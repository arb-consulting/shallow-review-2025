id,title,url,pageUrl,author,createdAt,score,commentCount,wordCount,tags,excerpt
ifechgnJRtJdduFGC,Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs,https://www.lesswrong.com/posts/ifechgnJRtJdduFGC/emergent-misalignment-narrow-finetuning-can-produce-broadly,https://www.lesswrong.com/posts/ifechgnJRtJdduFGC/emergent-misalignment-narrow-finetuning-can-produce-broadly,,2025-02-25T17:39:31.059000+00:00,333,92,,,
PwnadG4BFjaER3MGf,Interpretability Will Not Reliably Find Deceptive AI,https://www.lesswrong.com/posts/PwnadG4BFjaER3MGf/interpretability-will-not-reliably-find-deceptive-ai,https://www.lesswrong.com/posts/PwnadG4BFjaER3MGf/interpretability-will-not-reliably-find-deceptive-ai,,2025-05-04T16:32:29.643000+00:00,329,68,,,
PMc65HgRFvBimEpmJ,Legible vs. Illegible AI Safety Problems,https://www.lesswrong.com/posts/PMc65HgRFvBimEpmJ/legible-vs-illegible-ai-safety-problems,https://www.lesswrong.com/posts/PMc65HgRFvBimEpmJ/legible-vs-illegible-ai-safety-problems,,2025-11-04T21:39:07.202000+00:00,313,92,,,
5uw26uDdFbFQgKzih,Beware General Claims about “Generalizable Reasoning Capabilities” (of Modern AI Systems),https://www.lesswrong.com/posts/5uw26uDdFbFQgKzih/beware-general-claims-about-generalizable-reasoning,https://www.lesswrong.com/posts/5uw26uDdFbFQgKzih/beware-general-claims-about-generalizable-reasoning,,2025-06-11T19:27:33.648000+00:00,295,19,,,
dHLdf8SB8oW5L27gg,On Fleshling Safety: A Debate by Klurl and Trapaucius.,https://www.lesswrong.com/posts/dHLdf8SB8oW5L27gg/on-fleshling-safety-a-debate-by-klurl-and-trapaucius,https://www.lesswrong.com/posts/dHLdf8SB8oW5L27gg/on-fleshling-safety-a-debate-by-klurl-and-trapaucius,,2025-10-26T23:44:04.676000+00:00,238,52,,,
fF8pvsn3AGQhYsbjp,Safety researchers should take a public stance,https://www.lesswrong.com/posts/fF8pvsn3AGQhYsbjp/safety-researchers-should-take-a-public-stance,https://www.lesswrong.com/posts/fF8pvsn3AGQhYsbjp/safety-researchers-should-take-a-public-stance,,2025-09-19T18:55:54.209000+00:00,237,65,,,
7C4KJot4aN8ieEDoz,Will alignment-faking Claude accept a deal to reveal its misalignment?,https://www.lesswrong.com/posts/7C4KJot4aN8ieEDoz/will-alignment-faking-claude-accept-a-deal-to-reveal-its,https://www.lesswrong.com/posts/7C4KJot4aN8ieEDoz/will-alignment-faking-claude-accept-a-deal-to-reveal-its,,2025-01-31T16:49:47.316000+00:00,208,28,,,
mpMWWKzkzWqf57Yap,Eliezer's Lost Alignment Articles / The Arbital Sequence,https://www.lesswrong.com/posts/mpMWWKzkzWqf57Yap/eliezer-s-lost-alignment-articles-the-arbital-sequence,https://www.lesswrong.com/posts/mpMWWKzkzWqf57Yap/eliezer-s-lost-alignment-articles-the-arbital-sequence,,2025-02-20T00:48:10.338000+00:00,207,10,,,
ZEuDH2W3XdRaTwpjD,Hyperbolic model fits METR capabilities estimate worse than exponential model,https://www.lesswrong.com/posts/ZEuDH2W3XdRaTwpjD/hyperbolic-model-fits-metr-capabilities-estimate-worse-than,https://www.lesswrong.com/posts/ZEuDH2W3XdRaTwpjD/hyperbolic-model-fits-metr-capabilities-estimate-worse-than,,2025-08-19T15:12:38.446000+00:00,202,9,,,
pGMRzJByB67WfSvpy,Will Any Crap Cause Emergent Misalignment?,https://www.lesswrong.com/posts/pGMRzJByB67WfSvpy/will-any-crap-cause-emergent-misalignment,https://www.lesswrong.com/posts/pGMRzJByB67WfSvpy/will-any-crap-cause-emergent-misalignment,,2025-08-27T18:20:11.587000+00:00,194,37,,,
E3daBewppAiECN3Ao,Claude Sonnet 3.7 (often) knows when it’s in alignment evaluations,https://www.lesswrong.com/posts/E3daBewppAiECN3Ao/claude-sonnet-3-7-often-knows-when-it-s-in-alignment,https://www.lesswrong.com/posts/E3daBewppAiECN3Ao/claude-sonnet-3-7-often-knows-when-it-s-in-alignment,,2025-03-17T19:11:00.813000+00:00,188,9,,,
dHNKtQ3vTBxTfTPxu,What Is The Alignment Problem?,https://www.lesswrong.com/posts/dHNKtQ3vTBxTfTPxu/what-is-the-alignment-problem,https://www.lesswrong.com/posts/dHNKtQ3vTBxTfTPxu/what-is-the-alignment-problem,,2025-01-16T01:20:16.826000+00:00,181,49,,,
7xneDbsgj6yJDJMjK,Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety,https://www.lesswrong.com/posts/7xneDbsgj6yJDJMjK/chain-of-thought-monitorability-a-new-and-fragile,https://www.lesswrong.com/posts/7xneDbsgj6yJDJMjK/chain-of-thought-monitorability-a-new-and-fragile,,2025-07-15T16:23:17.588000+00:00,166,32,,,
jtqcsARGtmgogdcLT,Reducing LLM deception at scale with self-other overlap fine-tuning,https://www.lesswrong.com/posts/jtqcsARGtmgogdcLT/reducing-llm-deception-at-scale-with-self-other-overlap-fine,https://www.lesswrong.com/posts/jtqcsARGtmgogdcLT/reducing-llm-deception-at-scale-with-self-other-overlap-fine,,2025-03-13T19:09:43.620000+00:00,162,46,,,
ghESoA8mo3fv9Yx3E,Why Do Some Language Models Fake Alignment While Others Don't?,https://www.lesswrong.com/posts/ghESoA8mo3fv9Yx3E/why-do-some-language-models-fake-alignment-while-others-don,https://www.lesswrong.com/posts/ghESoA8mo3fv9Yx3E/why-do-some-language-models-fake-alignment-while-others-don,,2025-07-08T21:49:30.088000+00:00,158,14,,,
QkEyry3Mqo8umbhoK,Self-fulfilling misalignment data might be poisoning our AI models,https://www.lesswrong.com/posts/QkEyry3Mqo8umbhoK/self-fulfilling-misalignment-data-might-be-poisoning-our-ai,https://www.lesswrong.com/posts/QkEyry3Mqo8umbhoK/self-fulfilling-misalignment-data-might-be-poisoning-our-ai,,2025-03-02T19:51:14.775000+00:00,154,29,,,
bnnKGSCHJghAvqPjS,Foom & Doom 2: Technical alignment is hard,https://www.lesswrong.com/posts/bnnKGSCHJghAvqPjS/foom-and-doom-2-technical-alignment-is-hard,https://www.lesswrong.com/posts/bnnKGSCHJghAvqPjS/foom-and-doom-2-technical-alignment-is-hard,,2025-06-23T17:19:50.691000+00:00,152,65,,,
LtT24cCAazQp4NYc5,Open Global Investment as a Governance Model for AGI,https://www.lesswrong.com/posts/LtT24cCAazQp4NYc5/open-global-investment-as-a-governance-model-for-agi,https://www.lesswrong.com/posts/LtT24cCAazQp4NYc5/open-global-investment-as-a-governance-model-for-agi,,2025-08-27T17:42:00+00:00,152,47,,,
gYfpPbww3wQRaxAFD,Activation space interpretability may be doomed,https://www.lesswrong.com/posts/gYfpPbww3wQRaxAFD/activation-space-interpretability-may-be-doomed,https://www.lesswrong.com/posts/gYfpPbww3wQRaxAFD/activation-space-interpretability-may-be-doomed,,2025-01-08T12:49:38.421000+00:00,152,35,,,
TBk2dbWkg2F7dB3jb,It's hard to make scheming evals look realistic for LLMs,https://www.lesswrong.com/posts/TBk2dbWkg2F7dB3jb/it-s-hard-to-make-scheming-evals-look-realistic-for-llms,https://www.lesswrong.com/posts/TBk2dbWkg2F7dB3jb/it-s-hard-to-make-scheming-evals-look-realistic-for-llms,,2025-05-24T19:17:55.875000+00:00,150,29,,,
CFA8W6WCodEZdjqYE,AIs should also refuse to work on capabilities research,https://www.lesswrong.com/posts/CFA8W6WCodEZdjqYE/ais-should-also-refuse-to-work-on-capabilities-research,https://www.lesswrong.com/posts/CFA8W6WCodEZdjqYE/ais-should-also-refuse-to-work-on-capabilities-research,,2025-10-27T08:42:35.190000+00:00,150,20,,,
oDX5vcDTEei8WuoBx,Re: recent Anthropic safety research,https://www.lesswrong.com/posts/oDX5vcDTEei8WuoBx/re-recent-anthropic-safety-research,https://www.lesswrong.com/posts/oDX5vcDTEei8WuoBx/re-recent-anthropic-safety-research,,2025-08-06T22:52:44.203000+00:00,149,22,,,
zmtqmwetKH4nrxXcE,Which side of the AI safety community are you in?,https://www.lesswrong.com/posts/zmtqmwetKH4nrxXcE/which-side-of-the-ai-safety-community-are-you-in,https://www.lesswrong.com/posts/zmtqmwetKH4nrxXcE/which-side-of-the-ai-safety-community-are-you-in,,2025-10-22T21:17:28.465000+00:00,146,88,,,
Fr4QsQT52RFKHvCAH,Alignment Faking Revisited: Improved Classifiers and Open Source Extensions,https://www.lesswrong.com/posts/Fr4QsQT52RFKHvCAH/alignment-faking-revisited-improved-classifiers-and-open,https://www.lesswrong.com/posts/Fr4QsQT52RFKHvCAH/alignment-faking-revisited-improved-classifiers-and-open,,2025-04-08T17:32:55.315000+00:00,146,20,,,
neTbrpBziAsTH5Bn7,AI companies are unlikely to make high-assurance safety cases if timelines are short,https://www.lesswrong.com/posts/neTbrpBziAsTH5Bn7/ai-companies-are-unlikely-to-make-high-assurance-safety,https://www.lesswrong.com/posts/neTbrpBziAsTH5Bn7/ai-companies-are-unlikely-to-make-high-assurance-safety,,2025-01-23T18:41:40.546000+00:00,145,5,,,
qgehQxiTXj53X49mM,"Sonnet 4.5's eval gaming seriously undermines alignment evals, and this seems caused by training on alignment evals",https://www.lesswrong.com/posts/qgehQxiTXj53X49mM/sonnet-4-5-s-eval-gaming-seriously-undermines-alignment,https://www.lesswrong.com/posts/qgehQxiTXj53X49mM/sonnet-4-5-s-eval-gaming-seriously-undermines-alignment,,2025-10-30T15:34:32.336000+00:00,139,20,,,
gLDSqQm8pwNiq7qst,"Narrow Misalignment is Hard, Emergent Misalignment is Easy",https://www.lesswrong.com/posts/gLDSqQm8pwNiq7qst/narrow-misalignment-is-hard-emergent-misalignment-is-easy,https://www.lesswrong.com/posts/gLDSqQm8pwNiq7qst/narrow-misalignment-is-hard-emergent-misalignment-is-easy,,2025-07-14T21:05:57.653000+00:00,133,24,,,
n6Rsb2jDpYSfzsbns,Consider donating to AI safety champion Scott Wiener,https://www.lesswrong.com/posts/n6Rsb2jDpYSfzsbns/consider-donating-to-ai-safety-champion-scott-wiener,https://www.lesswrong.com/posts/n6Rsb2jDpYSfzsbns/consider-donating-to-ai-safety-champion-scott-wiener,,2025-10-22T18:40:41.963000+00:00,132,9,,,
qYPHryHTNiJ2y6Fhi,The Paris AI Anti-Safety Summit,https://www.lesswrong.com/posts/qYPHryHTNiJ2y6Fhi/the-paris-ai-anti-safety-summit,https://www.lesswrong.com/posts/qYPHryHTNiJ2y6Fhi/the-paris-ai-anti-safety-summit,,2025-02-12T14:00:07.383000+00:00,129,21,,,
E8n93nnEaFeXTbHn5,"Plans A, B, C, and D for misalignment risk",https://www.lesswrong.com/posts/E8n93nnEaFeXTbHn5/plans-a-b-c-and-d-for-misalignment-risk,https://www.lesswrong.com/posts/E8n93nnEaFeXTbHn5/plans-a-b-c-and-d-for-misalignment-risk,,2025-10-08T17:18:16.750000+00:00,127,69,,,
HLJoJYi52mxgomujc,Realistic Reward Hacking Induces Different and Deeper Misalignment,https://www.lesswrong.com/posts/HLJoJYi52mxgomujc/realistic-reward-hacking-induces-different-and-deeper-1,https://www.lesswrong.com/posts/HLJoJYi52mxgomujc/realistic-reward-hacking-induces-different-and-deeper-1,,2025-10-09T18:45:07.342000+00:00,127,2,,,
JmRfgNYCrYogCq7ny,Stress Testing Deliberative Alignment for Anti-Scheming Training,https://www.lesswrong.com/posts/JmRfgNYCrYogCq7ny/stress-testing-deliberative-alignment-for-anti-scheming,https://www.lesswrong.com/posts/JmRfgNYCrYogCq7ny/stress-testing-deliberative-alignment-for-anti-scheming,,2025-09-17T16:59:12.906000+00:00,125,19,,,
bGYQgBPEyHidnZCdE,Towards Alignment Auditing as a Numbers-Go-Up Science,https://www.lesswrong.com/posts/bGYQgBPEyHidnZCdE/towards-alignment-auditing-as-a-numbers-go-up-science,https://www.lesswrong.com/posts/bGYQgBPEyHidnZCdE/towards-alignment-auditing-as-a-numbers-go-up-science,,2025-08-04T22:30:52.101000+00:00,123,15,,,
J7Ju6t6QCpgbnYx4D,Please Donate to CAIP (Post 1 of 7 on AI Governance),https://www.lesswrong.com/posts/J7Ju6t6QCpgbnYx4D/please-donate-to-caip-post-1-of-7-on-ai-governance,https://www.lesswrong.com/posts/J7Ju6t6QCpgbnYx4D/please-donate-to-caip-post-1-of-7-on-ai-governance,,2025-05-07T17:13:46.761000+00:00,119,20,,,
26SHhxK2yYQbh7ors,Research directions Open Phil wants to fund in technical AI safety,https://www.lesswrong.com/posts/26SHhxK2yYQbh7ors/research-directions-open-phil-wants-to-fund-in-technical-ai,https://www.lesswrong.com/posts/26SHhxK2yYQbh7ors/research-directions-open-phil-wants-to-fund-in-technical-ai,,2025-02-08T01:40:00.968000+00:00,117,21,,,
TCGgiJAinGgcMEByt,“The Era of Experience” has an unsolved technical alignment problem,https://www.lesswrong.com/posts/TCGgiJAinGgcMEByt/the-era-of-experience-has-an-unsolved-technical-alignment,https://www.lesswrong.com/posts/TCGgiJAinGgcMEByt/the-era-of-experience-has-an-unsolved-technical-alignment,,2025-04-24T13:57:38.984000+00:00,115,48,,,
whkMnqFWKsBm7Gyd7,Recontextualization Mitigates Specification Gaming Without Modifying the Specification,https://www.lesswrong.com/posts/whkMnqFWKsBm7Gyd7/recontextualization-mitigates-specification-gaming-without,https://www.lesswrong.com/posts/whkMnqFWKsBm7Gyd7/recontextualization-mitigates-specification-gaming-without,,2025-10-14T00:53:06.161000+00:00,115,13,,,
jP9KDyMkchuv6tHwm,How To Become A Mechanistic Interpretability Researcher,https://www.lesswrong.com/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher,https://www.lesswrong.com/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher,,2025-09-02T23:38:43.780000+00:00,115,12,,,
gRc8KL2HLtKkFmNPr,Among Us: A Sandbox for Agentic Deception,https://www.lesswrong.com/posts/gRc8KL2HLtKkFmNPr/among-us-a-sandbox-for-agentic-deception,https://www.lesswrong.com/posts/gRc8KL2HLtKkFmNPr/among-us-a-sandbox-for-agentic-deception,,2025-04-05T06:24:49+00:00,114,7,,,
W3KfxjbqBAnifBQoi,We should try to automate AI safety work asap,https://www.lesswrong.com/posts/W3KfxjbqBAnifBQoi/we-should-try-to-automate-ai-safety-work-asap,https://www.lesswrong.com/posts/W3KfxjbqBAnifBQoi/we-should-try-to-automate-ai-safety-work-asap,,2025-04-26T16:35:43.770000+00:00,113,10,,,
tbnw7LbNApvxNLAg8,UK AISI’s Alignment Team: Research Agenda,https://www.lesswrong.com/posts/tbnw7LbNApvxNLAg8/uk-aisi-s-alignment-team-research-agenda,https://www.lesswrong.com/posts/tbnw7LbNApvxNLAg8/uk-aisi-s-alignment-team-research-agenda,,2025-05-07T16:33:41.176000+00:00,113,2,,,
wGRnzCFcowRCrpX4Y,Downstream applications as validation of interpretability progress,https://www.lesswrong.com/posts/wGRnzCFcowRCrpX4Y/downstream-applications-as-validation-of-interpretability,https://www.lesswrong.com/posts/wGRnzCFcowRCrpX4Y/downstream-applications-as-validation-of-interpretability,,2025-03-31T01:35:02.722000+00:00,112,3,,,
yHmJrDSJpFaNTZ9Tr,Model Organisms for Emergent Misalignment,https://www.lesswrong.com/posts/yHmJrDSJpFaNTZ9Tr/model-organisms-for-emergent-misalignment,https://www.lesswrong.com/posts/yHmJrDSJpFaNTZ9Tr/model-organisms-for-emergent-misalignment,,2025-06-16T15:46:41.739000+00:00,111,18,,,
wbJxRNxuezvsGFEWv,Open Philanthropy Technical AI Safety RFP - $40M Available Across 21 Research Areas,https://www.lesswrong.com/posts/wbJxRNxuezvsGFEWv/open-philanthropy-technical-ai-safety-rfp-usd40m-available,https://www.lesswrong.com/posts/wbJxRNxuezvsGFEWv/open-philanthropy-technical-ai-safety-rfp-usd40m-available,,2025-02-06T18:58:53.076000+00:00,111,0,,,
BjeesS4cosB2f4PAj,We're Not Advertising Enough (Post 3 of 7 on AI Governance),https://www.lesswrong.com/posts/BjeesS4cosB2f4PAj/we-re-not-advertising-enough-post-3-of-7-on-ai-governance,https://www.lesswrong.com/posts/BjeesS4cosB2f4PAj/we-re-not-advertising-enough-post-3-of-7-on-ai-governance,,2025-05-22T17:05:59.329000+00:00,110,10,,,
2pZWhCndKtLAiWXYv,Learnings from AI safety course so far,https://www.lesswrong.com/posts/2pZWhCndKtLAiWXYv/learnings-from-ai-safety-course-so-far,https://www.lesswrong.com/posts/2pZWhCndKtLAiWXYv/learnings-from-ai-safety-course-so-far,,2025-09-27T18:17:02.384000+00:00,106,6,,,
WkCfvqyjCzvRrwkaQ,AI Governance to Avoid Extinction: The Strategic Landscape and Actionable Research Questions,https://www.lesswrong.com/posts/WkCfvqyjCzvRrwkaQ/ai-governance-to-avoid-extinction-the-strategic-landscape,https://www.lesswrong.com/posts/WkCfvqyjCzvRrwkaQ/ai-governance-to-avoid-extinction-the-strategic-landscape,,2025-05-01T22:46:10.337000+00:00,105,7,,,
TJrCumJxhzTmNBsRz,A short course on AGI safety from the GDM Alignment team,https://www.lesswrong.com/posts/TJrCumJxhzTmNBsRz/a-short-course-on-agi-safety-from-the-gdm-alignment-team,https://www.lesswrong.com/posts/TJrCumJxhzTmNBsRz/a-short-course-on-agi-safety-from-the-gdm-alignment-team,,2025-02-14T15:43:50.903000+00:00,105,2,,,
9pGbTz6c78PGwJein,Detecting Strategic Deception Using Linear Probes,https://www.lesswrong.com/posts/9pGbTz6c78PGwJein/detecting-strategic-deception-using-linear-probes,https://www.lesswrong.com/posts/9pGbTz6c78PGwJein/detecting-strategic-deception-using-linear-probes,,2025-02-06T15:46:53.024000+00:00,104,9,,,
hQyrTDuTXpqkxrnoH,xAI's new safety framework is dreadful,https://www.lesswrong.com/posts/hQyrTDuTXpqkxrnoH/xai-s-new-safety-framework-is-dreadful,https://www.lesswrong.com/posts/hQyrTDuTXpqkxrnoH/xai-s-new-safety-framework-is-dreadful,,2025-09-02T15:00:43.396000+00:00,104,6,,,
wqz5CRzqWkvzoatBG,AGI Safety & Alignment @ Google DeepMind is hiring,https://www.lesswrong.com/posts/wqz5CRzqWkvzoatBG/agi-safety-and-alignment-google-deepmind-is-hiring,https://www.lesswrong.com/posts/wqz5CRzqWkvzoatBG/agi-safety-and-alignment-google-deepmind-is-hiring,,2025-02-17T21:11:18.970000+00:00,103,19,,,
cyYgdYJagkG4HGZBk,Reasons for and against working on technical AI safety at a frontier AI lab,https://www.lesswrong.com/posts/cyYgdYJagkG4HGZBk/reasons-for-and-against-working-on-technical-ai-safety-at-a,https://www.lesswrong.com/posts/cyYgdYJagkG4HGZBk/reasons-for-and-against-working-on-technical-ai-safety-at-a,,2025-01-05T14:49:53.529000+00:00,100,12,,,
6YxdpGjfHyrZb7F2G,Third-wave AI safety needs sociopolitical thinking,https://www.lesswrong.com/posts/6YxdpGjfHyrZb7F2G/third-wave-ai-safety-needs-sociopolitical-thinking,https://www.lesswrong.com/posts/6YxdpGjfHyrZb7F2G/third-wave-ai-safety-needs-sociopolitical-thinking,,2025-03-27T00:55:30.548000+00:00,99,23,,,
kcKnKHTHycHeRhcHF,"One-shot steering vectors cause emergent misalignment, too",https://www.lesswrong.com/posts/kcKnKHTHycHeRhcHF/one-shot-steering-vectors-cause-emergent-misalignment-too,https://www.lesswrong.com/posts/kcKnKHTHycHeRhcHF/one-shot-steering-vectors-cause-emergent-misalignment-too,,2025-04-14T06:40:41.503000+00:00,98,6,,,
6hy7tsB2pkpRHqazG,The Sweet Lesson: AI Safety Should Scale With Compute,https://www.lesswrong.com/posts/6hy7tsB2pkpRHqazG/the-sweet-lesson-ai-safety-should-scale-with-compute,https://www.lesswrong.com/posts/6hy7tsB2pkpRHqazG/the-sweet-lesson-ai-safety-should-scale-with-compute,,2025-05-05T19:03:28.748000+00:00,97,3,,,
HiTjDZyWdLEGCDzqu,Implications of the inference scaling paradigm for AI safety,https://www.lesswrong.com/posts/HiTjDZyWdLEGCDzqu/implications-of-the-inference-scaling-paradigm-for-ai-safety,https://www.lesswrong.com/posts/HiTjDZyWdLEGCDzqu/implications-of-the-inference-scaling-paradigm-for-ai-safety,,2025-01-14T02:14:53.562000+00:00,96,70,,,
kiNbFKcKoNQKdgTp8,Interview with Eliezer Yudkowsky on Rationality and Systematic Misunderstanding of AI Alignment,https://www.lesswrong.com/posts/kiNbFKcKoNQKdgTp8/interview-with-eliezer-yudkowsky-on-rationality-and,https://www.lesswrong.com/posts/kiNbFKcKoNQKdgTp8/interview-with-eliezer-yudkowsky-on-rationality-and,,2025-09-15T18:35:16.351000+00:00,93,21,,,
YeQe36XiY4BhrtRh5,ASI existential risk: Reconsidering Alignment as a Goal,https://www.lesswrong.com/posts/YeQe36XiY4BhrtRh5/asi-existential-risk-reconsidering-alignment-as-a-goal-1,https://www.lesswrong.com/posts/YeQe36XiY4BhrtRh5/asi-existential-risk-reconsidering-alignment-as-a-goal-1,,2025-04-15T19:57:42.547000+00:00,93,14,,,
3jnziqCF3vA2NXAKp,Six Thoughts on AI Safety,https://www.lesswrong.com/posts/3jnziqCF3vA2NXAKp/six-thoughts-on-ai-safety,https://www.lesswrong.com/posts/3jnziqCF3vA2NXAKp/six-thoughts-on-ai-safety,,2025-01-24T22:20:50.768000+00:00,92,55,,,
gT3wtWBAs7PKonbmy,Aesthetic Preferences Can Cause Emergent Misalignment,https://www.lesswrong.com/posts/gT3wtWBAs7PKonbmy/aesthetic-preferences-can-cause-emergent-misalignment,https://www.lesswrong.com/posts/gT3wtWBAs7PKonbmy/aesthetic-preferences-can-cause-emergent-misalignment,,2025-08-26T18:41:10.281000+00:00,92,16,,,
8XHBaugB5S3r27MG9,"Prover-Estimator Debate: 
A New Scalable Oversight Protocol",https://www.lesswrong.com/posts/8XHBaugB5S3r27MG9/prover-estimator-debate-a-new-scalable-oversight-protocol,https://www.lesswrong.com/posts/8XHBaugB5S3r27MG9/prover-estimator-debate-a-new-scalable-oversight-protocol,,2025-06-17T13:53:04.125000+00:00,88,18,,,
7BEcAzxCXenwcjXuE,On Emergent Misalignment,https://www.lesswrong.com/posts/7BEcAzxCXenwcjXuE/on-emergent-misalignment,https://www.lesswrong.com/posts/7BEcAzxCXenwcjXuE/on-emergent-misalignment,,2025-02-28T13:10:05.973000+00:00,88,5,,,
ksfjZJu3BFEfM6hHE,"Why Corrigibility is Hard and Important (i.e. ""Whence the high MIRI confidence in alignment difficulty?"")",https://www.lesswrong.com/posts/ksfjZJu3BFEfM6hHE/why-corrigibility-is-hard-and-important-i-e-whence-the-high,https://www.lesswrong.com/posts/ksfjZJu3BFEfM6hHE/why-corrigibility-is-hard-and-important-i-e-whence-the-high,,2025-09-30T00:12:04.934000+00:00,87,54,,,
PkeB4TLxgaNnSmddg,Scaling Sparse Feature Circuit Finding to Gemma 9B,https://www.lesswrong.com/posts/PkeB4TLxgaNnSmddg/scaling-sparse-feature-circuit-finding-to-gemma-9b,https://www.lesswrong.com/posts/PkeB4TLxgaNnSmddg/scaling-sparse-feature-circuit-finding-to-gemma-9b,,2025-01-10T11:08:11.999000+00:00,86,11,,,
PjeZxCivuoyKhs4JB,Claude 4 You: Safety and Alignment,https://www.lesswrong.com/posts/PjeZxCivuoyKhs4JB/claude-4-you-safety-and-alignment,https://www.lesswrong.com/posts/PjeZxCivuoyKhs4JB/claude-4-you-safety-and-alignment,,2025-05-25T14:00:04.528000+00:00,86,8,,,
dqd54wpEfjKJsJBk6,xAI's Grok 4 has no meaningful safety guardrails,https://www.lesswrong.com/posts/dqd54wpEfjKJsJBk6/xai-s-grok-4-has-no-meaningful-safety-guardrails,https://www.lesswrong.com/posts/dqd54wpEfjKJsJBk6/xai-s-grok-4-has-no-meaningful-safety-guardrails,,2025-07-13T18:22:31.350000+00:00,84,15,,,
AcTEiu5wYDgrbmXow,Open problems in emergent misalignment,https://www.lesswrong.com/posts/AcTEiu5wYDgrbmXow/open-problems-in-emergent-misalignment,https://www.lesswrong.com/posts/AcTEiu5wYDgrbmXow/open-problems-in-emergent-misalignment,,2025-03-01T09:47:58.889000+00:00,83,17,,,
o3sEHE8cqQ5hcqgkG,"What Makes an AI Startup ""Net Positive"" for Safety?",https://www.lesswrong.com/posts/o3sEHE8cqQ5hcqgkG/what-makes-an-ai-startup-net-positive-for-safety,https://www.lesswrong.com/posts/o3sEHE8cqQ5hcqgkG/what-makes-an-ai-startup-net-positive-for-safety,,2025-04-18T20:33:22.682000+00:00,82,23,,,
cJQZAueoPC6aTncKK,AIs at the current capability level may be important for future safety work,https://www.lesswrong.com/posts/cJQZAueoPC6aTncKK/ais-at-the-current-capability-level-may-be-important-for,https://www.lesswrong.com/posts/cJQZAueoPC6aTncKK/ais-at-the-current-capability-level-may-be-important-for,,2025-05-12T14:06:11.872000+00:00,82,2,,,
HE2WXbftEebdBLR9u,Anthropic is Quietly Backpedalling on its Safety Commitments,https://www.lesswrong.com/posts/HE2WXbftEebdBLR9u/anthropic-is-quietly-backpedalling-on-its-safety-commitments,https://www.lesswrong.com/posts/HE2WXbftEebdBLR9u/anthropic-is-quietly-backpedalling-on-its-safety-commitments,,2025-05-23T02:26:42.877000+00:00,81,7,,,
kCGk5tp5suHoGwhCa,Mistral Large 2 (123B) seems to exhibit alignment faking,https://www.lesswrong.com/posts/kCGk5tp5suHoGwhCa/mistral-large-2-123b-seems-to-exhibit-alignment-faking,https://www.lesswrong.com/posts/kCGk5tp5suHoGwhCa/mistral-large-2-123b-seems-to-exhibit-alignment-faking,,2025-03-27T15:39:02.176000+00:00,81,4,,,
RoWabfQxabWBiXwxP,"Go home GPT-4o, you’re drunk: emergent misalignment as lowered inhibitions",https://www.lesswrong.com/posts/RoWabfQxabWBiXwxP/go-home-gpt-4o-you-re-drunk-emergent-misalignment-as-lowered,https://www.lesswrong.com/posts/RoWabfQxabWBiXwxP/go-home-gpt-4o-you-re-drunk-emergent-misalignment-as-lowered,,2025-03-18T14:48:54.762000+00:00,80,12,,,
abd9ufFpLrn5kvnLn,Directly Try Solving Alignment for 5 weeks,https://www.lesswrong.com/posts/abd9ufFpLrn5kvnLn/directly-try-solving-alignment-for-5-weeks,https://www.lesswrong.com/posts/abd9ufFpLrn5kvnLn/directly-try-solving-alignment-for-5-weeks,,2025-07-21T21:51:55.056000+00:00,80,4,,,
F3j4xqpxjxgQD3xXh,AI for AI safety,https://www.lesswrong.com/posts/F3j4xqpxjxgQD3xXh/ai-for-ai-safety,https://www.lesswrong.com/posts/F3j4xqpxjxgQD3xXh/ai-for-ai-safety,,2025-03-14T15:00:23.491000+00:00,79,13,,,
Y49znC2qfL9SXkT7e,Why does LW not put much more focus on AI governance and outreach?,https://www.lesswrong.com/posts/Y49znC2qfL9SXkT7e/why-does-lw-not-put-much-more-focus-on-ai-governance-and,https://www.lesswrong.com/posts/Y49znC2qfL9SXkT7e/why-does-lw-not-put-much-more-focus-on-ai-governance-and,,2025-04-12T14:24:54.197000+00:00,78,31,,,
b8eeCGe3FWzHKbePF,Agentic Misalignment: How LLMs Could be Insider Threats,https://www.lesswrong.com/posts/b8eeCGe3FWzHKbePF/agentic-misalignment-how-llms-could-be-insider-threats-1,https://www.lesswrong.com/posts/b8eeCGe3FWzHKbePF/agentic-misalignment-how-llms-could-be-insider-threats-1,,2025-06-20T22:34:59.515000+00:00,78,13,,,
YXNeA3RyRrrRWS37A,A Problem to Solve Before Building a Deception Detector,https://www.lesswrong.com/posts/YXNeA3RyRrrRWS37A/a-problem-to-solve-before-building-a-deception-detector,https://www.lesswrong.com/posts/YXNeA3RyRrrRWS37A/a-problem-to-solve-before-building-a-deception-detector,,2025-02-07T19:35:23.307000+00:00,77,12,,,
TeTegzR8X5CuKgMc3,Misalignment and Strategic Underperformance: An Analysis of Sandbagging and Exploration Hacking,https://www.lesswrong.com/posts/TeTegzR8X5CuKgMc3/misalignment-and-strategic-underperformance-an-analysis-of,https://www.lesswrong.com/posts/TeTegzR8X5CuKgMc3/misalignment-and-strategic-underperformance-an-analysis-of,,2025-05-08T19:06:29.469000+00:00,77,3,,,
kffbZGa2yYhc6cakc,Petri: An open-source auditing tool to accelerate AI safety research,https://www.lesswrong.com/posts/kffbZGa2yYhc6cakc/petri-an-open-source-auditing-tool-to-accelerate-ai-safety,https://www.lesswrong.com/posts/kffbZGa2yYhc6cakc/petri-an-open-source-auditing-tool-to-accelerate-ai-safety,,2025-10-07T20:39:16.767000+00:00,77,0,,,
2TA7HqBYdhLdJBcZz,On closed-door AI safety research,https://www.lesswrong.com/posts/2TA7HqBYdhLdJBcZz/on-closed-door-ai-safety-research,https://www.lesswrong.com/posts/2TA7HqBYdhLdJBcZz/on-closed-door-ai-safety-research,,2025-08-18T21:59:19+00:00,76,11,,,
4yn8B8p2YiouxLABy,Claude Sonnet 4.5: System Card and Alignment,https://www.lesswrong.com/posts/4yn8B8p2YiouxLABy/claude-sonnet-4-5-system-card-and-alignment,https://www.lesswrong.com/posts/4yn8B8p2YiouxLABy/claude-sonnet-4-5-system-card-and-alignment,,2025-09-30T20:50:06.591000+00:00,75,5,,,
LhnqegFoykcjaXCYH,100+ concrete projects and open problems in evals,https://www.lesswrong.com/posts/LhnqegFoykcjaXCYH/100-concrete-projects-and-open-problems-in-evals,https://www.lesswrong.com/posts/LhnqegFoykcjaXCYH/100-concrete-projects-and-open-problems-in-evals,,2025-03-22T15:21:40.970000+00:00,75,1,,,
9tqpPP4FwSnv9AWsi,Research Note: Our scheming precursor evals had limited predictive power for our in-context scheming evals,https://www.lesswrong.com/posts/9tqpPP4FwSnv9AWsi/research-note-our-scheming-precursor-evals-had-limited,https://www.lesswrong.com/posts/9tqpPP4FwSnv9AWsi/research-note-our-scheming-precursor-evals-had-limited,,2025-07-03T15:57:07.779000+00:00,75,0,,,
LLJm97gLDacn8AscB,Introducing 11 New AI Safety Organizations - Catalyze's Winter 24/25 London Incubation Program Cohort,https://www.lesswrong.com/posts/LLJm97gLDacn8AscB/introducing-11-new-ai-safety-organizations-catalyze-s-winter,https://www.lesswrong.com/posts/LLJm97gLDacn8AscB/introducing-11-new-ai-safety-organizations-catalyze-s-winter,,2025-03-10T19:26:11.017000+00:00,75,0,,,
4XdxiqBsLKqiJ9xRM,LLM AGI may reason about its goals and discover misalignments by default,https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover,https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover,,2025-09-15T14:58:01.265000+00:00,74,6,,,
aKncW36ZdEnzxLo8A,"LLM AGI will have memory, and memory changes alignment",https://www.lesswrong.com/posts/aKncW36ZdEnzxLo8A/llm-agi-will-have-memory-and-memory-changes-alignment,https://www.lesswrong.com/posts/aKncW36ZdEnzxLo8A/llm-agi-will-have-memory-and-memory-changes-alignment,,2025-04-04T14:59:13.070000+00:00,73,15,,,
3ki4mt4BA6eTx56Tc,Google DeepMind: An Approach to Technical AGI Safety and Security,https://www.lesswrong.com/posts/3ki4mt4BA6eTx56Tc/google-deepmind-an-approach-to-technical-agi-safety-and,https://www.lesswrong.com/posts/3ki4mt4BA6eTx56Tc/google-deepmind-an-approach-to-technical-agi-safety-and,,2025-04-05T22:00:14.803000+00:00,73,12,,,
4aeshNuEKF8Ak356D,Omniscaling to MNIST,https://www.lesswrong.com/posts/4aeshNuEKF8Ak356D/omniscaling-to-mnist,https://www.lesswrong.com/posts/4aeshNuEKF8Ak356D/omniscaling-to-mnist,,2025-11-08T19:42:15.988000+00:00,73,3,,,
4kwyC8ZqGZLATezri,New scorecard evaluating AI companies on safety,https://www.lesswrong.com/posts/4kwyC8ZqGZLATezri/new-scorecard-evaluating-ai-companies-on-safety,https://www.lesswrong.com/posts/4kwyC8ZqGZLATezri/new-scorecard-evaluating-ai-companies-on-safety,,2025-05-26T16:00:15.629000+00:00,72,8,,,
fqDzevPyw3GGaF5o9,Paper: Open Problems in Mechanistic Interpretability,https://www.lesswrong.com/posts/fqDzevPyw3GGaF5o9/paper-open-problems-in-mechanistic-interpretability,https://www.lesswrong.com/posts/fqDzevPyw3GGaF5o9/paper-open-problems-in-mechanistic-interpretability,,2025-01-29T10:25:54.727000+00:00,71,0,,,
wFKZmvfRfNn24HNHp,Orphaned Policies (Post 5 of 7 on AI Governance),https://www.lesswrong.com/posts/wFKZmvfRfNn24HNHp/orphaned-policies-post-5-of-7-on-ai-governance,https://www.lesswrong.com/posts/wFKZmvfRfNn24HNHp/orphaned-policies-post-5-of-7-on-ai-governance,,2025-05-29T21:42:21.071000+00:00,70,5,,,
AzFxTMFfkTt4mhMKt,Alignment as uploading with more steps,https://www.lesswrong.com/posts/AzFxTMFfkTt4mhMKt/alignment-as-uploading-with-more-steps,https://www.lesswrong.com/posts/AzFxTMFfkTt4mhMKt/alignment-as-uploading-with-more-steps,,2025-09-14T04:08:36.418000+00:00,69,33,,,
irxuoCTKdufEdskSk,Alignment can be the ‘clean energy’ of AI,https://www.lesswrong.com/posts/irxuoCTKdufEdskSk/alignment-can-be-the-clean-energy-of-ai,https://www.lesswrong.com/posts/irxuoCTKdufEdskSk/alignment-can-be-the-clean-energy-of-ai,,2025-02-22T00:08:30.391000+00:00,68,8,,,
zzZ6jye3ukiNyMCmC,Thought Crime: Backdoors & Emergent Misalignment in Reasoning Models,https://www.lesswrong.com/posts/zzZ6jye3ukiNyMCmC/thought-crime-backdoors-and-emergent-misalignment-in,https://www.lesswrong.com/posts/zzZ6jye3ukiNyMCmC/thought-crime-backdoors-and-emergent-misalignment-in,,2025-06-16T16:43:12.826000+00:00,68,2,,,
umYzsh7SGHHKsRCaA,Convergent Linear Representations of Emergent Misalignment,https://www.lesswrong.com/posts/umYzsh7SGHHKsRCaA/convergent-linear-representations-of-emergent-misalignment,https://www.lesswrong.com/posts/umYzsh7SGHHKsRCaA/convergent-linear-representations-of-emergent-misalignment,,2025-06-16T15:47:15.487000+00:00,68,1,,,
PhgEKkB4cwYjwpGxb,Maintaining Alignment during RSI as a Feedback Control Problem,https://www.lesswrong.com/posts/PhgEKkB4cwYjwpGxb/maintaining-alignment-during-rsi-as-a-feedback-control,https://www.lesswrong.com/posts/PhgEKkB4cwYjwpGxb/maintaining-alignment-during-rsi-as-a-feedback-control,,2025-03-02T00:21:43.432000+00:00,67,6,,,
ZXxY2tccLapdjLbKm,Selective Generalization: Improving Capabilities While Maintaining Alignment,https://www.lesswrong.com/posts/ZXxY2tccLapdjLbKm/selective-generalization-improving-capabilities-while,https://www.lesswrong.com/posts/ZXxY2tccLapdjLbKm/selective-generalization-improving-capabilities-while,,2025-07-16T21:25:39.203000+00:00,67,4,,,
jzgqTtcHhXpSDNgcF,"Your AI Safety org could get EU funding up to €9.08M. Here’s how (+ free personalized support)
Update: Webinar 18/8 Link Below",https://www.lesswrong.com/posts/jzgqTtcHhXpSDNgcF/your-ai-safety-org-could-get-eu-funding-up-to-eur9-08m-here,https://www.lesswrong.com/posts/jzgqTtcHhXpSDNgcF/your-ai-safety-org-could-get-eu-funding-up-to-eur9-08m-here,,2025-07-20T01:30:55.308000+00:00,66,4,,,
rF7MQWGbqQjEkeLJA,Map of AI Safety v2,https://www.lesswrong.com/posts/rF7MQWGbqQjEkeLJA/map-of-ai-safety-v2,https://www.lesswrong.com/posts/rF7MQWGbqQjEkeLJA/map-of-ai-safety-v2,,2025-04-15T13:04:40.993000+00:00,64,4,,,
tG9LGHLzQezH3pvMs,Recommendations for Technical AI Safety Research Directions,https://www.lesswrong.com/posts/tG9LGHLzQezH3pvMs/recommendations-for-technical-ai-safety-research-directions,https://www.lesswrong.com/posts/tG9LGHLzQezH3pvMs/recommendations-for-technical-ai-safety-research-directions,,2025-01-10T19:34:04.920000+00:00,64,1,,,
TeF8Az2EiWenR9APF,When is it important that open-weight models aren't released? My thoughts on the benefits and dangers of open-weight models in response to developments in CBRN capabilities.,https://www.lesswrong.com/posts/TeF8Az2EiWenR9APF/when-is-it-important-that-open-weight-models-aren-t-released,https://www.lesswrong.com/posts/TeF8Az2EiWenR9APF/when-is-it-important-that-open-weight-models-aren-t-released,,2025-06-09T19:19:39.861000+00:00,63,11,,,
fMqgLGoeZFFQqAGyC,How do we solve the alignment problem?,https://www.lesswrong.com/posts/fMqgLGoeZFFQqAGyC/how-do-we-solve-the-alignment-problem,https://www.lesswrong.com/posts/fMqgLGoeZFFQqAGyC/how-do-we-solve-the-alignment-problem,,2025-02-13T18:27:27.712000+00:00,63,9,,,
inFW6hMG3QEx8tTfA,Rejecting Violence as an AI Safety Strategy,https://www.lesswrong.com/posts/inFW6hMG3QEx8tTfA/rejecting-violence-as-an-ai-safety-strategy,https://www.lesswrong.com/posts/inFW6hMG3QEx8tTfA/rejecting-violence-as-an-ai-safety-strategy,,2025-09-22T16:34:08.980000+00:00,63,5,,,
J7CyENFYXPxXQpsnD,SLT for AI Safety,https://www.lesswrong.com/posts/J7CyENFYXPxXQpsnD/slt-for-ai-safety,https://www.lesswrong.com/posts/J7CyENFYXPxXQpsnD/slt-for-ai-safety,,2025-07-01T04:52:21.093000+00:00,63,0,,,
AbnfsnEEmHFmprGzm,The EU Is Asking for Feedback on Frontier AI Regulation (Open to Global Experts)—This Post Breaks Down What’s at Stake for AI Safety,https://www.lesswrong.com/posts/AbnfsnEEmHFmprGzm/the-eu-is-asking-for-feedback-on-frontier-ai-regulation-open,https://www.lesswrong.com/posts/AbnfsnEEmHFmprGzm/the-eu-is-asking-for-feedback-on-frontier-ai-regulation-open,,2025-04-22T20:39:40.781000+00:00,62,13,,,
2RtuThoZwP4o8aEpS,Introducing the Epoch Capabilities Index (ECI),https://www.lesswrong.com/posts/2RtuThoZwP4o8aEpS/introducing-the-epoch-capabilities-index-eci,https://www.lesswrong.com/posts/2RtuThoZwP4o8aEpS/introducing-the-epoch-capabilities-index-eci,,2025-10-28T18:23:03.194000+00:00,62,9,,,
5rMwWzRdWFtRdHeuE,Not all capabilities will be created equal: focus on strategically superhuman agents,https://www.lesswrong.com/posts/5rMwWzRdWFtRdHeuE/not-all-capabilities-will-be-created-equal-focus-on,https://www.lesswrong.com/posts/5rMwWzRdWFtRdHeuE/not-all-capabilities-will-be-created-equal-focus-on,,2025-02-13T01:24:46.084000+00:00,62,9,,,
jzHhJJq2cFmisRKB2,"Misalignment classifiers: Why they’re hard to evaluate adversarially, and why we're studying them anyway",https://www.lesswrong.com/posts/jzHhJJq2cFmisRKB2/misalignment-classifiers-why-they-re-hard-to-evaluate,https://www.lesswrong.com/posts/jzHhJJq2cFmisRKB2/misalignment-classifiers-why-they-re-hard-to-evaluate,,2025-08-15T11:48:31.667000+00:00,61,3,,,
QGQiCuE33iHFv9jkv,"Softmax, Emmett Shear's new AI startup focused on ""Organic Alignment""",https://www.lesswrong.com/posts/QGQiCuE33iHFv9jkv/softmax-emmett-shear-s-new-ai-startup-focused-on-organic,https://www.lesswrong.com/posts/QGQiCuE33iHFv9jkv/softmax-emmett-shear-s-new-ai-startup-focused-on-organic,,2025-03-28T21:23:46.220000+00:00,61,2,,,
8KKujApx4g7FBm6hE,AI safety techniques leveraging distillation,https://www.lesswrong.com/posts/8KKujApx4g7FBm6hE/ai-safety-techniques-leveraging-distillation,https://www.lesswrong.com/posts/8KKujApx4g7FBm6hE/ai-safety-techniques-leveraging-distillation,,2025-06-19T14:31:02.632000+00:00,61,0,,,
jWFvsJnJieXnWBb9r,Alignment faking CTFs: Apply to my MATS stream,https://www.lesswrong.com/posts/jWFvsJnJieXnWBb9r/alignment-faking-ctfs-apply-to-my-mats-stream,https://www.lesswrong.com/posts/jWFvsJnJieXnWBb9r/alignment-faking-ctfs-apply-to-my-mats-stream,,2025-04-04T16:29:02.070000+00:00,61,0,,,
dcd2dPLZGFJPgtDzq,Shift Resources to Advocacy Now (Post 4 of 7 on AI Governance),https://www.lesswrong.com/posts/dcd2dPLZGFJPgtDzq/shift-resources-to-advocacy-now-post-4-of-7-on-ai-governance,https://www.lesswrong.com/posts/dcd2dPLZGFJPgtDzq/shift-resources-to-advocacy-now-post-4-of-7-on-ai-governance,,2025-05-28T01:19:27.307000+00:00,60,18,,,
adQueu9FFHfiBKDCt,Political Funding Expertise (Post 6 of 7 on AI Governance),https://www.lesswrong.com/posts/adQueu9FFHfiBKDCt/political-funding-expertise-post-6-of-7-on-ai-governance,https://www.lesswrong.com/posts/adQueu9FFHfiBKDCt/political-funding-expertise-post-6-of-7-on-ai-governance,,2025-06-19T14:14:31.909000+00:00,59,4,,,
BaigpPyZpkZuSjmAz,"The Need for Political Advertising
(Post 2 of 7 on AI Governance)",https://www.lesswrong.com/posts/BaigpPyZpkZuSjmAz/the-need-for-political-advertising-post-2-of-7-on-ai,https://www.lesswrong.com/posts/BaigpPyZpkZuSjmAz/the-need-for-political-advertising-post-2-of-7-on-ai,,2025-05-21T00:44:06.560000+00:00,59,2,,,
4QRvFCzhFbedmNfp4,"To be legible, evidence of misalignment probably has to be behavioral",https://www.lesswrong.com/posts/4QRvFCzhFbedmNfp4/to-be-legible-evidence-of-misalignment-probably-has-to-be,https://www.lesswrong.com/posts/4QRvFCzhFbedmNfp4/to-be-legible-evidence-of-misalignment-probably-has-to-be,,2025-04-15T18:14:53.022000+00:00,58,19,,,
QxJFjqT6oFY3jo47s,AI Safety as a YC Startup,https://www.lesswrong.com/posts/QxJFjqT6oFY3jo47s/ai-safety-as-a-yc-startup-1,https://www.lesswrong.com/posts/QxJFjqT6oFY3jo47s/ai-safety-as-a-yc-startup-1,,2025-01-08T10:46:29.042000+00:00,58,9,,,
xGNnBmtAL5F5vBKRf,AI Safety Law-a-thon: Turning Alignment Risks into Legal Strategy,https://www.lesswrong.com/posts/xGNnBmtAL5F5vBKRf/ai-safety-law-a-thon-turning-alignment-risks-into-legal,https://www.lesswrong.com/posts/xGNnBmtAL5F5vBKRf/ai-safety-law-a-thon-turning-alignment-risks-into-legal,,2025-09-10T10:22:16.082000+00:00,58,5,,,
Wi5keDzktqmANL422,On OpenAI’s Safety and Alignment Philosophy,https://www.lesswrong.com/posts/Wi5keDzktqmANL422/on-openai-s-safety-and-alignment-philosophy,https://www.lesswrong.com/posts/Wi5keDzktqmANL422/on-openai-s-safety-and-alignment-philosophy,,2025-03-05T14:00:07.302000+00:00,58,5,,,
nmaKpoHxmzjT8yXTk,New website analyzing AI companies' model evals,https://www.lesswrong.com/posts/nmaKpoHxmzjT8yXTk/new-website-analyzing-ai-companies-model-evals,https://www.lesswrong.com/posts/nmaKpoHxmzjT8yXTk/new-website-analyzing-ai-companies-model-evals,,2025-05-26T16:00:51.602000+00:00,58,0,,,
iELyAqizJkizBQbfr,An alignment safety case sketch based on debate,https://www.lesswrong.com/posts/iELyAqizJkizBQbfr/an-alignment-safety-case-sketch-based-on-debate,https://www.lesswrong.com/posts/iELyAqizJkizBQbfr/an-alignment-safety-case-sketch-based-on-debate,,2025-05-08T15:02:06.345000+00:00,57,21,,,
hvEikwtsbf6zaXG2s,On Google’s Safety Plan,https://www.lesswrong.com/posts/hvEikwtsbf6zaXG2s/on-google-s-safety-plan,https://www.lesswrong.com/posts/hvEikwtsbf6zaXG2s/on-google-s-safety-plan,,2025-04-11T12:51:12.112000+00:00,57,6,,,
vWYzSorAEWwoJnnXq,A sketch of an AI control safety case,https://www.lesswrong.com/posts/vWYzSorAEWwoJnnXq/a-sketch-of-an-ai-control-safety-case,https://www.lesswrong.com/posts/vWYzSorAEWwoJnnXq/a-sketch-of-an-ai-control-safety-case,,2025-01-30T17:28:47.992000+00:00,57,0,,,
RRvdRyWrSqKW2ANL9,Alignment Proposal: Adversarially Robust Augmentation and Distillation,https://www.lesswrong.com/posts/RRvdRyWrSqKW2ANL9/alignment-proposal-adversarially-robust-augmentation-and,https://www.lesswrong.com/posts/RRvdRyWrSqKW2ANL9/alignment-proposal-adversarially-robust-augmentation-and,,2025-05-25T12:58:55.336000+00:00,56,47,,,
y9QEHKdqsfsXBMAkb,Mainstream Grantmaking Expertise (Post 7 of 7 on AI Governance),https://www.lesswrong.com/posts/y9QEHKdqsfsXBMAkb/mainstream-grantmaking-expertise-post-7-of-7-on-ai,https://www.lesswrong.com/posts/y9QEHKdqsfsXBMAkb/mainstream-grantmaking-expertise-post-7-of-7-on-ai,,2025-06-23T01:39:43.466000+00:00,56,7,,,
cdPPr6XtPkCX5c8Ny,Predict 2025 AI capabilities (by Sunday),https://www.lesswrong.com/posts/cdPPr6XtPkCX5c8Ny/predict-2025-ai-capabilities-by-sunday,https://www.lesswrong.com/posts/cdPPr6XtPkCX5c8Ny/predict-2025-ai-capabilities-by-sunday,,2025-01-15T00:16:05.034000+00:00,55,3,,,
88xgGLnLo64AgjGco,Where are the AI safety replications?,https://www.lesswrong.com/posts/88xgGLnLo64AgjGco/where-are-the-ai-safety-replications,https://www.lesswrong.com/posts/88xgGLnLo64AgjGco/where-are-the-ai-safety-replications,,2025-07-26T21:29:48.774000+00:00,54,5,,,
qHudHZNLCiFrygRiy,Emergent Misalignment on a Budget,https://www.lesswrong.com/posts/qHudHZNLCiFrygRiy/emergent-misalignment-on-a-budget,https://www.lesswrong.com/posts/qHudHZNLCiFrygRiy/emergent-misalignment-on-a-budget,,2025-06-08T15:28:50.498000+00:00,54,0,,,
qe8LjXAtaZfrc8No7,Call for suggestions - AI safety course,https://www.lesswrong.com/posts/qe8LjXAtaZfrc8No7/call-for-suggestions-ai-safety-course,https://www.lesswrong.com/posts/qe8LjXAtaZfrc8No7/call-for-suggestions-ai-safety-course,,2025-07-03T14:30:41.147000+00:00,53,23,,,
bzYJCXicmwDHDpLZa,Reframing AI Safety as a Neverending Institutional Challenge,https://www.lesswrong.com/posts/bzYJCXicmwDHDpLZa/reframing-ai-safety-as-a-neverending-institutional-challenge,https://www.lesswrong.com/posts/bzYJCXicmwDHDpLZa/reframing-ai-safety-as-a-neverending-institutional-challenge,,2025-03-23T00:13:48.614000+00:00,53,12,,,
9tHEibBBhQCHEyFsa,"Do LLMs know what they're capable of? Why this matters for AI safety, and initial findings",https://www.lesswrong.com/posts/9tHEibBBhQCHEyFsa/do-llms-know-what-they-re-capable-of-why-this-matters-for-ai,https://www.lesswrong.com/posts/9tHEibBBhQCHEyFsa/do-llms-know-what-they-re-capable-of-why-this-matters-for-ai,,2025-07-13T19:54:53.974000+00:00,53,5,,,
a4EDinzAYtRwpNmx9,Towards data-centric interpretability with sparse autoencoders,https://www.lesswrong.com/posts/a4EDinzAYtRwpNmx9/towards-data-centric-interpretability-with-sparse,https://www.lesswrong.com/posts/a4EDinzAYtRwpNmx9/towards-data-centric-interpretability-with-sparse,,2025-08-15T20:10:55.825000+00:00,53,2,,,
u3ZysuXEjkyHhefrk,Against blanket arguments against interpretability,https://www.lesswrong.com/posts/u3ZysuXEjkyHhefrk/against-blanket-arguments-against-interpretability,https://www.lesswrong.com/posts/u3ZysuXEjkyHhefrk/against-blanket-arguments-against-interpretability,,2025-01-22T09:46:23.486000+00:00,52,4,,,
CSFa9rvGNGAfCzBk6,Problems with instruction-following as an alignment target,https://www.lesswrong.com/posts/CSFa9rvGNGAfCzBk6/problems-with-instruction-following-as-an-alignment-target,https://www.lesswrong.com/posts/CSFa9rvGNGAfCzBk6/problems-with-instruction-following-as-an-alignment-target,,2025-05-15T15:41:48.748000+00:00,51,14,,,
CJ4yywLBkdRALc4sT,On Deliberative Alignment,https://www.lesswrong.com/posts/CJ4yywLBkdRALc4sT/on-deliberative-alignment,https://www.lesswrong.com/posts/CJ4yywLBkdRALc4sT/on-deliberative-alignment,,2025-02-11T13:00:07.683000+00:00,51,2,,,
GvgmoDts5kphwGyS2,60 U.K. Lawmakers Accuse Google of Breaking AI Safety Pledge,https://www.lesswrong.com/posts/GvgmoDts5kphwGyS2/60-u-k-lawmakers-accuse-google-of-breaking-ai-safety-pledge,https://www.lesswrong.com/posts/GvgmoDts5kphwGyS2/60-u-k-lawmakers-accuse-google-of-breaking-ai-safety-pledge,,2025-08-29T16:09:08.998000+00:00,51,1,,,
JrTk2pbqp7BFwPAKw,Reward button alignment,https://www.lesswrong.com/posts/JrTk2pbqp7BFwPAKw/reward-button-alignment,https://www.lesswrong.com/posts/JrTk2pbqp7BFwPAKw/reward-button-alignment,,2025-05-22T17:36:50.078000+00:00,50,15,,,
nnzb7prw2XC8cFhnP,OpenAI #13: Altman at TED and OpenAI Cutting Corners on Safety Testing,https://www.lesswrong.com/posts/nnzb7prw2XC8cFhnP/openai-13-altman-at-ted-and-openai-cutting-corners-on-safety,https://www.lesswrong.com/posts/nnzb7prw2XC8cFhnP/openai-13-altman-at-ted-and-openai-cutting-corners-on-safety,,2025-04-15T15:30:02.518000+00:00,48,3,,,
zecxwyATrN8ZbinoC,"Interview with Steven Byrnes on Brain-like AGI, Foom & Doom, and Solving Technical Alignment",https://www.lesswrong.com/posts/zecxwyATrN8ZbinoC/interview-with-steven-byrnes-on-brain-like-agi-foom-and-doom,https://www.lesswrong.com/posts/zecxwyATrN8ZbinoC/interview-with-steven-byrnes-on-brain-like-agi-foom-and-doom,,2025-08-05T00:05:23.046000+00:00,48,1,,,
wzCtwYtojMabyEg2L,What is Inadequate about Bayesianism for AI Alignment: Motivating Infra-Bayesianism,https://www.lesswrong.com/posts/wzCtwYtojMabyEg2L/what-is-inadequate-about-bayesianism-for-ai-alignment,https://www.lesswrong.com/posts/wzCtwYtojMabyEg2L/what-is-inadequate-about-bayesianism-for-ai-alignment,,2025-05-01T19:06:15.711000+00:00,48,1,,,
aG9e5tHfHmBnDqrDy,The GDM AGI Safety+Alignment Team is Hiring for Applied Interpretability Research,https://www.lesswrong.com/posts/aG9e5tHfHmBnDqrDy/the-gdm-agi-safety-alignment-team-is-hiring-for-applied,https://www.lesswrong.com/posts/aG9e5tHfHmBnDqrDy/the-gdm-agi-safety-alignment-team-is-hiring-for-applied,,2025-02-24T02:17:12.991000+00:00,48,1,,,
nJcuj4rtuefeTRFHp,Can we safely automate alignment research?,https://www.lesswrong.com/posts/nJcuj4rtuefeTRFHp/can-we-safely-automate-alignment-research,https://www.lesswrong.com/posts/nJcuj4rtuefeTRFHp/can-we-safely-automate-alignment-research,,2025-04-30T17:37:13.193000+00:00,47,29,,,
hmds9eDjqFaadCk4F,Overview: AI Safety Outreach Grassroots Orgs,https://www.lesswrong.com/posts/hmds9eDjqFaadCk4F/overview-ai-safety-outreach-grassroots-orgs,https://www.lesswrong.com/posts/hmds9eDjqFaadCk4F/overview-ai-safety-outreach-grassroots-orgs,,2025-05-04T17:39:10.352000+00:00,47,8,,,
vGeuBKQ7nzPnn5f7A,Why modelling multi-objective homeostasis is essential for AI alignment (and how it helps with AI safety as well). Subtleties and Open Challenges.,https://www.lesswrong.com/posts/vGeuBKQ7nzPnn5f7A/why-modelling-multi-objective-homeostasis-is-essential-for,https://www.lesswrong.com/posts/vGeuBKQ7nzPnn5f7A/why-modelling-multi-objective-homeostasis-is-essential-for,,2025-01-12T03:37:59.692000+00:00,47,7,,,
KMbZWcTvGjChw9ynD,"Focus transparency on risk reports, not safety cases",https://www.lesswrong.com/posts/KMbZWcTvGjChw9ynD/focus-transparency-on-risk-reports-not-safety-cases,https://www.lesswrong.com/posts/KMbZWcTvGjChw9ynD/focus-transparency-on-risk-reports-not-safety-cases,,2025-09-22T15:27:49.280000+00:00,47,3,,,
dwpXvweBrJwErse3L,All the labs AI safety plans: 2025 edition,https://www.lesswrong.com/posts/dwpXvweBrJwErse3L/all-the-labs-ai-safety-plans-2025-edition,https://www.lesswrong.com/posts/dwpXvweBrJwErse3L/all-the-labs-ai-safety-plans-2025-edition,,2025-10-28T00:25:07.919000+00:00,47,2,,,
DJAZHYjWxMrcd2na3,Building and evaluating alignment auditing agents,https://www.lesswrong.com/posts/DJAZHYjWxMrcd2na3/building-and-evaluating-alignment-auditing-agents,https://www.lesswrong.com/posts/DJAZHYjWxMrcd2na3/building-and-evaluating-alignment-auditing-agents,,2025-07-24T19:22:26.195000+00:00,47,1,,,
CwJ2qWveb9JbaCGQ5,Harmless reward hacks can generalize to misalignment in LLMs,https://www.lesswrong.com/posts/CwJ2qWveb9JbaCGQ5/harmless-reward-hacks-can-generalize-to-misalignment-in-llms,https://www.lesswrong.com/posts/CwJ2qWveb9JbaCGQ5/harmless-reward-hacks-can-generalize-to-misalignment-in-llms,,2025-08-26T17:32:58.002000+00:00,46,7,,,
kyBGcHfzfZziHm5xL,Why I don't believe Superalignment will work,https://www.lesswrong.com/posts/kyBGcHfzfZziHm5xL/why-i-don-t-believe-superalignment-will-work,https://www.lesswrong.com/posts/kyBGcHfzfZziHm5xL/why-i-don-t-believe-superalignment-will-work,,2025-09-22T17:10:43.280000+00:00,46,6,,,
7ExkgcDudwhag73vw,Existing Safety Frameworks Imply Unreasonable Confidence,https://www.lesswrong.com/posts/7ExkgcDudwhag73vw/existing-safety-frameworks-imply-unreasonable-confidence,https://www.lesswrong.com/posts/7ExkgcDudwhag73vw/existing-safety-frameworks-imply-unreasonable-confidence,,2025-04-10T16:31:50.240000+00:00,46,3,,,
gwKyHqe4CL6TZNQxp,Why do many people who care about AI Safety not clearly endorse PauseAI?,https://www.lesswrong.com/posts/gwKyHqe4CL6TZNQxp/why-do-many-people-who-care-about-ai-safety-not-clearly,https://www.lesswrong.com/posts/gwKyHqe4CL6TZNQxp/why-do-many-people-who-care-about-ai-safety-not-clearly,,2025-03-30T18:06:32.426000+00:00,45,42,,,
5gmALpCetyjkSPEDr,Training AI to do alignment research we don’t already know how to do,https://www.lesswrong.com/posts/5gmALpCetyjkSPEDr/training-ai-to-do-alignment-research-we-don-t-already-know,https://www.lesswrong.com/posts/5gmALpCetyjkSPEDr/training-ai-to-do-alignment-research-we-don-t-already-know,,2025-02-24T19:19:43.067000+00:00,45,24,,,
sjr66DBEgyogAbfdf,Renormalization Redux: QFT Techniques for AI Interpretability,https://www.lesswrong.com/posts/sjr66DBEgyogAbfdf/renormalization-redux-qft-techniques-for-ai-interpretability,https://www.lesswrong.com/posts/sjr66DBEgyogAbfdf/renormalization-redux-qft-techniques-for-ai-interpretability,,2025-01-18T03:54:28.652000+00:00,45,12,,,
PejNckwQj3A2MGhMA,Systematic runaway-optimiser-like LLM failure modes on Biologically and Economically aligned AI safety benchmarks for LLMs with simplified observation format (BioBlue),https://www.lesswrong.com/posts/PejNckwQj3A2MGhMA/systematic-runaway-optimiser-like-llm-failure-modes-on,https://www.lesswrong.com/posts/PejNckwQj3A2MGhMA/systematic-runaway-optimiser-like-llm-failure-modes-on,,2025-03-16T23:23:30.989000+00:00,45,8,,,
25dsPH6CuRXPBkGHN,"No, We're Not Getting Meaningful Oversight of AI",https://www.lesswrong.com/posts/25dsPH6CuRXPBkGHN/no-we-re-not-getting-meaningful-oversight-of-ai,https://www.lesswrong.com/posts/25dsPH6CuRXPBkGHN/no-we-re-not-getting-meaningful-oversight-of-ai,,2025-07-09T11:10:54.429000+00:00,45,4,,,
ZdY4JzBPJEgaoCxTR,Emergent Misalignment & Realignment,https://www.lesswrong.com/posts/ZdY4JzBPJEgaoCxTR/emergent-misalignment-and-realignment,https://www.lesswrong.com/posts/ZdY4JzBPJEgaoCxTR/emergent-misalignment-and-realignment,,2025-06-27T21:31:43.367000+00:00,45,1,,,
etqbEF4yWoGBEaPro,On the Meta and DeepMind Safety Frameworks,https://www.lesswrong.com/posts/etqbEF4yWoGBEaPro/on-the-meta-and-deepmind-safety-frameworks,https://www.lesswrong.com/posts/etqbEF4yWoGBEaPro/on-the-meta-and-deepmind-safety-frameworks,,2025-02-07T13:10:08.449000+00:00,45,1,,,
gTt2J5uJ4mrTWkncF,Intelsat as a Model for International AGI Governance,https://www.lesswrong.com/posts/gTt2J5uJ4mrTWkncF/intelsat-as-a-model-for-international-agi-governance,https://www.lesswrong.com/posts/gTt2J5uJ4mrTWkncF/intelsat-as-a-model-for-international-agi-governance,,2025-03-13T12:58:11.692000+00:00,45,0,,,
kjbq7T7Z2vEDoPw95,Ketamine part 2: What do in vitro studies tell us about safety?,https://www.lesswrong.com/posts/kjbq7T7Z2vEDoPw95/ketamine-part-2-what-do-in-vitro-studies-tell-us-about,https://www.lesswrong.com/posts/kjbq7T7Z2vEDoPw95/ketamine-part-2-what-do-in-vitro-studies-tell-us-about,,2025-09-07T17:10:09.462000+00:00,44,0,,,
7zhAwcBri7yupStKy,Here’s 18 Applications of Deception Probes,https://www.lesswrong.com/posts/7zhAwcBri7yupStKy/here-s-18-applications-of-deception-probes,https://www.lesswrong.com/posts/7zhAwcBri7yupStKy/here-s-18-applications-of-deception-probes,,2025-08-28T18:59:37.797000+00:00,44,0,,,
h89L5FMAkEBNsZ3xM,A single principle related to many Alignment subproblems?,https://www.lesswrong.com/posts/h89L5FMAkEBNsZ3xM/a-single-principle-related-to-many-alignment-subproblems-2,https://www.lesswrong.com/posts/h89L5FMAkEBNsZ3xM/a-single-principle-related-to-many-alignment-subproblems-2,,2025-04-30T09:49:21.181000+00:00,43,34,,,
SAkFA5jHzzD5JWWxC,Alignment Is Not All You Need,https://www.lesswrong.com/posts/SAkFA5jHzzD5JWWxC/alignment-is-not-all-you-need,https://www.lesswrong.com/posts/SAkFA5jHzzD5JWWxC/alignment-is-not-all-you-need,,2025-01-02T17:50:00.486000+00:00,43,10,,,
kMiwjx6QyyBBTcjxt,Does the Universal Geometry of Embeddings paper have big implications for interpretability?,https://www.lesswrong.com/posts/kMiwjx6QyyBBTcjxt/does-the-universal-geometry-of-embeddings-paper-have-big,https://www.lesswrong.com/posts/kMiwjx6QyyBBTcjxt/does-the-universal-geometry-of-embeddings-paper-have-big,,2025-05-26T18:20:48.111000+00:00,43,6,,,
6DfWFtL7mcs3vnHPn,Should AI Developers Remove Discussion of AI Misalignment from AI Training Data?,https://www.lesswrong.com/posts/6DfWFtL7mcs3vnHPn/should-ai-developers-remove-discussion-of-ai-misalignment,https://www.lesswrong.com/posts/6DfWFtL7mcs3vnHPn/should-ai-developers-remove-discussion-of-ai-misalignment,,2025-10-23T15:12:51.392000+00:00,43,3,,,
XGHf7EY3CK4KorBpw,Understanding LLMs: Insights from Mechanistic Interpretability,https://www.lesswrong.com/posts/XGHf7EY3CK4KorBpw/understanding-llms-insights-from-mechanistic,https://www.lesswrong.com/posts/XGHf7EY3CK4KorBpw/understanding-llms-insights-from-mechanistic,,2025-08-30T16:50:07.429000+00:00,43,2,,,
vkdpw2vCnspK9t7nA,My January alignment theory Nanowrimo,https://www.lesswrong.com/posts/vkdpw2vCnspK9t7nA/my-january-alignment-theory-nanowrimo,https://www.lesswrong.com/posts/vkdpw2vCnspK9t7nA/my-january-alignment-theory-nanowrimo,,2025-01-02T00:07:24.050000+00:00,42,2,,,
kBgySGcASWa4FWdD9,Paths and waystations in AI safety,https://www.lesswrong.com/posts/kBgySGcASWa4FWdD9/paths-and-waystations-in-ai-safety-1,https://www.lesswrong.com/posts/kBgySGcASWa4FWdD9/paths-and-waystations-in-ai-safety-1,,2025-03-11T18:52:57.772000+00:00,42,1,,,
Ns7qfCqSgMjQNDZN8,"Sentinel's Global Risks Weekly Roundup #15/2025: Tariff yoyo, OpenAI slashing safety testing, Iran nuclear programme negotiations, 1K H5N1 confirmed herd infections.",https://www.lesswrong.com/posts/Ns7qfCqSgMjQNDZN8/sentinel-s-global-risks-weekly-roundup-15-2025-tariff-yoyo,https://www.lesswrong.com/posts/Ns7qfCqSgMjQNDZN8/sentinel-s-global-risks-weekly-roundup-15-2025-tariff-yoyo,,2025-04-14T19:11:20.977000+00:00,42,0,,,
yAwnYoeCz7PqeNrtL,Lectures on statistical learning theory for alignment researchers,https://www.lesswrong.com/posts/yAwnYoeCz7PqeNrtL/lectures-on-statistical-learning-theory-for-alignment,https://www.lesswrong.com/posts/yAwnYoeCz7PqeNrtL/lectures-on-statistical-learning-theory-for-alignment,,2025-10-01T08:36:52.525000+00:00,41,1,,,
Z8KLLHvsEkukxpTCD,‘GiveWell for AI Safety’: Lessons learned in a week,https://www.lesswrong.com/posts/Z8KLLHvsEkukxpTCD/givewell-for-ai-safety-lessons-learned-in-a-week,https://www.lesswrong.com/posts/Z8KLLHvsEkukxpTCD/givewell-for-ai-safety-lessons-learned-in-a-week,,2025-05-30T18:38:05.473000+00:00,41,0,,,
GvMakH65LS86RFn5x,Rolling Thresholds for AGI Scaling Regulation,https://www.lesswrong.com/posts/GvMakH65LS86RFn5x/rolling-thresholds-for-agi-scaling-regulation,https://www.lesswrong.com/posts/GvMakH65LS86RFn5x/rolling-thresholds-for-agi-scaling-regulation,,2025-01-12T01:30:23.797000+00:00,40,6,,,
eR69f3hi5ozxchhYg,Scaling Wargaming for Global Catastrophic Risks with AI,https://www.lesswrong.com/posts/eR69f3hi5ozxchhYg/scaling-wargaming-for-global-catastrophic-risks-with-ai,https://www.lesswrong.com/posts/eR69f3hi5ozxchhYg/scaling-wargaming-for-global-catastrophic-risks-with-ai,,2025-01-18T15:10:39.696000+00:00,40,2,,,
qhjNejRxbMGQp4wHt,How Fast Can Algorithms Advance Capabilities? | Epoch Gradient Update,https://www.lesswrong.com/posts/qhjNejRxbMGQp4wHt/how-fast-can-algorithms-advance-capabilities-or-epoch,https://www.lesswrong.com/posts/qhjNejRxbMGQp4wHt/how-fast-can-algorithms-advance-capabilities-or-epoch,,2025-05-16T21:38:46.822000+00:00,39,10,,,
CXYf7kGBecZMajrXC,Validating against a misalignment detector is very different to training against one,https://www.lesswrong.com/posts/CXYf7kGBecZMajrXC/validating-against-a-misalignment-detector-is-very-different,https://www.lesswrong.com/posts/CXYf7kGBecZMajrXC/validating-against-a-misalignment-detector-is-very-different,,2025-03-04T15:41:04.692000+00:00,39,4,,,
uwXajjhGeiYBGfz85,AI misbehaviour in the wild from Andon Labs' Safety Report,https://www.lesswrong.com/posts/uwXajjhGeiYBGfz85/ai-misbehaviour-in-the-wild-from-andon-labs-safety-report,https://www.lesswrong.com/posts/uwXajjhGeiYBGfz85/ai-misbehaviour-in-the-wild-from-andon-labs-safety-report,,2025-08-28T15:10:06.519000+00:00,39,0,,,
cus5CGmLrjBRgcPSF,"System 2 Alignment: Deliberation, Review, and Thought Management",https://www.lesswrong.com/posts/cus5CGmLrjBRgcPSF/system-2-alignment-deliberation-review-and-thought,https://www.lesswrong.com/posts/cus5CGmLrjBRgcPSF/system-2-alignment-deliberation-review-and-thought,,2025-02-13T19:17:56.868000+00:00,39,0,,,
p28GHSYskzsGKvABH,I underestimated safety research speedups from safe AI,https://www.lesswrong.com/posts/p28GHSYskzsGKvABH/i-underestimated-safety-research-speedups-from-safe-ai,https://www.lesswrong.com/posts/p28GHSYskzsGKvABH/i-underestimated-safety-research-speedups-from-safe-ai,,2025-06-29T13:29:44.904000+00:00,38,2,,,
wzTieM48mzYxLdYPi,AI Safety Research Futarchy: Using Prediction Markets to Choose Research Projects for MARS,https://www.lesswrong.com/posts/wzTieM48mzYxLdYPi/ai-safety-research-futarchy-using-prediction-markets-to,https://www.lesswrong.com/posts/wzTieM48mzYxLdYPi/ai-safety-research-futarchy-using-prediction-markets-to,,2025-09-30T15:37:49.366000+00:00,37,10,,,
5JJ4AxQRzJGWdj4pN,Building Big Science from the Bottom-Up: A Fractal Approach to AI Safety,https://www.lesswrong.com/posts/5JJ4AxQRzJGWdj4pN/building-big-science-from-the-bottom-up-a-fractal-approach,https://www.lesswrong.com/posts/5JJ4AxQRzJGWdj4pN/building-big-science-from-the-bottom-up-a-fractal-approach,,2025-01-07T03:08:51.447000+00:00,37,2,,,
x59FhzuM9yuvZHAHW,Scaling Laws for Scalable Oversight,https://www.lesswrong.com/posts/x59FhzuM9yuvZHAHW/scaling-laws-for-scalable-oversight,https://www.lesswrong.com/posts/x59FhzuM9yuvZHAHW/scaling-laws-for-scalable-oversight,,2025-04-30T12:13:32.412000+00:00,37,1,,,
qSDvzyh7LgsAJfehk,AI Safety x Physics Grand Challenge,https://www.lesswrong.com/posts/qSDvzyh7LgsAJfehk/ai-safety-x-physics-grand-challenge,https://www.lesswrong.com/posts/qSDvzyh7LgsAJfehk/ai-safety-x-physics-grand-challenge,,2025-07-23T21:41:18.023000+00:00,37,0,,,
zjqrSKZuRLnjAniyo,"Illusory Safety: Redteaming DeepSeek R1 and the Strongest Fine-Tunable Models of OpenAI, Anthropic, and Google",https://www.lesswrong.com/posts/zjqrSKZuRLnjAniyo/illusory-safety-redteaming-deepseek-r1-and-the-strongest,https://www.lesswrong.com/posts/zjqrSKZuRLnjAniyo/illusory-safety-redteaming-deepseek-r1-and-the-strongest,,2025-02-07T03:57:30.904000+00:00,37,0,,,
PK2EmWmzngC6hPPDM,"We don't want to post again ""This might be the last AI Safety Camp""",https://www.lesswrong.com/posts/PK2EmWmzngC6hPPDM/we-don-t-want-to-post-again-this-might-be-the-last-ai-safety,https://www.lesswrong.com/posts/PK2EmWmzngC6hPPDM/we-don-t-want-to-post-again-this-might-be-the-last-ai-safety,,2025-01-21T12:03:33.171000+00:00,36,17,,,
Kd2cbLXQxCCRRQDcH,Theory of Change for AI Safety Camp,https://www.lesswrong.com/posts/Kd2cbLXQxCCRRQDcH/theory-of-change-for-ai-safety-camp,https://www.lesswrong.com/posts/Kd2cbLXQxCCRRQDcH/theory-of-change-for-ai-safety-camp,,2025-01-22T22:07:10.664000+00:00,36,3,,,
sLZQrwQnPswNTEbWi,"Upcoming Workshop on Post-AGI Economics, Culture, and Governance",https://www.lesswrong.com/posts/sLZQrwQnPswNTEbWi/upcoming-workshop-on-post-agi-economics-culture-and,https://www.lesswrong.com/posts/sLZQrwQnPswNTEbWi/upcoming-workshop-on-post-agi-economics-culture-and,,2025-10-28T21:55:42.867000+00:00,36,1,,,
FqpAPC48CzAtvfx5C,Automating AI Safety: What we can do today,https://www.lesswrong.com/posts/FqpAPC48CzAtvfx5C/automating-ai-safety-what-we-can-do-today,https://www.lesswrong.com/posts/FqpAPC48CzAtvfx5C/automating-ai-safety-what-we-can-do-today,,2025-07-25T14:49:33.440000+00:00,36,0,,,
LSJx5EnQEW6s5Juw6,No-self as an alignment target,https://www.lesswrong.com/posts/LSJx5EnQEW6s5Juw6/no-self-as-an-alignment-target,https://www.lesswrong.com/posts/LSJx5EnQEW6s5Juw6/no-self-as-an-alignment-target,,2025-05-13T01:48:29.456000+00:00,35,5,,,
ciw6DCdywoXk7yrdw,New homepage for AI safety resources – AISafety.com redesign,https://www.lesswrong.com/posts/ciw6DCdywoXk7yrdw/new-homepage-for-ai-safety-resources-aisafety-com-redesign,https://www.lesswrong.com/posts/ciw6DCdywoXk7yrdw/new-homepage-for-ai-safety-resources-aisafety-com-redesign,,2025-11-05T10:33:04.719000+00:00,35,2,,,
MDWGcNHkZ3NPEzcnp,Call for Collaboration: Renormalization for AI safety ,https://www.lesswrong.com/posts/MDWGcNHkZ3NPEzcnp/call-for-collaboration-renormalization-for-ai-safety,https://www.lesswrong.com/posts/MDWGcNHkZ3NPEzcnp/call-for-collaboration-renormalization-for-ai-safety,,2025-03-31T21:01:56.500000+00:00,35,0,,,
bc5ohMwAyshdwJkDt,Forecasting Frontier Language Model Agent Capabilities,https://www.lesswrong.com/posts/bc5ohMwAyshdwJkDt/forecasting-frontier-language-model-agent-capabilities,https://www.lesswrong.com/posts/bc5ohMwAyshdwJkDt/forecasting-frontier-language-model-agent-capabilities,,2025-02-24T16:51:32.022000+00:00,35,0,,,
Em9sihEZmbofZKc2t,"A Concrete Roadmap towards Safety Cases based on
Chain-of-Thought Monitoring",https://www.lesswrong.com/posts/Em9sihEZmbofZKc2t/a-concrete-roadmap-towards-safety-cases-based-on-chain-of,https://www.lesswrong.com/posts/Em9sihEZmbofZKc2t/a-concrete-roadmap-towards-safety-cases-based-on-chain-of,,2025-10-23T11:34:55.397000+00:00,34,2,,,
NDotm7oLHfR56g4sD,Why do misalignment risks increase as AIs get more capable?,https://www.lesswrong.com/posts/NDotm7oLHfR56g4sD/why-do-misalignment-risks-increase-as-ais-get-more-capable,https://www.lesswrong.com/posts/NDotm7oLHfR56g4sD/why-do-misalignment-risks-increase-as-ais-get-more-capable,,2025-04-11T03:06:50.928000+00:00,33,6,,,
x6ffKSHXxxbueYrHE,Widening AI Safety's talent pipeline by meeting people where they are,https://www.lesswrong.com/posts/x6ffKSHXxxbueYrHE/widening-ai-safety-s-talent-pipeline-by-meeting-people-where,https://www.lesswrong.com/posts/x6ffKSHXxxbueYrHE/widening-ai-safety-s-talent-pipeline-by-meeting-people-where,,2025-09-25T20:50:44.452000+00:00,33,3,,,
EgRJtwQurNzz8CEfJ,Dodging systematic human errors in scalable oversight,https://www.lesswrong.com/posts/EgRJtwQurNzz8CEfJ/dodging-systematic-human-errors-in-scalable-oversight,https://www.lesswrong.com/posts/EgRJtwQurNzz8CEfJ/dodging-systematic-human-errors-in-scalable-oversight,,2025-05-14T15:19:39.352000+00:00,33,3,,,
vxSGDLGRtfcf6FWBg,"Top AI safety newsletters, books, podcasts, etc – new AISafety.com resource",https://www.lesswrong.com/posts/vxSGDLGRtfcf6FWBg/top-ai-safety-newsletters-books-podcasts-etc-new-aisafety,https://www.lesswrong.com/posts/vxSGDLGRtfcf6FWBg/top-ai-safety-newsletters-books-podcasts-etc-new-aisafety,,2025-03-04T17:01:18.758000+00:00,33,2,,,
y5cYisQ2QHiSbQbhk,"Prospects for Alignment Automation:
Interpretability Case Study",https://www.lesswrong.com/posts/y5cYisQ2QHiSbQbhk/prospects-for-alignment-automation-interpretability-case,https://www.lesswrong.com/posts/y5cYisQ2QHiSbQbhk/prospects-for-alignment-automation-interpretability-case,,2025-03-21T14:05:51.528000+00:00,32,5,,,
bWYisdRccDDHbista,"Why did interest in ""AI risk"" and ""AI safety"" spike in June and July 2025? (Google Trends)",https://www.lesswrong.com/posts/bWYisdRccDDHbista/why-did-interest-in-ai-risk-and-ai-safety-spike-in-june-and,https://www.lesswrong.com/posts/bWYisdRccDDHbista/why-did-interest-in-ai-risk-and-ai-safety-spike-in-june-and,,2025-08-16T15:22:46.237000+00:00,32,4,,,
yEJwJzG2o3cwDSDqP,Why I'm Posting AI-Safety-Related Clips On TikTok,https://www.lesswrong.com/posts/yEJwJzG2o3cwDSDqP/why-i-m-posting-ai-safety-related-clips-on-tiktok,https://www.lesswrong.com/posts/yEJwJzG2o3cwDSDqP/why-i-m-posting-ai-safety-related-clips-on-tiktok,,2025-08-12T22:50:22.487000+00:00,32,1,,,
CCT7Qc8rSeRs7r5GL,Beliefs about formal methods and AI safety,https://www.lesswrong.com/posts/CCT7Qc8rSeRs7r5GL/beliefs-about-formal-methods-and-ai-safety,https://www.lesswrong.com/posts/CCT7Qc8rSeRs7r5GL/beliefs-about-formal-methods-and-ai-safety,,2025-10-23T16:43:24.911000+00:00,32,0,,,
DqaoPNqhQhwBFqWue,Principles for Picking Practical Interpretability Projects,https://www.lesswrong.com/posts/DqaoPNqhQhwBFqWue/principles-for-picking-practical-interpretability-projects,https://www.lesswrong.com/posts/DqaoPNqhQhwBFqWue/principles-for-picking-practical-interpretability-projects,,2025-07-15T17:38:25.221000+00:00,32,0,,,
xAsviBJGSBBtgBiCw,The Best Way to Align an LLM: Is Inner Alignment Now a Solved Problem?,https://www.lesswrong.com/posts/xAsviBJGSBBtgBiCw/the-best-way-to-align-an-llm-is-inner-alignment-now-a-solved,https://www.lesswrong.com/posts/xAsviBJGSBBtgBiCw/the-best-way-to-align-an-llm-is-inner-alignment-now-a-solved,,2025-05-28T06:21:42.324000+00:00,31,34,,,
SebmGh9HYdd8GZtHA,"""The Urgency of Interpretability"" (Dario Amodei)",https://www.lesswrong.com/posts/SebmGh9HYdd8GZtHA/the-urgency-of-interpretability-dario-amodei,https://www.lesswrong.com/posts/SebmGh9HYdd8GZtHA/the-urgency-of-interpretability-dario-amodei,,2025-04-27T04:31:50.090000+00:00,31,23,,,
syEwQzC6LQywQDrFi,What is it to solve the alignment problem?,https://www.lesswrong.com/posts/syEwQzC6LQywQDrFi/what-is-it-to-solve-the-alignment-problem-2,https://www.lesswrong.com/posts/syEwQzC6LQywQDrFi/what-is-it-to-solve-the-alignment-problem-2,,2025-02-13T18:42:07.215000+00:00,31,6,,,
LH9SoGvgSwqGtcFwk,Misalignment and Roleplaying: Are Misaligned LLMs Acting Out Sci-Fi Stories?,https://www.lesswrong.com/posts/LH9SoGvgSwqGtcFwk/misalignment-and-roleplaying-are-misaligned-llms-acting-out,https://www.lesswrong.com/posts/LH9SoGvgSwqGtcFwk/misalignment-and-roleplaying-are-misaligned-llms-acting-out,,2025-09-24T02:09:24.362000+00:00,31,5,,,
KbFuuaBKRP7FcAADL,Transformers Don't Need LayerNorm at Inference Time: Implications for Interpretability,https://www.lesswrong.com/posts/KbFuuaBKRP7FcAADL/transformers-don-t-need-layernorm-at-inference-time,https://www.lesswrong.com/posts/KbFuuaBKRP7FcAADL/transformers-don-t-need-layernorm-at-inference-time,,2025-07-23T14:55:13.283000+00:00,31,0,,,
eaEqAzGN3uJfpfGoc,"Trusted monitoring, but with deception probes.",https://www.lesswrong.com/posts/eaEqAzGN3uJfpfGoc/trusted-monitoring-but-with-deception-probes,https://www.lesswrong.com/posts/eaEqAzGN3uJfpfGoc/trusted-monitoring-but-with-deception-probes,,2025-07-23T05:26:52.399000+00:00,31,0,,,
nZtAkGmDELMnLJMQ5,AXRP Episode 45 - Samuel Albanie on DeepMind’s AGI Safety Approach,https://www.lesswrong.com/posts/nZtAkGmDELMnLJMQ5/axrp-episode-45-samuel-albanie-on-deepmind-s-agi-safety,https://www.lesswrong.com/posts/nZtAkGmDELMnLJMQ5/axrp-episode-45-samuel-albanie-on-deepmind-s-agi-safety,,2025-07-06T23:00:03.659000+00:00,31,0,,,
pCMmLiBcHbKohQgwA,"I replicated the Anthropic alignment faking experiment on other models, and they didn't fake alignment",https://www.lesswrong.com/posts/pCMmLiBcHbKohQgwA/i-replicated-the-anthropic-alignment-faking-experiment-on,https://www.lesswrong.com/posts/pCMmLiBcHbKohQgwA/i-replicated-the-anthropic-alignment-faking-experiment-on,,2025-05-30T18:57:11.169000+00:00,31,0,,,
yacqE5gD5jHywiFKC,The Alignment Mapping Program: Forging Independent Thinkers in AI Safety - A Pilot Retrospective,https://www.lesswrong.com/posts/yacqE5gD5jHywiFKC/the-alignment-mapping-program-forging-independent-thinkers,https://www.lesswrong.com/posts/yacqE5gD5jHywiFKC/the-alignment-mapping-program-forging-independent-thinkers,,2025-01-10T16:22:16.905000+00:00,31,0,,,
tdrK7r4QA3ifbt2Ty,Is AI Alignment Enough?,https://www.lesswrong.com/posts/tdrK7r4QA3ifbt2Ty/is-ai-alignment-enough,https://www.lesswrong.com/posts/tdrK7r4QA3ifbt2Ty/is-ai-alignment-enough,,2025-01-10T18:57:48.409000+00:00,30,6,,,
nwx6duiDZcHatbpPT,Early Signs of Steganographic Capabilities in Frontier LLMs,https://www.lesswrong.com/posts/nwx6duiDZcHatbpPT/untitled-draft-6osz,https://www.lesswrong.com/posts/nwx6duiDZcHatbpPT/untitled-draft-6osz,,2025-07-04T16:36:54.136000+00:00,30,5,,,
QpaWHYEQomyQTBKw5,Nonpartisan AI safety,https://www.lesswrong.com/posts/QpaWHYEQomyQTBKw5/nonpartisan-ai-safety,https://www.lesswrong.com/posts/QpaWHYEQomyQTBKw5/nonpartisan-ai-safety,,2025-02-10T14:55:50.913000+00:00,30,4,,,
Ws6KS7DtQ3F9Gv8ym,Announcing Trajectory Labs - A Toronto AI Safety Office,https://www.lesswrong.com/posts/Ws6KS7DtQ3F9Gv8ym/announcing-trajectory-labs-a-toronto-ai-safety-office,https://www.lesswrong.com/posts/Ws6KS7DtQ3F9Gv8ym/announcing-trajectory-labs-a-toronto-ai-safety-office,,2025-05-13T21:04:11.271000+00:00,30,3,,,
rRLPycsLdjFpZ4cKe,AI Safety Law-a-thon: We need more technical AI Safety researchers to join!,https://www.lesswrong.com/events/rRLPycsLdjFpZ4cKe/ai-safety-law-a-thon-we-need-more-technical-ai-safety,https://www.lesswrong.com/events/rRLPycsLdjFpZ4cKe/ai-safety-law-a-thon-we-need-more-technical-ai-safety,,2025-09-10T10:12:29.352000+00:00,30,2,,,
HYkg6kwqhCQT5uYuK,EIS XV: A New Proof of Concept for Useful Interpretability,https://www.lesswrong.com/posts/HYkg6kwqhCQT5uYuK/eis-xv-a-new-proof-of-concept-for-useful-interpretability,https://www.lesswrong.com/posts/HYkg6kwqhCQT5uYuK/eis-xv-a-new-proof-of-concept-for-useful-interpretability,,2025-03-17T20:05:30.580000+00:00,30,2,,,
8QjAnWyuE9fktPRgS,AI Safety Field Growth Analysis 2025,https://www.lesswrong.com/posts/8QjAnWyuE9fktPRgS/ai-safety-field-growth-analysis-2025,https://www.lesswrong.com/posts/8QjAnWyuE9fktPRgS/ai-safety-field-growth-analysis-2025,,2025-09-27T17:03:08.766000+00:00,29,13,,,
XLNxrFfkyrdktuzqn,Why would AI companies use human-level AI to do alignment research?,https://www.lesswrong.com/posts/XLNxrFfkyrdktuzqn/why-would-ai-companies-use-human-level-ai-to-do-alignment,https://www.lesswrong.com/posts/XLNxrFfkyrdktuzqn/why-would-ai-companies-use-human-level-ai-to-do-alignment,,2025-04-25T19:12:56.202000+00:00,29,8,,,
Ckhek3mXXq7TWvvEh,Lessons from a year of university AI safety field building,https://www.lesswrong.com/posts/Ckhek3mXXq7TWvvEh/lessons-from-a-year-of-university-ai-safety-field-building,https://www.lesswrong.com/posts/Ckhek3mXXq7TWvvEh/lessons-from-a-year-of-university-ai-safety-field-building,,2025-06-06T14:35:14.533000+00:00,29,3,,,
f9dgyttsuC4sK3Nn7,Contest for Better AGI Safety Plans,https://www.lesswrong.com/posts/f9dgyttsuC4sK3Nn7/contest-for-better-agi-safety-plans,https://www.lesswrong.com/posts/f9dgyttsuC4sK3Nn7/contest-for-better-agi-safety-plans,,2025-07-03T17:02:41.064000+00:00,29,1,,,
wKTwdgZDo479EhmJL,The Alignment Project by UK AISI,https://www.lesswrong.com/posts/wKTwdgZDo479EhmJL/the-alignment-project-by-uk-aisi-1,https://www.lesswrong.com/posts/wKTwdgZDo479EhmJL/the-alignment-project-by-uk-aisi-1,,2025-08-01T09:52:37.314000+00:00,29,0,,,
8buEtNxCScYpjzgW8,We won’t get AIs smart enough to solve alignment but too dumb to rebel,https://www.lesswrong.com/posts/8buEtNxCScYpjzgW8/we-won-t-get-ais-smart-enough-to-solve-alignment-but-too,https://www.lesswrong.com/posts/8buEtNxCScYpjzgW8/we-won-t-get-ais-smart-enough-to-solve-alignment-but-too,,2025-10-06T21:49:05.595000+00:00,28,16,,,
De5eNbSpmmSwhuivW,Is theory good or bad for AI safety?,https://www.lesswrong.com/posts/De5eNbSpmmSwhuivW/is-theory-good-or-bad-for-ai-safety,https://www.lesswrong.com/posts/De5eNbSpmmSwhuivW/is-theory-good-or-bad-for-ai-safety,,2025-01-19T10:32:08.772000+00:00,28,1,,,
ZwXspKtgbXLGKFx4B,How we spent our first two weeks as an independent AI safety research group,https://www.lesswrong.com/posts/ZwXspKtgbXLGKFx4B/how-we-spent-our-first-two-weeks-as-an-independent-ai-safety,https://www.lesswrong.com/posts/ZwXspKtgbXLGKFx4B/how-we-spent-our-first-two-weeks-as-an-independent-ai-safety,,2025-08-11T19:32:39.371000+00:00,28,0,,,
y6rBarAPTLmuhn9PJ,Takeaways from sketching a control safety case,https://www.lesswrong.com/posts/y6rBarAPTLmuhn9PJ/takeaways-from-sketching-a-control-safety-case,https://www.lesswrong.com/posts/y6rBarAPTLmuhn9PJ/takeaways-from-sketching-a-control-safety-case,,2025-01-31T04:43:45.917000+00:00,28,0,,,
TQbptN7F4ijPnQRLy,Video and transcript of talk on automating alignment research,https://www.lesswrong.com/posts/TQbptN7F4ijPnQRLy/video-and-transcript-of-talk-on-automating-alignment,https://www.lesswrong.com/posts/TQbptN7F4ijPnQRLy/video-and-transcript-of-talk-on-automating-alignment,,2025-04-30T17:43:06.557000+00:00,27,0,,,
9htmQx5wiePqTtZuL,Deceptive Alignment and Homuncularity,https://www.lesswrong.com/posts/9htmQx5wiePqTtZuL/deceptive-alignment-and-homuncularity,https://www.lesswrong.com/posts/9htmQx5wiePqTtZuL/deceptive-alignment-and-homuncularity,,2025-01-16T13:55:19.161000+00:00,26,12,,,
kiCZzHDkRtupek2mc,My Failed AI Safety Research Projects (Q1/Q2 2025),https://www.lesswrong.com/posts/kiCZzHDkRtupek2mc/my-failed-ai-safety-research-projects-q1-q2-2025,https://www.lesswrong.com/posts/kiCZzHDkRtupek2mc/my-failed-ai-safety-research-projects-q1-q2-2025,,2025-06-19T03:55:40.363000+00:00,26,3,,,
46kxzHbTMXBhrSgZL,Will Non-Dual Crap Cause Emergent Misalignment?,https://www.lesswrong.com/posts/46kxzHbTMXBhrSgZL/will-non-dual-crap-cause-emergent-misalignment,https://www.lesswrong.com/posts/46kxzHbTMXBhrSgZL/will-non-dual-crap-cause-emergent-misalignment,,2025-09-02T00:12:09.867000+00:00,26,2,,,
EFohZdFPGj2iphnvB,Do Self-Perceived Superintelligent LLMs Exhibit Misalignment?,https://www.lesswrong.com/posts/EFohZdFPGj2iphnvB/do-self-perceived-superintelligent-llms-exhibit-misalignment,https://www.lesswrong.com/posts/EFohZdFPGj2iphnvB/do-self-perceived-superintelligent-llms-exhibit-misalignment,,2025-06-29T11:06:42.799000+00:00,26,2,,,
TFToqpaKMhcjAEY5E,AXRP Episode 40 - Jason Gross on Compact Proofs and Interpretability,https://www.lesswrong.com/posts/TFToqpaKMhcjAEY5E/axrp-episode-40-jason-gross-on-compact-proofs-and,https://www.lesswrong.com/posts/TFToqpaKMhcjAEY5E/axrp-episode-40-jason-gross-on-compact-proofs-and,,2025-03-28T18:40:01.856000+00:00,26,0,,,
tgHps2cxiGDkNxNZN,Finding Emergent Misalignment,https://www.lesswrong.com/posts/tgHps2cxiGDkNxNZN/finding-emergent-misalignment,https://www.lesswrong.com/posts/tgHps2cxiGDkNxNZN/finding-emergent-misalignment,,2025-03-26T17:33:46.792000+00:00,26,0,,,
iJzDm6h5a2CK9etYZ,A Conservative Vision For AI Alignment,https://www.lesswrong.com/posts/iJzDm6h5a2CK9etYZ/a-conservative-vision-for-ai-alignment,https://www.lesswrong.com/posts/iJzDm6h5a2CK9etYZ/a-conservative-vision-for-ai-alignment,,2025-08-21T18:14:11.139000+00:00,25,34,,,
524pFXTPD8iDWmX4x,Technical Acceleration Methods for AI Safety: Summary from October 2025 Symposium,https://www.lesswrong.com/posts/524pFXTPD8iDWmX4x/technical-acceleration-methods-for-ai-safety-summary-from,https://www.lesswrong.com/posts/524pFXTPD8iDWmX4x/technical-acceleration-methods-for-ai-safety-summary-from,,2025-10-22T21:33:02.358000+00:00,25,2,,,
b8vhTpQiQsqbmi3tx,"Profanity causes emergent misalignment, but with qualitatively different results than insecure code",https://www.lesswrong.com/posts/b8vhTpQiQsqbmi3tx/profanity-causes-emergent-misalignment-but-with,https://www.lesswrong.com/posts/b8vhTpQiQsqbmi3tx/profanity-causes-emergent-misalignment-but-with,,2025-08-28T08:22:08.111000+00:00,25,2,,,
rGcg4XDPDzBFuqNJz,Research Areas in AI Control (The Alignment Project by UK AISI),https://www.lesswrong.com/posts/rGcg4XDPDzBFuqNJz/research-areas-in-ai-control-the-alignment-project-by-uk,https://www.lesswrong.com/posts/rGcg4XDPDzBFuqNJz/research-areas-in-ai-control-the-alignment-project-by-uk,,2025-08-01T10:27:09.500000+00:00,25,0,,,
g3RXozhPmcLm2yDps,"Constraining Minds, Not Goals: A Structural Approach to AI Alignment",https://www.lesswrong.com/posts/g3RXozhPmcLm2yDps/constraining-minds-not-goals-a-structural-approach-to-ai,https://www.lesswrong.com/posts/g3RXozhPmcLm2yDps/constraining-minds-not-goals-a-structural-approach-to-ai,,2025-06-13T21:06:40.984000+00:00,25,0,,,
e3CpMJrZQjbXeqA6C,Examples of self-fulfilling prophecies in AI alignment?,https://www.lesswrong.com/posts/e3CpMJrZQjbXeqA6C/examples-of-self-fulfilling-prophecies-in-ai-alignment,https://www.lesswrong.com/posts/e3CpMJrZQjbXeqA6C/examples-of-self-fulfilling-prophecies-in-ai-alignment,,2025-03-03T02:45:51.619000+00:00,24,9,,,
XfGN9K4fmr2oLYed6,Interpretability through two lenses: biology and physics,https://www.lesswrong.com/posts/XfGN9K4fmr2oLYed6/interpretability-through-two-lenses-biology-and-physics,https://www.lesswrong.com/posts/XfGN9K4fmr2oLYed6/interpretability-through-two-lenses-biology-and-physics,,2025-08-12T20:25:15.590000+00:00,24,4,,,
x85YnN8kzmpdjmGWg,14+ AI Safety Advisors You Can Speak to – New AISafety.com Resource,https://www.lesswrong.com/posts/x85YnN8kzmpdjmGWg/14-ai-safety-advisors-you-can-speak-to-new-aisafety-com,https://www.lesswrong.com/posts/x85YnN8kzmpdjmGWg/14-ai-safety-advisors-you-can-speak-to-new-aisafety-com,,2025-01-21T17:34:02.170000+00:00,24,0,,,
L7j4JkeWMeBsweq5b,Who is marketing AI alignment?,https://www.lesswrong.com/posts/L7j4JkeWMeBsweq5b/who-is-marketing-ai-alignment,https://www.lesswrong.com/posts/L7j4JkeWMeBsweq5b/who-is-marketing-ai-alignment,,2025-01-19T21:37:30.477000+00:00,23,4,,,
RoGdEq6Cz8yWyX4kp,What is a circuit? [in interpretability],https://www.lesswrong.com/posts/RoGdEq6Cz8yWyX4kp/what-is-a-circuit-in-interpretability,https://www.lesswrong.com/posts/RoGdEq6Cz8yWyX4kp/what-is-a-circuit-in-interpretability,,2025-02-14T04:40:42.978000+00:00,23,1,,,
qRnupMmFG7dxQTTYh,The Strange Science of Interpretability: Recent Papers and a Reading List for the Philosophy of Interpretability,https://www.lesswrong.com/posts/qRnupMmFG7dxQTTYh/the-strange-science-of-interpretability-recent-papers-and-a,https://www.lesswrong.com/posts/qRnupMmFG7dxQTTYh/the-strange-science-of-interpretability-recent-papers-and-a,,2025-08-17T23:38:23.301000+00:00,23,0,,,
HmdprC38DbjDnNmgt,Improving Our Safety Cases Using Upper and Lower Bounds,https://www.lesswrong.com/posts/HmdprC38DbjDnNmgt/improving-our-safety-cases-using-upper-and-lower-bounds,https://www.lesswrong.com/posts/HmdprC38DbjDnNmgt/improving-our-safety-cases-using-upper-and-lower-bounds,,2025-01-16T00:01:49.043000+00:00,23,0,,,
a7QEmdiqdi37PetjY,Why haven't we auto-translated all AI alignment content?,https://www.lesswrong.com/posts/a7QEmdiqdi37PetjY/why-haven-t-we-auto-translated-all-ai-alignment-content,https://www.lesswrong.com/posts/a7QEmdiqdi37PetjY/why-haven-t-we-auto-translated-all-ai-alignment-content,,2025-07-16T15:33:30.950000+00:00,22,10,,,
4rGJ75ZSym9uWfoaB,Can we ever ensure AI alignment if we can only test AI personas?,https://www.lesswrong.com/posts/4rGJ75ZSym9uWfoaB/can-we-ever-ensure-ai-alignment-if-we-can-only-test-ai,https://www.lesswrong.com/posts/4rGJ75ZSym9uWfoaB/can-we-ever-ensure-ai-alignment-if-we-can-only-test-ai,,2025-03-16T08:06:42.345000+00:00,22,8,,,
rLd7NWNKnRFdnJEgD,Intent alignment seems incoherent,https://www.lesswrong.com/posts/rLd7NWNKnRFdnJEgD/intent-alignment-seems-incoherent,https://www.lesswrong.com/posts/rLd7NWNKnRFdnJEgD/intent-alignment-seems-incoherent,,2025-10-07T23:01:49.160000+00:00,22,2,,,
dT7mvHzuX46vydt9K,Avoiding AI Deception: Lie Detectors can either Induce Honesty or Evasion,https://www.lesswrong.com/posts/dT7mvHzuX46vydt9K/avoiding-ai-deception-lie-detectors-can-either-induce,https://www.lesswrong.com/posts/dT7mvHzuX46vydt9K/avoiding-ai-deception-lie-detectors-can-either-induce,,2025-06-05T23:07:59.889000+00:00,22,2,,,
NPBjELgHFEeHTgDrK,Is weak-to-strong generalization an alignment technique?,https://www.lesswrong.com/posts/NPBjELgHFEeHTgDrK/is-weak-to-strong-generalization-an-alignment-technique,https://www.lesswrong.com/posts/NPBjELgHFEeHTgDrK/is-weak-to-strong-generalization-an-alignment-technique,,2025-01-31T07:13:03.332000+00:00,22,1,,,
87iDbBM3Qaf4q4gN4,The Future of Interpretability is Geometric,https://www.lesswrong.com/posts/87iDbBM3Qaf4q4gN4/the-future-of-interpretability-is-geometric,https://www.lesswrong.com/posts/87iDbBM3Qaf4q4gN4/the-future-of-interpretability-is-geometric,,2025-10-24T18:32:25.390000+00:00,22,0,,,
qFKH5jhfKpcreCTkt,Feedback request: Is the time right for an AI Safety stack exchange?,https://www.lesswrong.com/posts/qFKH5jhfKpcreCTkt/feedback-request-is-the-time-right-for-an-ai-safety-stack,https://www.lesswrong.com/posts/qFKH5jhfKpcreCTkt/feedback-request-is-the-time-right-for-an-ai-safety-stack,,2025-09-26T09:14:05.422000+00:00,22,0,,,
CwdCYmsutwXwnYtEF,[Paper] Safety by Measurement: A Systematic Literature Review of AI Safety Evaluation Methods,https://www.lesswrong.com/posts/CwdCYmsutwXwnYtEF/paper-safety-by-measurement-a-systematic-literature-review,https://www.lesswrong.com/posts/CwdCYmsutwXwnYtEF/paper-safety-by-measurement-a-systematic-literature-review,,2025-05-19T10:38:22.570000+00:00,22,0,,,
wkGmouy7JnTNtWAbc,Opportunity Space: Renormalization for AI Safety ,https://www.lesswrong.com/posts/wkGmouy7JnTNtWAbc/opportunity-space-renormalization-for-ai-safety,https://www.lesswrong.com/posts/wkGmouy7JnTNtWAbc/opportunity-space-renormalization-for-ai-safety,,2025-03-31T20:55:52.155000+00:00,22,0,,,
A9SLAx8XFp2gCNJCJ,Moral Alignment: An Idea I'm Embarrassed I Didn't Think of Myself,https://www.lesswrong.com/posts/A9SLAx8XFp2gCNJCJ/moral-alignment-an-idea-i-m-embarrassed-i-didn-t-think-of,https://www.lesswrong.com/posts/A9SLAx8XFp2gCNJCJ/moral-alignment-an-idea-i-m-embarrassed-i-didn-t-think-of,,2025-06-18T15:42:33.491000+00:00,21,54,,,
TGQrqqptazFeRfHBR,Will we survive if AI solves engineering before deception?,https://www.lesswrong.com/posts/TGQrqqptazFeRfHBR/will-we-survive-if-ai-solves-engineering-before-deception,https://www.lesswrong.com/posts/TGQrqqptazFeRfHBR/will-we-survive-if-ai-solves-engineering-before-deception,,2025-05-17T19:22:57.925000+00:00,21,13,,,
ffKE9ohuuSFZDepeZ,Scaling AI Safety in Europe: From Local Groups to International Coordination,https://www.lesswrong.com/posts/ffKE9ohuuSFZDepeZ/scaling-ai-safety-in-europe-from-local-groups-to,https://www.lesswrong.com/posts/ffKE9ohuuSFZDepeZ/scaling-ai-safety-in-europe-from-local-groups-to,,2025-09-02T23:36:38.859000+00:00,21,3,,,
HFcriD29cw3E5QLCR,Selective regularization for alignment-focused representation engineering,https://www.lesswrong.com/posts/HFcriD29cw3E5QLCR/selective-regularization-for-alignment-focused,https://www.lesswrong.com/posts/HFcriD29cw3E5QLCR/selective-regularization-for-alignment-focused,,2025-05-20T12:54:09.111000+00:00,21,3,,,
wgENfqD8HgADgq4rv,Why “Solving Alignment” Is Likely a Category Mistake,https://www.lesswrong.com/posts/wgENfqD8HgADgq4rv/why-solving-alignment-is-likely-a-category-mistake,https://www.lesswrong.com/posts/wgENfqD8HgADgq4rv/why-solving-alignment-is-likely-a-category-mistake,,2025-05-05T04:26:50.825000+00:00,21,3,,,
TxPeQ85yxpcdfq2wg,Metacrisis as a Framework for AI Governance,https://www.lesswrong.com/posts/TxPeQ85yxpcdfq2wg/metacrisis-as-a-framework-for-ai-governance,https://www.lesswrong.com/posts/TxPeQ85yxpcdfq2wg/metacrisis-as-a-framework-for-ai-governance,,2025-09-21T21:30:59.395000+00:00,21,1,,,
6aXe9nipTgwK5LxaP,Do safety-relevant LLM steering vectors optimized on a single example generalize?,https://www.lesswrong.com/posts/6aXe9nipTgwK5LxaP/do-safety-relevant-llm-steering-vectors-optimized-on-a,https://www.lesswrong.com/posts/6aXe9nipTgwK5LxaP/do-safety-relevant-llm-steering-vectors-optimized-on-a,,2025-02-28T12:01:12.514000+00:00,21,1,,,
C6ETL8KSKwsgB4KcR,Output and CoE Monitoring of Customer Service Representatives Shows Default Alignment,https://www.lesswrong.com/posts/C6ETL8KSKwsgB4KcR/output-and-coe-monitoring-of-customer-service,https://www.lesswrong.com/posts/C6ETL8KSKwsgB4KcR/output-and-coe-monitoring-of-customer-service,,2025-08-09T21:31:16.295000+00:00,21,0,,,
mk3qkvBv8ciFeXGdL,Definition of alignment science I like,https://www.lesswrong.com/posts/mk3qkvBv8ciFeXGdL/definition-of-alignment-science-i-like,https://www.lesswrong.com/posts/mk3qkvBv8ciFeXGdL/definition-of-alignment-science-i-like,,2025-01-06T20:40:38.187000+00:00,21,0,,,
gXyMCnjrMfBbnYyZ4,How far along Metr's law can AI start automating or helping with alignment research?,https://www.lesswrong.com/posts/gXyMCnjrMfBbnYyZ4/how-far-along-metr-s-law-can-ai-start-automating-or-helping,https://www.lesswrong.com/posts/gXyMCnjrMfBbnYyZ4/how-far-along-metr-s-law-can-ai-start-automating-or-helping,,2025-03-20T15:58:08.369000+00:00,20,21,,,
55CBrLrXiQgHBb2gF,Alignment Faking Demo for Congressional Staffers,https://www.lesswrong.com/posts/55CBrLrXiQgHBb2gF/alignment-faking-demo-for-congressional-staffers,https://www.lesswrong.com/posts/55CBrLrXiQgHBb2gF/alignment-faking-demo-for-congressional-staffers,,2025-10-06T01:44:03.594000+00:00,20,2,,,
gbJJpm92jtxiD9zag,Inverse Scaling in Test-Time Compute,https://www.lesswrong.com/posts/gbJJpm92jtxiD9zag/inverse-scaling-in-test-time-compute-2,https://www.lesswrong.com/posts/gbJJpm92jtxiD9zag/inverse-scaling-in-test-time-compute-2,,2025-07-22T22:06:30.168000+00:00,20,2,,,
wneX9x8ZKnnAenjfL,Why Eliminating Deception Won’t Align AI,https://www.lesswrong.com/posts/wneX9x8ZKnnAenjfL/why-eliminating-deception-won-t-align-ai,https://www.lesswrong.com/posts/wneX9x8ZKnnAenjfL/why-eliminating-deception-won-t-align-ai,,2025-07-15T09:21:18.695000+00:00,19,6,,,
ykTBKmvZJHssYo4kd,AI Governance Strategy Builder: A Browser Game,https://www.lesswrong.com/posts/ykTBKmvZJHssYo4kd/ai-governance-strategy-builder-a-browser-game,https://www.lesswrong.com/posts/ykTBKmvZJHssYo4kd/ai-governance-strategy-builder-a-browser-game,,2025-09-13T23:30:36.496000+00:00,19,3,,,
bqWihHtDnDseyfF2T,Edge Cases in AI Alignment,https://www.lesswrong.com/posts/bqWihHtDnDseyfF2T/edge-cases-in-ai-alignment-2,https://www.lesswrong.com/posts/bqWihHtDnDseyfF2T/edge-cases-in-ai-alignment-2,,2025-03-24T09:27:58.164000+00:00,19,3,,,
ntLxPrHvCShDnAPrH,Creating a Standard for TAI Governance ,https://www.lesswrong.com/posts/ntLxPrHvCShDnAPrH/creating-a-standard-for-tai-governance,https://www.lesswrong.com/posts/ntLxPrHvCShDnAPrH/creating-a-standard-for-tai-governance,,2025-09-11T13:23:21.646000+00:00,19,2,,,
JNL2bmDXmaG7YnRbF,MAISU - Minimal AI Safety Unconference ,https://www.lesswrong.com/events/JNL2bmDXmaG7YnRbF/maisu-minimal-ai-safety-unconference,https://www.lesswrong.com/events/JNL2bmDXmaG7YnRbF/maisu-minimal-ai-safety-unconference,,2025-02-21T11:36:25.202000+00:00,19,2,,,
nssvbGtDN8pwsCEKh,Offer: Team Conflict Counseling for AI Safety Orgs,https://www.lesswrong.com/posts/nssvbGtDN8pwsCEKh/offer-team-conflict-counseling-for-ai-safety-orgs,https://www.lesswrong.com/posts/nssvbGtDN8pwsCEKh/offer-team-conflict-counseling-for-ai-safety-orgs,,2025-04-14T15:17:00.835000+00:00,19,1,,,
nuDJNyG5XLQjtvaeg,Is alignment reducible to becoming more coherent?,https://www.lesswrong.com/posts/nuDJNyG5XLQjtvaeg/is-alignment-reducible-to-becoming-more-coherent,https://www.lesswrong.com/posts/nuDJNyG5XLQjtvaeg/is-alignment-reducible-to-becoming-more-coherent,,2025-04-22T23:47:34.531000+00:00,19,0,,,
orCtTgQkWwwD3XN87,Misspecification in Inverse Reinforcement Learning,https://www.lesswrong.com/posts/orCtTgQkWwwD3XN87/misspecification-in-inverse-reinforcement-learning,https://www.lesswrong.com/posts/orCtTgQkWwwD3XN87/misspecification-in-inverse-reinforcement-learning,,2025-02-28T19:24:49.204000+00:00,19,0,,,
zavyum4dxEAqs6wHt,Undergrad AI Safety Conference,https://www.lesswrong.com/posts/zavyum4dxEAqs6wHt/undergrad-ai-safety-conference,https://www.lesswrong.com/posts/zavyum4dxEAqs6wHt/undergrad-ai-safety-conference,,2025-02-19T03:43:47.969000+00:00,19,0,,,
4sfW4xKwfhvxRzvAX,AI safety content you could create,https://www.lesswrong.com/posts/4sfW4xKwfhvxRzvAX/ai-safety-content-you-could-create,https://www.lesswrong.com/posts/4sfW4xKwfhvxRzvAX/ai-safety-content-you-could-create,,2025-01-06T15:35:56.167000+00:00,19,0,,,
9jhrWnxYkoZPxMZMj,"Women Want Safety, Men Want Respect",https://www.lesswrong.com/posts/9jhrWnxYkoZPxMZMj/women-want-safety-men-want-respect,https://www.lesswrong.com/posts/9jhrWnxYkoZPxMZMj/women-want-safety-men-want-respect,,2025-07-23T19:10:01.886000+00:00,18,31,,,
sDuWXb8cPXZ2yHdH4,"Alignment first, intelligence later",https://www.lesswrong.com/posts/sDuWXb8cPXZ2yHdH4/alignment-first-intelligence-later,https://www.lesswrong.com/posts/sDuWXb8cPXZ2yHdH4/alignment-first-intelligence-later,,2025-03-30T22:26:55.302000+00:00,18,5,,,
7rM4BKvbk82C3FgAF,Building AI safety benchmark environments on themes of universal human values,https://www.lesswrong.com/posts/7rM4BKvbk82C3FgAF/building-ai-safety-benchmark-environments-on-themes-of,https://www.lesswrong.com/posts/7rM4BKvbk82C3FgAF/building-ai-safety-benchmark-environments-on-themes-of,,2025-01-03T04:24:36.186000+00:00,18,3,,,
FWs6dNq4AddtDfpGe,Category-Theoretic Wanderings into Interpretability,https://www.lesswrong.com/posts/FWs6dNq4AddtDfpGe/category-theoretic-wanderings-into-interpretability,https://www.lesswrong.com/posts/FWs6dNq4AddtDfpGe/category-theoretic-wanderings-into-interpretability,,2025-09-02T00:03:32.363000+00:00,18,2,,,
CpftMXCEnwqbWreHD,Safety cases for Pessimism,https://www.lesswrong.com/posts/CpftMXCEnwqbWreHD/safety-cases-for-pessimism,https://www.lesswrong.com/posts/CpftMXCEnwqbWreHD/safety-cases-for-pessimism,,2025-09-08T13:26:58.295000+00:00,18,1,,,
9rqMPLdpctxig2iAg,The Week in AI Governance,https://www.lesswrong.com/posts/9rqMPLdpctxig2iAg/the-week-in-ai-governance,https://www.lesswrong.com/posts/9rqMPLdpctxig2iAg/the-week-in-ai-governance,,2025-08-01T12:20:06.672000+00:00,18,1,,,
9i6fHMn2vTqyzAi9o,When does Claude sabotage code? An Agentic Misalignment follow-up,https://www.lesswrong.com/posts/9i6fHMn2vTqyzAi9o/when-does-claude-sabotage-code-an-agentic-misalignment,https://www.lesswrong.com/posts/9i6fHMn2vTqyzAi9o/when-does-claude-sabotage-code-an-agentic-misalignment,,2025-11-09T23:11:43.916000+00:00,18,0,,,
3sjtEXzbwDpyALR4H,AI Safety Camp 10 Outputs,https://www.lesswrong.com/posts/3sjtEXzbwDpyALR4H/ai-safety-camp-10-outputs,https://www.lesswrong.com/posts/3sjtEXzbwDpyALR4H/ai-safety-camp-10-outputs,,2025-09-05T08:27:34.179000+00:00,18,0,,,
evo7ou4qaA5PHGXAQ,AI Safety course intro blog,https://www.lesswrong.com/posts/evo7ou4qaA5PHGXAQ/ai-safety-course-intro-blog,https://www.lesswrong.com/posts/evo7ou4qaA5PHGXAQ/ai-safety-course-intro-blog,,2025-07-21T02:35:55.167000+00:00,18,0,,,
rzCF2T7iLPEgNa59Y,Rational Animations' video about scalable oversight and sandwiching,https://www.lesswrong.com/posts/rzCF2T7iLPEgNa59Y/rational-animations-video-about-scalable-oversight-and,https://www.lesswrong.com/posts/rzCF2T7iLPEgNa59Y/rational-animations-video-about-scalable-oversight-and,,2025-07-06T14:00:41.186000+00:00,18,0,,,
tTyQvHedrjFWjurGk,Schmidt Sciences Technical AI Safety RFP on Inference-Time Compute – Deadline: April 30,https://www.lesswrong.com/posts/tTyQvHedrjFWjurGk/schmidt-sciences-technical-ai-safety-rfp-on-inference-time,https://www.lesswrong.com/posts/tTyQvHedrjFWjurGk/schmidt-sciences-technical-ai-safety-rfp-on-inference-time,,2025-03-18T18:05:34.757000+00:00,18,0,,,
GwZvpYR7Hv2smv8By,Share AI Safety Ideas: Both Crazy and Not,https://www.lesswrong.com/posts/GwZvpYR7Hv2smv8By/share-ai-safety-ideas-both-crazy-and-not,https://www.lesswrong.com/posts/GwZvpYR7Hv2smv8By/share-ai-safety-ideas-both-crazy-and-not,,2025-03-01T19:08:25.605000+00:00,17,28,,,
TxiB6hvnQqxXB5XDJ,Why Future AIs will Require New Alignment Methods,https://www.lesswrong.com/posts/TxiB6hvnQqxXB5XDJ/why-future-ais-will-require-new-alignment-methods,https://www.lesswrong.com/posts/TxiB6hvnQqxXB5XDJ/why-future-ais-will-require-new-alignment-methods,,2025-10-10T14:27:44.233000+00:00,17,7,,,
tQzeafo9HjCeXn7ZF,AI companies should be safety-testing the most capable versions of their models,https://www.lesswrong.com/posts/tQzeafo9HjCeXn7ZF/ai-companies-should-be-safety-testing-the-most-capable,https://www.lesswrong.com/posts/tQzeafo9HjCeXn7ZF/ai-companies-should-be-safety-testing-the-most-capable,,2025-03-26T19:03:41.790000+00:00,17,6,,,
brBATybmh2eEZSwdg,How useful would alien alignment research be? ,https://www.lesswrong.com/posts/brBATybmh2eEZSwdg/how-useful-would-alien-alignment-research-be,https://www.lesswrong.com/posts/brBATybmh2eEZSwdg/how-useful-would-alien-alignment-research-be,,2025-01-23T10:59:22.330000+00:00,17,5,,,
iS4g58qQEJzjMzYZJ,What AI safety plans are there?,https://www.lesswrong.com/posts/iS4g58qQEJzjMzYZJ/what-ai-safety-plans-are-there,https://www.lesswrong.com/posts/iS4g58qQEJzjMzYZJ/what-ai-safety-plans-are-there,,2025-04-23T22:58:06.885000+00:00,17,3,,,
pxn5C6Lq2FqMGJGDz,An argument for discussing AI safety in person being underused,https://www.lesswrong.com/posts/pxn5C6Lq2FqMGJGDz/an-argument-for-discussing-ai-safety-in-person-being,https://www.lesswrong.com/posts/pxn5C6Lq2FqMGJGDz/an-argument-for-discussing-ai-safety-in-person-being,,2025-09-24T11:36:19.321000+00:00,17,1,,,
Jo6LPyp7t3rPuf8Ao,Black-box interpretability methodology blueprint: Probing runaway optimisation in LLMs,https://www.lesswrong.com/posts/Jo6LPyp7t3rPuf8Ao/black-box-interpretability-methodology-blueprint-probing,https://www.lesswrong.com/posts/Jo6LPyp7t3rPuf8Ao/black-box-interpretability-methodology-blueprint-probing,,2025-06-22T18:16:19.078000+00:00,17,0,,,
B2o6nrxwKxLPsSYdh,Do LLMs Comply Differently During Tests? Is This a Hidden Variable in Safety Evaluation? And Can We Steer That?,https://www.lesswrong.com/posts/B2o6nrxwKxLPsSYdh/do-llms-comply-differently-during-tests-is-this-a-hidden,https://www.lesswrong.com/posts/B2o6nrxwKxLPsSYdh/do-llms-comply-differently-during-tests-is-this-a-hidden,,2025-06-16T13:52:03.008000+00:00,17,0,,,
3JeE2dXSKHue2hhZj,Interested in working from a new Boston AI Safety Hub? ,https://www.lesswrong.com/posts/3JeE2dXSKHue2hhZj/interested-in-working-from-a-new-boston-ai-safety-hub,https://www.lesswrong.com/posts/3JeE2dXSKHue2hhZj/interested-in-working-from-a-new-boston-ai-safety-hub,,2025-03-17T13:42:19.509000+00:00,17,0,,,
bTzk32t9aWJwLuNhi,Workshop: Interpretability in LLMs using Geometric and Statistical Methods,https://www.lesswrong.com/posts/bTzk32t9aWJwLuNhi/workshop-interpretability-in-llms-using-geometric-and,https://www.lesswrong.com/posts/bTzk32t9aWJwLuNhi/workshop-interpretability-in-llms-using-geometric-and,,2025-02-22T09:39:26.446000+00:00,17,0,,,
jrkrHyrymv95CX5NC,Alignment Can Reduce Performance on Simple Ethical Questions,https://www.lesswrong.com/posts/jrkrHyrymv95CX5NC/alignment-can-reduce-performance-on-simple-ethical-questions,https://www.lesswrong.com/posts/jrkrHyrymv95CX5NC/alignment-can-reduce-performance-on-simple-ethical-questions,,2025-02-03T19:35:42.895000+00:00,16,7,,,
s9z4mgjtWTPpDLxFy,Agentic Interpretability: A Strategy Against Gradual Disempowerment,https://www.lesswrong.com/posts/s9z4mgjtWTPpDLxFy/agentic-interpretability-a-strategy-against-gradual,https://www.lesswrong.com/posts/s9z4mgjtWTPpDLxFy/agentic-interpretability-a-strategy-against-gradual,,2025-06-17T14:52:55.695000+00:00,16,6,,,
xCzKwWmhcEKkeytys,What can we learn from parent-child-alignment for AI?,https://www.lesswrong.com/posts/xCzKwWmhcEKkeytys/what-can-we-learn-from-parent-child-alignment-for-ai,https://www.lesswrong.com/posts/xCzKwWmhcEKkeytys/what-can-we-learn-from-parent-child-alignment-for-ai,,2025-10-29T08:02:56.220000+00:00,16,4,,,
ySojRcfMddtyFCYEe,How Can Average People Contribute to AI Safety?,https://www.lesswrong.com/posts/ySojRcfMddtyFCYEe/how-can-average-people-contribute-to-ai-safety,https://www.lesswrong.com/posts/ySojRcfMddtyFCYEe/how-can-average-people-contribute-to-ai-safety,,2025-03-06T22:50:12.288000+00:00,16,4,,,
6oF6pRr2FgjTmiHus,Topological Data Analysis and Mechanistic Interpretability,https://www.lesswrong.com/posts/6oF6pRr2FgjTmiHus/topological-data-analysis-and-mechanistic-interpretability,https://www.lesswrong.com/posts/6oF6pRr2FgjTmiHus/topological-data-analysis-and-mechanistic-interpretability,,2025-02-24T19:56:02.498000+00:00,16,4,,,
ysHERGduadwJFkKhS,Lessons from organizing a technical AI safety bootcamp,https://www.lesswrong.com/posts/ysHERGduadwJFkKhS/lessons-from-organizing-a-technical-ai-safety-bootcamp,https://www.lesswrong.com/posts/ysHERGduadwJFkKhS/lessons-from-organizing-a-technical-ai-safety-bootcamp,,2025-09-28T13:48:13.474000+00:00,16,3,,,
