title,link,karma,date,keep
Eliciting secret knowledge from language models,https://www.alignmentforum.org/posts/Mv3yg7wMXfns3NPaz/eliciting-secret-knowledge-from-language-models-1,20,2025-10-02T20:57:13.496Z,y
Lectures on statistical learning theory for alignment researchers,https://www.alignmentforum.org/posts/yAwnYoeCz7PqeNrtL/lectures-on-statistical-learning-theory-for-alignment,15,2025-10-01T08:36:52.525Z,
AI Induced Psychosis: A shallow investigation,https://www.alignmentforum.org/posts/iGF7YcnQkEbwvYLPA/ai-induced-psychosis-a-shallow-investigation,83,2025-08-26T20:03:53.308Z,
Stress Testing Deliberative Alignment for Anti-Scheming Training,https://www.alignmentforum.org/posts/JmRfgNYCrYogCq7ny/stress-testing-deliberative-alignment-for-anti-scheming,57,2025-09-17T16:59:12.906Z,y
Four ways learning Econ makes people dumber re: future AI,https://www.alignmentforum.org/posts/xJWBofhLQjf3KmRgg/four-ways-learning-econ-makes-people-dumber-re-future-ai,119,2025-08-21T17:52:46.684Z,
Research Agenda: Synthesizing Standalone World-Models,https://www.alignmentforum.org/posts/LngR93YwiEpJ3kiWh/research-agenda-synthesizing-standalone-world-models,35,2025-09-22T19:06:14.592Z,y
"What, if not agency?",https://www.alignmentforum.org/posts/tQ9vWm4b57HFqbaRj/what-if-not-agency,40,2025-09-15T03:14:12.301Z,y
"Focus transparency on risk reports, not safety cases",https://www.alignmentforum.org/posts/KMbZWcTvGjChw9ynD/focus-transparency-on-risk-reports-not-safety-cases,24,2025-09-22T15:27:49.280Z,
"Synthesizing Standalone World-Models, Part 4: Metaphysical Justifications",https://www.alignmentforum.org/posts/gR96ANcNXQgh22qtG/synthesizing-standalone-world-models-part-4-metaphysical,12,2025-09-26T18:00:29.049Z,
"Trust me bro, just one more RL scale up, this one will be the real scale up with the good environments, the actually legit one, trust me bro",https://www.alignmentforum.org/posts/HsLWpZ2zad43nzvWi/trust-me-bro-just-one-more-rl-scale-up-this-one-will-be-the,57,2025-09-03T13:21:35.726Z,
LLM AGI may reason about its goals and discover misalignments by default,https://www.alignmentforum.org/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover,32,2025-09-15T14:58:01.265Z,
Subliminal Learning: LLMs Transmit Behavioral Traits via Hidden Signals in Data,https://www.alignmentforum.org/posts/cGcwQDKAKbQ68BGuR/subliminal-learning-llms-transmit-behavioral-traits-via,105,2025-07-22T16:37:22.475Z,y
Natural Latents: Latent Variables Stable Across Ontologies,https://www.alignmentforum.org/posts/Qdgo2jYAuFRMeMRJT/natural-latents-latent-variables-stable-across-ontologies,50,2025-09-04T00:33:14.621Z,y
From SLT to AIT: NN generalisation out-of-distribution,https://www.alignmentforum.org/posts/2MX2bXreTtntB85Zy/from-slt-to-ait-nn-generalisation-out-of-distribution,34,2025-09-04T15:20:02.296Z,y
Alignment as uploading with more steps,https://www.alignmentforum.org/posts/AzFxTMFfkTt4mhMKt/alignment-as-uploading-with-more-steps,23,2025-09-14T04:08:36.418Z,
How To Become A Mechanistic Interpretability Researcher,https://www.alignmentforum.org/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher,36,2025-09-02T23:38:43.780Z,
Prospects for studying actual schemers,https://www.alignmentforum.org/posts/gALJWSfoTkq2ky5os/prospects-for-studying-actual-schemers,28,2025-09-19T14:11:28.816Z,
My AGI timeline updates from GPT-5 (and 2025 so far),https://www.alignmentforum.org/posts/2ssPfDpdrjaM2rMbn/my-agi-timeline-updates-from-gpt-5-and-2025-so-far-1,56,2025-08-20T16:11:16.656Z,
"Synthesizing Standalone World-Models, Part 1: Abstraction Hierarchies",https://www.alignmentforum.org/posts/xvCNjLL3GZ6w2BWeb/synthesizing-standalone-world-models-part-1-abstraction,11,2025-09-23T17:01:29.964Z,y
Draconian measures can increase the risk of irrevocable catastrophe,https://www.alignmentforum.org/posts/xSxdtEAnum56e8dqH/draconian-measures-can-increase-the-risk-of-irrevocable-1,6,2025-09-23T21:40:31.403Z,
Lessons from Studying Two-Hop Latent Reasoning,https://www.alignmentforum.org/posts/MdKWqFrNstiZQ3G6K/lessons-from-studying-two-hop-latent-reasoning,29,2025-09-11T17:53:58.503Z,y
"Synthesizing Standalone World-Models, Part 2: Shifting Structures",https://www.alignmentforum.org/posts/kNyMwXQxctWtaRZhs/synthesizing-standalone-world-models-part-2-shifting,8,2025-09-24T19:02:14.662Z,
"Synthesizing Standalone World-Models, Part 3: Dataset-Assembly",https://www.alignmentforum.org/posts/eQX93nxapSdxQYdkL/synthesizing-standalone-world-models-part-3-dataset-assembly,7,2025-09-25T19:21:37.730Z,
(∃ Stochastic Natural Latent) Implies (∃ Deterministic Natural Latent),https://www.alignmentforum.org/posts/Gd36HT7qLr684SYuQ/stochastic-natural-latent-implies-deterministic-natural,52,2025-08-22T21:46:37.235Z,
Crisp Supra-Decision Processes,https://www.alignmentforum.org/posts/mt82ZhdEsfh6CNYse/crisp-supra-decision-processes,19,2025-09-17T15:56:40.549Z,
AI 2027: What Superintelligence Looks Like,https://www.alignmentforum.org/posts/TpSFoqoG2M5MAAesg/ai-2027-what-superintelligence-looks-like-1,190,2025-04-03T16:23:44.619Z,y
Attaching requirements to model releases has serious downsides (relative to a different deadline for these requirements),https://www.alignmentforum.org/posts/Eh7WdKTrpLch5Kvkz/attaching-requirements-to-model-releases-has-serious,49,2025-08-27T17:04:23.590Z,
Optimizing The Final Output Can Obfuscate CoT (Research Note),https://www.alignmentforum.org/posts/CM7AsQoBxDW4vhkP3/optimizing-the-final-output-can-obfuscate-cot-research-note,77,2025-07-30T21:26:42.796Z,
the void,https://www.alignmentforum.org/posts/3EzbtNLdcnZe8og8b/the-void-1,102,2025-06-11T03:19:18.538Z,
"If anyone builds it, everyone will plausibly be fine",https://www.alignmentforum.org/posts/pozSWmqLqqc7Z2mQW/if-anyone-builds-it-everyone-will-plausibly-be-fine-2,16,2025-09-18T20:03:33.366Z,
Training a Reward Hacker Despite Perfect Labels,https://www.alignmentforum.org/posts/dbYEoG7jNZbeWX39o/training-a-reward-hacker-despite-perfect-labels,53,2025-08-14T23:57:21.054Z,y
What training data should developers filter to reduce risk from misaligned AI? An initial narrow proposal,https://www.alignmentforum.org/posts/dEiBJDtSmbC8dChwe/what-training-data-should-developers-filter-to-reduce-risk-1,19,2025-09-17T15:30:23.839Z,
Foom & Doom 1: “Brain in a box in a basement”,https://www.alignmentforum.org/posts/yew6zFWAKG4AGs3Wk/foom-and-doom-1-brain-in-a-box-in-a-basement,86,2025-06-23T17:18:54.237Z,y
METR's Evaluation of GPT-5,https://www.alignmentforum.org/posts/SuvWoLaGiNjPDcA7d/metr-s-evaluation-of-gpt-5,53,2025-08-07T22:17:42.947Z,y
AIs will greatly change engineering in AI companies well before AGI,https://www.alignmentforum.org/posts/uRdJio8pnTqHpWa4t/ais-will-greatly-change-engineering-in-ai-companies-well,24,2025-09-09T16:58:33.031Z,
Video and transcript of talk on giving AIs safe motivations,https://www.alignmentforum.org/posts/LQsoCMGDsgZJPDbSG/video-and-transcript-of-talk-on-giving-ais-safe-motivations,7,2025-09-22T16:43:09.032Z,
Beware General Claims about “Generalizable Reasoning Capabilities” (of Modern AI Systems),https://www.alignmentforum.org/posts/5uw26uDdFbFQgKzih/beware-general-claims-about-generalizable-reasoning,106,2025-06-11T19:27:33.648Z,
Narrow Finetuning Leaves Clearly Readable Traces in Activation Differences,https://www.alignmentforum.org/posts/sBSjEBykQkmSfqrwt/narrow-finetuning-leaves-clearly-readable-traces-in,23,2025-09-05T12:11:49.617Z,
Towards Alignment Auditing as a Numbers-Go-Up Science,https://www.alignmentforum.org/posts/bGYQgBPEyHidnZCdE/towards-alignment-auditing-as-a-numbers-go-up-science,59,2025-08-04T22:30:52.101Z,
Do-Divergence: A Bound for Maxwell's Demon,https://www.alignmentforum.org/posts/DHSY697pRWYto6LsF/do-divergence-a-bound-for-maxwell-s-demon,29,2025-08-26T17:07:16.441Z,
Decision Theory Guarding is Sufficient for Scheming,https://www.alignmentforum.org/posts/NccvE4GAhHbFim5Eb/decision-theory-guarding-is-sufficient-for-scheming,26,2025-09-09T14:49:17.388Z,
Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety,https://www.alignmentforum.org/posts/7xneDbsgj6yJDJMjK/chain-of-thought-monitorability-a-new-and-fragile,80,2025-07-15T16:23:17.588Z,y
Hidden Reasoning in LLMs: A Taxonomy,https://www.alignmentforum.org/posts/ZrgFfeWuckpwK5Lyi/hidden-reasoning-in-llms-a-taxonomy,21,2025-08-25T22:43:05.838Z,
Do Not Tile the Lightcone with Your Confused Ontology,https://www.alignmentforum.org/posts/Y8zS8iG5HhqKcQBtA/do-not-tile-the-lightcone-with-your-confused-ontology,63,2025-06-13T12:45:23.325Z,
Large Language Models and the Critical Brain Hypothesis,https://www.alignmentforum.org/posts/Ntdwc5nrPGZMicAWz/large-language-models-and-the-critical-brain-hypothesis-1,15,2025-09-09T15:45:37.805Z,
Interpretability Will Not Reliably Find Deceptive AI,https://www.alignmentforum.org/posts/PwnadG4BFjaER3MGf/interpretability-will-not-reliably-find-deceptive-ai,117,2025-05-04T16:32:29.643Z,y
Distillation Robustifies Unlearning,https://www.alignmentforum.org/posts/anX4QrNjhJqGFvrBr/distillation-robustifies-unlearning,87,2025-06-13T13:45:26.261Z,y
A recurrent CNN finds maze paths by filling dead-ends,https://www.alignmentforum.org/posts/HKvFHbKfjryqXhuuu/a-recurrent-cnn-finds-maze-paths-by-filling-dead-ends,12,2025-09-15T20:49:23.145Z,
Resampling Conserves Redundancy (Approximately),https://www.alignmentforum.org/posts/cpxxfagD92ivLZCp4/resampling-conserves-redundancy-approximately,29,2025-08-21T22:43:20.894Z,
Why Do Some Language Models Fake Alignment While Others Don't?,https://www.alignmentforum.org/posts/ghESoA8mo3fv9Yx3E/why-do-some-language-models-fake-alignment-while-others-don,74,2025-07-08T21:49:30.088Z,y
AI companies have started saying safeguards are load-bearing,https://www.alignmentforum.org/posts/Bz2gPtqRJJDWyKxnX/ai-companies-have-started-saying-safeguards-are-load-bearing,22,2025-08-27T13:00:36.942Z,
Being honest with AIs,https://www.alignmentforum.org/posts/uuikfACQBm4KJZp4w/being-honest-with-ais,31,2025-08-21T03:57:17.392Z,
New Paper on Reflective Oracles & Grain of Truth Problem,https://www.alignmentforum.org/posts/PuGxDb27xhRPBPbiv/new-paper-on-reflective-oracles-and-grain-of-truth-problem,27,2025-08-26T00:18:35.862Z,
Mech interp is not pre-paradigmatic,https://www.alignmentforum.org/posts/beREnXhBnzxbJtr8k/mech-interp-is-not-pre-paradigmatic,78,2025-06-10T13:39:26.457Z,y
AI companies' eval reports mostly don't support their claims,https://www.alignmentforum.org/posts/AK6AihHGjirdoiJg6/ai-companies-eval-reports-mostly-don-t-support-their-claims,80,2025-06-09T13:00:40.942Z,
"Narrow Misalignment is Hard, Emergent Misalignment is Easy",https://www.alignmentforum.org/posts/gLDSqQm8pwNiq7qst/narrow-misalignment-is-hard-emergent-misalignment-is-easy,61,2025-07-14T21:05:57.653Z,y
Notes on cooperating with unaligned AIs,https://www.alignmentforum.org/posts/oLzoHA9ZtF2ygYgx4/notes-on-cooperating-with-unaligned-ais,28,2025-08-24T04:19:21.018Z,
Harmless reward hacks can generalize to misalignment in LLMs,https://www.alignmentforum.org/posts/CwJ2qWveb9JbaCGQ5/harmless-reward-hacks-can-generalize-to-misalignment-in-llms,22,2025-08-26T17:32:58.002Z,
How AI Takeover Might Happen in 2 Years,https://www.alignmentforum.org/posts/KFJ2LFogYqzfGB3uX/how-ai-takeover-might-happen-in-2-years,115,2025-02-07T17:10:10.530Z,
Comparing risk from internally-deployed AI to insider and outsider threats from humans,https://www.alignmentforum.org/posts/DCQ8GfzCqoBzgziew/comparing-risk-from-internally-deployed-ai-to-insider-and,58,2025-06-23T17:47:21.408Z,
Discovering Backdoor Triggers,https://www.alignmentforum.org/posts/kmNqsbgKWJHGqhj4g/discovering-backdoor-triggers,24,2025-08-19T06:24:33.088Z,
Thoughts on Gradual Disempowerment,https://www.alignmentforum.org/posts/ct6SMDuexe9uBwDoL/thoughts-on-gradual-disempowerment,26,2025-08-15T11:56:13.191Z,
MATS 8.0 Research Projects,https://www.alignmentforum.org/posts/3semWY8cZJN3pD66g/mats-8-0-research-projects,8,2025-09-09T01:29:41.189Z,
Foom & Doom 2: Technical alignment is hard,https://www.alignmentforum.org/posts/bnnKGSCHJghAvqPjS/foom-and-doom-2-technical-alignment-is-hard,56,2025-06-23T17:19:50.691Z,y
Tracing the Thoughts of a Large Language Model,https://www.alignmentforum.org/posts/zsr4rWRASxwmgXfmq/tracing-the-thoughts-of-a-large-language-model,104,2025-03-27T17:20:02.162Z,y
"Misalignment classifiers: Why they’re hard to evaluate adversarially, and why we're studying them anyway",https://www.alignmentforum.org/posts/jzHhJJq2cFmisRKB2/misalignment-classifiers-why-they-re-hard-to-evaluate,28,2025-08-15T11:48:31.667Z,y
CoT May Be Highly Informative Despite “Unfaithfulness” [METR],https://www.alignmentforum.org/posts/WAxkA6gDgrschZovx/cot-may-be-highly-informative-despite-unfaithfulness-metr,31,2025-08-11T21:47:35.493Z,y
Good Research Takes are Not Sufficient for Good Strategic Takes,https://www.alignmentforum.org/posts/P5zWiPF5cPJZSkiAK/good-research-takes-are-not-sufficient-for-good-strategic,101,2025-03-22T10:13:38.257Z,
Could one country outgrow the rest of the world?,https://www.alignmentforum.org/posts/x8uzeok9zhHGeCKAq/could-one-country-outgrow-the-rest-of-the-world,23,2025-08-21T15:32:39.524Z,
Towards data-centric interpretability with sparse autoencoders,https://www.alignmentforum.org/posts/a4EDinzAYtRwpNmx9/towards-data-centric-interpretability-with-sparse,17,2025-08-15T20:10:55.825Z,
Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs,https://www.alignmentforum.org/posts/ifechgnJRtJdduFGC/emergent-misalignment-narrow-finetuning-can-produce-broadly,110,2025-02-25T17:39:31.059Z,y
The Industrial Explosion,https://www.alignmentforum.org/posts/Na2CBmNY7otypEmto/the-industrial-explosion,51,2025-06-26T14:41:41.238Z,
Safety cases for Pessimism,https://www.alignmentforum.org/posts/CpftMXCEnwqbWreHD/safety-cases-for-pessimism,10,2025-09-08T13:26:58.295Z,
"Claude, GPT, and Gemini All Struggle to Evade Monitors",https://www.alignmentforum.org/posts/dwEgSEPxpKjz3Fw5k/claude-gpt-and-gemini-all-struggle-to-evade-monitors,24,2025-08-06T20:28:26.671Z,y
Statistical suggestions for mech interp research and beyond,https://www.alignmentforum.org/posts/GxhtzqMwdTHo6326y/statistical-suggestions-for-mech-interp-research-and-beyond,26,2025-08-06T12:45:34.081Z,
Recent Redwood Research project proposals,https://www.alignmentforum.org/posts/RRxhzshdpneyTzKfq/recent-redwood-research-project-proposals,47,2025-07-14T22:27:01.367Z,
Steering Out-of-Distribution Generalization with Concept Ablation Fine-Tuning,https://www.alignmentforum.org/posts/BxeZNpiTvoEqTXndJ/steering-out-of-distribution-generalization-with-concept,37,2025-07-23T14:57:40.199Z,y
Perils of under- vs over-sculpting AGI desires,https://www.alignmentforum.org/posts/grgb2ipxQf2wzNDEG/perils-of-under-vs-over-sculpting-agi-desires,25,2025-08-05T18:13:32.628Z,y
Making deals with early schemers,https://www.alignmentforum.org/posts/psqkwsKrKHCfkhrQx/making-deals-with-early-schemers,54,2025-06-20T18:21:43.288Z,
What We Learned Trying to Diff Base and Chat Models (And Why It Matters),https://www.alignmentforum.org/posts/xmpauEXEerzYcJKNm/what-we-learned-trying-to-diff-base-and-chat-models-and-why,42,2025-06-30T17:17:42.719Z,y
On the functional self of LLMs,https://www.alignmentforum.org/posts/29aWbJARGF4ybAa5d/on-the-functional-self-of-llms,33,2025-07-07T15:39:29.881Z,
METR: Measuring AI Ability to Complete Long Tasks,https://www.alignmentforum.org/posts/deesrjitvXM4xYGZd/metr-measuring-ai-ability-to-complete-long-tasks,77,2025-03-19T16:00:54.874Z,y
Extract-and-Evaluate Monitoring Can Significantly Enhance CoT Monitor Performance (Research Note),https://www.alignmentforum.org/posts/nRcKDYi2KfRTXdvDF/extract-and-evaluate-monitoring-can-significantly-enhance,19,2025-08-08T10:41:13.514Z,
What’s the short timeline plan?,https://www.alignmentforum.org/posts/bb5Tnjdrptu89rcyY/what-s-the-short-timeline-plan,130,2025-01-02T14:59:20.026Z,y
Four places where you can put LLM monitoring,https://www.alignmentforum.org/posts/AmcEyFErJc9TQ5ySF/four-places-where-you-can-put-llm-monitoring,30,2025-08-09T23:10:26.504Z,y
An Introduction to Credal Sets and Infra-Bayes Learnability,https://www.alignmentforum.org/posts/rkhaRnAc6dLzQT2sJ/an-introduction-to-credal-sets-and-infra-bayes-learnability-1,15,2025-08-22T13:03:07.815Z,y
Proof Section to Crisp Supra-Decision Processes,https://www.alignmentforum.org/posts/2HDydzwuEGE4MNsjx/proof-section-to-crisp-supra-decision-processes,4,2025-09-17T15:57:07.064Z,
Model Organisms for Emergent Misalignment,https://www.alignmentforum.org/posts/yHmJrDSJpFaNTZ9Tr/model-organisms-for-emergent-misalignment,55,2025-06-16T15:46:41.739Z,y
How Can You Tell if You've Instilled a False Belief in Your LLM?,https://www.alignmentforum.org/posts/5G46ooS85ihDxtBvm/how-can-you-tell-if-you-ve-instilled-a-false-belief-in-your,8,2025-09-06T16:45:27.218Z,
White Box Control at UK AISI - Update on Sandbagging Investigations,https://www.alignmentforum.org/posts/pPEeMdgjpjHZWCDFw/white-box-control-at-uk-aisi-update-on-sandbagging,41,2025-07-10T13:37:05.487Z,y
Selective Generalization: Improving Capabilities While Maintaining Alignment,https://www.alignmentforum.org/posts/ZXxY2tccLapdjLbKm/selective-generalization-improving-capabilities-while,30,2025-07-16T21:25:39.203Z,
Self-preservation or Instruction Ambiguity? Examining the Causes of Shutdown Resistance,https://www.alignmentforum.org/posts/wnzkjSmrgWZaBa2aC/self-preservation-or-instruction-ambiguity-examining-the,41,2025-07-14T14:52:44.149Z,y
We Built a Tool to Protect Your Dataset From Simple Scrapers,https://www.alignmentforum.org/posts/DA3vbSEfABLdoCt59/we-built-a-tool-to-protect-your-dataset-from-simple-scrapers,29,2025-07-25T05:44:55.006Z,
“Behaviorist” RL reward functions lead to scheming,https://www.alignmentforum.org/posts/FNJF3SoNiwceAQ69W/behaviorist-rl-reward-functions-lead-to-scheming,25,2025-07-23T16:55:18.326Z,y
Sleeping Experts in the (reflective) Solomonoff Prior,https://www.alignmentforum.org/posts/Go2mQBP4AXRw3iNMk/sleeping-experts-in-the-reflective-solomonoff-prior,8,2025-08-31T04:55:48.633Z,
Research Note: Our scheming precursor evals had limited predictive power for our in-context scheming evals,https://www.alignmentforum.org/posts/9tqpPP4FwSnv9AWsi/research-note-our-scheming-precursor-evals-had-limited,43,2025-07-03T15:57:07.779Z,
Short Timelines Don't Devalue Long Horizon Research,https://www.alignmentforum.org/posts/3NdpbA6M5AM2gHvTW/short-timelines-don-t-devalue-long-horizon-research,68,2025-04-09T00:42:07.324Z,
Unfaithful chain-of-thought as nudged reasoning,https://www.alignmentforum.org/posts/vPAFPpRDEg3vjhNFi/unfaithful-chain-of-thought-as-nudged-reasoning,29,2025-07-22T22:35:15.481Z,
"Prover-Estimator Debate: 
A New Scalable Oversight Protocol",https://www.alignmentforum.org/posts/8XHBaugB5S3r27MG9/prover-estimator-debate-a-new-scalable-oversight-protocol,42,2025-06-17T13:53:04.125Z,y
Circuits in Superposition 2: Now with Less Wrong Math,https://www.alignmentforum.org/posts/FWkZYQceEzL84tNej/circuits-in-superposition-2-now-with-less-wrong-math,33,2025-06-30T10:25:01.981Z,
Claude Sonnet 3.7 (often) knows when it’s in alignment evaluations,https://www.alignmentforum.org/posts/E3daBewppAiECN3Ao/claude-sonnet-3-7-often-knows-when-it-s-in-alignment,71,2025-03-17T19:11:00.813Z,
Building and evaluating alignment auditing agents,https://www.alignmentforum.org/posts/DJAZHYjWxMrcd2na3/building-and-evaluating-alignment-auditing-agents,29,2025-07-24T19:22:26.195Z,
Agentic Misalignment: How LLMs Could be Insider Threats,https://www.alignmentforum.org/posts/b8eeCGe3FWzHKbePF/agentic-misalignment-how-llms-could-be-insider-threats-1,34,2025-06-20T22:34:59.515Z,y
OpenAI: Detecting misbehavior in frontier reasoning models,https://www.alignmentforum.org/posts/7wFdXj9oR8M9AiFht/openai-detecting-misbehavior-in-frontier-reasoning-models,73,2025-03-11T02:17:21.026Z,y
Gradual Disempowerment: Concrete Research Projects,https://www.alignmentforum.org/posts/GAv4DRGyDHe2orvwB/gradual-disempowerment-concrete-research-projects,36,2025-05-29T18:55:15.723Z,
“Sharp Left Turn” discourse: An opinionated review,https://www.alignmentforum.org/posts/2yLyT6kB7BQvTfEuZ/sharp-left-turn-discourse-an-opinionated-review,86,2025-01-28T18:47:04.395Z,
Jankily controlling superintelligence,https://www.alignmentforum.org/posts/ainn5APCKHTFxuHKv/jankily-controlling-superintelligence,35,2025-06-27T14:05:53.134Z,
A Simple Explanation of AGI Risk,https://www.alignmentforum.org/posts/W43vm8aD9jf9peAFf/a-simple-explanation-of-agi-risk,32,2025-07-01T16:18:07.914Z,
Why it's hard to make settings for high-stakes control research,https://www.alignmentforum.org/posts/xGaFncekAXEWq8Mrv/why-it-s-hard-to-make-settings-for-high-stakes-control,28,2025-07-18T16:33:18.896Z,
Alignment Faking Revisited: Improved Classifiers and Open Source Extensions,https://www.alignmentforum.org/posts/Fr4QsQT52RFKHvCAH/alignment-faking-revisited-improved-classifiers-and-open,70,2025-04-08T17:32:55.315Z,
"If you can generate obfuscated chain-of-thought, can you monitor it?",https://www.alignmentforum.org/posts/ZEdP6rYirxPxRSfTb/if-you-can-generate-obfuscated-chain-of-thought-can-you,13,2025-08-04T15:46:22.330Z,
Paradigms for computation,https://www.alignmentforum.org/posts/APP8cbeDaqhGjqH8X/paradigms-for-computation,31,2025-06-30T00:37:20.194Z,
On the Rationality of Deterring ASI,https://www.alignmentforum.org/posts/XsYQyBgm8eKjd3Sqw/on-the-rationality-of-deterring-asi,65,2025-03-05T16:11:37.855Z,y
UK AISI’s Alignment Team: Research Agenda,https://www.alignmentforum.org/posts/tbnw7LbNApvxNLAg8/uk-aisi-s-alignment-team-research-agenda,47,2025-05-07T16:33:41.176Z,y
There are two fundamentally different constraints on schemers,https://www.alignmentforum.org/posts/qDWm7E9sfwLDBWfMw/there-are-two-fundamentally-different-constraints-on,39,2025-07-02T15:51:59.332Z,
Will alignment-faking Claude accept a deal to reveal its misalignment?,https://www.alignmentforum.org/posts/7C4KJot4aN8ieEDoz/will-alignment-faking-claude-accept-a-deal-to-reveal-its,81,2025-01-31T16:49:47.316Z,
SLT for AI Safety,https://www.alignmentforum.org/posts/J7CyENFYXPxXQpsnD/slt-for-ai-safety,18,2025-07-01T04:52:21.093Z,y
Optimally Combining Probe Monitors and Black Box Monitors,https://www.alignmentforum.org/posts/FhixwyymPxF8TZX39/optimally-combining-probe-monitors-and-black-box-monitors,11,2025-07-27T19:13:47.732Z,
AI-enabled coups: a small group could use AI to seize power,https://www.alignmentforum.org/posts/6kBMqrK9bREuGsrnd/ai-enabled-coups-a-small-group-could-use-ai-to-seize-power-1,52,2025-04-16T16:51:29.561Z,
"Do LLMs know what they're capable of? Why this matters for AI safety, and initial findings",https://www.alignmentforum.org/posts/9tHEibBBhQCHEyFsa/do-llms-know-what-they-re-capable-of-why-this-matters-for-ai,19,2025-07-13T19:54:53.974Z,
Reducing LLM deception at scale with self-other overlap fine-tuning,https://www.alignmentforum.org/posts/jtqcsARGtmgogdcLT/reducing-llm-deception-at-scale-with-self-other-overlap-fine,32,2025-03-13T19:09:43.620Z,y
How quick and big would a software intelligence explosion be?,https://www.alignmentforum.org/posts/s2yvH4xdcYNheSohZ/how-quick-and-big-would-a-software-intelligence-explosion-be,24,2025-07-22T12:58:35.285Z,
Evaluating and monitoring for AI scheming,https://www.alignmentforum.org/posts/JvYF5kosLeYGvvLpP/evaluating-and-monitoring-for-ai-scheming,34,2025-07-10T14:24:02.008Z,
How much novel security-critical infrastructure do you need during the singularity?,https://www.alignmentforum.org/posts/qKz2hBahahmb4uDty/how-much-novel-security-critical-infrastructure-do-you-need,31,2025-07-04T16:54:53.285Z,
"Why ""training against scheming"" is hard",https://www.alignmentforum.org/posts/HwuMFFnb6CaTwzhye/why-training-against-scheming-is-hard,34,2025-06-24T19:08:37.836Z,
Ctrl-Z: Controlling AI Agents via Resampling,https://www.alignmentforum.org/posts/LPHMMMZFAWog6ty5x/ctrl-z-controlling-ai-agents-via-resampling,56,2025-04-16T16:21:23.781Z,y
"What's worse, spies or schemers?",https://www.alignmentforum.org/posts/o5g4bhHgoZrewKwFH/what-s-worse-spies-or-schemers,32,2025-07-09T14:37:25.169Z,
Linkpost: Redwood Research reading list,https://www.alignmentforum.org/posts/aYNYmaKFXT6wHNzoz/linkpost-redwood-research-reading-list,28,2025-07-10T18:39:29.677Z,
“The Era of Experience” has an unsolved technical alignment problem,https://www.alignmentforum.org/posts/TCGgiJAinGgcMEByt/the-era-of-experience-has-an-unsolved-technical-alignment,46,2025-04-24T13:57:38.984Z,
We should try to automate AI safety work asap,https://www.alignmentforum.org/posts/W3KfxjbqBAnifBQoi/we-should-try-to-automate-ai-safety-work-asap,44,2025-04-26T16:35:43.770Z,
Have LLMs Generated Novel Insights?Q,https://www.alignmentforum.org/posts/GADJFwHzNZKg2Ndti/have-llms-generated-novel-insights,52,2025-02-23T18:22:12.763Z,
When does training a model change its goals?,https://www.alignmentforum.org/posts/yvuXPi5m4vCvSGTjo/when-does-training-a-model-change-its-goals,40,2025-06-12T18:43:05.187Z,
Catastrophe through Chaos,https://www.alignmentforum.org/posts/fbfujF7foACS5aJSL/catastrophe-through-chaos,63,2025-01-31T14:19:08.399Z,
Convergent Linear Representations of Emergent Misalignment,https://www.alignmentforum.org/posts/umYzsh7SGHHKsRCaA/convergent-linear-representations-of-emergent-misalignment,36,2025-06-16T15:47:15.487Z,y
The Pando Problem: Rethinking AI Individuality,https://www.alignmentforum.org/posts/wQKskToGofs4osdJ3/the-pando-problem-rethinking-ai-individuality,42,2025-03-28T21:03:28.374Z,
Self-fulfilling misalignment data might be poisoning our AI models,https://www.alignmentforum.org/posts/QkEyry3Mqo8umbhoK/self-fulfilling-misalignment-data-might-be-poisoning-our-ai,50,2025-03-02T19:51:14.775Z,
Unfaithful Reasoning Can Fool Chain-of-Thought Monitoring,https://www.alignmentforum.org/posts/QYAfjdujzRv8hx6xo/unfaithful-reasoning-can-fool-chain-of-thought-monitoring,36,2025-06-02T19:08:42.396Z,y
Instrumental Goals Are A Different And Friendlier Kind Of Thing Than Terminal Goals,https://www.alignmentforum.org/posts/7Z4WC4AFgfmZ3fCDC/instrumental-goals-are-a-different-and-friendlier-kind-of,64,2025-01-24T20:20:28.881Z,
What Is The Alignment Problem?,https://www.alignmentforum.org/posts/dHNKtQ3vTBxTfTPxu/what-is-the-alignment-problem,64,2025-01-16T01:20:16.826Z,y
Auditing language models for hidden objectives,https://www.alignmentforum.org/posts/wSKPuBfgkkqfTpmWJ/auditing-language-models-for-hidden-objectives,82,2025-03-13T19:18:32.638Z,y
The Sweet Lesson: AI Safety Should Scale With Compute,https://www.alignmentforum.org/posts/6hy7tsB2pkpRHqazG/the-sweet-lesson-ai-safety-should-scale-with-compute,32,2025-05-05T19:03:28.748Z,
AI safety techniques leveraging distillation,https://www.alignmentforum.org/posts/8KKujApx4g7FBm6hE/ai-safety-techniques-leveraging-distillation,36,2025-06-19T14:31:02.632Z,
Reasoning-Finetuning Repurposes Latent Representations in Base Models,https://www.alignmentforum.org/posts/J9BiKfJ4YvNd2Lwbh/reasoning-finetuning-repurposes-latent-representations-in,20,2025-07-23T16:18:43.544Z,
Slow corporations as an intuition pump for AI R&D automation,https://www.alignmentforum.org/posts/hMSuXTsEHvk4NG6pm/slow-corporations-as-an-intuition-pump-for-ai-r-and-d,48,2025-05-09T14:49:38.987Z,
Two proposed projects on abstract analogies for scheming,https://www.alignmentforum.org/posts/5zsLpcTMtesgF7c8p/two-proposed-projects-on-abstract-analogies-for-scheming,31,2025-07-04T16:03:36.081Z,
Negative Results for SAEs On Downstream Tasks and Deprioritising SAE Research (GDM Mech Interp Team Progress Update #2),https://www.alignmentforum.org/posts/4uXCAJNuPKtKBsi28/negative-results-for-saes-on-downstream-tasks,58,2025-03-26T19:07:48.710Z,y
"The best approaches for mitigating ""the intelligence curse"" (or gradual disempowerment); my quick guesses at the best object-level interventions",https://www.alignmentforum.org/posts/BXW2bqxmYbLuBrm7E/the-best-approaches-for-mitigating-the-intelligence-curse-or,35,2025-05-31T18:20:43.710Z,
Gradual Disempowerment: Systemic Existential Risks from Incremental AI Development,https://www.alignmentforum.org/posts/pZhEQieM9otKXhxmd/gradual-disempowerment-systemic-existential-risks-from,61,2025-01-30T17:03:45.545Z,y
How training-gamers might function (and win),https://www.alignmentforum.org/posts/ntDA4Q7BaYhWPgzuq/reward-seekers,56,2025-04-11T21:26:18.669Z,
When is it important that open-weight models aren't released? My thoughts on the benefits and dangers of open-weight models in response to developments in CBRN capabilities.,https://www.alignmentforum.org/posts/TeF8Az2EiWenR9APF/when-is-it-important-that-open-weight-models-aren-t-released,32,2025-06-09T19:19:39.861Z,
How will we update about scheming?,https://www.alignmentforum.org/posts/aEguDPoCzt3287CCD/how-will-we-update-about-scheming,86,2025-01-06T20:21:52.281Z,
AIs at the current capability level may be important for future safety work,https://www.alignmentforum.org/posts/cJQZAueoPC6aTncKK/ais-at-the-current-capability-level-may-be-important-for,39,2025-05-12T14:06:11.872Z,
7+ tractable directions in AI control,https://www.alignmentforum.org/posts/wwshEdNhwwT4r9RQN/7-tractable-directions-in-ai-control,52,2025-04-28T17:12:58.363Z,
Proof Section to an Introduction to Credal Sets and Infra-Bayes Learnability,https://www.alignmentforum.org/posts/pFtCk9xezCssuzvjs/proof-section-to-an-introduction-to-credal-sets-and-infra,5,2025-08-21T23:11:16.801Z,
A computational no-coincidence principle,https://www.alignmentforum.org/posts/Xt9r4SNNuYxW83tmo/a-computational-no-coincidence-principle,62,2025-02-14T21:39:39.277Z,
Research Areas in AI Control (The Alignment Project by UK AISI),https://www.alignmentforum.org/posts/rGcg4XDPDzBFuqNJz/research-areas-in-ai-control-the-alignment-project-by-uk,16,2025-08-01T10:27:09.500Z,y
Do reasoning models use their scratchpad like we do? Evidence from distilling paraphrases,https://www.alignmentforum.org/posts/ywzLszRuGRDpabjCk/do-reasoning-models-use-their-scratchpad-like-we-do-evidence,63,2025-03-11T11:52:38.994Z,y
Downstream applications as validation of interpretability progress,https://www.alignmentforum.org/posts/wGRnzCFcowRCrpX4Y/downstream-applications-as-validation-of-interpretability,58,2025-03-31T01:35:02.722Z,y
[Paper] Stochastic Parameter Decomposition,https://www.alignmentforum.org/posts/yjrpmCmqurDmbMztW/paper-stochastic-parameter-decomposition,17,2025-06-27T16:54:16.228Z,y
Pitfalls of Building UDT Agents,https://www.alignmentforum.org/posts/y3zTP6sixGjAkz7xE/pitfalls-of-building-udt-agents,9,2025-07-30T03:27:45.592Z,
Recent and forecasted rates of software and hardware progress,https://www.alignmentforum.org/posts/ATSyAdBnxxDDAwhgu/recent-and-forecasted-rates-of-software-and-hardware,27,2025-06-26T22:37:27.074Z,
SAE on activation differences,https://www.alignmentforum.org/posts/XPNJSa3BxMAN4ZXc7/sae-on-activation-differences,18,2025-06-30T17:50:54.821Z,y
Individual AI representatives don't solve Gradual Disempowerement,https://www.alignmentforum.org/posts/E96XcEPECbsipAvFi/individual-ai-representatives-don-t-solve-gradual,31,2025-06-04T01:26:15.761Z,
Misalignment and Strategic Underperformance: An Analysis of Sandbagging and Exploration Hacking,https://www.alignmentforum.org/posts/TeTegzR8X5CuKgMc3/misalignment-and-strategic-underperformance-an-analysis-of,50,2025-05-08T19:06:29.469Z,y
The bitter lesson of misuse detection,https://www.alignmentforum.org/posts/RvDkMho6quHcRiTva/the-bitter-lesson-of-misuse-detection-1,16,2025-07-10T14:50:19.960Z,y
Unbounded Embedded Agency: AEDT w.r.t. rOSI,https://www.alignmentforum.org/posts/B6gumHyuxzR5yn5tH/unbounded-embedded-agency-aedt-w-r-t-rosi,15,2025-07-20T23:46:47.885Z,
Planning for Extreme AI Risks,https://www.alignmentforum.org/posts/8vgi3fBWPFDLBBcAx/planning-for-extreme-ai-risks,65,2025-01-29T18:33:14.844Z,
Highly Opinionated Advice on How to Write ML Papers,https://www.alignmentforum.org/posts/eJGptPbbFPZGLpjsp/highly-opinionated-advice-on-how-to-write-ml-papers,27,2025-05-12T01:59:26.664Z,
AI companies are unlikely to make high-assurance safety cases if timelines are short,https://www.alignmentforum.org/posts/neTbrpBziAsTH5Bn7/ai-companies-are-unlikely-to-make-high-assurance-safety,71,2025-01-23T18:41:40.546Z,
What's going on with AI progress and trends? (As of 5/2025),https://www.alignmentforum.org/posts/v7LtZx6Qk5e9s7zj3/what-s-going-on-with-ai-progress-and-trends-as-of-5-2025,38,2025-05-02T19:00:40.276Z,
"Gradual Disempowerment, Shell Games and Flinches",https://www.alignmentforum.org/posts/a6FKqvdf6XjFpvKEb/gradual-disempowerment-shell-games-and-flinches,42,2025-02-02T14:47:53.404Z,
Emergent Misalignment on a Budget,https://www.alignmentforum.org/posts/qHudHZNLCiFrygRiy/emergent-misalignment-on-a-budget,19,2025-06-08T15:28:50.498Z,
Activation space interpretability may be doomed,https://www.alignmentforum.org/posts/gYfpPbww3wQRaxAFD/activation-space-interpretability-may-be-doomed,57,2025-01-08T12:49:38.421Z,y
Ten people on the inside,https://www.alignmentforum.org/posts/WSNnKcKCYAffcnrt2/ten-people-on-the-inside,63,2025-01-28T16:41:22.990Z,
Judgements: Merging Prediction & Evidence,https://www.alignmentforum.org/posts/3hs6MniiEssfL8rPz/judgements-merging-prediction-and-evidence,46,2025-02-23T19:35:51.488Z,
Working through a small tiling result,https://www.alignmentforum.org/posts/akuMwu8SkmQSdospi/working-through-a-small-tiling-result,35,2025-05-13T20:28:35.725Z,
Thought Anchors: Which LLM Reasoning Steps Matter?,https://www.alignmentforum.org/posts/iLHe3vLur3NgrFPFy/thought-anchors-which-llm-reasoning-steps-matter,16,2025-07-02T20:16:03.355Z,
Towards a scale-free theory of intelligent agency,https://www.alignmentforum.org/posts/5tYTKX4pNpiG4vzYg/towards-a-scale-free-theory-of-intelligent-agency,38,2025-03-21T01:39:42.251Z,
AI Control May Increase Existential Risk,https://www.alignmentforum.org/posts/rZcyemEpBHgb2hqLP/ai-control-may-increase-existential-risk,44,2025-03-11T14:30:05.972Z,y
"Tell me about yourself:
LLMs are aware of their learned behaviors",https://www.alignmentforum.org/posts/xrv2fNJtqabN3h6Aj/tell-me-about-yourself-llms-are-aware-of-their-learned,51,2025-01-22T00:47:15.023Z,y
Approximating Human Preferences Using a Multi-Judge Learned System,https://www.alignmentforum.org/posts/i4tuCLF9yjTHJQpkw/approximating-human-preferences-using-a-multi-judge-learned-1,4,2025-07-31T18:01:33.503Z,
Training on Documents About Reward Hacking Induces Reward Hacking,https://www.alignmentforum.org/posts/qXYLvjGL9QvD3aFSW/training-on-documents-about-reward-hacking-induces-reward,60,2025-01-21T21:32:24.691Z,y
Interpreting the METR Time Horizons Post,https://www.alignmentforum.org/posts/fRiqwFPiaasKxtJuZ/interpreting-the-metr-time-horizons-post,27,2025-04-30T03:03:19.928Z,
Principles for Picking Practical Interpretability Projects,https://www.alignmentforum.org/posts/DqaoPNqhQhwBFqWue/principles-for-picking-practical-interpretability-projects,16,2025-07-15T17:38:25.221Z,
Research directions Open Phil wants to fund in technical AI safety,https://www.alignmentforum.org/posts/26SHhxK2yYQbh7ors/research-directions-open-phil-wants-to-fund-in-technical-ai,55,2025-02-08T01:40:00.968Z,y
AXRP Episode 45 - Samuel Albanie on DeepMind’s AGI Safety Approach,https://www.alignmentforum.org/posts/nZtAkGmDELMnLJMQ5/axrp-episode-45-samuel-albanie-on-deepmind-s-agi-safety,15,2025-07-06T23:00:03.659Z,
Steelmanning heuristic arguments,https://www.alignmentforum.org/posts/CYDakfFgjHFB7DGXk/steelmanning-heuristic-arguments,40,2025-04-13T01:09:33.392Z,
Modifying LLM Beliefs with Synthetic Document Finetuning,https://www.alignmentforum.org/posts/ARQs7KYY9vJHeYsGc/modifying-llm-beliefs-with-synthetic-document-finetuning,45,2025-04-24T21:15:17.366Z,
Building AI Research Fleets,https://www.alignmentforum.org/posts/WJ7y8S9WdKRvrzJmR/building-ai-research-fleets,46,2025-01-12T18:23:09.682Z,
New Paper: Infra-Bayesian Decision-Estimation Theory,https://www.alignmentforum.org/posts/LgLez8aeK24PbyyQJ/new-paper-infra-bayesian-decision-estimation-theory,29,2025-04-10T09:17:38.966Z,
A quick list of reward hacking interventions,https://www.alignmentforum.org/posts/spZyuEGPzqPhnehyk/a-quick-list-of-reward-hacking-interventions,29,2025-06-10T00:58:37.361Z,
LLMs are Capable of Misaligned Behavior Under Explicit Prohibition and Surveillance,https://www.alignmentforum.org/posts/Phjqz3hjYDGoqGR65/llms-are-capable-of-misaligned-behavior-under-explicit-1,11,2025-07-08T11:50:47.964Z,
A short course on AGI safety from the GDM Alignment team,https://www.alignmentforum.org/posts/TJrCumJxhzTmNBsRz/a-short-course-on-agi-safety-from-the-gdm-alignment-team,48,2025-02-14T15:43:50.903Z,
Research Areas in Learning Theory (The Alignment Project by UK AISI),https://www.alignmentforum.org/posts/Wut4Y7LRwNpfZzG3y/research-areas-in-learning-theory-the-alignment-project-by,8,2025-08-01T10:26:16.594Z,y
Will compute bottlenecks prevent a software intelligence explosion?,https://www.alignmentforum.org/posts/XDF6ovePBJf6hsxGj/will-compute-bottlenecks-prevent-a-software-intelligence-1,37,2025-04-04T17:41:37.088Z,
Defining Corrigible and Useful Goals,https://www.alignmentforum.org/posts/HLns982j8iTn7d2km/defining-corrigible-and-useful-goals,14,2025-06-25T03:51:57.160Z,
Mistral Large 2 (123B) seems to exhibit alignment faking,https://www.alignmentforum.org/posts/kCGk5tp5suHoGwhCa/mistral-large-2-123b-seems-to-exhibit-alignment-faking,29,2025-03-27T15:39:02.176Z,
Exploration hacking: can reasoning models subvert RL?,https://www.alignmentforum.org/posts/Dft9vpMnEeWFE3Gc6/exploration-hacking-can-reasoning-models-subvert-rl-1,9,2025-07-30T22:02:20.230Z,
AI for AI safety,https://www.alignmentforum.org/posts/F3j4xqpxjxgQD3xXh/ai-for-ai-safety,21,2025-03-14T15:00:23.491Z,
Inverse Scaling in Test-Time Compute,https://www.alignmentforum.org/posts/gbJJpm92jtxiD9zag/inverse-scaling-in-test-time-compute-2,7,2025-07-22T22:06:30.168Z,
Reward button alignment,https://www.alignmentforum.org/posts/JrTk2pbqp7BFwPAKw/reward-button-alignment,25,2025-05-22T17:36:50.078Z,
"We should start looking for scheming ""in the wild""",https://www.alignmentforum.org/posts/HvWQCWQoYh4WoGZfR/we-should-start-looking-for-scheming-in-the-wild,39,2025-03-06T13:49:39.739Z,
An alignment safety case sketch based on debate,https://www.alignmentforum.org/posts/iELyAqizJkizBQbfr/an-alignment-safety-case-sketch-based-on-debate,35,2025-05-08T15:02:06.345Z,
Google DeepMind: An Approach to Technical AGI Safety and Security,https://www.alignmentforum.org/posts/3ki4mt4BA6eTx56Tc/google-deepmind-an-approach-to-technical-agi-safety-and,41,2025-04-05T22:00:14.803Z,
Research Areas in Evaluation and Guarantees in Reinforcement Learning (The Alignment Project by UK AISI),https://www.alignmentforum.org/posts/3eeKWC62thz5Lry8t/research-areas-in-evaluation-and-guarantees-in-reinforcement,8,2025-08-01T09:53:19.122Z,y
Detecting Strategic Deception Using Linear Probes,https://www.alignmentforum.org/posts/9pGbTz6c78PGwJein/detecting-strategic-deception-using-linear-probes,46,2025-02-06T15:46:53.024Z,
The need to relativise in debate,https://www.alignmentforum.org/posts/XycoFucvxAPhcgJQa/the-need-to-relativise-in-debate-1,14,2025-06-26T16:23:48.995Z,
Research Areas in Interpretability (The Alignment Project by UK AISI),https://www.alignmentforum.org/posts/dgcsY8CHcPQiZ5v8P/research-areas-in-interpretability-the-alignment-project-by,9,2025-08-01T10:26:46.914Z,
Attribution-based parameter decomposition,https://www.alignmentforum.org/posts/EPefYWjuHNcNH4C7E/attribution-based-parameter-decomposition,45,2025-01-25T13:12:11.031Z,
What goals will AIs have? A list of hypotheses,https://www.alignmentforum.org/posts/r86BBAqLHXrZ4mWWA/what-goals-will-ais-have-a-list-of-hypotheses,42,2025-03-03T20:08:31.539Z,
Steering Gemini with BiDPO,https://www.alignmentforum.org/posts/WqjkqrEyFDXoHzz9K/steering-gemini-with-bidpo,52,2025-01-31T02:37:55.839Z,y
"Go home GPT-4o, you’re drunk: emergent misalignment as lowered inhibitions",https://www.alignmentforum.org/posts/RoWabfQxabWBiXwxP/go-home-gpt-4o-you-re-drunk-emergent-misalignment-as-lowered,33,2025-03-18T14:48:54.762Z,
New Paper: Ambiguous Online Learning,https://www.alignmentforum.org/posts/Y9NuKpb6dsyiYFxWK/new-paper-ambiguous-online-learning,15,2025-06-25T09:14:14.872Z,
The case for countermeasures to memetic spread of misaligned values,https://www.alignmentforum.org/posts/qjCk73Hu4wv9ocmRF/the-case-for-countermeasures-to-memetic-spread-of-misaligned,29,2025-05-28T21:12:32.929Z,
Prefix cache untrusted monitors: a method to apply after you catch your AI,https://www.alignmentforum.org/posts/Mxucm6BmJyCvxptPu/prefix-cache-untrusted-monitors-a-method-to-apply-after-you,22,2025-06-20T15:56:00.353Z,
Problems with instruction-following as an alignment target,https://www.alignmentforum.org/posts/CSFa9rvGNGAfCzBk6/problems-with-instruction-following-as-an-alignment-target,15,2025-05-15T15:41:48.748Z,
100+ concrete projects and open problems in evals,https://www.alignmentforum.org/posts/LhnqegFoykcjaXCYH/100-concrete-projects-and-open-problems-in-evals,39,2025-03-22T15:21:40.970Z,
"How I Think About My Research Process: Explore, Understand, Distill",https://www.alignmentforum.org/posts/hjMy4ZxS5ogA9cTYK/how-i-think-about-my-research-process-explore-understand,22,2025-04-26T10:31:34.305Z,
Research Areas in Methods for Post-training and Elicitation (The Alignment Project by UK AISI),https://www.alignmentforum.org/posts/tEfuDmXTGLGegsbmv/research-areas-in-methods-for-post-training-and-elicitation,6,2025-08-01T10:27:02.618Z,y
Research Areas in Cognitive Science (The Alignment Project by UK AISI),https://www.alignmentforum.org/posts/dzjpmq4uWyxGduRai/research-areas-in-cognitive-science-the-alignment-project-by,9,2025-08-01T10:26:38.362Z,y
Glass box learners want to be black box,https://www.alignmentforum.org/posts/boodbr2PXpEEMGrfx/glass-box-learners-want-to-be-black-box,18,2025-05-10T11:05:23.003Z,
The 80/20 playbook for mitigating AI scheming in 2025,https://www.alignmentforum.org/posts/YFxpsrph83H25aCLW/the-80-20-playbook-for-mitigating-ai-scheming-in-2025,14,2025-05-31T21:17:44.304Z,
How LLM Beliefs Change During Chain-of-Thought Reasoning,https://www.alignmentforum.org/posts/GwvWtAwnKBKjmknag/how-llm-beliefs-change-during-chain-of-thought-reasoning-2,11,2025-06-16T16:18:29.324Z,
Explaining your life with self-reflective AIXI (an interlude),https://www.alignmentforum.org/posts/yTue8urmngTxRviZT/explaining-your-life-with-self-reflective-aixi-an-interlude,6,2025-07-23T00:57:13.284Z,
How can we solve diffuse threats like research sabotage with AI control?,https://www.alignmentforum.org/posts/Mf5Hnpi2KcqZdmFDq/how-can-we-solve-diffuse-threats-like-research-sabotage-with,33,2025-04-30T19:23:19.999Z,
Renormalization Roadmap,https://www.alignmentforum.org/posts/74wSgnCKPHAuqExe7/renormalization-roadmap,32,2025-03-31T20:34:16.352Z,
Putting up Bumpers,https://www.alignmentforum.org/posts/HXJXPjzWyS5aAoRCw/putting-up-bumpers,31,2025-04-23T16:05:05.476Z,
"To be legible, evidence of misalignment probably has to be behavioral",https://www.alignmentforum.org/posts/4QRvFCzhFbedmNfp4/to-be-legible-evidence-of-misalignment-probably-has-to-be,28,2025-04-15T18:14:53.022Z,
Formalizing Embeddedness Failures in Universal Artificial Intelligence,https://www.alignmentforum.org/posts/FSm92N8bcDujRZPMH/formalizing-embeddedness-failures-in-universal-artificial,20,2025-05-26T12:36:41.067Z,
How might we safely pass the buck to AI?,https://www.alignmentforum.org/posts/TTFsKxQThrqgWeXYJ/how-might-we-safely-pass-the-buck-to-ai,44,2025-02-19T17:48:32.249Z,
The Weighted Perplexity Benchmark: Tokenizer-Normalized Evaluation for Language Model Comparison,https://www.alignmentforum.org/posts/csNk8ECk9SiKHkw35/the-weighted-perplexity-benchmark-tokenizer-normalized,13,2025-07-07T21:43:41.672Z,
Selective modularity: a research agenda,https://www.alignmentforum.org/posts/tAnHM3L25LwuASdpF/selective-modularity-a-research-agenda,26,2025-03-24T04:12:44.822Z,
Tips and Code for Empirical Research Workflows,https://www.alignmentforum.org/posts/6P8GYb4AjtPXx6LLB/tips-and-code-for-empirical-research-workflows,40,2025-01-20T22:31:51.498Z,
What is Inadequate about Bayesianism for AI Alignment: Motivating Infra-Bayesianism,https://www.alignmentforum.org/posts/wzCtwYtojMabyEg2L/what-is-inadequate-about-bayesianism-for-ai-alignment,16,2025-05-01T19:06:15.711Z,y
Unlearning Needs to be More Selective [Progress Report],https://www.alignmentforum.org/posts/QYzofMbzmbgiwfqy8/unlearning-needs-to-be-more-selective-progress-report,14,2025-06-27T16:38:00.136Z,
Research Areas in Benchmark Design and Evaluation (The Alignment Project by UK AISI),https://www.alignmentforum.org/posts/NFKFesJJkwDDRs3Mx/research-areas-in-benchmark-design-and-evaluation-the,7,2025-08-01T10:26:54.931Z,
Can we safely automate alignment research?,https://www.alignmentforum.org/posts/nJcuj4rtuefeTRFHp/can-we-safely-automate-alignment-research,25,2025-04-30T17:37:13.193Z,
Thoughts on the conservative assumptions in AI control,https://www.alignmentforum.org/posts/rHyPtvfnvWeMv7Lkb/thoughts-on-the-conservative-assumptions-in-ai-control,42,2025-01-17T19:23:38.575Z,
AI 2027 - Rogue Replication Timeline,https://www.alignmentforum.org/posts/66NjHDJFChe8cfPac/ai-2027-rogue-replication-timeline,10,2025-05-30T13:46:41.454Z,
Steganography via internal activations is already possible in small language models — a potential first step toward persistent hidden reasoning.,https://www.alignmentforum.org/posts/dRmeXo6REf5n8xGug/steganography-via-internal-activations-is-already-possible,2,2025-08-09T11:44:25.546Z,
Political sycophancy as a model organism of scheming,https://www.alignmentforum.org/posts/bhxgkb7YtRNwBxLMd/political-sycophancy-as-a-model-organism-of-scheming,26,2025-05-12T17:49:19.605Z,
LLM in-context learning as (approximating) Solomonoff induction,https://www.alignmentforum.org/posts/xyYss3oCzovibHxAF/llm-in-context-learning-as-approximating-solomonoff,13,2025-06-05T17:45:28.385Z,
The subset parity learning problem: much more than you wanted to know,https://www.alignmentforum.org/posts/Mcrfi3DBJBzfoLctA/the-subset-parity-learning-problem-much-more-than-you-wanted,36,2025-01-03T09:13:59.245Z,
A simple explanation of incomplete models,https://www.alignmentforum.org/posts/CGzAu8F3fii7gdgMC/a-simple-explanation-of-incomplete-models,5,2025-07-06T19:09:25.917Z,
Interim Research Report: Mechanisms of Awareness,https://www.alignmentforum.org/posts/m8WKfNxp9eDLRkCk9/interim-research-report-mechanisms-of-awareness,22,2025-05-02T20:29:35.140Z,
It Is Untenable That Near-Future AI Scenario Models Like “AI 2027” Don't Include Open Source AI,https://www.alignmentforum.org/posts/vyWzz5exnyKbPJ4zt/it-is-untenable-that-near-future-ai-scenario-models-like-ai,14,2025-05-16T02:20:48.115Z,
A single principle related to many Alignment subproblems?,https://www.alignmentforum.org/posts/h89L5FMAkEBNsZ3xM/a-single-principle-related-to-many-alignment-subproblems-2,13,2025-04-30T09:49:21.181Z,
A Problem to Solve Before Building a Deception Detector,https://www.alignmentforum.org/posts/YXNeA3RyRrrRWS37A/a-problem-to-solve-before-building-a-deception-detector,35,2025-02-07T19:35:23.307Z,
"My Research Process: Key Mindsets - Truth-Seeking, Prioritisation, Moving Fast",https://www.alignmentforum.org/posts/cbBwwm4jW6AZctymL/my-research-process-key-mindsets-truth-seeking,19,2025-04-27T14:38:36.794Z,
What can be learned from scary demos? A snitching case study,https://www.alignmentforum.org/posts/5JyepxdN6Hq62TPCK/what-can-be-learned-from-scary-demos-a-snitching-case-study,13,2025-06-24T08:40:50.903Z,
Language Models Use Trigonometry to Do Addition,https://www.alignmentforum.org/posts/E7z89FKLsHk5DkmDL/language-models-use-trigonometry-to-do-addition-1,25,2025-02-05T13:50:08.243Z,y
Prioritizing threats for AI control,https://www.alignmentforum.org/posts/fCazYoZSSMadiT6sf/prioritizing-threats-for-ai-control,35,2025-03-19T17:09:45.044Z,
Anti-Slop Interventions?,https://www.alignmentforum.org/posts/PdtHzEb3cebnWCjoj/anti-slop-interventions,25,2025-02-04T19:50:29.127Z,
MONA: Managed Myopia with Approval Feedback,https://www.alignmentforum.org/posts/zWySWKuXnhMDhgwc3/mona-managed-myopia-with-approval-feedback-2,44,2025-01-23T12:24:18.108Z,y
Unexploitable search: blocking malicious use of free parameters,https://www.alignmentforum.org/posts/CuneN5HmLnztsLRzD/unexploitable-search-blocking-malicious-use-of-free-1,16,2025-05-21T17:23:45.463Z,
"Constraining Minds, Not Goals: A Structural Approach to AI Alignment",https://www.alignmentforum.org/posts/g3RXozhPmcLm2yDps/constraining-minds-not-goals-a-structural-approach-to-ai,12,2025-06-13T21:06:40.984Z,
Training-time schemers vs behavioral schemers,https://www.alignmentforum.org/posts/m5nWc9v6MTsWXKpCy/training-time-schemers-vs-behavioral-schemers,27,2025-04-24T19:07:55.256Z,
The Best Way to Align an LLM: Is Inner Alignment Now a Solved Problem?,https://www.alignmentforum.org/posts/xAsviBJGSBBtgBiCw/the-best-way-to-align-an-llm-is-inner-alignment-now-a-solved,7,2025-05-28T06:21:42.324Z,
MATS Applications + Research Directions I'm Currently Excited About,https://www.alignmentforum.org/posts/qGKq4G3HGRcSBc94C/mats-applications-research-directions-i-m-currently-excited,31,2025-02-06T11:03:40.093Z,y
AXRP Episode 41 - Lee Sharkey on Attribution-based Parameter Decomposition,https://www.alignmentforum.org/posts/gnyna4Rb2S7KdzxvJ/axrp-episode-41-lee-sharkey-on-attribution-based-parameter,17,2025-06-03T03:40:02.640Z,y
Notes on countermeasures for exploration hacking (aka sandbagging),https://www.alignmentforum.org/posts/abmzgwfJA9acBoFEX/notes-on-countermeasures-for-exploration-hacking-aka,32,2025-03-24T18:39:36.665Z,
Fuzzing LLMs sometimes makes them reveal their secrets,https://www.alignmentforum.org/posts/GE6pcmmLc3kdpNJja/fuzzing-llms-sometimes-makes-them-reveal-their-secrets,34,2025-02-26T16:48:48.878Z,
An Advent of Thought,https://www.alignmentforum.org/posts/nkeYxjdrWBJvwbnTr/an-advent-of-thought,29,2025-03-17T14:21:08.765Z,
Quickly Assessing Reward Hacking-like Behavior in LLMs and its Sensitivity to Prompt Variations,https://www.alignmentforum.org/posts/quTGGNhGEiTCBEAX5/quickly-assessing-reward-hacking-like-behavior-in-llms-and,17,2025-06-04T07:22:56.081Z,
Reframing AI Safety as a Neverending Institutional Challenge,https://www.alignmentforum.org/posts/bzYJCXicmwDHDpLZa/reframing-ai-safety-as-a-neverending-institutional-challenge,17,2025-03-23T00:13:48.614Z,
Dodging systematic human errors in scalable oversight,https://www.alignmentforum.org/posts/EgRJtwQurNzz8CEfJ/dodging-systematic-human-errors-in-scalable-oversight,20,2025-05-14T15:19:39.352Z,
Research Areas in Information Theory and Cryptography (The Alignment Project by UK AISI),https://www.alignmentforum.org/posts/5rBXBew4T3cnnvBsh/research-areas-in-information-theory-and-cryptography-the,5,2025-08-01T10:25:28.366Z,
Research Areas in Computational Complexity Theory (The Alignment Project by UK AISI),https://www.alignmentforum.org/posts/BxwBBaLonX88dwDqT/research-areas-in-computational-complexity-theory-the,5,2025-08-01T10:25:47.492Z,
Defining Monitorable and Useful Goals,https://www.alignmentforum.org/posts/ixnRxJ2bjiwHryQJr/defining-monitorable-and-useful-goals,5,2025-07-15T23:06:00.338Z,
Paper: Open Problems in Mechanistic Interpretability,https://www.alignmentforum.org/posts/fqDzevPyw3GGaF5o9/paper-open-problems-in-mechanistic-interpretability,27,2025-01-29T10:25:54.727Z,y
"New, improved multiple-choice TruthfulQA",https://www.alignmentforum.org/posts/Bunfwz6JsNd44kgLT/new-improved-multiple-choice-truthfulqa,39,2025-01-15T23:32:09.202Z,
Handling schemers if shutdown is not an option,https://www.alignmentforum.org/posts/XxjScx4niRLWTfuD5/handling-schemers-if-shutdown-is-not-an-option,24,2025-04-18T14:39:18.609Z,
"Numberwang: LLMs Doing Autonomous Research, and a Call for Input",https://www.alignmentforum.org/posts/RSqfcyAW9ZkveGQ5u/numberwang-llms-doing-autonomous-research-and-a-call-for-1,28,2025-01-16T17:20:37.552Z,
"Inference-Time-Compute: More Faithful? 
A Research Note
",https://www.alignmentforum.org/posts/C8HAa2mf5kcBrpjkX/inference-time-compute-more-faithful-a-research-note,34,2025-01-15T04:43:00.631Z,y
Automated Researchers Can Subtly Sandbag,https://www.alignmentforum.org/posts/niBXcbthcQRXG5M5Y/automated-researchers-can-subtly-sandbag,23,2025-03-26T19:13:26.879Z,
Doing good... best?,https://www.alignmentforum.org/posts/3EqToCdzMXcmKd2ct/doing-good-best,0,2025-08-22T15:48:41.820Z,
Modeling versus Implementation,https://www.alignmentforum.org/posts/rysmFDWKT5L7uNTqD/modeling-versus-implementation,13,2025-05-18T13:38:20.421Z,
Layered AI Defenses Have Holes: Vulnerabilities and Key Recommendations,https://www.alignmentforum.org/posts/Z8tAn3jp2uaQf34Mm/layered-ai-defenses-have-holes-vulnerabilities-and-key,6,2025-07-04T00:07:17.845Z,
An Easily Overlooked Post on the Automation of Wisdom and Philosophy,https://www.alignmentforum.org/posts/WBMcKgrTpmniTK8HG/an-easily-overlooked-post-on-the-automation-of-wisdom-and,8,2025-06-12T02:54:50.303Z,
"For scheming, we should first focus on detection and then on prevention",https://www.alignmentforum.org/posts/bAWPsgbmtLf8ptay6/for-scheming-we-should-first-focus-on-detection-and-then-on,25,2025-03-04T15:22:06.105Z,
Research Areas in Economic Theory and Game Theory (The Alignment Project by UK AISI),https://www.alignmentforum.org/posts/o3rvXNLJTNh44oTy3/research-areas-in-economic-theory-and-game-theory-the,3,2025-08-01T10:25:58.917Z,
The Self-Hating Attention Head: A Deep Dive in GPT-2,https://www.alignmentforum.org/posts/wxPvdBwWeaneAsWRB/the-self-hating-attention-head-a-deep-dive-in-gpt-2-1,1,2025-07-04T13:07:44.841Z,
Gaming TruthfulQA: Simple Heuristics Exposed Dataset Weaknesses,https://www.alignmentforum.org/posts/57k6xNcWtAtsSTcor/gaming-truthfulqa-simple-heuristics-exposed-dataset,31,2025-01-16T02:14:35.098Z,
We Have No Plan for Preventing Loss of Control in Open Models,https://www.alignmentforum.org/posts/QSyshep2CRs8JTPwK/we-have-no-plan-for-preventing-loss-of-control-in-open,12,2025-03-10T15:35:12.597Z,
Agentic Interpretability: A Strategy Against Gradual Disempowerment,https://www.alignmentforum.org/posts/s9z4mgjtWTPpDLxFy/agentic-interpretability-a-strategy-against-gradual,7,2025-06-17T14:52:55.695Z,y
"Dream, Truth, & Good",https://www.alignmentforum.org/posts/DPjvL62kskHpp2SZg/dream-truth-and-good,23,2025-02-24T16:59:05.045Z,
Recommendations for Technical AI Safety Research Directions,https://www.alignmentforum.org/posts/tG9LGHLzQezH3pvMs/recommendations-for-technical-ai-safety-research-directions,34,2025-01-10T19:34:04.920Z,y
Recent progress on the science of evaluations,https://www.alignmentforum.org/posts/m2qMj7ovncbqKtzNt/recent-progress-on-the-science-of-evaluations,7,2025-06-23T09:41:44.185Z,
A sketch of an AI control safety case,https://www.alignmentforum.org/posts/vWYzSorAEWwoJnnXq/a-sketch-of-an-ai-control-safety-case,33,2025-01-30T17:28:47.992Z,
An overview of control measures,https://www.alignmentforum.org/posts/G8WwLmcGFa4H6Ld9d/an-overview-of-control-measures,29,2025-03-24T23:16:49.400Z,y
Forecasting time to automated superhuman coders [AI 2027 Timelines Forecast],https://www.alignmentforum.org/posts/ggqSg7bSLChanfunf/forecasting-time-to-automated-superhuman-coders-ai-2027,13,2025-04-10T23:10:23.063Z,y
The ultimate goal,https://www.alignmentforum.org/posts/PCxevdhZtesKurYHD/the-ultimate-goal,3,2025-07-05T19:10:51.243Z,
AXRP Episode 44 - Peter Salib on AI Rights for Human Safety,https://www.alignmentforum.org/posts/vHDowQtsiy2xK38H4/axrp-episode-44-peter-salib-on-ai-rights-for-human-safety,6,2025-06-28T01:40:04.658Z,
"With enough knowledge, any conscious agent acts morally",https://www.alignmentforum.org/posts/EbRLty44fFtoWduj4/with-enough-knowledge-any-conscious-agent-acts-morally,0,2025-08-22T15:44:07.587Z,
Selection Pressures on LM Personas,https://www.alignmentforum.org/posts/LdBhgAhpvbdEep79F/selection-pressures-on-lm-personas,17,2025-03-28T20:33:09.918Z,
Three Types of Intelligence Explosion,https://www.alignmentforum.org/posts/PzbEpSGvwH3NnegDB/three-types-of-intelligence-explosion,16,2025-03-17T14:47:46.696Z,
My Research Process: Understanding and Cultivating Research Taste,https://www.alignmentforum.org/posts/Ldrss6o3tiKT6NdMm/my-research-process-understanding-and-cultivating-research,13,2025-05-01T23:08:42.212Z,
Paths and waystations in AI safety,https://www.alignmentforum.org/posts/kBgySGcASWa4FWdD9/paths-and-waystations-in-ai-safety-1,22,2025-03-11T18:52:57.772Z,
SHIFT relies on token-level features to de-bias Bias in Bios probes,https://www.alignmentforum.org/posts/QdxwGz9AeDu5du4Rk/shift-relies-on-token-level-features-to-de-bias-bias-in-bios,21,2025-03-19T21:29:15.974Z,
Research Areas in Probabilistic Methods (The Alignment Project by UK AISI),https://www.alignmentforum.org/posts/Mv5qE29RvWcau2pGA/research-areas-in-probabilistic-methods-the-alignment,3,2025-08-01T10:26:07.666Z,
Why do misalignment risks increase as AIs get more capable?,https://www.alignmentforum.org/posts/NDotm7oLHfR56g4sD/why-do-misalignment-risks-increase-as-ais-get-more-capable,18,2025-04-11T03:06:50.928Z,
Non-Monotonic Infra-Bayesian Physicalism,https://www.alignmentforum.org/posts/DobZ62XMdiPigii9H/non-monotonic-infra-bayesian-physicalism,13,2025-04-02T12:14:19.783Z,
Video & transcript: Challenges for Safe & Beneficial Brain-Like AGI,https://www.alignmentforum.org/posts/YKmyay3bWF2ofAGNo/video-and-transcript-challenges-for-safe-and-beneficial,14,2025-05-08T21:11:49.901Z,
Testing for Scheming with Model Deletion,https://www.alignmentforum.org/posts/D5kGGGhsnfH7G8v9f/testing-for-scheming-with-model-deletion,26,2025-01-07T01:54:13.550Z,
MONA: Three Month Later - Updates and Steganography Without Optimization Pressure,https://www.alignmentforum.org/posts/zF5gXf3KJhmvojDRf/mona-three-month-later-updates-and-steganography-without,17,2025-04-12T23:15:07.964Z,y
"Superintelligent Agents Pose Catastrophic Risks:
Can Scientist AI Offer a Safer Path?",https://www.alignmentforum.org/posts/p5gBcoQeBsvsMShvT/superintelligent-agents-pose-catastrophic-risks-can,13,2025-02-24T18:31:48.580Z,y
Absolute Zero: Alpha Zero for LLM,https://www.alignmentforum.org/posts/dkWicxcPKdSx9npK4/absolute-zero-alpha-zero-for-llm,10,2025-05-11T20:42:51.937Z,
Subversion Strategy Eval: Can language models statelessly strategize to subvert control protocols?,https://www.alignmentforum.org/posts/RxRKGwBkCX3YsthfK/subversion-strategy-eval-can-language-models-statelessly,24,2025-03-24T17:55:59.358Z,
What is the functional role of SAE errors?,https://www.alignmentforum.org/posts/WzHPpMz2kRongsA7q/what-is-the-functional-role-of-sae-errors,5,2025-06-20T18:11:45.373Z,
An Introduction to Reinforcement Learning for Understanding Infra-Bayesianism,https://www.alignmentforum.org/posts/gd4ALPL9nSyTgzccz/an-introduction-to-reinforcement-learning-for-understanding,10,2025-05-17T02:34:50.413Z,y
How to evaluate control measures for LLM agents? A trajectory from today to superintelligence,https://www.alignmentforum.org/posts/2XYcK9WHACzxfHJju/how-to-evaluate-control-measures-for-llm-agents-a-trajectory,18,2025-04-14T16:45:46.584Z,
One more reason for AI capable of independent moral reasoning: alignment itself and cause prioritisation,https://www.alignmentforum.org/posts/tYCinFKtbBEg42gy9/one-more-reason-for-ai-capable-of-independent-moral,1,2025-08-22T15:53:36.568Z,
Validating against a misalignment detector is very different to training against one,https://www.alignmentforum.org/posts/CXYf7kGBecZMajrXC/validating-against-a-misalignment-detector-is-very-different,23,2025-03-04T15:41:04.692Z,
Self-dialogue: Do behaviorist rewards make scheming AGIs?,https://www.alignmentforum.org/posts/SFgLBQsLB3rprdxsq/self-dialogue-do-behaviorist-rewards-make-scheming-agis,22,2025-02-13T18:39:37.770Z,y
An overview of areas of control work,https://www.alignmentforum.org/posts/Eeo9NrXeotWuHCgQW/an-overview-of-areas-of-control-work,22,2025-03-25T22:02:16.178Z,
Subjective Naturalism in Decision Theory: Savage vs. Jeffrey–Bolker,https://www.alignmentforum.org/posts/FL8RunWvyS5L8uJEw/subjective-naturalism-in-decision-theory-savage-vs-jeffrey,14,2025-02-04T20:34:22.625Z,
Reasoning models don't always say what they think,https://www.alignmentforum.org/posts/PrcBFPkoRNGWrvdPk/reasoning-models-don-t-always-say-what-they-think-1,12,2025-04-09T19:48:58.733Z,
"Prospects for Alignment Automation:
Interpretability Case Study",https://www.alignmentforum.org/posts/y5cYisQ2QHiSbQbhk/prospects-for-alignment-automation-interpretability-case,14,2025-03-21T14:05:51.528Z,
Sufficiently Decentralized Intelligence is Indistinguishable from Synchronicity,https://www.alignmentforum.org/posts/SePsaQzDbGGTMzaJZ/sufficiently-decentralized-intelligence-is-indistinguishable,17,2025-03-07T21:50:32.231Z,
"Policy Entropy, Learning, and Alignment (Or Maybe Your LLM Needs Therapy)",https://www.alignmentforum.org/posts/C4tvfHn2DfxyYYwaL/policy-entropy-learning-and-alignment-or-maybe-your-llm,7,2025-05-31T22:09:51.411Z,
Role embeddings: making authorship more salient to LLMs,https://www.alignmentforum.org/posts/HEzNZ9gvgYwT3aZFS/role-embeddings-making-authorship-more-salient-to-llms,25,2025-01-07T20:13:16.677Z,
Introducing MASK: A Benchmark for Measuring Honesty in AI Systems,https://www.alignmentforum.org/posts/TgDymNrGRoxPv4SWj/the-mask-benchmark-disentangling-honesty-from-accuracy-in-ai-3,17,2025-03-05T22:56:46.155Z,
In response to critiques of Guaranteed Safe AI,https://www.alignmentforum.org/posts/DZuBHHKao6jsDDreH/in-response-to-critiques-of-guaranteed-safe-ai,19,2025-01-31T01:43:05.787Z,
Corrupted by Reasoning: Reasoning Language Models Become Free-Riders in Public Goods Games,https://www.alignmentforum.org/posts/M6dXdCbdoLSpHt8v3/corrupted-by-reasoning-reasoning-language-models-become-free,13,2025-04-22T19:25:35.302Z,
Proof idea: SLT to AIT,https://www.alignmentforum.org/posts/3ZBmKDpAJJahRM248/proof-idea-slt-to-ait,17,2025-02-10T23:14:24.538Z,y
AXRP Episode 42 - Owain Evans on LLM Psychology,https://www.alignmentforum.org/posts/ZiacrsqoWHczctTBS/axrp-episode-42-owain-evans-on-llm-psychology,7,2025-06-06T20:20:03.549Z,
Video and transcript of talk on automating alignment research,https://www.alignmentforum.org/posts/TQbptN7F4ijPnQRLy/video-and-transcript-of-talk-on-automating-alignment,11,2025-04-30T17:43:06.557Z,
Renormalization Redux: QFT Techniques for AI Interpretability,https://www.alignmentforum.org/posts/sjr66DBEgyogAbfdf/renormalization-redux-qft-techniques-for-ai-interpretability,18,2025-01-18T03:54:28.652Z,
How to mitigate sandbagging,https://www.alignmentforum.org/posts/Qv5PkrJYAaiBuEJjB/how-to-mitigate-sandbagging-1,12,2025-03-23T17:19:07.452Z,
Why modelling multi-objective homeostasis is essential for AI alignment (and how it helps with AI safety as well). Subtleties and Open Challenges.,https://www.alignmentforum.org/posts/vGeuBKQ7nzPnn5f7A/why-modelling-multi-objective-homeostasis-is-essential-for,12,2025-01-12T03:37:59.692Z,
Why care about AI personhood?,https://www.alignmentforum.org/posts/Gh4fyTxGCpWS4jT82/why-care-about-ai-personhood,20,2025-01-26T11:24:45.596Z,
EIS XV: A New Proof of Concept for Useful Interpretability,https://www.alignmentforum.org/posts/HYkg6kwqhCQT5uYuK/eis-xv-a-new-proof-of-concept-for-useful-interpretability,20,2025-03-17T20:05:30.580Z,
Forecasting AI Forecasting,https://www.alignmentforum.org/posts/LczkzW4uPaQS3joj8/forecasting-ai-forecasting,2,2025-06-23T13:39:41.097Z,
Potentially Useful Projects in Wise AI,https://www.alignmentforum.org/posts/24RvfBgyt72XzfEYS/potentially-useful-projects-in-wise-ai,4,2025-06-05T08:13:42.839Z,
The Internal Model Principle: A Straightforward Explanation,https://www.alignmentforum.org/posts/fsQzYmuFN65vHcEcH/the-internal-model-principle-a-straightforward-explanation,6,2025-04-12T10:58:51.479Z,
Estimating the Probability of Sampling a Trained Neural Network at Random,https://www.alignmentforum.org/posts/ubhqr7n57S4nwgc56/estimating-the-probability-of-sampling-a-trained-neural,17,2025-03-01T02:11:56.313Z,
"Extended analogy between humans, corporations, and AIs.",https://www.alignmentforum.org/posts/bsTzgG3cRrsgbGtCc/extended-analogy-between-humans-corporations-and-ais,17,2025-02-13T00:03:13.956Z,
AXRP Episode 40 - Jason Gross on Compact Proofs and Interpretability,https://www.alignmentforum.org/posts/TFToqpaKMhcjAEY5E/axrp-episode-40-jason-gross-on-compact-proofs-and,13,2025-03-28T18:40:01.856Z,
"Illusory Safety: Redteaming DeepSeek R1 and the Strongest Fine-Tunable Models of OpenAI, Anthropic, and Google",https://www.alignmentforum.org/posts/zjqrSKZuRLnjAniyo/illusory-safety-redteaming-deepseek-r1-and-the-strongest,16,2025-02-07T03:57:30.904Z,
Takeaways From Our Recent Work on SAE Probing,https://www.alignmentforum.org/posts/osNKnwiJWHxDYvQTD/takeaways-from-our-recent-work-on-sae-probing,12,2025-03-03T19:50:16.692Z,
Is alignment reducible to becoming more coherent?,https://www.alignmentforum.org/posts/nuDJNyG5XLQjtvaeg/is-alignment-reducible-to-becoming-more-coherent,9,2025-04-22T23:47:34.531Z,
Existing UDTs test the limits of Bayesianism (and consistency),https://www.alignmentforum.org/posts/w2QmWzZBTBJ76xuwH/existing-udts-test-the-limits-of-bayesianism-and-consistency,7,2025-03-12T04:09:11.615Z,
Abstract Mathematical Concepts vs. Abstractions Over Real-World Systems,https://www.alignmentforum.org/posts/T6xSXiXF3WF6TmCyN/abstract-mathematical-concepts-vs-abstractions-over-real,19,2025-02-18T18:04:46.717Z,
Deep sparse autoencoders yield interpretable features too,https://www.alignmentforum.org/posts/tLCBJn3NcSNzi5xng/deep-sparse-autoencoders-yield-interpretable-features-too,13,2025-02-23T05:46:59.189Z,
SimpleStories: A Better Synthetic Dataset and Tiny Models for Interpretability,https://www.alignmentforum.org/posts/mjzj3pfXrjspAPkbM/simplestories-a-better-synthetic-dataset-and-tiny-models-for,3,2025-05-03T14:04:08.871Z,
Eliciting bad contexts,https://www.alignmentforum.org/posts/inkzPmpTFBdXoKLqC/eliciting-bad-contexts,21,2025-01-24T10:39:39.358Z,
Notes on handling non-concentrated failures with AI control: high level methods and different regimes,https://www.alignmentforum.org/posts/D5H5vcnhBz8G4dh6v/notes-on-handling-non-concentrated-failures-with-ai-control,16,2025-03-24T01:00:38.222Z,
Building Big Science from the Bottom-Up: A Fractal Approach to AI Safety,https://www.alignmentforum.org/posts/5JJ4AxQRzJGWdj4pN/building-big-science-from-the-bottom-up-a-fractal-approach,16,2025-01-07T03:08:51.447Z,
"Why Aligning an LLM is Hard, and How to Make it Easier",https://www.alignmentforum.org/posts/XdpJsY6QGdCbvo2dS/why-aligning-an-llm-is-hard-and-how-to-make-it-easier,11,2025-01-23T06:44:04.048Z,
AI Safety & Entrepreneurship v1.0,https://www.alignmentforum.org/posts/WbBe7LKNwv7fBgqii/ai-safety-and-entrepreneurship-v1-0,7,2025-04-26T14:37:11.037Z,
What if Agent-4 breaks out?,https://www.alignmentforum.org/posts/3GawHDBhsgg3CGbNb/what-if-agent-4-breaks-out,3,2025-05-15T09:15:17.340Z,
The Theoretical Reward Learning Research Agenda: Introduction and Motivation,https://www.alignmentforum.org/posts/pJ3mDD7LfEwp3s5vG/the-theoretical-reward-learning-research-agenda-introduction,15,2025-02-28T19:20:30.168Z,
"S-Expressions as a Design Language: A Tool for Deconfusion in Alignment
",https://www.alignmentforum.org/posts/aeBAkCPqWHscrAZKA/s-expressions-as-a-design-language-a-tool-for-deconfusion-in,5,2025-06-19T19:03:13.418Z,
"Grammars, subgrammars, and combinatorics of generalization in transformers",https://www.alignmentforum.org/posts/sjoW35fgBJ82ADuND/grammars-subgrammars-and-combinatorics-of-generalization-in,18,2025-01-02T09:37:23.191Z,
Investigating Accidental Misalignment: Causal Effects of Fine-Tuning Data on Model Vulnerability,https://www.alignmentforum.org/posts/zmSafQK7JjmfydAtT/investigating-accidental-misalignment-causal-effects-of-fine,1,2025-06-11T19:30:17.553Z,
"Alignment first, intelligence later",https://www.alignmentforum.org/posts/sDuWXb8cPXZ2yHdH4/alignment-first-intelligence-later,0,2025-03-30T22:26:55.302Z,
"Sleep peacefully: no hidden reasoning detected in LLMs. Well, at least in small ones. ",https://www.alignmentforum.org/posts/6GgHMBsAjD74KZAsB/sleep-peacefully-no-hidden-reasoning-detected-in-llms-well,9,2025-04-04T20:49:59.031Z,
"Don't you mean ""the most *conditionally* forbidden technique?""",https://www.alignmentforum.org/posts/jokdGg3wciyoeZTN2/don-t-you-mean-the-most-conditionally-forbidden-technique-1,6,2025-04-26T03:45:38.026Z,
Takeaways from sketching a control safety case,https://www.alignmentforum.org/posts/y6rBarAPTLmuhn9PJ/takeaways-from-sketching-a-control-safety-case,21,2025-01-31T04:43:45.917Z,
Will AI R&D Automation Cause a Software Intelligence Explosion?,https://www.alignmentforum.org/posts/tMHPQ5SDzm8gugoBN/will-ai-r-and-d-automation-cause-a-software-intelligence-1,10,2025-03-26T18:12:02.524Z,
[Replication] Crosscoder-based Stage-Wise Model Diffing,https://www.alignmentforum.org/posts/hxxramAB82tjtpiQu/replication-crosscoder-based-stage-wise-model-diffing-2,7,2025-03-22T18:35:19.003Z,
Early Experiments in Human Auditing for AI Control,https://www.alignmentforum.org/posts/2CJyfgaJQk8pyRSCp/auditing-1,16,2025-01-23T01:34:31.682Z,
Extending control evaluations to non-scheming threats,https://www.alignmentforum.org/posts/ArK2YhmpTy8XftubN/extending-control-evaluations-to-non-scheming-threats,15,2025-01-12T01:42:54.614Z,
When should we worry about AI power-seeking?,https://www.alignmentforum.org/posts/QyaJQ82FSsKXrvj7t/when-should-we-worry-about-ai-power-seeking,13,2025-02-19T19:44:25.062Z,
Outer Alignment is the Necessary Compliment to AI 2027's Best Case Scenario,https://www.alignmentforum.org/posts/rpKPgzjr3tPkDZChg/outer-alignment-is-the-necessary-compliment-to-ai-2027-s,2,2025-06-09T15:43:40.505Z,
When does capability elicitation bound risk?,https://www.alignmentforum.org/posts/u3taQsgxqCzrgErMM/when-does-capability-elicitation-bound-risk,18,2025-01-22T03:42:36.289Z,
Literature Review of Text AutoEncoders,https://www.alignmentforum.org/posts/PjEtMQ65sux4mgrCT/literature-review-of-text-autoencoders-1,4,2025-02-19T21:54:14.905Z,
Misspecification in Inverse Reinforcement Learning,https://www.alignmentforum.org/posts/orCtTgQkWwwD3XN87/misspecification-in-inverse-reinforcement-learning,10,2025-02-28T19:24:49.204Z,
Open Challenges in Representation Engineering,https://www.alignmentforum.org/posts/4nfDXmacAFgdhPwo3/open-challenges-in-representation-engineering-2,8,2025-04-03T19:21:45.971Z,y
Rebuttals for ~all criticisms of AIXI,https://www.alignmentforum.org/posts/TXmXrnFqLqKDx882e/rebuttals-for-all-criticisms-of-aixi,13,2025-01-07T17:41:10.557Z,
Can LLM-based models do model-based planning?,https://www.alignmentforum.org/posts/d2pohPvyve5sbTsTZ/can-llm-based-models-do-model-based-planning-1,5,2025-04-16T12:38:00.793Z,
Revising Stages-Oversight Reveals Greater Situational Awareness in LLMs,https://www.alignmentforum.org/posts/5naJwQnbb5bwPCCFz/revising-stages-oversight,4,2025-03-12T17:56:31.910Z,
Measuring Beliefs of Language Models During Chain-of-Thought Reasoning,https://www.alignmentforum.org/posts/a86uAnPykqNtmEbDH/measuring-beliefs-of-language-models-during-chain-of-thought-1,5,2025-04-18T22:56:28.727Z,
Is weak-to-strong generalization an alignment technique?Q,https://www.alignmentforum.org/posts/NPBjELgHFEeHTgDrK/is-weak-to-strong-generalization-an-alignment-technique,14,2025-01-31T07:13:03.332Z,
AI threatens to orchestrate sustainable social reform,https://www.alignmentforum.org/posts/MSt4ZmFE7nD9CetMT/ai-threatens-to-orchestrate-sustainable-social-reform,7,2025-04-02T03:04:43.182Z,
Is AI Physical?,https://www.alignmentforum.org/posts/yiqcFdAq8nqfMPGmS/is-ai-physical,9,2025-01-14T21:21:39.999Z,
Are we trying to figure out if AI is conscious?,https://www.alignmentforum.org/posts/TqBTdiWzAhA7qa5ke/are-we-trying-to-figure-out-if-ai-is-conscious-1,6,2025-01-27T01:05:07.001Z,
An idea for avoiding neuralese architectures,https://www.alignmentforum.org/posts/3W8HZe8mcyoo4qGkB/an-idea-for-avoiding-neuralese-architectures-1,2,2025-04-03T22:23:21.653Z,
How to Contribute to Theoretical Reward Learning Research,https://www.alignmentforum.org/posts/ByG7g3eSYhzduqg6s/how-to-contribute-to-theoretical-reward-learning-research,10,2025-02-28T19:27:52.552Z,
Other Papers About the Theory of Reward Learning,https://www.alignmentforum.org/posts/chbFoBYzkap2y46QD/other-papers-about-the-theory-of-reward-learning,10,2025-02-28T19:26:11.490Z,
Partial Identifiability in Reward Learning,https://www.alignmentforum.org/posts/nk4ifEfJYG7J38qwv/partial-identifiability-in-reward-learning,9,2025-02-28T19:23:30.738Z,
Measuring Schelling Coordination - Reflections on Subversion Strategy Eval,https://www.alignmentforum.org/posts/vsAxLSShMgy5Qjub5/measuring-schelling-coordination-reflections-on-subversion,4,2025-05-12T19:06:00.449Z,
Defining and Characterising Reward Hacking,https://www.alignmentforum.org/posts/vnNdpaXehmefXSe2H/defining-and-characterising-reward-hacking,9,2025-02-28T19:25:42.777Z,
When is it Better to Train on the Alignment Proxy?,https://www.alignmentforum.org/posts/zLvt2xHopdbArfvbM/when-is-it-better-to-train-on-the-alignment-proxy,9,2025-03-11T13:35:51.152Z,
Powerful Predictions,https://www.alignmentforum.org/posts/LT73KyjG9aFxuvvoT/powerful-predictions,2,2025-06-05T10:44:33.457Z,
What makes a theory of intelligence useful?,https://www.alignmentforum.org/posts/3donrE5vFHeMJFLY9/what-makes-a-theory-of-intelligence-useful,4,2025-02-20T19:22:29.725Z,
Won't vs. Can't: Sandbagging-like Behavior from Claude Models,https://www.alignmentforum.org/posts/oJHgwBCJ2tnrCpZun/won-t-vs-can-t-sandbagging-like-behavior-from-claude-models-2,6,2025-02-19T20:47:06.792Z,
Will the Need to Retrain AI Models from Scratch Block a Software Intelligence Explosion?,https://www.alignmentforum.org/posts/5CgxLpD2Fi9FkDFD4/will-the-need-to-retrain-ai-models-from-scratch-block-a-1,6,2025-03-28T14:12:02.163Z,
TAMing The Alignment Problem,https://www.alignmentforum.org/posts/MoW6wNKdaB4DbqzGr/taming-the-alignment-problem,2,2025-04-07T08:47:22.080Z,
Half-baked idea: a straightforward method for learning environmental goals?,https://www.alignmentforum.org/posts/hzXFthtujYhDXMgFd/tbd-3,4,2025-02-04T06:56:31.813Z,
"Linkpost to a Summary of ""Imagining and building wise machines: The centrality of AI metacognition"" by Johnson, Karimi, Bengio, et al.",https://www.alignmentforum.org/posts/axKyBatdWtce48Zda/linkpost-to-a-summary-of-imagining-and-building-wise,4,2025-04-10T11:54:37.484Z,
Defense Against the Dark Prompts: Mitigating Best-of-N Jailbreaking with Prompt Evaluation,https://www.alignmentforum.org/posts/5MyB8k8hSJRfa3udi/defense-against-the-dark-prompts-mitigating-best-of-n,11,2025-01-31T15:36:01.050Z,
Commitment Races are a technical problem ASI can easily solve,https://www.alignmentforum.org/posts/ReFEceKCnCsqHLkTY/commitment-races-are-a-technical-problem-asi-can-easily-1,3,2025-04-12T22:22:47.790Z,
Measuring Nonlinear Feature Interactions in Sparse Crosscoders [Project Proposal],https://www.alignmentforum.org/posts/RjrGAqJbk849Q7PHP/measuring-nonlinear-feature-interactions-in-sparse,8,2025-01-06T04:22:12.633Z,
Proof Section to an Introduction to Reinforcement Learning for Understanding Infra-Bayesianism,https://www.alignmentforum.org/posts/KRPWJiCfcmdB5smdb/proof-section-to-an-introduction-to-reinforcement-learning,2,2025-05-17T02:36:22.105Z,
[Research sprint] Single-model crosscoder feature ablation and steering,https://www.alignmentforum.org/posts/TEx9J7r5YsRfMuhpX/research-sprint-single-model-crosscoder-feature-ablation-and,3,2025-04-06T14:42:30.357Z,
Mind the Coherence Gap: Lessons from Steering Llama with Goodfire,https://www.alignmentforum.org/posts/6dpKhtniqR3rnstnL/mind-the-coherence-gap-lessons-from-steering-llama-with-1,2,2025-05-09T21:29:35.232Z,
Tied Crosscoders: Explaining Chat Behavior from Base Model,https://www.alignmentforum.org/posts/3T8eKyaPvDDm2wzor/research-question,4,2025-03-22T18:07:21.751Z,
Levels of analysis for thinking about agency,https://www.alignmentforum.org/posts/9w62Pjz5enFkzHW59/levels-of-analysis-for-thinking-about-agency,4,2025-02-26T04:24:24.583Z,
Forecasting AGI: Insights from Prediction Markets and Metaculus,https://www.alignmentforum.org/posts/dRbvHfEwb6Cuf6xn3/forecasting-agi-insights-from-prediction-markets-and-1,4,2025-02-04T13:03:45.927Z,
STARC: A General Framework For Quantifying Differences Between Reward Functions,https://www.alignmentforum.org/posts/EH5YPCAoy6urmz5sF/starc-a-general-framework-for-quantifying-differences,6,2025-02-28T19:24:52.965Z,
Musings on Scenario Forecasting and AI,https://www.alignmentforum.org/posts/ZK3SCtu2gz9vq9ffn/musings-on-scenario-forecasting-and-ai,2,2025-03-06T12:28:19.662Z,
Transformer Dynamics: a neuro-inspired approach to MechInterp,https://www.alignmentforum.org/posts/vucxxwdJARR3cqaPc/transformer-dynamics-a-neuro-inspired-approach-to-mechinterp,4,2025-02-22T21:33:23.855Z,
The Elicitation Game: Evaluating capability elicitation techniques,https://www.alignmentforum.org/posts/6QA5eHBEqpAicCwbh/the-elicitation-game-evaluating-capability-elicitation,4,2025-02-27T20:33:24.861Z,
Using Prompt Evaluation to Combat Bio-Weapon Research,https://www.alignmentforum.org/posts/sfucF8Mhcn7zmWQ8Y/using-prompt-evaluation-to-combat-bio-weapon-research,7,2025-02-19T12:39:00.491Z,
Misspecification in Inverse Reinforcement Learning - Part II,https://www.alignmentforum.org/posts/iKiREYhxLSjCkDGPa/misspecification-in-inverse-reinforcement-learning-part-ii,4,2025-02-28T19:24:59.570Z,
Amplifying the Computational No-Coincidence Conjecture,https://www.alignmentforum.org/posts/PxhCvDSfSqJYNXtaw/amplifying-the-computational-no-coincidence-conjecture,4,2025-03-07T21:29:54.933Z,
Emergent Misalignment and Emergent Alignment,https://www.alignmentforum.org/posts/aFtCG6JcG3ciaev9G/emergent-misalignment-and-emergent-alignment,2,2025-04-03T08:04:38.150Z,
"Training Data Attribution:
Examining Its Adoption & Use Cases",https://www.alignmentforum.org/posts/aHgvu6mz8gqQQqJwP/training-data-attribution-examining-its-adoption-and-use,1,2025-01-22T15:41:19.744Z,
Are SAE features from the Base Model still meaningful to LLaVA?,https://www.alignmentforum.org/posts/8JTi7N3nQmjoRRuMD/are-sae-features-from-the-base-model-still-meaningful-to-1,2,2025-02-18T22:16:14.449Z,
AXRP Episode 38.6 - Joel Lehman on Positive Visions of AI,https://www.alignmentforum.org/posts/5XC66LN3gS3w6mYnR/axrp-episode-38-6-joel-lehman-on-positive-visions-of-ai,7,2025-01-24T23:00:07.562Z,
How Language Models Understand Nullability,https://www.alignmentforum.org/posts/QqE7Gmcqjm5CX6zxE/how-language-models-understand-nullability,2,2025-03-11T15:57:28.686Z,
AXRP Episode 38.5 - Adrià Garriga-Alonso on Detecting AI Scheming,https://www.alignmentforum.org/posts/MpLmcLBiEbpzv2awg/axrp-episode-38-5-adria-garriga-alonso-on-detecting-ai,7,2025-01-20T00:40:07.077Z,
Sleeping Beauty: an Accuracy-based Approach,https://www.alignmentforum.org/posts/ECLEsydXxvtK3XxMs/sleeping-beauty-an-accuracy-based-approach,1,2025-02-10T15:40:29.619Z,
Cross-Layer Feature Alignment and Steering in Large Language Model,https://www.alignmentforum.org/posts/feknAa3hQgLG2ZAna/cross-layer-feature-alignment-and-steering-in-large-language-2,0,2025-02-08T20:18:20.331Z,y
SAE Training Dataset Influence in Feature Matching and a Hypothesis on Position Features,https://www.alignmentforum.org/posts/ATsvzF77ZsfWzyTak/dataset-sensitivity-in-feature-matching-and-a-hypothesis-on-1,0,2025-02-26T17:05:18.265Z,y
Forecasting AI Futures Resource Hub,https://www.alignmentforum.org/posts/jAPDX72dugrYrvFsd/forecasting-ai-futures-resource-hub,1,2025-03-19T17:26:28.059Z,
"Expanding HarmBench: Investigating Gaps & Extending Adversarial LLM Testing

",https://www.alignmentforum.org/posts/rh2Hzi7NLFdyxYogb/expanding-harmbench-investigating-gaps-and-extending,0,2025-03-03T19:23:20.687Z,y
Simplex Progress Report - July 2025,https://www.alignmentforum.org/posts/fhkurwqhjZopx8DKK/simplex-progress-report-july-2025,48,2025-07-28T21:58:56.847Z,y
Fifty Years Requests for Startups,https://www.alignmentforum.org/posts/7rYphMipLtiEXArPf/fifty-years-requests-for-startups,8,2025-09-15T18:37:56.488Z,
The Alignment Project by UK AISI,https://www.alignmentforum.org/posts/wKTwdgZDo479EhmJL/the-alignment-project-by-uk-aisi-1,13,2025-08-01T09:52:37.314Z,y
Open Philanthropy Technical AI Safety RFP - $40M Available Across 21 Research Areas,https://www.alignmentforum.org/posts/wbJxRNxuezvsGFEWv/open-philanthropy-technical-ai-safety-rfp-usd40m-available,54,2025-02-06T18:58:53.076Z,y
Announcing ILIAD2: ODYSSEY,https://www.alignmentforum.org/posts/WP7TbzzM39agMS77e/announcing-iliad2-odyssey-1,35,2025-04-03T17:01:06.004Z,y
AGI Safety & Alignment @ Google DeepMind is hiring,https://www.alignmentforum.org/posts/wqz5CRzqWkvzoatBG/agi-safety-and-alignment-google-deepmind-is-hiring,52,2025-02-17T21:11:18.970Z,y
Timaeus in 2024,https://www.alignmentforum.org/posts/gGAXSfQaiGBCwBJH5/timaeus-in-2024,38,2025-02-20T23:54:56.939Z,y
OpenAI now has an RL API which is broadly accessible,https://www.alignmentforum.org/posts/HevgiEWLMfzAAC6CD/openai-now-has-an-rl-api-which-is-broadly-accessible,21,2025-06-11T23:39:30.340Z,
Neel Nanda MATS Applications Open (Due Aug 29),https://www.alignmentforum.org/posts/cToqfmDuTX6CvkdKk/neel-nanda-mats-applications-open-due-aug-29,13,2025-07-30T00:55:20.101Z,
Alignment faking CTFs: Apply to my MATS stream,https://www.alignmentforum.org/posts/jWFvsJnJieXnWBb9r/alignment-faking-ctfs-apply-to-my-mats-stream,38,2025-04-04T16:29:02.070Z,y
Agent Foundations 2025 at CMU,https://www.alignmentforum.org/posts/cuf4oMFHEQNKMXRvr/agent-foundations-2025-at-cmu,42,2025-01-19T23:48:22.569Z,y
"Softmax, Emmett Shear's new AI startup focused on ""Organic Alignment""",https://www.alignmentforum.org/posts/QGQiCuE33iHFv9jkv/softmax-emmett-shear-s-new-ai-startup-focused-on-organic,24,2025-03-28T21:23:46.220Z,
Introducing BenchBench: An Industry Standard Benchmark for AI Strength,https://www.alignmentforum.org/posts/vyvsKNFS64WGZbBMb/introducing-benchbench-an-industry-standard-benchmark-for-ai-1,21,2025-04-02T02:11:41.555Z,y
Apply now to Human-Aligned AI Summer School 2025,https://www.alignmentforum.org/posts/JdrmKSzsWmRbgMumf/apply-now-to-human-aligned-ai-summer-school-2025,17,2025-06-06T19:31:40.085Z,
Timaeus is hiring researchers & engineers,https://www.alignmentforum.org/posts/g8e4pz7aHGCpahFR4/timaeus-is-hiring-researchers-and-engineers,27,2025-01-17T19:13:14.739Z,y
Announcement: Learning Theory Online Course,https://www.alignmentforum.org/posts/sPAA9X6basAXsWhau/announcement-learning-theory-online-course,26,2025-01-20T19:55:57.598Z,
The GDM AGI Safety+Alignment Team is Hiring for Applied Interpretability Research,https://www.alignmentforum.org/posts/aG9e5tHfHmBnDqrDy/the-gdm-agi-safety-alignment-team-is-hiring-for-applied,26,2025-02-24T02:17:12.991Z,y
AXRP Episode 43 - David Lindner on Myopic Optimization with Non-myopic Approval,https://www.alignmentforum.org/posts/jHxJ6y8fSx4mqweHp/axrp-episode-43-david-lindner-on-myopic-optimization-with,8,2025-06-15T01:20:02.873Z,
Schmidt Sciences Technical AI Safety RFP on Inference-Time Compute – Deadline: April 30,https://www.alignmentforum.org/posts/tTyQvHedrjFWjurGk/schmidt-sciences-technical-ai-safety-rfp-on-inference-time,10,2025-03-18T18:05:34.757Z,
What is the most impressive game LLMs can play well?Q,https://www.alignmentforum.org/posts/3c5tx5WjZ5Yvniq6Y/what-is-the-most-impressive-game-llms-can-play-well,9,2025-01-08T19:38:18.530Z,