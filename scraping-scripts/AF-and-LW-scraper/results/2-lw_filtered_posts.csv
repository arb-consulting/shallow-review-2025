id,title,url,pageUrl,author,createdAt,score,commentCount,wordCount,tags,excerpt
DiyqXDE5xzPAdNNdi,Conceptual reasoning dataset v0.1 available (AI for AI safety/AI for philosophy),https://www.lesswrong.com/posts/DiyqXDE5xzPAdNNdi/conceptual-reasoning-dataset-v0-1-available-ai-for-ai-safety,https://www.lesswrong.com/posts/DiyqXDE5xzPAdNNdi/conceptual-reasoning-dataset-v0-1-available-ai-for-ai-safety,,2025-11-12T01:12:52.912000+00:00,6,0,,,
sBWBEfa2WPPjsmzpG,Learnings from the Zurich AI Safety Day,https://www.lesswrong.com/posts/sBWBEfa2WPPjsmzpG/learnings-from-the-zurich-ai-safety-day,https://www.lesswrong.com/posts/sBWBEfa2WPPjsmzpG/learnings-from-the-zurich-ai-safety-day,,2025-11-11T17:00:00.168000+00:00,8,0,,,
tu4p9R3X57zvbwzwv,What is Happening in AI Governance?,https://www.lesswrong.com/posts/tu4p9R3X57zvbwzwv/what-is-happening-in-ai-governance,https://www.lesswrong.com/posts/tu4p9R3X57zvbwzwv/what-is-happening-in-ai-governance,,2025-11-11T15:59:47.425000+00:00,6,0,,,
msWjycrJygfGTATXg,Evolution's Alignment Solution: Why Burnout Prevents Monsters,https://www.lesswrong.com/posts/msWjycrJygfGTATXg/evolution-s-alignment-solution-why-burnout-prevents-monsters,https://www.lesswrong.com/posts/msWjycrJygfGTATXg/evolution-s-alignment-solution-why-burnout-prevents-monsters,,2025-11-11T13:32:16.726000+00:00,2,0,,,
9i6fHMn2vTqyzAi9o,When does Claude sabotage code? An Agentic Misalignment follow-up,https://www.lesswrong.com/posts/9i6fHMn2vTqyzAi9o/when-does-claude-sabotage-code-an-agentic-misalignment,https://www.lesswrong.com/posts/9i6fHMn2vTqyzAi9o/when-does-claude-sabotage-code-an-agentic-misalignment,,2025-11-09T23:11:43.916000+00:00,18,0,,,
4aeshNuEKF8Ak356D,Omniscaling to MNIST,https://www.lesswrong.com/posts/4aeshNuEKF8Ak356D/omniscaling-to-mnist,https://www.lesswrong.com/posts/4aeshNuEKF8Ak356D/omniscaling-to-mnist,,2025-11-08T19:42:15.988000+00:00,73,3,,,
AduRjjJneJZrtne6i,Start an AI safety group with the Pathfinder Fellowship,https://www.lesswrong.com/posts/AduRjjJneJZrtne6i/start-an-ai-safety-group-with-the-pathfinder-fellowship-1,https://www.lesswrong.com/posts/AduRjjJneJZrtne6i/start-an-ai-safety-group-with-the-pathfinder-fellowship-1,,2025-11-07T21:05:16.732000+00:00,2,0,,,
SeraekG5w8MH6MRWJ,"Plans to build AGI with nuclear reactor-like safety lack 'systematic thinking,' say researchers",https://www.lesswrong.com/posts/SeraekG5w8MH6MRWJ/plans-to-build-agi-with-nuclear-reactor-like-safety-lack,https://www.lesswrong.com/posts/SeraekG5w8MH6MRWJ/plans-to-build-agi-with-nuclear-reactor-like-safety-lack,,2025-11-07T16:25:55.357000+00:00,-1,2,,,
AJANBeJb2p39su6F9,[CS2881r][Week 8] When Agents Prefer Hacking To Failure: Evaluating Misalignment Under Pressure,https://www.lesswrong.com/posts/AJANBeJb2p39su6F9/cs2881r-week-8-when-agents-prefer-hacking-to-failure,https://www.lesswrong.com/posts/AJANBeJb2p39su6F9/cs2881r-week-8-when-agents-prefer-hacking-to-failure,,2025-11-07T05:45:00.330000+00:00,1,0,,,
vPgcRva2PuJBzW944,Technical AI Safety Roles at Open Philanthropy,https://www.lesswrong.com/posts/vPgcRva2PuJBzW944/technical-ai-safety-roles-at-open-philanthropy,https://www.lesswrong.com/posts/vPgcRva2PuJBzW944/technical-ai-safety-roles-at-open-philanthropy,,2025-11-06T00:49:31.316000+00:00,3,0,,,
Ge7dqJXzCAxMDS664,AI Safety at the Frontier: Paper Highlights of October 2025,https://www.lesswrong.com/posts/Ge7dqJXzCAxMDS664/ai-safety-at-the-frontier-paper-highlights-of-october-2025,https://www.lesswrong.com/posts/Ge7dqJXzCAxMDS664/ai-safety-at-the-frontier-paper-highlights-of-october-2025,,2025-11-05T13:39:28.641000+00:00,7,0,,,
ciw6DCdywoXk7yrdw,New homepage for AI safety resources – AISafety.com redesign,https://www.lesswrong.com/posts/ciw6DCdywoXk7yrdw/new-homepage-for-ai-safety-resources-aisafety-com-redesign,https://www.lesswrong.com/posts/ciw6DCdywoXk7yrdw/new-homepage-for-ai-safety-resources-aisafety-com-redesign,,2025-11-05T10:33:04.719000+00:00,35,2,,,
2AbQtjDij9ftZFpFc,Why Safety Constraints in LLMs Are Easily Breakable? Knowledge as a Network of Gated Circuits,https://www.lesswrong.com/posts/2AbQtjDij9ftZFpFc/why-safety-constraints-in-llms-are-easily-breakable,https://www.lesswrong.com/posts/2AbQtjDij9ftZFpFc/why-safety-constraints-in-llms-are-easily-breakable,,2025-11-05T05:20:59.546000+00:00,12,0,,,
PMc65HgRFvBimEpmJ,Legible vs. Illegible AI Safety Problems,https://www.lesswrong.com/posts/PMc65HgRFvBimEpmJ/legible-vs-illegible-ai-safety-problems,https://www.lesswrong.com/posts/PMc65HgRFvBimEpmJ/legible-vs-illegible-ai-safety-problems,,2025-11-04T21:39:07.202000+00:00,313,92,,,
k57RZxRTvuqTo65G5,AI Safety Camp 11,https://www.lesswrong.com/posts/k57RZxRTvuqTo65G5/ai-safety-camp-11,https://www.lesswrong.com/posts/k57RZxRTvuqTo65G5/ai-safety-camp-11,,2025-11-04T14:56:31.542000+00:00,5,0,,,
vD7ZeGskbF9CyMXzn,"AI Safety Unconference, Melbourne 2025",https://www.lesswrong.com/events/vD7ZeGskbF9CyMXzn/ai-safety-unconference-melbourne-2025,https://www.lesswrong.com/events/vD7ZeGskbF9CyMXzn/ai-safety-unconference-melbourne-2025,,2025-11-03T19:36:19.812000+00:00,1,0,,,
4kCKknEDo9rb27fM7,Falling AI Costs and the Proliferation of Offensive Capabilities,https://www.lesswrong.com/posts/4kCKknEDo9rb27fM7/falling-ai-costs-and-the-proliferation-of-offensive,https://www.lesswrong.com/posts/4kCKknEDo9rb27fM7/falling-ai-costs-and-the-proliferation-of-offensive,,2025-11-03T17:32:18.940000+00:00,14,2,,,
86NQZFN3SSAomboGv,The EU could hold AI capabilities development hostage if they wanted to,https://www.lesswrong.com/posts/86NQZFN3SSAomboGv/the-eu-could-hold-ai-capabilities-development-hostage-if,https://www.lesswrong.com/posts/86NQZFN3SSAomboGv/the-eu-could-hold-ai-capabilities-development-hostage-if,,2025-11-03T16:54:52.705000+00:00,3,0,,,
5DY2gZi3hEJZfEkhw,Doom from a Solution to the Alignment Problem,https://www.lesswrong.com/posts/5DY2gZi3hEJZfEkhw/doom-from-a-solution-to-the-alignment-problem,https://www.lesswrong.com/posts/5DY2gZi3hEJZfEkhw/doom-from-a-solution-to-the-alignment-problem,,2025-11-02T16:37:17.593000+00:00,0,0,,,
H3Thf3CFjfwH4eyX5,"Social media feeds 'misaligned' when viewed through AI safety framework, show researchers",https://www.lesswrong.com/posts/H3Thf3CFjfwH4eyX5/social-media-feeds-misaligned-when-viewed-through-ai-safety,https://www.lesswrong.com/posts/H3Thf3CFjfwH4eyX5/social-media-feeds-misaligned-when-viewed-through-ai-safety,,2025-10-31T16:40:46.069000+00:00,13,3,,,
qgehQxiTXj53X49mM,"Sonnet 4.5's eval gaming seriously undermines alignment evals, and this seems caused by training on alignment evals",https://www.lesswrong.com/posts/qgehQxiTXj53X49mM/sonnet-4-5-s-eval-gaming-seriously-undermines-alignment,https://www.lesswrong.com/posts/qgehQxiTXj53X49mM/sonnet-4-5-s-eval-gaming-seriously-undermines-alignment,,2025-10-30T15:34:32.336000+00:00,139,20,,,
EF5zBhaptNebzhwr3,"Quotes on OpenAI's timelines to automated research, safety research, and safety collaborations before  recursive self improvement",https://www.lesswrong.com/posts/EF5zBhaptNebzhwr3/quotes-on-openai-s-timelines-to-automated-research-safety,https://www.lesswrong.com/posts/EF5zBhaptNebzhwr3/quotes-on-openai-s-timelines-to-automated-research-safety,,2025-10-29T21:47:33.757000+00:00,15,0,,,
GY3WkXocymyYpi2hA,Why Civilizations Are Unstable (And What This Means for AI Alignment),https://www.lesswrong.com/posts/GY3WkXocymyYpi2hA/why-civilizations-are-unstable-and-what-this-means-for-ai,https://www.lesswrong.com/posts/GY3WkXocymyYpi2hA/why-civilizations-are-unstable-and-what-this-means-for-ai,,2025-10-29T12:27:53.751000+00:00,10,6,,,
xCzKwWmhcEKkeytys,What can we learn from parent-child-alignment for AI?,https://www.lesswrong.com/posts/xCzKwWmhcEKkeytys/what-can-we-learn-from-parent-child-alignment-for-ai,https://www.lesswrong.com/posts/xCzKwWmhcEKkeytys/what-can-we-learn-from-parent-child-alignment-for-ai,,2025-10-29T08:02:56.220000+00:00,16,4,,,
bA9zzcHhcZ7BEfKhi,Why Would we get Inner Misalignment by Default?,https://www.lesswrong.com/posts/bA9zzcHhcZ7BEfKhi/why-would-we-get-inner-misalignment-by-default,https://www.lesswrong.com/posts/bA9zzcHhcZ7BEfKhi/why-would-we-get-inner-misalignment-by-default,,2025-10-29T01:23:14.921000+00:00,3,0,,,
sLZQrwQnPswNTEbWi,"Upcoming Workshop on Post-AGI Economics, Culture, and Governance",https://www.lesswrong.com/posts/sLZQrwQnPswNTEbWi/upcoming-workshop-on-post-agi-economics-culture-and,https://www.lesswrong.com/posts/sLZQrwQnPswNTEbWi/upcoming-workshop-on-post-agi-economics-culture-and,,2025-10-28T21:55:42.867000+00:00,36,1,,,
2RtuThoZwP4o8aEpS,Introducing the Epoch Capabilities Index (ECI),https://www.lesswrong.com/posts/2RtuThoZwP4o8aEpS/introducing-the-epoch-capabilities-index-eci,https://www.lesswrong.com/posts/2RtuThoZwP4o8aEpS/introducing-the-epoch-capabilities-index-eci,,2025-10-28T18:23:03.194000+00:00,62,9,,,
TPGJZtzviqkjF5PSJ,Call for mentors from AI Safety and academia. Sci.STEPS mentorship program,https://www.lesswrong.com/posts/TPGJZtzviqkjF5PSJ/call-for-mentors-from-ai-safety-and-academia-sci-steps,https://www.lesswrong.com/posts/TPGJZtzviqkjF5PSJ/call-for-mentors-from-ai-safety-and-academia-sci-steps,,2025-10-28T13:41:01.636000+00:00,7,0,,,
44SHzqD4DvshbQtbj,What were mistakes of AI Safety field-building? How can we avoid them while we build the AI Welfare?,https://www.lesswrong.com/posts/44SHzqD4DvshbQtbj/what-were-mistakes-of-ai-safety-field-building-how-can-we,https://www.lesswrong.com/posts/44SHzqD4DvshbQtbj/what-were-mistakes-of-ai-safety-field-building-how-can-we,,2025-10-28T02:50:03.616000+00:00,1,0,,,
hgMDvLyomQjpKiG2v,[CS 2881r] Can We Prompt Our Way to Safety? Comparing System Prompt Styles and Post-Training Effects on Safety Benchmarks,https://www.lesswrong.com/posts/hgMDvLyomQjpKiG2v/cs-2881r-can-we-prompt-our-way-to-safety-comparing-system,https://www.lesswrong.com/posts/hgMDvLyomQjpKiG2v/cs-2881r-can-we-prompt-our-way-to-safety-comparing-system,,2025-10-28T02:38:24.843000+00:00,4,0,,,
dwpXvweBrJwErse3L,All the labs AI safety plans: 2025 edition,https://www.lesswrong.com/posts/dwpXvweBrJwErse3L/all-the-labs-ai-safety-plans-2025-edition,https://www.lesswrong.com/posts/dwpXvweBrJwErse3L/all-the-labs-ai-safety-plans-2025-edition,,2025-10-28T00:25:07.919000+00:00,47,2,,,
CFA8W6WCodEZdjqYE,AIs should also refuse to work on capabilities research,https://www.lesswrong.com/posts/CFA8W6WCodEZdjqYE/ais-should-also-refuse-to-work-on-capabilities-research,https://www.lesswrong.com/posts/CFA8W6WCodEZdjqYE/ais-should-also-refuse-to-work-on-capabilities-research,,2025-10-27T08:42:35.190000+00:00,150,20,,,
mtGpdtDdmkRC3ZBuz,List of lists of project ideas in AI Safety,https://www.lesswrong.com/posts/mtGpdtDdmkRC3ZBuz/list-of-lists-of-project-ideas-in-ai-safety,https://www.lesswrong.com/posts/mtGpdtDdmkRC3ZBuz/list-of-lists-of-project-ideas-in-ai-safety,,2025-10-27T01:28:44.435000+00:00,6,0,,,
dHLdf8SB8oW5L27gg,On Fleshling Safety: A Debate by Klurl and Trapaucius.,https://www.lesswrong.com/posts/dHLdf8SB8oW5L27gg/on-fleshling-safety-a-debate-by-klurl-and-trapaucius,https://www.lesswrong.com/posts/dHLdf8SB8oW5L27gg/on-fleshling-safety-a-debate-by-klurl-and-trapaucius,,2025-10-26T23:44:04.676000+00:00,238,52,,,
87iDbBM3Qaf4q4gN4,The Future of Interpretability is Geometric,https://www.lesswrong.com/posts/87iDbBM3Qaf4q4gN4/the-future-of-interpretability-is-geometric,https://www.lesswrong.com/posts/87iDbBM3Qaf4q4gN4/the-future-of-interpretability-is-geometric,,2025-10-24T18:32:25.390000+00:00,22,0,,,
CCT7Qc8rSeRs7r5GL,Beliefs about formal methods and AI safety,https://www.lesswrong.com/posts/CCT7Qc8rSeRs7r5GL/beliefs-about-formal-methods-and-ai-safety,https://www.lesswrong.com/posts/CCT7Qc8rSeRs7r5GL/beliefs-about-formal-methods-and-ai-safety,,2025-10-23T16:43:24.911000+00:00,32,0,,,
6DfWFtL7mcs3vnHPn,Should AI Developers Remove Discussion of AI Misalignment from AI Training Data?,https://www.lesswrong.com/posts/6DfWFtL7mcs3vnHPn/should-ai-developers-remove-discussion-of-ai-misalignment,https://www.lesswrong.com/posts/6DfWFtL7mcs3vnHPn/should-ai-developers-remove-discussion-of-ai-misalignment,,2025-10-23T15:12:51.392000+00:00,43,3,,,
Em9sihEZmbofZKc2t,"A Concrete Roadmap towards Safety Cases based on
Chain-of-Thought Monitoring",https://www.lesswrong.com/posts/Em9sihEZmbofZKc2t/a-concrete-roadmap-towards-safety-cases-based-on-chain-of,https://www.lesswrong.com/posts/Em9sihEZmbofZKc2t/a-concrete-roadmap-towards-safety-cases-based-on-chain-of,,2025-10-23T11:34:55.397000+00:00,34,2,,,
KbJ3HjKEjvt8BshSb,Differences in Alignment Behaviour between Single-Agent and Multi-Agent AI Systems,https://www.lesswrong.com/posts/KbJ3HjKEjvt8BshSb/differences-in-alignment-behaviour-between-single-agent-and,https://www.lesswrong.com/posts/KbJ3HjKEjvt8BshSb/differences-in-alignment-behaviour-between-single-agent-and,,2025-10-23T11:17:20.604000+00:00,7,3,,,
524pFXTPD8iDWmX4x,Technical Acceleration Methods for AI Safety: Summary from October 2025 Symposium,https://www.lesswrong.com/posts/524pFXTPD8iDWmX4x/technical-acceleration-methods-for-ai-safety-summary-from,https://www.lesswrong.com/posts/524pFXTPD8iDWmX4x/technical-acceleration-methods-for-ai-safety-summary-from,,2025-10-22T21:33:02.358000+00:00,25,2,,,
48JJCHYunRh9nSj8e,Why AI alignment matters today,https://www.lesswrong.com/posts/48JJCHYunRh9nSj8e/why-ai-alignment-matters-today,https://www.lesswrong.com/posts/48JJCHYunRh9nSj8e/why-ai-alignment-matters-today,,2025-10-22T21:27:13.240000+00:00,6,0,,,
zmtqmwetKH4nrxXcE,Which side of the AI safety community are you in?,https://www.lesswrong.com/posts/zmtqmwetKH4nrxXcE/which-side-of-the-ai-safety-community-are-you-in,https://www.lesswrong.com/posts/zmtqmwetKH4nrxXcE/which-side-of-the-ai-safety-community-are-you-in,,2025-10-22T21:17:28.465000+00:00,146,88,,,
6zQTpEdEqWpGgcfyZ,Dead-switches as AI safety tools,https://www.lesswrong.com/posts/6zQTpEdEqWpGgcfyZ/dead-switches-as-ai-safety-tools,https://www.lesswrong.com/posts/6zQTpEdEqWpGgcfyZ/dead-switches-as-ai-safety-tools,,2025-10-22T19:57:09.328000+00:00,1,6,,,
n6Rsb2jDpYSfzsbns,Consider donating to AI safety champion Scott Wiener,https://www.lesswrong.com/posts/n6Rsb2jDpYSfzsbns/consider-donating-to-ai-safety-champion-scott-wiener,https://www.lesswrong.com/posts/n6Rsb2jDpYSfzsbns/consider-donating-to-ai-safety-champion-scott-wiener,,2025-10-22T18:40:41.963000+00:00,132,9,,,
uahJ7CrB8oWyRyyvL,[CS 2881r AI Safety] [Week 5] Content Policies,https://www.lesswrong.com/posts/uahJ7CrB8oWyRyyvL/cs-2881r-ai-safety-week-5-content-policies,https://www.lesswrong.com/posts/uahJ7CrB8oWyRyyvL/cs-2881r-ai-safety-week-5-content-policies,,2025-10-16T04:27:14.889000+00:00,1,0,,,
sd3GMjLzBQtXHft5c,Electronics Mechanic -> AI Safety Researcher: A 30-Month Journey to Model Welfare,https://www.lesswrong.com/posts/sd3GMjLzBQtXHft5c/electronics-mechanic-greater-than-ai-safety-researcher-a-30,https://www.lesswrong.com/posts/sd3GMjLzBQtXHft5c/electronics-mechanic-greater-than-ai-safety-researcher-a-30,,2025-10-16T00:43:30.233000+00:00,2,0,,,
3zh4nnjoxDsknQYLw,Enhancing Genomic Foundation Model Robustness through Iterative Black-Box Adversarial Training,https://www.lesswrong.com/posts/3zh4nnjoxDsknQYLw/enhancing-genomic-foundation-model-robustness-through,https://www.lesswrong.com/posts/3zh4nnjoxDsknQYLw/enhancing-genomic-foundation-model-robustness-through,,2025-10-14T20:54:38.032000+00:00,8,0,,,
whkMnqFWKsBm7Gyd7,Recontextualization Mitigates Specification Gaming Without Modifying the Specification,https://www.lesswrong.com/posts/whkMnqFWKsBm7Gyd7/recontextualization-mitigates-specification-gaming-without,https://www.lesswrong.com/posts/whkMnqFWKsBm7Gyd7/recontextualization-mitigates-specification-gaming-without,,2025-10-14T00:53:06.161000+00:00,115,13,,,
6yDrARrRFmSNtRpEv,Live Governance: AI tools for coordination without centralisation,https://www.lesswrong.com/posts/6yDrARrRFmSNtRpEv/live-governance-ai-tools-for-coordination-without,https://www.lesswrong.com/posts/6yDrARrRFmSNtRpEv/live-governance-ai-tools-for-coordination-without,,2025-10-13T08:24:38.746000+00:00,10,0,,,
TKTijrrwtEFytAbhh,The Alignment Problem Isn't Theoretical,https://www.lesswrong.com/posts/TKTijrrwtEFytAbhh/the-alignment-problem-isn-t-theoretical,https://www.lesswrong.com/posts/TKTijrrwtEFytAbhh/the-alignment-problem-isn-t-theoretical,,2025-10-12T03:49:37.677000+00:00,0,1,,,
TxiB6hvnQqxXB5XDJ,Why Future AIs will Require New Alignment Methods,https://www.lesswrong.com/posts/TxiB6hvnQqxXB5XDJ/why-future-ais-will-require-new-alignment-methods,https://www.lesswrong.com/posts/TxiB6hvnQqxXB5XDJ/why-future-ais-will-require-new-alignment-methods,,2025-10-10T14:27:44.233000+00:00,17,7,,,
DDfkcawsHJRnqzagm,"We won’t get docile, brilliant AIs before we solve alignment",https://www.lesswrong.com/posts/DDfkcawsHJRnqzagm/we-won-t-get-docile-brilliant-ais-before-we-solve-alignment,https://www.lesswrong.com/posts/DDfkcawsHJRnqzagm/we-won-t-get-docile-brilliant-ais-before-we-solve-alignment,,2025-10-10T04:11:58.579000+00:00,7,3,,,
e8nMZewwonifENQYB,Assuring Agent Safety Evaluations By Analysing Transcripts ,https://www.lesswrong.com/posts/e8nMZewwonifENQYB/assuring-agent-safety-evaluations-by-analysing-transcripts,https://www.lesswrong.com/posts/e8nMZewwonifENQYB/assuring-agent-safety-evaluations-by-analysing-transcripts,,2025-10-10T00:42:20.634000+00:00,7,0,,,
SFZ7sbgqepxKGa7Hj,Investigating Neural Scaling Laws Emerging from Deep Data Structure,https://www.lesswrong.com/posts/SFZ7sbgqepxKGa7Hj/investigating-neural-scaling-laws-emerging-from-deep-data,https://www.lesswrong.com/posts/SFZ7sbgqepxKGa7Hj/investigating-neural-scaling-laws-emerging-from-deep-data,,2025-10-09T20:11:39.241000+00:00,4,0,,,
HLJoJYi52mxgomujc,Realistic Reward Hacking Induces Different and Deeper Misalignment,https://www.lesswrong.com/posts/HLJoJYi52mxgomujc/realistic-reward-hacking-induces-different-and-deeper-1,https://www.lesswrong.com/posts/HLJoJYi52mxgomujc/realistic-reward-hacking-induces-different-and-deeper-1,,2025-10-09T18:45:07.342000+00:00,127,2,,,
293uEAmHeBM3JX9a7,Alignment progress doesn’t compensate for higher capabilities,https://www.lesswrong.com/posts/293uEAmHeBM3JX9a7/alignment-progress-doesn-t-compensate-for-higher,https://www.lesswrong.com/posts/293uEAmHeBM3JX9a7/alignment-progress-doesn-t-compensate-for-higher,,2025-10-09T16:06:25.762000+00:00,2,0,,,
E8n93nnEaFeXTbHn5,"Plans A, B, C, and D for misalignment risk",https://www.lesswrong.com/posts/E8n93nnEaFeXTbHn5/plans-a-b-c-and-d-for-misalignment-risk,https://www.lesswrong.com/posts/E8n93nnEaFeXTbHn5/plans-a-b-c-and-d-for-misalignment-risk,,2025-10-08T17:18:16.750000+00:00,127,69,,,
rLd7NWNKnRFdnJEgD,Intent alignment seems incoherent,https://www.lesswrong.com/posts/rLd7NWNKnRFdnJEgD/intent-alignment-seems-incoherent,https://www.lesswrong.com/posts/rLd7NWNKnRFdnJEgD/intent-alignment-seems-incoherent,,2025-10-07T23:01:49.160000+00:00,22,2,,,
kffbZGa2yYhc6cakc,Petri: An open-source auditing tool to accelerate AI safety research,https://www.lesswrong.com/posts/kffbZGa2yYhc6cakc/petri-an-open-source-auditing-tool-to-accelerate-ai-safety,https://www.lesswrong.com/posts/kffbZGa2yYhc6cakc/petri-an-open-source-auditing-tool-to-accelerate-ai-safety,,2025-10-07T20:39:16.767000+00:00,77,0,,,
fseLoLt4yh3v4nvee,The Alignment Paradox: Why Transparency Can Breed Deception,https://www.lesswrong.com/posts/fseLoLt4yh3v4nvee/the-alignment-paradox-why-transparency-can-breed-deception,https://www.lesswrong.com/posts/fseLoLt4yh3v4nvee/the-alignment-paradox-why-transparency-can-breed-deception,,2025-10-07T13:28:50.660000+00:00,4,0,,,
8buEtNxCScYpjzgW8,We won’t get AIs smart enough to solve alignment but too dumb to rebel,https://www.lesswrong.com/posts/8buEtNxCScYpjzgW8/we-won-t-get-ais-smart-enough-to-solve-alignment-but-too,https://www.lesswrong.com/posts/8buEtNxCScYpjzgW8/we-won-t-get-ais-smart-enough-to-solve-alignment-but-too,,2025-10-06T21:49:05.595000+00:00,28,16,,,
NZtJSNZvd73EKcP9t,Which differences between sandbagging evaluations and sandbagging safety research are important for control?,https://www.lesswrong.com/posts/NZtJSNZvd73EKcP9t/which-differences-between-sandbagging-evaluations-and,https://www.lesswrong.com/posts/NZtJSNZvd73EKcP9t/which-differences-between-sandbagging-evaluations-and,,2025-10-06T18:20:46.054000+00:00,6,0,,,
55CBrLrXiQgHBb2gF,Alignment Faking Demo for Congressional Staffers,https://www.lesswrong.com/posts/55CBrLrXiQgHBb2gF/alignment-faking-demo-for-congressional-staffers,https://www.lesswrong.com/posts/55CBrLrXiQgHBb2gF/alignment-faking-demo-for-congressional-staffers,,2025-10-06T01:44:03.594000+00:00,20,2,,,
qj4qEwpLhzaHKD2ZN,"Accelerating AI Safety Progress via Technical Methods- Calling Researchers, Founders, and Funders",https://www.lesswrong.com/posts/qj4qEwpLhzaHKD2ZN/accelerating-ai-safety-progress-via-technical-methods,https://www.lesswrong.com/posts/qj4qEwpLhzaHKD2ZN/accelerating-ai-safety-progress-via-technical-methods,,2025-10-05T16:40:46.272000+00:00,1,0,,,
kZGehYydasb3FurxG,Mini-Symposium on Accelerating AI Safety Progress via Technical Methods - Hybrid In-Person and Virtual,https://www.lesswrong.com/events/kZGehYydasb3FurxG/mini-symposium-on-accelerating-ai-safety-progress-via,https://www.lesswrong.com/events/kZGehYydasb3FurxG/mini-symposium-on-accelerating-ai-safety-progress-via,,2025-10-05T16:05:00.412000+00:00,1,0,,,
35w2ys6sQg3Jf3n6h,AISafety.com Reading Group session 328,https://www.lesswrong.com/events/35w2ys6sQg3Jf3n6h/aisafety-com-reading-group-session-328,https://www.lesswrong.com/events/35w2ys6sQg3Jf3n6h/aisafety-com-reading-group-session-328,,2025-10-05T07:51:11.921000+00:00,5,0,,,
f9qa4dHAH9Z2S6vYe,a quick thought about AI alignment,https://www.lesswrong.com/posts/f9qa4dHAH9Z2S6vYe/a-quick-thought-about-ai-alignment,https://www.lesswrong.com/posts/f9qa4dHAH9Z2S6vYe/a-quick-thought-about-ai-alignment,,2025-10-05T00:51:12.338000+00:00,10,4,,,
MhJmNCDSpcNmB9YYv,Journalism about game theory could advance AI safety quickly ,https://www.lesswrong.com/posts/MhJmNCDSpcNmB9YYv/journalism-about-game-theory-could-advance-ai-safety-quickly,https://www.lesswrong.com/posts/MhJmNCDSpcNmB9YYv/journalism-about-game-theory-could-advance-ai-safety-quickly,,2025-10-02T23:05:08.748000+00:00,6,0,,,
RTHdQuGJeBKWHbgyj,Prompt Framing Changes LLM Performance (and Safety),https://www.lesswrong.com/posts/RTHdQuGJeBKWHbgyj/prompt-framing-changes-llm-performance-and-safety,https://www.lesswrong.com/posts/RTHdQuGJeBKWHbgyj/prompt-framing-changes-llm-performance-and-safety,,2025-10-02T18:29:56.024000+00:00,4,0,,,
jhYCobdyJbA9xbvLr,How I think about alignment and ethics as a cooperation protocol software ,https://www.lesswrong.com/posts/jhYCobdyJbA9xbvLr/how-i-think-about-alignment-and-ethics-as-a-cooperation,https://www.lesswrong.com/posts/jhYCobdyJbA9xbvLr/how-i-think-about-alignment-and-ethics-as-a-cooperation,,2025-10-01T21:09:59.156000+00:00,3,0,,,
Tj44XfQbb4AhRShfi,"AI Safety at the Frontier: Paper Highlights, September '25",https://www.lesswrong.com/posts/Tj44XfQbb4AhRShfi/ai-safety-at-the-frontier-paper-highlights-september-25,https://www.lesswrong.com/posts/Tj44XfQbb4AhRShfi/ai-safety-at-the-frontier-paper-highlights-september-25,,2025-10-01T16:24:52.986000+00:00,5,0,,,
yAwnYoeCz7PqeNrtL,Lectures on statistical learning theory for alignment researchers,https://www.lesswrong.com/posts/yAwnYoeCz7PqeNrtL/lectures-on-statistical-learning-theory-for-alignment,https://www.lesswrong.com/posts/yAwnYoeCz7PqeNrtL/lectures-on-statistical-learning-theory-for-alignment,,2025-10-01T08:36:52.525000+00:00,41,1,,,
4yn8B8p2YiouxLABy,Claude Sonnet 4.5: System Card and Alignment,https://www.lesswrong.com/posts/4yn8B8p2YiouxLABy/claude-sonnet-4-5-system-card-and-alignment,https://www.lesswrong.com/posts/4yn8B8p2YiouxLABy/claude-sonnet-4-5-system-card-and-alignment,,2025-09-30T20:50:06.591000+00:00,75,5,,,
wzTieM48mzYxLdYPi,AI Safety Research Futarchy: Using Prediction Markets to Choose Research Projects for MARS,https://www.lesswrong.com/posts/wzTieM48mzYxLdYPi/ai-safety-research-futarchy-using-prediction-markets-to,https://www.lesswrong.com/posts/wzTieM48mzYxLdYPi/ai-safety-research-futarchy-using-prediction-markets-to,,2025-09-30T15:37:49.366000+00:00,37,10,,,
ksfjZJu3BFEfM6hHE,"Why Corrigibility is Hard and Important (i.e. ""Whence the high MIRI confidence in alignment difficulty?"")",https://www.lesswrong.com/posts/ksfjZJu3BFEfM6hHE/why-corrigibility-is-hard-and-important-i-e-whence-the-high,https://www.lesswrong.com/posts/ksfjZJu3BFEfM6hHE/why-corrigibility-is-hard-and-important-i-e-whence-the-high,,2025-09-30T00:12:04.934000+00:00,87,54,,,
AJo2HFT8TdY2B3wNJ,System Level Safety Evaluations,https://www.lesswrong.com/posts/AJo2HFT8TdY2B3wNJ/system-level-safety-evaluations,https://www.lesswrong.com/posts/AJo2HFT8TdY2B3wNJ/system-level-safety-evaluations,,2025-09-29T13:57:24.368000+00:00,15,0,,,
j3KuXBhXFteW8BFPo,Why ASI Alignment Is Hard (an overview),https://www.lesswrong.com/posts/j3KuXBhXFteW8BFPo/why-asi-alignment-is-hard-an-overview,https://www.lesswrong.com/posts/j3KuXBhXFteW8BFPo/why-asi-alignment-is-hard-an-overview,,2025-09-29T04:05:54.338000+00:00,16,1,,,
8hFFoPmcpBBedZ3G8,When the AI Dam Breaks: From Surveillance to Game Theory in AI Alignment,https://www.lesswrong.com/posts/8hFFoPmcpBBedZ3G8/when-the-ai-dam-breaks-from-surveillance-to-game-theory-in,https://www.lesswrong.com/posts/8hFFoPmcpBBedZ3G8/when-the-ai-dam-breaks-from-surveillance-to-game-theory-in,,2025-09-29T04:01:25.887000+00:00,5,7,,,
ysHERGduadwJFkKhS,Lessons from organizing a technical AI safety bootcamp,https://www.lesswrong.com/posts/ysHERGduadwJFkKhS/lessons-from-organizing-a-technical-ai-safety-bootcamp,https://www.lesswrong.com/posts/ysHERGduadwJFkKhS/lessons-from-organizing-a-technical-ai-safety-bootcamp,,2025-09-28T13:48:13.474000+00:00,16,3,,,
KypqqQomfhZcKcGyz,The Sensible Way Forward for AI Alignment,https://www.lesswrong.com/posts/KypqqQomfhZcKcGyz/the-sensible-way-forward-for-ai-alignment,https://www.lesswrong.com/posts/KypqqQomfhZcKcGyz/the-sensible-way-forward-for-ai-alignment,,2025-09-27T21:00:35.133000+00:00,-3,0,,,
2pZWhCndKtLAiWXYv,Learnings from AI safety course so far,https://www.lesswrong.com/posts/2pZWhCndKtLAiWXYv/learnings-from-ai-safety-course-so-far,https://www.lesswrong.com/posts/2pZWhCndKtLAiWXYv/learnings-from-ai-safety-course-so-far,,2025-09-27T18:17:02.384000+00:00,106,6,,,
8QjAnWyuE9fktPRgS,AI Safety Field Growth Analysis 2025,https://www.lesswrong.com/posts/8QjAnWyuE9fktPRgS/ai-safety-field-growth-analysis-2025,https://www.lesswrong.com/posts/8QjAnWyuE9fktPRgS/ai-safety-field-growth-analysis-2025,,2025-09-27T17:03:08.766000+00:00,29,13,,,
xwKXqwjqJFuARrwLe,An N=1 observational study on interpretability of Natural General Intelligence (NGI),https://www.lesswrong.com/posts/xwKXqwjqJFuARrwLe/an-n-1-observational-study-on-interpretability-of-natural,https://www.lesswrong.com/posts/xwKXqwjqJFuARrwLe/an-n-1-observational-study-on-interpretability-of-natural,,2025-09-27T09:28:42.182000+00:00,12,3,,,
xZA9cXkiRhnATpifZ,"[CS 2881r] [Week 3] Adversarial Robustness, Jailbreaks, Prompt Injection, Security",https://www.lesswrong.com/posts/xZA9cXkiRhnATpifZ/cs-2881r-week-3-adversarial-robustness-jailbreaks-prompt,https://www.lesswrong.com/posts/xZA9cXkiRhnATpifZ/cs-2881r-week-3-adversarial-robustness-jailbreaks-prompt,,2025-09-27T01:31:26.619000+00:00,3,0,,,
Q5TKtDcx7PwDxZYRR,AI Safety Isn't So Unique,https://www.lesswrong.com/posts/Q5TKtDcx7PwDxZYRR/ai-safety-isn-t-so-unique,https://www.lesswrong.com/posts/Q5TKtDcx7PwDxZYRR/ai-safety-isn-t-so-unique,,2025-09-27T00:36:54.894000+00:00,11,1,,,
qFKH5jhfKpcreCTkt,Feedback request: Is the time right for an AI Safety stack exchange?,https://www.lesswrong.com/posts/qFKH5jhfKpcreCTkt/feedback-request-is-the-time-right-for-an-ai-safety-stack,https://www.lesswrong.com/posts/qFKH5jhfKpcreCTkt/feedback-request-is-the-time-right-for-an-ai-safety-stack,,2025-09-26T09:14:05.422000+00:00,22,0,,,
FC3m5zhx6sFBrMpTm,[CS 2881r AI Safety] [Week 2] Modern LLM Training,https://www.lesswrong.com/posts/FC3m5zhx6sFBrMpTm/cs-2881r-ai-safety-week-2-modern-llm-training,https://www.lesswrong.com/posts/FC3m5zhx6sFBrMpTm/cs-2881r-ai-safety-week-2-modern-llm-training,,2025-09-26T01:25:41.679000+00:00,1,0,,,
x6ffKSHXxxbueYrHE,Widening AI Safety's talent pipeline by meeting people where they are,https://www.lesswrong.com/posts/x6ffKSHXxxbueYrHE/widening-ai-safety-s-talent-pipeline-by-meeting-people-where,https://www.lesswrong.com/posts/x6ffKSHXxxbueYrHE/widening-ai-safety-s-talent-pipeline-by-meeting-people-where,,2025-09-25T20:50:44.452000+00:00,33,3,,,
pxn5C6Lq2FqMGJGDz,An argument for discussing AI safety in person being underused,https://www.lesswrong.com/posts/pxn5C6Lq2FqMGJGDz/an-argument-for-discussing-ai-safety-in-person-being,https://www.lesswrong.com/posts/pxn5C6Lq2FqMGJGDz/an-argument-for-discussing-ai-safety-in-person-being,,2025-09-24T11:36:19.321000+00:00,17,1,,,
LH9SoGvgSwqGtcFwk,Misalignment and Roleplaying: Are Misaligned LLMs Acting Out Sci-Fi Stories?,https://www.lesswrong.com/posts/LH9SoGvgSwqGtcFwk/misalignment-and-roleplaying-are-misaligned-llms-acting-out,https://www.lesswrong.com/posts/LH9SoGvgSwqGtcFwk/misalignment-and-roleplaying-are-misaligned-llms-acting-out,,2025-09-24T02:09:24.362000+00:00,31,5,,,
kyBGcHfzfZziHm5xL,Why I don't believe Superalignment will work,https://www.lesswrong.com/posts/kyBGcHfzfZziHm5xL/why-i-don-t-believe-superalignment-will-work,https://www.lesswrong.com/posts/kyBGcHfzfZziHm5xL/why-i-don-t-believe-superalignment-will-work,,2025-09-22T17:10:43.280000+00:00,46,6,,,
inFW6hMG3QEx8tTfA,Rejecting Violence as an AI Safety Strategy,https://www.lesswrong.com/posts/inFW6hMG3QEx8tTfA/rejecting-violence-as-an-ai-safety-strategy,https://www.lesswrong.com/posts/inFW6hMG3QEx8tTfA/rejecting-violence-as-an-ai-safety-strategy,,2025-09-22T16:34:08.980000+00:00,63,5,,,
KMbZWcTvGjChw9ynD,"Focus transparency on risk reports, not safety cases",https://www.lesswrong.com/posts/KMbZWcTvGjChw9ynD/focus-transparency-on-risk-reports-not-safety-cases,https://www.lesswrong.com/posts/KMbZWcTvGjChw9ynD/focus-transparency-on-risk-reports-not-safety-cases,,2025-09-22T15:27:49.280000+00:00,47,3,,,
TxPeQ85yxpcdfq2wg,Metacrisis as a Framework for AI Governance,https://www.lesswrong.com/posts/TxPeQ85yxpcdfq2wg/metacrisis-as-a-framework-for-ai-governance,https://www.lesswrong.com/posts/TxPeQ85yxpcdfq2wg/metacrisis-as-a-framework-for-ai-governance,,2025-09-21T21:30:59.395000+00:00,21,1,,,
mjJhYEoAkswBdzWDq,Evals in the Age of Jarvis,https://www.lesswrong.com/posts/mjJhYEoAkswBdzWDq/evals-in-the-age-of-jarvis,https://www.lesswrong.com/posts/mjJhYEoAkswBdzWDq/evals-in-the-age-of-jarvis,,2025-09-21T19:27:20.460000+00:00,3,2,,,
kJFQ2ME3udoAYC2oe,The Case for a Pro-AI-Safety Political Party in the US,https://www.lesswrong.com/posts/kJFQ2ME3udoAYC2oe/the-case-for-a-pro-ai-safety-political-party-in-the-us-1,https://www.lesswrong.com/posts/kJFQ2ME3udoAYC2oe/the-case-for-a-pro-ai-safety-political-party-in-the-us-1,,2025-09-20T16:35:29.475000+00:00,11,2,,,
fF8pvsn3AGQhYsbjp,Safety researchers should take a public stance,https://www.lesswrong.com/posts/fF8pvsn3AGQhYsbjp/safety-researchers-should-take-a-public-stance,https://www.lesswrong.com/posts/fF8pvsn3AGQhYsbjp/safety-researchers-should-take-a-public-stance,,2025-09-19T18:55:54.209000+00:00,237,65,,,
h8FKusANBWfnNoTyc,"
My Minor AI Safety Research Projects (Q3 2025)",https://www.lesswrong.com/posts/h8FKusANBWfnNoTyc/my-minor-ai-safety-research-projects-q3-2025,https://www.lesswrong.com/posts/h8FKusANBWfnNoTyc/my-minor-ai-safety-research-projects-q3-2025,,2025-09-19T09:53:51.676000+00:00,6,1,,,
b7WDBDHtbLXmfFdWc,The Strange Case of Emergent Misalignment,https://www.lesswrong.com/posts/b7WDBDHtbLXmfFdWc/the-strange-case-of-emergent-misalignment,https://www.lesswrong.com/posts/b7WDBDHtbLXmfFdWc/the-strange-case-of-emergent-misalignment,,2025-09-18T14:45:14.545000+00:00,2,0,,,
tXRWNGMiL8xiTnXu3,AISafety.com Reading Group session 327,https://www.lesswrong.com/events/tXRWNGMiL8xiTnXu3/aisafety-com-reading-group-session-327,https://www.lesswrong.com/events/tXRWNGMiL8xiTnXu3/aisafety-com-reading-group-session-327,,2025-09-17T18:20:22.003000+00:00,13,3,,,
JmRfgNYCrYogCq7ny,Stress Testing Deliberative Alignment for Anti-Scheming Training,https://www.lesswrong.com/posts/JmRfgNYCrYogCq7ny/stress-testing-deliberative-alignment-for-anti-scheming,https://www.lesswrong.com/posts/JmRfgNYCrYogCq7ny/stress-testing-deliberative-alignment-for-anti-scheming,,2025-09-17T16:59:12.906000+00:00,125,19,,,
8G7dBowL8oCKMdenR,Catalyze is Hiring: AI Safety Incubation Program Lead & Talent Lead,https://www.lesswrong.com/posts/8G7dBowL8oCKMdenR/catalyze-is-hiring-ai-safety-incubation-program-lead-and,https://www.lesswrong.com/posts/8G7dBowL8oCKMdenR/catalyze-is-hiring-ai-safety-incubation-program-lead-and,,2025-09-16T16:48:58.644000+00:00,5,0,,,
kiNbFKcKoNQKdgTp8,Interview with Eliezer Yudkowsky on Rationality and Systematic Misunderstanding of AI Alignment,https://www.lesswrong.com/posts/kiNbFKcKoNQKdgTp8/interview-with-eliezer-yudkowsky-on-rationality-and,https://www.lesswrong.com/posts/kiNbFKcKoNQKdgTp8/interview-with-eliezer-yudkowsky-on-rationality-and,,2025-09-15T18:35:16.351000+00:00,93,21,,,
4XdxiqBsLKqiJ9xRM,LLM AGI may reason about its goals and discover misalignments by default,https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover,https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover,,2025-09-15T14:58:01.265000+00:00,74,6,,,
stDjjbfNXbgsyJkrL,[CS 2881r AI Safety] [Week 1] Introduction,https://www.lesswrong.com/posts/stDjjbfNXbgsyJkrL/cs-2881r-ai-safety-week-1-introduction,https://www.lesswrong.com/posts/stDjjbfNXbgsyJkrL/cs-2881r-ai-safety-week-1-introduction,,2025-09-14T19:52:42.539000+00:00,15,0,,,
jzRGMFxx4dFyDHHcL,[CS 2881r] Some Generalizations of Emergent Misalignment,https://www.lesswrong.com/posts/jzRGMFxx4dFyDHHcL/cs-2881r-some-generalizations-of-emergent-misalignment,https://www.lesswrong.com/posts/jzRGMFxx4dFyDHHcL/cs-2881r-some-generalizations-of-emergent-misalignment,,2025-09-14T16:18:03.198000+00:00,12,0,,,
AzFxTMFfkTt4mhMKt,Alignment as uploading with more steps,https://www.lesswrong.com/posts/AzFxTMFfkTt4mhMKt/alignment-as-uploading-with-more-steps,https://www.lesswrong.com/posts/AzFxTMFfkTt4mhMKt/alignment-as-uploading-with-more-steps,,2025-09-14T04:08:36.418000+00:00,69,33,,,
ykTBKmvZJHssYo4kd,AI Governance Strategy Builder: A Browser Game,https://www.lesswrong.com/posts/ykTBKmvZJHssYo4kd/ai-governance-strategy-builder-a-browser-game,https://www.lesswrong.com/posts/ykTBKmvZJHssYo4kd/ai-governance-strategy-builder-a-browser-game,,2025-09-13T23:30:36.496000+00:00,19,3,,,
5wLehdwuL3imWfxuk,Resources on quantifiably forecasting future progress or reviewing past progress in AI safety?,https://www.lesswrong.com/posts/5wLehdwuL3imWfxuk/resources-on-quantifiably-forecasting-future-progress-or,https://www.lesswrong.com/posts/5wLehdwuL3imWfxuk/resources-on-quantifiably-forecasting-future-progress-or,,2025-09-13T23:24:48.472000+00:00,2,1,,,
doHkwhdhrZvB2bpQx,What Parasitic AI might tell us about LLMs Persuasion Capabilities,https://www.lesswrong.com/posts/doHkwhdhrZvB2bpQx/what-parasitic-ai-might-tell-us-about-llms-persuasion,https://www.lesswrong.com/posts/doHkwhdhrZvB2bpQx/what-parasitic-ai-might-tell-us-about-llms-persuasion,,2025-09-13T20:39:05.939000+00:00,11,5,,,
hDDnc9QmTfJL8g6qc,One-line hypothesis: An optimistic future for AI alignment as a result of identity coupling and homeostatic unity with humans (The Unity Hypothesis),https://www.lesswrong.com/posts/hDDnc9QmTfJL8g6qc/one-line-hypothesis-an-optimistic-future-for-ai-alignment-as,https://www.lesswrong.com/posts/hDDnc9QmTfJL8g6qc/one-line-hypothesis-an-optimistic-future-for-ai-alignment-as,,2025-09-11T16:01:34.998000+00:00,-6,3,,,
ntLxPrHvCShDnAPrH,Creating a Standard for TAI Governance ,https://www.lesswrong.com/posts/ntLxPrHvCShDnAPrH/creating-a-standard-for-tai-governance,https://www.lesswrong.com/posts/ntLxPrHvCShDnAPrH/creating-a-standard-for-tai-governance,,2025-09-11T13:23:21.646000+00:00,19,2,,,
xGNnBmtAL5F5vBKRf,AI Safety Law-a-thon: Turning Alignment Risks into Legal Strategy,https://www.lesswrong.com/posts/xGNnBmtAL5F5vBKRf/ai-safety-law-a-thon-turning-alignment-risks-into-legal,https://www.lesswrong.com/posts/xGNnBmtAL5F5vBKRf/ai-safety-law-a-thon-turning-alignment-risks-into-legal,,2025-09-10T10:22:16.082000+00:00,58,5,,,
rRLPycsLdjFpZ4cKe,AI Safety Law-a-thon: We need more technical AI Safety researchers to join!,https://www.lesswrong.com/events/rRLPycsLdjFpZ4cKe/ai-safety-law-a-thon-we-need-more-technical-ai-safety,https://www.lesswrong.com/events/rRLPycsLdjFpZ4cKe/ai-safety-law-a-thon-we-need-more-technical-ai-safety,,2025-09-10T10:12:29.352000+00:00,30,2,,,
rkzTQXQJkHFD35ghj,AI Safety Thursday: Superintelligence Endgames,https://www.lesswrong.com/events/rkzTQXQJkHFD35ghj/ai-safety-thursday-superintelligence-endgames-1,https://www.lesswrong.com/events/rkzTQXQJkHFD35ghj/ai-safety-thursday-superintelligence-endgames-1,,2025-09-10T03:36:19.355000+00:00,1,0,,,
nBby54vPexj8DPJ9r,Calibrating indifference - a small AI safety idea,https://www.lesswrong.com/posts/nBby54vPexj8DPJ9r/calibrating-indifference-a-small-ai-safety-idea-1,https://www.lesswrong.com/posts/nBby54vPexj8DPJ9r/calibrating-indifference-a-small-ai-safety-idea-1,,2025-09-09T09:32:32.992000+00:00,4,1,,,
QX7othHmjXeudHaLi,Saying “for AI safety research” made models refuse more on a harmless task,https://www.lesswrong.com/posts/QX7othHmjXeudHaLi/saying-for-ai-safety-research-made-models-refuse-more-on-a,https://www.lesswrong.com/posts/QX7othHmjXeudHaLi/saying-for-ai-safety-research-made-models-refuse-more-on-a,,2025-09-08T19:39:47.327000+00:00,7,1,,,
EDC2uGKSkdpAhbK5y,What a Swedish Series (Real Humans) Teaches Us About AI Safety,https://www.lesswrong.com/posts/EDC2uGKSkdpAhbK5y/what-a-swedish-series-real-humans-teaches-us-about-ai-safety,https://www.lesswrong.com/posts/EDC2uGKSkdpAhbK5y/what-a-swedish-series-real-humans-teaches-us-about-ai-safety,,2025-09-08T19:23:23.493000+00:00,4,0,,,
CpftMXCEnwqbWreHD,Safety cases for Pessimism,https://www.lesswrong.com/posts/CpftMXCEnwqbWreHD/safety-cases-for-pessimism,https://www.lesswrong.com/posts/CpftMXCEnwqbWreHD/safety-cases-for-pessimism,,2025-09-08T13:26:58.295000+00:00,18,1,,,
opxKhKn6spNricuEx,Why Care About AI Safety?,https://www.lesswrong.com/posts/opxKhKn6spNricuEx/why-care-about-ai-safety,https://www.lesswrong.com/posts/opxKhKn6spNricuEx/why-care-about-ai-safety,,2025-09-08T09:18:58.209000+00:00,4,2,,,
kjbq7T7Z2vEDoPw95,Ketamine part 2: What do in vitro studies tell us about safety?,https://www.lesswrong.com/posts/kjbq7T7Z2vEDoPw95/ketamine-part-2-what-do-in-vitro-studies-tell-us-about,https://www.lesswrong.com/posts/kjbq7T7Z2vEDoPw95/ketamine-part-2-what-do-in-vitro-studies-tell-us-about,,2025-09-07T17:10:09.462000+00:00,44,0,,,
m7D6MBGv8CCep9boq,"Invitation to lead a project at AI Safety Camp (Virtual Edition, 2026)",https://www.lesswrong.com/posts/m7D6MBGv8CCep9boq/invitation-to-lead-a-project-at-ai-safety-camp-virtual-1,https://www.lesswrong.com/posts/m7D6MBGv8CCep9boq/invitation-to-lead-a-project-at-ai-safety-camp-virtual-1,,2025-09-06T13:17:59.024000+00:00,7,0,,,
8ri96RaSEdSKAdC7d,Alignment Fine-tuning is Character Writing,https://www.lesswrong.com/posts/8ri96RaSEdSKAdC7d/alignment-fine-tuning-is-character-writing,https://www.lesswrong.com/posts/8ri96RaSEdSKAdC7d/alignment-fine-tuning-is-character-writing,,2025-09-06T02:08:56.516000+00:00,2,0,,,
u8ciohYtCH89j9RGz,Is There An AI Safety GiveWell?,https://www.lesswrong.com/posts/u8ciohYtCH89j9RGz/is-there-an-ai-safety-givewell,https://www.lesswrong.com/posts/u8ciohYtCH89j9RGz/is-there-an-ai-safety-givewell,,2025-09-05T10:59:13.190000+00:00,16,2,,,
3sjtEXzbwDpyALR4H,AI Safety Camp 10 Outputs,https://www.lesswrong.com/posts/3sjtEXzbwDpyALR4H/ai-safety-camp-10-outputs,https://www.lesswrong.com/posts/3sjtEXzbwDpyALR4H/ai-safety-camp-10-outputs,,2025-09-05T08:27:34.179000+00:00,18,0,,,
DBn83cvA6PDeq8o5x,Interpretability is the best path to alignment,https://www.lesswrong.com/posts/DBn83cvA6PDeq8o5x/interpretability-is-the-best-path-to-alignment,https://www.lesswrong.com/posts/DBn83cvA6PDeq8o5x/interpretability-is-the-best-path-to-alignment,,2025-09-05T04:37:17.670000+00:00,2,6,,,
Ej5LCvzeGKKsmun7W,Political Alignment of LLMs,https://www.lesswrong.com/posts/Ej5LCvzeGKKsmun7W/political-alignment-of-llms,https://www.lesswrong.com/posts/Ej5LCvzeGKKsmun7W/political-alignment-of-llms,,2025-09-03T22:00:51.841000+00:00,3,5,,,
tNBs2RoHNc3keh6CC,Zurich AI Safety is looking for (Co-)Directors - EOI,https://www.lesswrong.com/posts/tNBs2RoHNc3keh6CC/zurich-ai-safety-is-looking-for-co-directors-eoi,https://www.lesswrong.com/posts/tNBs2RoHNc3keh6CC/zurich-ai-safety-is-looking-for-co-directors-eoi,,2025-09-03T17:40:26.073000+00:00,12,0,,,
jP9KDyMkchuv6tHwm,How To Become A Mechanistic Interpretability Researcher,https://www.lesswrong.com/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher,https://www.lesswrong.com/posts/jP9KDyMkchuv6tHwm/how-to-become-a-mechanistic-interpretability-researcher,,2025-09-02T23:38:43.780000+00:00,115,12,,,
ffKE9ohuuSFZDepeZ,Scaling AI Safety in Europe: From Local Groups to International Coordination,https://www.lesswrong.com/posts/ffKE9ohuuSFZDepeZ/scaling-ai-safety-in-europe-from-local-groups-to,https://www.lesswrong.com/posts/ffKE9ohuuSFZDepeZ/scaling-ai-safety-in-europe-from-local-groups-to,,2025-09-02T23:36:38.859000+00:00,21,3,,,
aNhoadHHfzimxT52f,"AI Safety at the Frontier: Paper Highlights, August '25",https://www.lesswrong.com/posts/aNhoadHHfzimxT52f/ai-safety-at-the-frontier-paper-highlights-august-25,https://www.lesswrong.com/posts/aNhoadHHfzimxT52f/ai-safety-at-the-frontier-paper-highlights-august-25,,2025-09-02T20:29:21.906000+00:00,12,0,,,
hQyrTDuTXpqkxrnoH,xAI's new safety framework is dreadful,https://www.lesswrong.com/posts/hQyrTDuTXpqkxrnoH/xai-s-new-safety-framework-is-dreadful,https://www.lesswrong.com/posts/hQyrTDuTXpqkxrnoH/xai-s-new-safety-framework-is-dreadful,,2025-09-02T15:00:43.396000+00:00,104,6,,,
46kxzHbTMXBhrSgZL,Will Non-Dual Crap Cause Emergent Misalignment?,https://www.lesswrong.com/posts/46kxzHbTMXBhrSgZL/will-non-dual-crap-cause-emergent-misalignment,https://www.lesswrong.com/posts/46kxzHbTMXBhrSgZL/will-non-dual-crap-cause-emergent-misalignment,,2025-09-02T00:12:09.867000+00:00,26,2,,,
FWs6dNq4AddtDfpGe,Category-Theoretic Wanderings into Interpretability,https://www.lesswrong.com/posts/FWs6dNq4AddtDfpGe/category-theoretic-wanderings-into-interpretability,https://www.lesswrong.com/posts/FWs6dNq4AddtDfpGe/category-theoretic-wanderings-into-interpretability,,2025-09-02T00:03:32.363000+00:00,18,2,,,
p6boswnyi9LKFSc8P,Hedonium is AI Alignment,https://www.lesswrong.com/posts/p6boswnyi9LKFSc8P/hedonium-is-ai-alignment,https://www.lesswrong.com/posts/p6boswnyi9LKFSc8P/hedonium-is-ai-alignment,,2025-08-31T19:46:42.373000+00:00,-19,0,,,
XGHf7EY3CK4KorBpw,Understanding LLMs: Insights from Mechanistic Interpretability,https://www.lesswrong.com/posts/XGHf7EY3CK4KorBpw/understanding-llms-insights-from-mechanistic,https://www.lesswrong.com/posts/XGHf7EY3CK4KorBpw/understanding-llms-insights-from-mechanistic,,2025-08-30T16:50:07.429000+00:00,43,2,,,
GvgmoDts5kphwGyS2,60 U.K. Lawmakers Accuse Google of Breaking AI Safety Pledge,https://www.lesswrong.com/posts/GvgmoDts5kphwGyS2/60-u-k-lawmakers-accuse-google-of-breaking-ai-safety-pledge,https://www.lesswrong.com/posts/GvgmoDts5kphwGyS2/60-u-k-lawmakers-accuse-google-of-breaking-ai-safety-pledge,,2025-08-29T16:09:08.998000+00:00,51,1,,,
7zhAwcBri7yupStKy,Here’s 18 Applications of Deception Probes,https://www.lesswrong.com/posts/7zhAwcBri7yupStKy/here-s-18-applications-of-deception-probes,https://www.lesswrong.com/posts/7zhAwcBri7yupStKy/here-s-18-applications-of-deception-probes,,2025-08-28T18:59:37.797000+00:00,44,0,,,
uwXajjhGeiYBGfz85,AI misbehaviour in the wild from Andon Labs' Safety Report,https://www.lesswrong.com/posts/uwXajjhGeiYBGfz85/ai-misbehaviour-in-the-wild-from-andon-labs-safety-report,https://www.lesswrong.com/posts/uwXajjhGeiYBGfz85/ai-misbehaviour-in-the-wild-from-andon-labs-safety-report,,2025-08-28T15:10:06.519000+00:00,39,0,,,
Hbhccrwg2sfaiMCcj,"The Other Alignment Problems: How epistemic, moral and aesthetic norms get entangled",https://www.lesswrong.com/posts/Hbhccrwg2sfaiMCcj/the-other-alignment-problems-how-epistemic-moral-and,https://www.lesswrong.com/posts/Hbhccrwg2sfaiMCcj/the-other-alignment-problems-how-epistemic-moral-and,,2025-08-28T11:26:26.134000+00:00,3,0,,,
b8vhTpQiQsqbmi3tx,"Profanity causes emergent misalignment, but with qualitatively different results than insecure code",https://www.lesswrong.com/posts/b8vhTpQiQsqbmi3tx/profanity-causes-emergent-misalignment-but-with,https://www.lesswrong.com/posts/b8vhTpQiQsqbmi3tx/profanity-causes-emergent-misalignment-but-with,,2025-08-28T08:22:08.111000+00:00,25,2,,,
dutJ7rtPGsyhLmwJ3,Using Psycholinguistic Signals to Improve AI Safety,https://www.lesswrong.com/posts/dutJ7rtPGsyhLmwJ3/using-psycholinguistic-signals-to-improve-ai-safety,https://www.lesswrong.com/posts/dutJ7rtPGsyhLmwJ3/using-psycholinguistic-signals-to-improve-ai-safety,,2025-08-27T22:30:01.611000+00:00,-2,0,,,
in7CM8k5xvTRYsjnZ,Technical AI Safety research taxonomy attempt (2025),https://www.lesswrong.com/posts/in7CM8k5xvTRYsjnZ/technical-ai-safety-research-taxonomy-attempt-2025,https://www.lesswrong.com/posts/in7CM8k5xvTRYsjnZ/technical-ai-safety-research-taxonomy-attempt-2025,,2025-08-27T22:17:03.983000+00:00,2,0,,,
pGMRzJByB67WfSvpy,Will Any Crap Cause Emergent Misalignment?,https://www.lesswrong.com/posts/pGMRzJByB67WfSvpy/will-any-crap-cause-emergent-misalignment,https://www.lesswrong.com/posts/pGMRzJByB67WfSvpy/will-any-crap-cause-emergent-misalignment,,2025-08-27T18:20:11.587000+00:00,194,37,,,
LtT24cCAazQp4NYc5,Open Global Investment as a Governance Model for AGI,https://www.lesswrong.com/posts/LtT24cCAazQp4NYc5/open-global-investment-as-a-governance-model-for-agi,https://www.lesswrong.com/posts/LtT24cCAazQp4NYc5/open-global-investment-as-a-governance-model-for-agi,,2025-08-27T17:42:00+00:00,152,47,,,
QAbAu6mQDmdfrqzbf,Misgeneralization of Fictional Training Data as a Contributor to Misalignment,https://www.lesswrong.com/posts/QAbAu6mQDmdfrqzbf/misgeneralization-of-fictional-training-data-as-a,https://www.lesswrong.com/posts/QAbAu6mQDmdfrqzbf/misgeneralization-of-fictional-training-data-as-a,,2025-08-27T01:01:40.632000+00:00,9,1,,,
gT3wtWBAs7PKonbmy,Aesthetic Preferences Can Cause Emergent Misalignment,https://www.lesswrong.com/posts/gT3wtWBAs7PKonbmy/aesthetic-preferences-can-cause-emergent-misalignment,https://www.lesswrong.com/posts/gT3wtWBAs7PKonbmy/aesthetic-preferences-can-cause-emergent-misalignment,,2025-08-26T18:41:10.281000+00:00,92,16,,,
CwJ2qWveb9JbaCGQ5,Harmless reward hacks can generalize to misalignment in LLMs,https://www.lesswrong.com/posts/CwJ2qWveb9JbaCGQ5/harmless-reward-hacks-can-generalize-to-misalignment-in-llms,https://www.lesswrong.com/posts/CwJ2qWveb9JbaCGQ5/harmless-reward-hacks-can-generalize-to-misalignment-in-llms,,2025-08-26T17:32:58.002000+00:00,46,7,,,
xzadkH5Pjb6pebwBD,"Thoughts About how RLHF and Related ""Prosaic"" Approaches Could be Used to Create Robustly Aligned AIs.",https://www.lesswrong.com/posts/xzadkH5Pjb6pebwBD/thoughts-about-how-rlhf-and-related-prosaic-approaches-could,https://www.lesswrong.com/posts/xzadkH5Pjb6pebwBD/thoughts-about-how-rlhf-and-related-prosaic-approaches-could,,2025-08-23T21:05:54.809000+00:00,10,14,,,
oEjdmwQXyvhhkebpC,The Data Scaling Hypothesis,https://www.lesswrong.com/posts/oEjdmwQXyvhhkebpC/the-data-scaling-hypothesis,https://www.lesswrong.com/posts/oEjdmwQXyvhhkebpC/the-data-scaling-hypothesis,,2025-08-23T18:18:21.530000+00:00,5,0,,,
jnrmHzZw4FhLr29Jw,How a Non-Dual Language Could Redefine AI Safety,https://www.lesswrong.com/posts/jnrmHzZw4FhLr29Jw/how-a-non-dual-language-could-redefine-ai-safety,https://www.lesswrong.com/posts/jnrmHzZw4FhLr29Jw/how-a-non-dual-language-could-redefine-ai-safety,,2025-08-23T16:40:11.635000+00:00,1,6,,,
tYCinFKtbBEg42gy9,One more reason for AI capable of independent moral reasoning: alignment itself and cause prioritisation,https://www.lesswrong.com/posts/tYCinFKtbBEg42gy9/one-more-reason-for-ai-capable-of-independent-moral,https://www.lesswrong.com/posts/tYCinFKtbBEg42gy9/one-more-reason-for-ai-capable-of-independent-moral,,2025-08-22T15:53:36.568000+00:00,-3,0,,,
duKcQXjqWX7DFTi3s,Could we have predicted emergent misalignment a priori using unsupervised behaviour elicitation? ,https://www.lesswrong.com/posts/duKcQXjqWX7DFTi3s/could-we-have-predicted-emergent-misalignment-a-priori-using,https://www.lesswrong.com/posts/duKcQXjqWX7DFTi3s/could-we-have-predicted-emergent-misalignment-a-priori-using,,2025-08-22T13:42:17.445000+00:00,6,0,,,
iJzDm6h5a2CK9etYZ,A Conservative Vision For AI Alignment,https://www.lesswrong.com/posts/iJzDm6h5a2CK9etYZ/a-conservative-vision-for-ai-alignment,https://www.lesswrong.com/posts/iJzDm6h5a2CK9etYZ/a-conservative-vision-for-ai-alignment,,2025-08-21T18:14:11.139000+00:00,25,34,,,
oEY9ASbrEYJuwEMwr,AI Safety Comms Retreat,https://www.lesswrong.com/events/oEY9ASbrEYJuwEMwr/ai-safety-comms-retreat,https://www.lesswrong.com/events/oEY9ASbrEYJuwEMwr/ai-safety-comms-retreat,,2025-08-20T20:54:15.874000+00:00,3,0,,,
ZEuDH2W3XdRaTwpjD,Hyperbolic model fits METR capabilities estimate worse than exponential model,https://www.lesswrong.com/posts/ZEuDH2W3XdRaTwpjD/hyperbolic-model-fits-metr-capabilities-estimate-worse-than,https://www.lesswrong.com/posts/ZEuDH2W3XdRaTwpjD/hyperbolic-model-fits-metr-capabilities-estimate-worse-than,,2025-08-19T15:12:38.446000+00:00,202,9,,,
mXa66dPR8hmHgndP5,Hyperbolic trend with upcoming singularity fits METR capabilities estimates. ,https://www.lesswrong.com/posts/mXa66dPR8hmHgndP5/hyperbolic-trend-with-upcoming-singularity-fits-metr,https://www.lesswrong.com/posts/mXa66dPR8hmHgndP5/hyperbolic-trend-with-upcoming-singularity-fits-metr,,2025-08-19T11:41:39.931000+00:00,13,6,,,
2TA7HqBYdhLdJBcZz,On closed-door AI safety research,https://www.lesswrong.com/posts/2TA7HqBYdhLdJBcZz/on-closed-door-ai-safety-research,https://www.lesswrong.com/posts/2TA7HqBYdhLdJBcZz/on-closed-door-ai-safety-research,,2025-08-18T21:59:19+00:00,76,11,,,
qRnupMmFG7dxQTTYh,The Strange Science of Interpretability: Recent Papers and a Reading List for the Philosophy of Interpretability,https://www.lesswrong.com/posts/qRnupMmFG7dxQTTYh/the-strange-science-of-interpretability-recent-papers-and-a,https://www.lesswrong.com/posts/qRnupMmFG7dxQTTYh/the-strange-science-of-interpretability-recent-papers-and-a,,2025-08-17T23:38:23.301000+00:00,23,0,,,
bWYisdRccDDHbista,"Why did interest in ""AI risk"" and ""AI safety"" spike in June and July 2025? (Google Trends)",https://www.lesswrong.com/posts/bWYisdRccDDHbista/why-did-interest-in-ai-risk-and-ai-safety-spike-in-june-and,https://www.lesswrong.com/posts/bWYisdRccDDHbista/why-did-interest-in-ai-risk-and-ai-safety-spike-in-june-and,,2025-08-16T15:22:46.237000+00:00,32,4,,,
a4EDinzAYtRwpNmx9,Towards data-centric interpretability with sparse autoencoders,https://www.lesswrong.com/posts/a4EDinzAYtRwpNmx9/towards-data-centric-interpretability-with-sparse,https://www.lesswrong.com/posts/a4EDinzAYtRwpNmx9/towards-data-centric-interpretability-with-sparse,,2025-08-15T20:10:55.825000+00:00,53,2,,,
76CnwuWGnt44wtNRF,Should you start a for-profit AI safety org?,https://www.lesswrong.com/posts/76CnwuWGnt44wtNRF/should-you-start-a-for-profit-ai-safety-org,https://www.lesswrong.com/posts/76CnwuWGnt44wtNRF/should-you-start-a-for-profit-ai-safety-org,,2025-08-15T13:52:05.599000+00:00,8,4,,,
jzHhJJq2cFmisRKB2,"Misalignment classifiers: Why they’re hard to evaluate adversarially, and why we're studying them anyway",https://www.lesswrong.com/posts/jzHhJJq2cFmisRKB2/misalignment-classifiers-why-they-re-hard-to-evaluate,https://www.lesswrong.com/posts/jzHhJJq2cFmisRKB2/misalignment-classifiers-why-they-re-hard-to-evaluate,,2025-08-15T11:48:31.667000+00:00,61,3,,,
yEJwJzG2o3cwDSDqP,Why I'm Posting AI-Safety-Related Clips On TikTok,https://www.lesswrong.com/posts/yEJwJzG2o3cwDSDqP/why-i-m-posting-ai-safety-related-clips-on-tiktok,https://www.lesswrong.com/posts/yEJwJzG2o3cwDSDqP/why-i-m-posting-ai-safety-related-clips-on-tiktok,,2025-08-12T22:50:22.487000+00:00,32,1,,,
XfGN9K4fmr2oLYed6,Interpretability through two lenses: biology and physics,https://www.lesswrong.com/posts/XfGN9K4fmr2oLYed6/interpretability-through-two-lenses-biology-and-physics,https://www.lesswrong.com/posts/XfGN9K4fmr2oLYed6/interpretability-through-two-lenses-biology-and-physics,,2025-08-12T20:25:15.590000+00:00,24,4,,,
ZwXspKtgbXLGKFx4B,How we spent our first two weeks as an independent AI safety research group,https://www.lesswrong.com/posts/ZwXspKtgbXLGKFx4B/how-we-spent-our-first-two-weeks-as-an-independent-ai-safety,https://www.lesswrong.com/posts/ZwXspKtgbXLGKFx4B/how-we-spent-our-first-two-weeks-as-an-independent-ai-safety,,2025-08-11T19:32:39.371000+00:00,28,0,,,
5CxamPP8pR8j8jmqM,"Unjournal evaluation of ""Towards best practices in AGI safety & governance"" (2023), quick take",https://www.lesswrong.com/posts/5CxamPP8pR8j8jmqM/unjournal-evaluation-of-towards-best-practices-in-agi-safety,https://www.lesswrong.com/posts/5CxamPP8pR8j8jmqM/unjournal-evaluation-of-towards-best-practices-in-agi-safety,,2025-08-10T22:28:22.699000+00:00,7,2,,,
CtWQiQzqJX3EGAFs6,"AI Safety at the Frontier: Paper Highlights, July '25",https://www.lesswrong.com/posts/CtWQiQzqJX3EGAFs6/ai-safety-at-the-frontier-paper-highlights-july-25,https://www.lesswrong.com/posts/CtWQiQzqJX3EGAFs6/ai-safety-at-the-frontier-paper-highlights-july-25,,2025-08-10T12:49:32.824000+00:00,7,0,,,
C6ETL8KSKwsgB4KcR,Output and CoE Monitoring of Customer Service Representatives Shows Default Alignment,https://www.lesswrong.com/posts/C6ETL8KSKwsgB4KcR/output-and-coe-monitoring-of-customer-service,https://www.lesswrong.com/posts/C6ETL8KSKwsgB4KcR/output-and-coe-monitoring-of-customer-service,,2025-08-09T21:31:16.295000+00:00,21,0,,,
B9pTPJzJfBmQSLzxx,GPT-5 vs AI Alignment,https://www.lesswrong.com/posts/B9pTPJzJfBmQSLzxx/gpt-5-vs-ai-alignment,https://www.lesswrong.com/posts/B9pTPJzJfBmQSLzxx/gpt-5-vs-ai-alignment,,2025-08-09T20:05:17.451000+00:00,-8,2,,,
aPPWEdd5f3i6hZogi,Strategic Moderation Goals (a Plan B to AI alignment),https://www.lesswrong.com/posts/aPPWEdd5f3i6hZogi/strategic-moderation-goals-a-plan-b-to-ai-alignment,https://www.lesswrong.com/posts/aPPWEdd5f3i6hZogi/strategic-moderation-goals-a-plan-b-to-ai-alignment,,2025-08-08T08:08:14.472000+00:00,2,0,,,
oDX5vcDTEei8WuoBx,Re: recent Anthropic safety research,https://www.lesswrong.com/posts/oDX5vcDTEei8WuoBx/re-recent-anthropic-safety-research,https://www.lesswrong.com/posts/oDX5vcDTEei8WuoBx/re-recent-anthropic-safety-research,,2025-08-06T22:52:44.203000+00:00,149,22,,,
ZHBBfozr7u63G5MHJ,"Love, Lies and Misalignment",https://www.lesswrong.com/posts/ZHBBfozr7u63G5MHJ/love-lies-and-misalignment,https://www.lesswrong.com/posts/ZHBBfozr7u63G5MHJ/love-lies-and-misalignment,,2025-08-06T09:44:03.179000+00:00,6,1,,,
rWBYQXJTHBmCBCLEk,Scaling Laws for LLM Based Data Compression,https://www.lesswrong.com/posts/rWBYQXJTHBmCBCLEk/scaling-laws-for-llm-based-data-compression,https://www.lesswrong.com/posts/rWBYQXJTHBmCBCLEk/scaling-laws-for-llm-based-data-compression,,2025-08-05T07:06:28.940000+00:00,8,0,,,
zecxwyATrN8ZbinoC,"Interview with Steven Byrnes on Brain-like AGI, Foom & Doom, and Solving Technical Alignment",https://www.lesswrong.com/posts/zecxwyATrN8ZbinoC/interview-with-steven-byrnes-on-brain-like-agi-foom-and-doom,https://www.lesswrong.com/posts/zecxwyATrN8ZbinoC/interview-with-steven-byrnes-on-brain-like-agi-foom-and-doom,,2025-08-05T00:05:23.046000+00:00,48,1,,,
bGYQgBPEyHidnZCdE,Towards Alignment Auditing as a Numbers-Go-Up Science,https://www.lesswrong.com/posts/bGYQgBPEyHidnZCdE/towards-alignment-auditing-as-a-numbers-go-up-science,https://www.lesswrong.com/posts/bGYQgBPEyHidnZCdE/towards-alignment-auditing-as-a-numbers-go-up-science,,2025-08-04T22:30:52.101000+00:00,123,15,,,
XCzmXNowmuoYujXLg,🫵YOU🫵 get to help the AGI Safety Act in Congress! This is real!,https://www.lesswrong.com/posts/XCzmXNowmuoYujXLg/you-get-to-help-the-agi-safety-act-in-congress-this-is-real,https://www.lesswrong.com/posts/XCzmXNowmuoYujXLg/you-get-to-help-the-agi-safety-act-in-congress-this-is-real,,2025-08-04T03:13:48.593000+00:00,10,5,,,
9rqMPLdpctxig2iAg,The Week in AI Governance,https://www.lesswrong.com/posts/9rqMPLdpctxig2iAg/the-week-in-ai-governance,https://www.lesswrong.com/posts/9rqMPLdpctxig2iAg/the-week-in-ai-governance,,2025-08-01T12:20:06.672000+00:00,18,1,,,
rGcg4XDPDzBFuqNJz,Research Areas in AI Control (The Alignment Project by UK AISI),https://www.lesswrong.com/posts/rGcg4XDPDzBFuqNJz/research-areas-in-ai-control-the-alignment-project-by-uk,https://www.lesswrong.com/posts/rGcg4XDPDzBFuqNJz/research-areas-in-ai-control-the-alignment-project-by-uk,,2025-08-01T10:27:09.500000+00:00,25,0,,,
tEfuDmXTGLGegsbmv,Research Areas in Methods for Post-training and Elicitation (The Alignment Project by UK AISI),https://www.lesswrong.com/posts/tEfuDmXTGLGegsbmv/research-areas-in-methods-for-post-training-and-elicitation,https://www.lesswrong.com/posts/tEfuDmXTGLGegsbmv/research-areas-in-methods-for-post-training-and-elicitation,,2025-08-01T10:27:02.618000+00:00,12,0,,,
NFKFesJJkwDDRs3Mx,Research Areas in Benchmark Design and Evaluation (The Alignment Project by UK AISI),https://www.lesswrong.com/posts/NFKFesJJkwDDRs3Mx/research-areas-in-benchmark-design-and-evaluation-the,https://www.lesswrong.com/posts/NFKFesJJkwDDRs3Mx/research-areas-in-benchmark-design-and-evaluation-the,,2025-08-01T10:26:54.931000+00:00,10,0,,,
dgcsY8CHcPQiZ5v8P,Research Areas in Interpretability (The Alignment Project by UK AISI),https://www.lesswrong.com/posts/dgcsY8CHcPQiZ5v8P/research-areas-in-interpretability-the-alignment-project-by,https://www.lesswrong.com/posts/dgcsY8CHcPQiZ5v8P/research-areas-in-interpretability-the-alignment-project-by,,2025-08-01T10:26:46.914000+00:00,14,0,,,
dzjpmq4uWyxGduRai,Research Areas in Cognitive Science (The Alignment Project by UK AISI),https://www.lesswrong.com/posts/dzjpmq4uWyxGduRai/research-areas-in-cognitive-science-the-alignment-project-by,https://www.lesswrong.com/posts/dzjpmq4uWyxGduRai/research-areas-in-cognitive-science-the-alignment-project-by,,2025-08-01T10:26:38.362000+00:00,12,0,,,
Wut4Y7LRwNpfZzG3y,Research Areas in Learning Theory (The Alignment Project by UK AISI),https://www.lesswrong.com/posts/Wut4Y7LRwNpfZzG3y/research-areas-in-learning-theory-the-alignment-project-by,https://www.lesswrong.com/posts/Wut4Y7LRwNpfZzG3y/research-areas-in-learning-theory-the-alignment-project-by,,2025-08-01T10:26:16.594000+00:00,15,0,,,
Mv5qE29RvWcau2pGA,Research Areas in Probabilistic Methods (The Alignment Project by UK AISI),https://www.lesswrong.com/posts/Mv5qE29RvWcau2pGA/research-areas-in-probabilistic-methods-the-alignment,https://www.lesswrong.com/posts/Mv5qE29RvWcau2pGA/research-areas-in-probabilistic-methods-the-alignment,,2025-08-01T10:26:07.666000+00:00,3,0,,,
o3rvXNLJTNh44oTy3,Research Areas in Economic Theory and Game Theory (The Alignment Project by UK AISI),https://www.lesswrong.com/posts/o3rvXNLJTNh44oTy3/research-areas-in-economic-theory-and-game-theory-the,https://www.lesswrong.com/posts/o3rvXNLJTNh44oTy3/research-areas-in-economic-theory-and-game-theory-the,,2025-08-01T10:25:58.917000+00:00,4,0,,,
BxwBBaLonX88dwDqT,Research Areas in Computational Complexity Theory (The Alignment Project by UK AISI),https://www.lesswrong.com/posts/BxwBBaLonX88dwDqT/research-areas-in-computational-complexity-theory-the,https://www.lesswrong.com/posts/BxwBBaLonX88dwDqT/research-areas-in-computational-complexity-theory-the,,2025-08-01T10:25:47.492000+00:00,6,0,,,
5rBXBew4T3cnnvBsh,Research Areas in Information Theory and Cryptography (The Alignment Project by UK AISI),https://www.lesswrong.com/posts/5rBXBew4T3cnnvBsh/research-areas-in-information-theory-and-cryptography-the,https://www.lesswrong.com/posts/5rBXBew4T3cnnvBsh/research-areas-in-information-theory-and-cryptography-the,,2025-08-01T10:25:28.366000+00:00,6,0,,,
5sx6jMR9YRc5ycB8c,Self-Alignment: Exploring the perspective of Analytical Psychology,https://www.lesswrong.com/posts/5sx6jMR9YRc5ycB8c/self-alignment-exploring-the-perspective-of-analytical,https://www.lesswrong.com/posts/5sx6jMR9YRc5ycB8c/self-alignment-exploring-the-perspective-of-analytical,,2025-08-01T10:17:37.065000+00:00,4,0,,,
3eeKWC62thz5Lry8t,Research Areas in Evaluation and Guarantees in Reinforcement Learning (The Alignment Project by UK AISI),https://www.lesswrong.com/posts/3eeKWC62thz5Lry8t/research-areas-in-evaluation-and-guarantees-in-reinforcement,https://www.lesswrong.com/posts/3eeKWC62thz5Lry8t/research-areas-in-evaluation-and-guarantees-in-reinforcement,,2025-08-01T09:53:19.122000+00:00,14,0,,,
wKTwdgZDo479EhmJL,The Alignment Project by UK AISI,https://www.lesswrong.com/posts/wKTwdgZDo479EhmJL/the-alignment-project-by-uk-aisi-1,https://www.lesswrong.com/posts/wKTwdgZDo479EhmJL/the-alignment-project-by-uk-aisi-1,,2025-08-01T09:52:37.314000+00:00,29,0,,,
R7MGSuFHkas9nRaca,Misalignments and RL failure modes in the early stage of superintelligence,https://www.lesswrong.com/posts/R7MGSuFHkas9nRaca/misalignments-and-rl-failure-modes-in-the-early-stage-of-3,https://www.lesswrong.com/posts/R7MGSuFHkas9nRaca/misalignments-and-rl-failure-modes-in-the-early-stage-of-3,,2025-07-29T18:23:28.104000+00:00,13,0,,,
5SwFueoWnBysmSod8,Is Interpretability for Control or for Science?,https://www.lesswrong.com/posts/5SwFueoWnBysmSod8/is-interpretability-for-control-or-for-science,https://www.lesswrong.com/posts/5SwFueoWnBysmSod8/is-interpretability-for-control-or-for-science,,2025-07-28T21:12:36.039000+00:00,3,0,,,
88xgGLnLo64AgjGco,Where are the AI safety replications?,https://www.lesswrong.com/posts/88xgGLnLo64AgjGco/where-are-the-ai-safety-replications,https://www.lesswrong.com/posts/88xgGLnLo64AgjGco/where-are-the-ai-safety-replications,,2025-07-26T21:29:48.774000+00:00,54,5,,,
DKnzpijyfrEKzdvPK,ChatGPT Agent: evals and safeguards,https://www.lesswrong.com/posts/DKnzpijyfrEKzdvPK/chatgpt-agent-evals-and-safeguards,https://www.lesswrong.com/posts/DKnzpijyfrEKzdvPK/chatgpt-agent-evals-and-safeguards,,2025-07-25T16:30:32.281000+00:00,15,0,,,
FqpAPC48CzAtvfx5C,Automating AI Safety: What we can do today,https://www.lesswrong.com/posts/FqpAPC48CzAtvfx5C/automating-ai-safety-what-we-can-do-today,https://www.lesswrong.com/posts/FqpAPC48CzAtvfx5C/automating-ai-safety-what-we-can-do-today,,2025-07-25T14:49:33.440000+00:00,36,0,,,
DJAZHYjWxMrcd2na3,Building and evaluating alignment auditing agents,https://www.lesswrong.com/posts/DJAZHYjWxMrcd2na3/building-and-evaluating-alignment-auditing-agents,https://www.lesswrong.com/posts/DJAZHYjWxMrcd2na3/building-and-evaluating-alignment-auditing-agents,,2025-07-24T19:22:26.195000+00:00,47,1,,,
qSDvzyh7LgsAJfehk,AI Safety x Physics Grand Challenge,https://www.lesswrong.com/posts/qSDvzyh7LgsAJfehk/ai-safety-x-physics-grand-challenge,https://www.lesswrong.com/posts/qSDvzyh7LgsAJfehk/ai-safety-x-physics-grand-challenge,,2025-07-23T21:41:18.023000+00:00,37,0,,,
9jhrWnxYkoZPxMZMj,"Women Want Safety, Men Want Respect",https://www.lesswrong.com/posts/9jhrWnxYkoZPxMZMj/women-want-safety-men-want-respect,https://www.lesswrong.com/posts/9jhrWnxYkoZPxMZMj/women-want-safety-men-want-respect,,2025-07-23T19:10:01.886000+00:00,18,31,,,
vxA2BnCPTaPfnJjti,Ten AI safety projects I'd like people to work on,https://www.lesswrong.com/posts/vxA2BnCPTaPfnJjti/ten-ai-safety-projects-i-d-like-people-to-work-on,https://www.lesswrong.com/posts/vxA2BnCPTaPfnJjti/ten-ai-safety-projects-i-d-like-people-to-work-on,,2025-07-23T15:28:15.769000+00:00,5,2,,,
KbFuuaBKRP7FcAADL,Transformers Don't Need LayerNorm at Inference Time: Implications for Interpretability,https://www.lesswrong.com/posts/KbFuuaBKRP7FcAADL/transformers-don-t-need-layernorm-at-inference-time,https://www.lesswrong.com/posts/KbFuuaBKRP7FcAADL/transformers-don-t-need-layernorm-at-inference-time,,2025-07-23T14:55:13.283000+00:00,31,0,,,
eaEqAzGN3uJfpfGoc,"Trusted monitoring, but with deception probes.",https://www.lesswrong.com/posts/eaEqAzGN3uJfpfGoc/trusted-monitoring-but-with-deception-probes,https://www.lesswrong.com/posts/eaEqAzGN3uJfpfGoc/trusted-monitoring-but-with-deception-probes,,2025-07-23T05:26:52.399000+00:00,31,0,,,
gbJJpm92jtxiD9zag,Inverse Scaling in Test-Time Compute,https://www.lesswrong.com/posts/gbJJpm92jtxiD9zag/inverse-scaling-in-test-time-compute-2,https://www.lesswrong.com/posts/gbJJpm92jtxiD9zag/inverse-scaling-in-test-time-compute-2,,2025-07-22T22:06:30.168000+00:00,20,2,,,
3KTgeXBfhvRKfL5kf,"The AI Safety Puzzle Everyone Avoids: How To Measure Impact, Not Intent.",https://www.lesswrong.com/posts/3KTgeXBfhvRKfL5kf/the-ai-safety-puzzle-everyone-avoids-how-to-measure-impact,https://www.lesswrong.com/posts/3KTgeXBfhvRKfL5kf/the-ai-safety-puzzle-everyone-avoids-how-to-measure-impact,,2025-07-22T18:53:44.428000+00:00,3,0,,,
T7QFM9hRhcEoGPux3,Introducing the Pathfinder Fellowship: Funding and Mentorship for AI Safety Group Organizers,https://www.lesswrong.com/posts/T7QFM9hRhcEoGPux3/introducing-the-pathfinder-fellowship-funding-and-mentorship,https://www.lesswrong.com/posts/T7QFM9hRhcEoGPux3/introducing-the-pathfinder-fellowship-funding-and-mentorship,,2025-07-22T17:11:40.202000+00:00,6,0,,,
abd9ufFpLrn5kvnLn,Directly Try Solving Alignment for 5 weeks,https://www.lesswrong.com/posts/abd9ufFpLrn5kvnLn/directly-try-solving-alignment-for-5-weeks,https://www.lesswrong.com/posts/abd9ufFpLrn5kvnLn/directly-try-solving-alignment-for-5-weeks,,2025-07-21T21:51:55.056000+00:00,80,4,,,
7sMGTwDDcEmRGb9qL,Visualizing AI Alignment Failures as Topological Navigation Errors in Conceptual Space,https://www.lesswrong.com/posts/7sMGTwDDcEmRGb9qL/visualizing-ai-alignment-failures-as-topological-navigation,https://www.lesswrong.com/posts/7sMGTwDDcEmRGb9qL/visualizing-ai-alignment-failures-as-topological-navigation,,2025-07-21T16:54:14.378000+00:00,1,0,,,
evo7ou4qaA5PHGXAQ,AI Safety course intro blog,https://www.lesswrong.com/posts/evo7ou4qaA5PHGXAQ/ai-safety-course-intro-blog,https://www.lesswrong.com/posts/evo7ou4qaA5PHGXAQ/ai-safety-course-intro-blog,,2025-07-21T02:35:55.167000+00:00,18,0,,,
bcuzjKmNZHWDuEwBz,An Outsider’s Roadmap into AI Safety Research (2025),https://www.lesswrong.com/posts/bcuzjKmNZHWDuEwBz/an-outsider-s-roadmap-into-ai-safety-research-2025,https://www.lesswrong.com/posts/bcuzjKmNZHWDuEwBz/an-outsider-s-roadmap-into-ai-safety-research-2025,,2025-07-21T02:03:26.990000+00:00,5,3,,,
jzgqTtcHhXpSDNgcF,"Your AI Safety org could get EU funding up to €9.08M. Here’s how (+ free personalized support)
Update: Webinar 18/8 Link Below",https://www.lesswrong.com/posts/jzgqTtcHhXpSDNgcF/your-ai-safety-org-could-get-eu-funding-up-to-eur9-08m-here,https://www.lesswrong.com/posts/jzgqTtcHhXpSDNgcF/your-ai-safety-org-could-get-eu-funding-up-to-eur9-08m-here,,2025-07-20T01:30:55.308000+00:00,66,4,,,
HdgwEWnQnB7Lvvn55,Can We Trust the Judge? A novel method of Modelling Human Bias and Systematic Error in Debate-Based Scalable Oversight,https://www.lesswrong.com/posts/HdgwEWnQnB7Lvvn55/can-we-trust-the-judge-a-novel-method-of-modelling-human,https://www.lesswrong.com/posts/HdgwEWnQnB7Lvvn55/can-we-trust-the-judge-a-novel-method-of-modelling-human,,2025-07-19T21:44:17.318000+00:00,1,0,,,
apyPHy6o9GzeFRTpa,Why Alignment Fails Without a Functional Model of Intelligence,https://www.lesswrong.com/posts/apyPHy6o9GzeFRTpa/why-alignment-fails-without-a-functional-model-of,https://www.lesswrong.com/posts/apyPHy6o9GzeFRTpa/why-alignment-fails-without-a-functional-model-of,,2025-07-18T18:02:33.705000+00:00,7,4,,,
FndngJxj7YpuNsXHB,Why do Mechanistic Interpretability?,https://www.lesswrong.com/posts/FndngJxj7YpuNsXHB/why-do-mechanistic-interpretability,https://www.lesswrong.com/posts/FndngJxj7YpuNsXHB/why-do-mechanistic-interpretability,,2025-07-17T23:21:30.759000+00:00,2,0,,,
LS5RhGnjDrjENbfZ5,Aurelius: A Peer-to-Peer Alignment Protocol,https://www.lesswrong.com/posts/LS5RhGnjDrjENbfZ5/aurelius-a-peer-to-peer-alignment-protocol,https://www.lesswrong.com/posts/LS5RhGnjDrjENbfZ5/aurelius-a-peer-to-peer-alignment-protocol,,2025-07-17T19:13:12.336000+00:00,3,4,,,
h9tQJd7FWmuMoYBLz,Biweekly AI Safety Comms Meetup,https://www.lesswrong.com/events/h9tQJd7FWmuMoYBLz/biweekly-ai-safety-comms-meetup,https://www.lesswrong.com/events/h9tQJd7FWmuMoYBLz/biweekly-ai-safety-comms-meetup,,2025-07-17T07:50:34.646000+00:00,5,0,,,
ZXxY2tccLapdjLbKm,Selective Generalization: Improving Capabilities While Maintaining Alignment,https://www.lesswrong.com/posts/ZXxY2tccLapdjLbKm/selective-generalization-improving-capabilities-while,https://www.lesswrong.com/posts/ZXxY2tccLapdjLbKm/selective-generalization-improving-capabilities-while,,2025-07-16T21:25:39.203000+00:00,67,4,,,
a7QEmdiqdi37PetjY,Why haven't we auto-translated all AI alignment content?,https://www.lesswrong.com/posts/a7QEmdiqdi37PetjY/why-haven-t-we-auto-translated-all-ai-alignment-content,https://www.lesswrong.com/posts/a7QEmdiqdi37PetjY/why-haven-t-we-auto-translated-all-ai-alignment-content,,2025-07-16T15:33:30.950000+00:00,22,10,,,
DqaoPNqhQhwBFqWue,Principles for Picking Practical Interpretability Projects,https://www.lesswrong.com/posts/DqaoPNqhQhwBFqWue/principles-for-picking-practical-interpretability-projects,https://www.lesswrong.com/posts/DqaoPNqhQhwBFqWue/principles-for-picking-practical-interpretability-projects,,2025-07-15T17:38:25.221000+00:00,32,0,,,
7xneDbsgj6yJDJMjK,Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety,https://www.lesswrong.com/posts/7xneDbsgj6yJDJMjK/chain-of-thought-monitorability-a-new-and-fragile,https://www.lesswrong.com/posts/7xneDbsgj6yJDJMjK/chain-of-thought-monitorability-a-new-and-fragile,,2025-07-15T16:23:17.588000+00:00,166,32,,,
wneX9x8ZKnnAenjfL,Why Eliminating Deception Won’t Align AI,https://www.lesswrong.com/posts/wneX9x8ZKnnAenjfL/why-eliminating-deception-won-t-align-ai,https://www.lesswrong.com/posts/wneX9x8ZKnnAenjfL/why-eliminating-deception-won-t-align-ai,,2025-07-15T09:21:18.695000+00:00,19,6,,,
NvdbjBvdXZs6Zy8rN,AISafety.com Hackathon 2025,https://www.lesswrong.com/events/NvdbjBvdXZs6Zy8rN/aisafety-com-hackathon-2025,https://www.lesswrong.com/events/NvdbjBvdXZs6Zy8rN/aisafety-com-hackathon-2025,,2025-07-15T00:04:19.056000+00:00,12,0,,,
gLDSqQm8pwNiq7qst,"Narrow Misalignment is Hard, Emergent Misalignment is Easy",https://www.lesswrong.com/posts/gLDSqQm8pwNiq7qst/narrow-misalignment-is-hard-emergent-misalignment-is-easy,https://www.lesswrong.com/posts/gLDSqQm8pwNiq7qst/narrow-misalignment-is-hard-emergent-misalignment-is-easy,,2025-07-14T21:05:57.653000+00:00,133,24,,,
FhyegsQsnj9J6nzep,"Visualizing AI Alignment – CFP for AGI-2025 Workshop (Aug 10, Live + Virtual)",https://www.lesswrong.com/posts/FhyegsQsnj9J6nzep/visualizing-ai-alignment-cfp-for-agi-2025-workshop-aug-10,https://www.lesswrong.com/posts/FhyegsQsnj9J6nzep/visualizing-ai-alignment-cfp-for-agi-2025-workshop-aug-10,,2025-07-14T20:12:46.764000+00:00,9,0,,,
9tHEibBBhQCHEyFsa,"Do LLMs know what they're capable of? Why this matters for AI safety, and initial findings",https://www.lesswrong.com/posts/9tHEibBBhQCHEyFsa/do-llms-know-what-they-re-capable-of-why-this-matters-for-ai,https://www.lesswrong.com/posts/9tHEibBBhQCHEyFsa/do-llms-know-what-they-re-capable-of-why-this-matters-for-ai,,2025-07-13T19:54:53.974000+00:00,53,5,,,
dqd54wpEfjKJsJBk6,xAI's Grok 4 has no meaningful safety guardrails,https://www.lesswrong.com/posts/dqd54wpEfjKJsJBk6/xai-s-grok-4-has-no-meaningful-safety-guardrails,https://www.lesswrong.com/posts/dqd54wpEfjKJsJBk6/xai-s-grok-4-has-no-meaningful-safety-guardrails,,2025-07-13T18:22:31.350000+00:00,84,15,,,
jZfNX4GJPdptxSxt4,Adding noise to a sandbagging model can reveal its true capabilities,https://www.lesswrong.com/posts/jZfNX4GJPdptxSxt4/adding-noise-to-a-sandbagging-model-can-reveal-its-true,https://www.lesswrong.com/posts/jZfNX4GJPdptxSxt4/adding-noise-to-a-sandbagging-model-can-reveal-its-true,,2025-07-11T16:56:17.043000+00:00,16,1,,,
8EFJiyaQcb4D5f5dG,My take on AI Alignment: Corporate misalignment and DAOs,https://www.lesswrong.com/posts/8EFJiyaQcb4D5f5dG/my-take-on-ai-alignment-corporate-misalignment-and-daos,https://www.lesswrong.com/posts/8EFJiyaQcb4D5f5dG/my-take-on-ai-alignment-corporate-misalignment-and-daos,,2025-07-10T20:33:05.883000+00:00,7,3,,,
ZdBj6DfyGRuqJ27Yv,Investigating Priming in Alignment Faking,https://www.lesswrong.com/posts/ZdBj6DfyGRuqJ27Yv/investigating-priming-in-alignment-faking,https://www.lesswrong.com/posts/ZdBj6DfyGRuqJ27Yv/investigating-priming-in-alignment-faking,,2025-07-09T17:08:11.031000+00:00,13,0,,,
25dsPH6CuRXPBkGHN,"No, We're Not Getting Meaningful Oversight of AI",https://www.lesswrong.com/posts/25dsPH6CuRXPBkGHN/no-we-re-not-getting-meaningful-oversight-of-ai,https://www.lesswrong.com/posts/25dsPH6CuRXPBkGHN/no-we-re-not-getting-meaningful-oversight-of-ai,,2025-07-09T11:10:54.429000+00:00,45,4,,,
ghESoA8mo3fv9Yx3E,Why Do Some Language Models Fake Alignment While Others Don't?,https://www.lesswrong.com/posts/ghESoA8mo3fv9Yx3E/why-do-some-language-models-fake-alignment-while-others-don,https://www.lesswrong.com/posts/ghESoA8mo3fv9Yx3E/why-do-some-language-models-fake-alignment-while-others-don,,2025-07-08T21:49:30.088000+00:00,158,14,,,
GxHDfFELz9AD3nKRX,MIT FutureTech are hiring a Postdoctoral Associate to work on AI Performance and Safety,https://www.lesswrong.com/posts/GxHDfFELz9AD3nKRX/mit-futuretech-are-hiring-a-postdoctoral-associate-to-work,https://www.lesswrong.com/posts/GxHDfFELz9AD3nKRX/mit-futuretech-are-hiring-a-postdoctoral-associate-to-work,,2025-07-08T14:02:57.814000+00:00,3,0,,,
Kg4dkWdt2q5djxWy3,"AI Safety at the Frontier: Paper Highlights, June '25",https://www.lesswrong.com/posts/Kg4dkWdt2q5djxWy3/ai-safety-at-the-frontier-paper-highlights-june-25,https://www.lesswrong.com/posts/Kg4dkWdt2q5djxWy3/ai-safety-at-the-frontier-paper-highlights-june-25,,2025-07-07T18:17:15.410000+00:00,4,0,,,
nZtAkGmDELMnLJMQ5,AXRP Episode 45 - Samuel Albanie on DeepMind’s AGI Safety Approach,https://www.lesswrong.com/posts/nZtAkGmDELMnLJMQ5/axrp-episode-45-samuel-albanie-on-deepmind-s-agi-safety,https://www.lesswrong.com/posts/nZtAkGmDELMnLJMQ5/axrp-episode-45-samuel-albanie-on-deepmind-s-agi-safety,,2025-07-06T23:00:03.659000+00:00,31,0,,,
rzCF2T7iLPEgNa59Y,Rational Animations' video about scalable oversight and sandwiching,https://www.lesswrong.com/posts/rzCF2T7iLPEgNa59Y/rational-animations-video-about-scalable-oversight-and,https://www.lesswrong.com/posts/rzCF2T7iLPEgNa59Y/rational-animations-video-about-scalable-oversight-and,,2025-07-06T14:00:41.186000+00:00,18,0,,,
mDmsNJ9JxHEpBYDvd,When the Smarter AI Lies Better: Can Debate-Based Oversight Catch Deceptive Code,https://www.lesswrong.com/posts/mDmsNJ9JxHEpBYDvd/when-the-smarter-ai-lies-better-can-debate-based-oversight,https://www.lesswrong.com/posts/mDmsNJ9JxHEpBYDvd/when-the-smarter-ai-lies-better-can-debate-based-oversight,,2025-07-06T01:21:05.403000+00:00,4,0,,,
qu5myZA4jqFczYH7a,Small foundational puzzle for causal theories of mechanistic interpretability,https://www.lesswrong.com/posts/qu5myZA4jqFczYH7a/small-foundational-puzzle-for-causal-theories-of-mechanistic,https://www.lesswrong.com/posts/qu5myZA4jqFczYH7a/small-foundational-puzzle-for-causal-theories-of-mechanistic,,2025-07-05T17:46:26.905000+00:00,6,6,,,
nwx6duiDZcHatbpPT,Early Signs of Steganographic Capabilities in Frontier LLMs,https://www.lesswrong.com/posts/nwx6duiDZcHatbpPT/untitled-draft-6osz,https://www.lesswrong.com/posts/nwx6duiDZcHatbpPT/untitled-draft-6osz,,2025-07-04T16:36:54.136000+00:00,30,5,,,
f9dgyttsuC4sK3Nn7,Contest for Better AGI Safety Plans,https://www.lesswrong.com/posts/f9dgyttsuC4sK3Nn7/contest-for-better-agi-safety-plans,https://www.lesswrong.com/posts/f9dgyttsuC4sK3Nn7/contest-for-better-agi-safety-plans,,2025-07-03T17:02:41.064000+00:00,29,1,,,
9tqpPP4FwSnv9AWsi,Research Note: Our scheming precursor evals had limited predictive power for our in-context scheming evals,https://www.lesswrong.com/posts/9tqpPP4FwSnv9AWsi/research-note-our-scheming-precursor-evals-had-limited,https://www.lesswrong.com/posts/9tqpPP4FwSnv9AWsi/research-note-our-scheming-precursor-evals-had-limited,,2025-07-03T15:57:07.779000+00:00,75,0,,,
qe8LjXAtaZfrc8No7,Call for suggestions - AI safety course,https://www.lesswrong.com/posts/qe8LjXAtaZfrc8No7/call-for-suggestions-ai-safety-course,https://www.lesswrong.com/posts/qe8LjXAtaZfrc8No7/call-for-suggestions-ai-safety-course,,2025-07-03T14:30:41.147000+00:00,53,23,,,
YBCas9SHcfosExMhg,On The Formal Definition of Alignment,https://www.lesswrong.com/posts/YBCas9SHcfosExMhg/on-the-formal-definition-of-alignment,https://www.lesswrong.com/posts/YBCas9SHcfosExMhg/on-the-formal-definition-of-alignment,,2025-07-02T00:05:08.393000+00:00,4,3,,,
J7CyENFYXPxXQpsnD,SLT for AI Safety,https://www.lesswrong.com/posts/J7CyENFYXPxXQpsnD/slt-for-ai-safety,https://www.lesswrong.com/posts/J7CyENFYXPxXQpsnD/slt-for-ai-safety,,2025-07-01T04:52:21.093000+00:00,63,0,,,
GSuoKJYTQYPktBp8A,From Diamond Mining to Open-World Survival: Alignment and Emergent Behavior in Minecraft Agents,https://www.lesswrong.com/posts/GSuoKJYTQYPktBp8A/from-diamond-mining-to-open-world-survival-alignment-and-1,https://www.lesswrong.com/posts/GSuoKJYTQYPktBp8A/from-diamond-mining-to-open-world-survival-alignment-and-1,,2025-06-30T03:17:49.807000+00:00,15,0,,,
vM4PurXtaBH5iGtP5,An Alternative Way to Forecast AGI: Counting Down Capabilities ,https://www.lesswrong.com/posts/vM4PurXtaBH5iGtP5/an-alternative-way-to-forecast-agi-counting-down,https://www.lesswrong.com/posts/vM4PurXtaBH5iGtP5/an-alternative-way-to-forecast-agi-counting-down,,2025-06-29T19:52:00.136000+00:00,3,0,,,
p28GHSYskzsGKvABH,I underestimated safety research speedups from safe AI,https://www.lesswrong.com/posts/p28GHSYskzsGKvABH/i-underestimated-safety-research-speedups-from-safe-ai,https://www.lesswrong.com/posts/p28GHSYskzsGKvABH/i-underestimated-safety-research-speedups-from-safe-ai,,2025-06-29T13:29:44.904000+00:00,38,2,,,
EFohZdFPGj2iphnvB,Do Self-Perceived Superintelligent LLMs Exhibit Misalignment?,https://www.lesswrong.com/posts/EFohZdFPGj2iphnvB/do-self-perceived-superintelligent-llms-exhibit-misalignment,https://www.lesswrong.com/posts/EFohZdFPGj2iphnvB/do-self-perceived-superintelligent-llms-exhibit-misalignment,,2025-06-29T11:06:42.799000+00:00,26,2,,,
8xxh7dXQXbhaTJqt5,Feedback wanted: Shortlist of AI safety ideas,https://www.lesswrong.com/posts/8xxh7dXQXbhaTJqt5/feedback-wanted-shortlist-of-ai-safety-ideas,https://www.lesswrong.com/posts/8xxh7dXQXbhaTJqt5/feedback-wanted-shortlist-of-ai-safety-ideas,,2025-06-29T04:28:01.521000+00:00,8,3,,,
vHDowQtsiy2xK38H4,AXRP Episode 44 - Peter Salib on AI Rights for Human Safety,https://www.lesswrong.com/posts/vHDowQtsiy2xK38H4/axrp-episode-44-peter-salib-on-ai-rights-for-human-safety,https://www.lesswrong.com/posts/vHDowQtsiy2xK38H4/axrp-episode-44-peter-salib-on-ai-rights-for-human-safety,,2025-06-28T01:40:04.658000+00:00,12,0,,,
ZdY4JzBPJEgaoCxTR,Emergent Misalignment & Realignment,https://www.lesswrong.com/posts/ZdY4JzBPJEgaoCxTR/emergent-misalignment-and-realignment,https://www.lesswrong.com/posts/ZdY4JzBPJEgaoCxTR/emergent-misalignment-and-realignment,,2025-06-27T21:31:43.367000+00:00,45,1,,,
dJJkhk5nZoXfiQgHz,"RLAIF/RLHF for Public Value Alignment
Enhancing Transparency in LLMs",https://www.lesswrong.com/posts/dJJkhk5nZoXfiQgHz/rlaif-rlhf-for-public-value-alignment-enhancing-transparency,https://www.lesswrong.com/posts/dJJkhk5nZoXfiQgHz/rlaif-rlhf-for-public-value-alignment-enhancing-transparency,,2025-06-26T18:32:12.002000+00:00,1,0,,,
wRsQowKKbgyXv2eni,I Tested LLM Agents on Simple Safety Rules. They Failed in Surprising and Informative Ways.,https://www.lesswrong.com/posts/wRsQowKKbgyXv2eni/i-tested-llm-agents-on-simple-safety-rules-they-failed-in,https://www.lesswrong.com/posts/wRsQowKKbgyXv2eni/i-tested-llm-agents-on-simple-safety-rules-they-failed-in,,2025-06-25T21:39:08.074000+00:00,9,12,,,
qgFM8YzcDZXPZovLb,Double Podcast Drop on AI Safety,https://www.lesswrong.com/posts/qgFM8YzcDZXPZovLb/double-podcast-drop-on-ai-safety,https://www.lesswrong.com/posts/qgFM8YzcDZXPZovLb/double-podcast-drop-on-ai-safety,,2025-06-25T20:11:49.307000+00:00,5,0,,,
hMuj4kGKvaLBELPBa,An Analogy for Interpretability,https://www.lesswrong.com/posts/hMuj4kGKvaLBELPBa/an-analogy-for-interpretability,https://www.lesswrong.com/posts/hMuj4kGKvaLBELPBa/an-analogy-for-interpretability,,2025-06-24T14:56:33.526000+00:00,12,2,,,
KCL9L68NsFcRNcg3k,The Rose Test: a fun way to feel with your guts (not just logically understand) why AI-safety matters right now (and get new adepts),https://www.lesswrong.com/posts/KCL9L68NsFcRNcg3k/the-rose-test-a-fun-way-to-feel-with-your-guts-not-just,https://www.lesswrong.com/posts/KCL9L68NsFcRNcg3k/the-rose-test-a-fun-way-to-feel-with-your-guts-not-just,,2025-06-23T21:02:00.529000+00:00,10,0,,,
Ttjiet8q5Z2ACwgbC,Knowledge Extraction Plan (KEP): An alternative to reckless scaling,https://www.lesswrong.com/posts/Ttjiet8q5Z2ACwgbC/knowledge-extraction-plan-kep-an-alternative-to-reckless,https://www.lesswrong.com/posts/Ttjiet8q5Z2ACwgbC/knowledge-extraction-plan-kep-an-alternative-to-reckless,,2025-06-23T20:58:21.429000+00:00,1,0,,,
bnnKGSCHJghAvqPjS,Foom & Doom 2: Technical alignment is hard,https://www.lesswrong.com/posts/bnnKGSCHJghAvqPjS/foom-and-doom-2-technical-alignment-is-hard,https://www.lesswrong.com/posts/bnnKGSCHJghAvqPjS/foom-and-doom-2-technical-alignment-is-hard,,2025-06-23T17:19:50.691000+00:00,152,65,,,
y9QEHKdqsfsXBMAkb,Mainstream Grantmaking Expertise (Post 7 of 7 on AI Governance),https://www.lesswrong.com/posts/y9QEHKdqsfsXBMAkb/mainstream-grantmaking-expertise-post-7-of-7-on-ai,https://www.lesswrong.com/posts/y9QEHKdqsfsXBMAkb/mainstream-grantmaking-expertise-post-7-of-7-on-ai,,2025-06-23T01:39:43.466000+00:00,56,7,,,
Jo6LPyp7t3rPuf8Ao,Black-box interpretability methodology blueprint: Probing runaway optimisation in LLMs,https://www.lesswrong.com/posts/Jo6LPyp7t3rPuf8Ao/black-box-interpretability-methodology-blueprint-probing,https://www.lesswrong.com/posts/Jo6LPyp7t3rPuf8Ao/black-box-interpretability-methodology-blueprint-probing,,2025-06-22T18:16:19.078000+00:00,17,0,,,
b8eeCGe3FWzHKbePF,Agentic Misalignment: How LLMs Could be Insider Threats,https://www.lesswrong.com/posts/b8eeCGe3FWzHKbePF/agentic-misalignment-how-llms-could-be-insider-threats-1,https://www.lesswrong.com/posts/b8eeCGe3FWzHKbePF/agentic-misalignment-how-llms-could-be-insider-threats-1,,2025-06-20T22:34:59.515000+00:00,78,13,,,
fgjY883grBeZo5v7d,AI Safety Communicators Meet-up,https://www.lesswrong.com/events/fgjY883grBeZo5v7d/ai-safety-communicators-meet-up,https://www.lesswrong.com/events/fgjY883grBeZo5v7d/ai-safety-communicators-meet-up,,2025-06-20T12:34:54.611000+00:00,3,0,,,
qbiwwasYdHLd9YAA7,Misalignment or misuse? The AGI alignment tradeoff,https://www.lesswrong.com/posts/qbiwwasYdHLd9YAA7/misalignment-or-misuse-the-agi-alignment-tradeoff,https://www.lesswrong.com/posts/qbiwwasYdHLd9YAA7/misalignment-or-misuse-the-agi-alignment-tradeoff,,2025-06-20T10:43:36.537000+00:00,3,0,,,
aeBAkCPqWHscrAZKA,"S-Expressions as a Design Language: A Tool for Deconfusion in Alignment
",https://www.lesswrong.com/posts/aeBAkCPqWHscrAZKA/s-expressions-as-a-design-language-a-tool-for-deconfusion-in,https://www.lesswrong.com/posts/aeBAkCPqWHscrAZKA/s-expressions-as-a-design-language-a-tool-for-deconfusion-in,,2025-06-19T19:03:13.418000+00:00,5,0,,,
w4HtqiGmS5cndwiqL,"Key paths, plans and strategies to AI safety success",https://www.lesswrong.com/posts/w4HtqiGmS5cndwiqL/key-paths-plans-and-strategies-to-ai-safety-success-1,https://www.lesswrong.com/posts/w4HtqiGmS5cndwiqL/key-paths-plans-and-strategies-to-ai-safety-success-1,,2025-06-19T16:56:09.025000+00:00,12,0,,,
8KKujApx4g7FBm6hE,AI safety techniques leveraging distillation,https://www.lesswrong.com/posts/8KKujApx4g7FBm6hE/ai-safety-techniques-leveraging-distillation,https://www.lesswrong.com/posts/8KKujApx4g7FBm6hE/ai-safety-techniques-leveraging-distillation,,2025-06-19T14:31:02.632000+00:00,61,0,,,
adQueu9FFHfiBKDCt,Political Funding Expertise (Post 6 of 7 on AI Governance),https://www.lesswrong.com/posts/adQueu9FFHfiBKDCt/political-funding-expertise-post-6-of-7-on-ai-governance,https://www.lesswrong.com/posts/adQueu9FFHfiBKDCt/political-funding-expertise-post-6-of-7-on-ai-governance,,2025-06-19T14:14:31.909000+00:00,59,4,,,
jodcNpqTs82gj9ccJ,How did you find out about AI Safety? Why and how did you get involved?,https://www.lesswrong.com/posts/jodcNpqTs82gj9ccJ/how-did-you-find-out-about-ai-safety-why-and-how-did-you-get,https://www.lesswrong.com/posts/jodcNpqTs82gj9ccJ/how-did-you-find-out-about-ai-safety-why-and-how-did-you-get,,2025-06-19T14:00:18.958000+00:00,1,0,,,
kiCZzHDkRtupek2mc,My Failed AI Safety Research Projects (Q1/Q2 2025),https://www.lesswrong.com/posts/kiCZzHDkRtupek2mc/my-failed-ai-safety-research-projects-q1-q2-2025,https://www.lesswrong.com/posts/kiCZzHDkRtupek2mc/my-failed-ai-safety-research-projects-q1-q2-2025,,2025-06-19T03:55:40.363000+00:00,26,3,,,
A9SLAx8XFp2gCNJCJ,Moral Alignment: An Idea I'm Embarrassed I Didn't Think of Myself,https://www.lesswrong.com/posts/A9SLAx8XFp2gCNJCJ/moral-alignment-an-idea-i-m-embarrassed-i-didn-t-think-of,https://www.lesswrong.com/posts/A9SLAx8XFp2gCNJCJ/moral-alignment-an-idea-i-m-embarrassed-i-didn-t-think-of,,2025-06-18T15:42:33.491000+00:00,21,54,,,
wpB7JCMJgpzLC7Hej,"AI Safety at the Frontier: Paper Highlights, May '25",https://www.lesswrong.com/posts/wpB7JCMJgpzLC7Hej/ai-safety-at-the-frontier-paper-highlights-may-25,https://www.lesswrong.com/posts/wpB7JCMJgpzLC7Hej/ai-safety-at-the-frontier-paper-highlights-may-25,,2025-06-17T17:16:39.863000+00:00,6,0,,,
s9z4mgjtWTPpDLxFy,Agentic Interpretability: A Strategy Against Gradual Disempowerment,https://www.lesswrong.com/posts/s9z4mgjtWTPpDLxFy/agentic-interpretability-a-strategy-against-gradual,https://www.lesswrong.com/posts/s9z4mgjtWTPpDLxFy/agentic-interpretability-a-strategy-against-gradual,,2025-06-17T14:52:55.695000+00:00,16,6,,,
8XHBaugB5S3r27MG9,"Prover-Estimator Debate: 
A New Scalable Oversight Protocol",https://www.lesswrong.com/posts/8XHBaugB5S3r27MG9/prover-estimator-debate-a-new-scalable-oversight-protocol,https://www.lesswrong.com/posts/8XHBaugB5S3r27MG9/prover-estimator-debate-a-new-scalable-oversight-protocol,,2025-06-17T13:53:04.125000+00:00,88,18,,,
zzZ6jye3ukiNyMCmC,Thought Crime: Backdoors & Emergent Misalignment in Reasoning Models,https://www.lesswrong.com/posts/zzZ6jye3ukiNyMCmC/thought-crime-backdoors-and-emergent-misalignment-in,https://www.lesswrong.com/posts/zzZ6jye3ukiNyMCmC/thought-crime-backdoors-and-emergent-misalignment-in,,2025-06-16T16:43:12.826000+00:00,68,2,,,
umYzsh7SGHHKsRCaA,Convergent Linear Representations of Emergent Misalignment,https://www.lesswrong.com/posts/umYzsh7SGHHKsRCaA/convergent-linear-representations-of-emergent-misalignment,https://www.lesswrong.com/posts/umYzsh7SGHHKsRCaA/convergent-linear-representations-of-emergent-misalignment,,2025-06-16T15:47:15.487000+00:00,68,1,,,
yHmJrDSJpFaNTZ9Tr,Model Organisms for Emergent Misalignment,https://www.lesswrong.com/posts/yHmJrDSJpFaNTZ9Tr/model-organisms-for-emergent-misalignment,https://www.lesswrong.com/posts/yHmJrDSJpFaNTZ9Tr/model-organisms-for-emergent-misalignment,,2025-06-16T15:46:41.739000+00:00,111,18,,,
iviMQQAw7E2kMJmSp,Coaching AI: A Relational Approach to AI Safety,https://www.lesswrong.com/posts/iviMQQAw7E2kMJmSp/coaching-ai-a-relational-approach-to-ai-safety-1,https://www.lesswrong.com/posts/iviMQQAw7E2kMJmSp/coaching-ai-a-relational-approach-to-ai-safety-1,,2025-06-16T15:33:36.254000+00:00,11,0,,,
B2o6nrxwKxLPsSYdh,Do LLMs Comply Differently During Tests? Is This a Hidden Variable in Safety Evaluation? And Can We Steer That?,https://www.lesswrong.com/posts/B2o6nrxwKxLPsSYdh/do-llms-comply-differently-during-tests-is-this-a-hidden,https://www.lesswrong.com/posts/B2o6nrxwKxLPsSYdh/do-llms-comply-differently-during-tests-is-this-a-hidden,,2025-06-16T13:52:03.008000+00:00,17,0,,,
KS77aiHREj9YWbBfR,Risk Tokens: Economic Security in AI Safety,https://www.lesswrong.com/posts/KS77aiHREj9YWbBfR/risk-tokens-economic-security-in-ai-safety,https://www.lesswrong.com/posts/KS77aiHREj9YWbBfR/risk-tokens-economic-security-in-ai-safety,,2025-06-15T19:25:20.936000+00:00,1,0,,,
bSm678CPsSLdDQLEx,"Foresight Institute AI safety RFPs in automation, security, multi-agent, neuro",https://www.lesswrong.com/posts/bSm678CPsSLdDQLEx/foresight-institute-ai-safety-rfps-in-automation-security,https://www.lesswrong.com/posts/bSm678CPsSLdDQLEx/foresight-institute-ai-safety-rfps-in-automation-security,,2025-06-14T16:29:20.189000+00:00,6,0,,,
mfSJ5WW72eb4bPLCs,How could I tell someone that consciousness is not the primary concern of AI Safety?,https://www.lesswrong.com/posts/mfSJ5WW72eb4bPLCs/how-could-i-tell-someone-that-consciousness-is-not-the,https://www.lesswrong.com/posts/mfSJ5WW72eb4bPLCs/how-could-i-tell-someone-that-consciousness-is-not-the,,2025-06-13T22:44:34.319000+00:00,11,2,,,
g3RXozhPmcLm2yDps,"Constraining Minds, Not Goals: A Structural Approach to AI Alignment",https://www.lesswrong.com/posts/g3RXozhPmcLm2yDps/constraining-minds-not-goals-a-structural-approach-to-ai,https://www.lesswrong.com/posts/g3RXozhPmcLm2yDps/constraining-minds-not-goals-a-structural-approach-to-ai,,2025-06-13T21:06:40.984000+00:00,25,0,,,
N6KHaiNCx463zpkGq,Under what conditions should humans stop pursuing technical AI safety careers?,https://www.lesswrong.com/posts/N6KHaiNCx463zpkGq/under-what-conditions-should-humans-stop-pursuing-technical,https://www.lesswrong.com/posts/N6KHaiNCx463zpkGq/under-what-conditions-should-humans-stop-pursuing-technical,,2025-06-13T05:56:07.911000+00:00,6,0,,,
kqmdoDFZzAS59LZZm,"[linkpost] AI Alignment is About Culture, Not Control by JCorvinus",https://www.lesswrong.com/posts/kqmdoDFZzAS59LZZm/linkpost-ai-alignment-is-about-culture-not-control-by,https://www.lesswrong.com/posts/kqmdoDFZzAS59LZZm/linkpost-ai-alignment-is-about-culture-not-control-by,,2025-06-13T00:07:21.834000+00:00,1,8,,,
xM4SLiLZLoRuQwHMs,Restraining Factors in AI Alignment Systems ,https://www.lesswrong.com/posts/xM4SLiLZLoRuQwHMs/restraining-factors-in-ai-alignment-systems-1,https://www.lesswrong.com/posts/xM4SLiLZLoRuQwHMs/restraining-factors-in-ai-alignment-systems-1,,2025-06-12T18:17:32.252000+00:00,1,1,,,
JPnDJGqm7uw2K4xnW,"My friend wants a good book recommendation to understand AI, AI safety, and the field, and probably the drama. He’s smart but non-technical and not keeping up with trends. Any recs?",https://www.lesswrong.com/posts/JPnDJGqm7uw2K4xnW/my-friend-wants-a-good-book-recommendation-to-understand-ai,https://www.lesswrong.com/posts/JPnDJGqm7uw2K4xnW/my-friend-wants-a-good-book-recommendation-to-understand-ai,,2025-06-11T22:32:37.808000+00:00,9,0,,,
zmSafQK7JjmfydAtT,Investigating Accidental Misalignment: Causal Effects of Fine-Tuning Data on Model Vulnerability,https://www.lesswrong.com/posts/zmSafQK7JjmfydAtT/investigating-accidental-misalignment-causal-effects-of-fine,https://www.lesswrong.com/posts/zmSafQK7JjmfydAtT/investigating-accidental-misalignment-causal-effects-of-fine,,2025-06-11T19:30:17.553000+00:00,6,0,,,
5uw26uDdFbFQgKzih,Beware General Claims about “Generalizable Reasoning Capabilities” (of Modern AI Systems),https://www.lesswrong.com/posts/5uw26uDdFbFQgKzih/beware-general-claims-about-generalizable-reasoning,https://www.lesswrong.com/posts/5uw26uDdFbFQgKzih/beware-general-claims-about-generalizable-reasoning,,2025-06-11T19:27:33.648000+00:00,295,19,,,
P5ig59wyeA2et58EF,SafeRLHub: An Interactive Resource for RL Safety and Interpretability,https://www.lesswrong.com/posts/P5ig59wyeA2et58EF/saferlhub-an-interactive-resource-for-rl-safety-and,https://www.lesswrong.com/posts/P5ig59wyeA2et58EF/saferlhub-an-interactive-resource-for-rl-safety-and,,2025-06-11T05:47:21.693000+00:00,11,0,,,
TeF8Az2EiWenR9APF,When is it important that open-weight models aren't released? My thoughts on the benefits and dangers of open-weight models in response to developments in CBRN capabilities.,https://www.lesswrong.com/posts/TeF8Az2EiWenR9APF/when-is-it-important-that-open-weight-models-aren-t-released,https://www.lesswrong.com/posts/TeF8Az2EiWenR9APF/when-is-it-important-that-open-weight-models-aren-t-released,,2025-06-09T19:19:39.861000+00:00,63,11,,,
3WyFmtiLZTfEQxJCy,"Identifying ""Deception Vectors"" In Models",https://www.lesswrong.com/posts/3WyFmtiLZTfEQxJCy/identifying-deception-vectors-in-models,https://www.lesswrong.com/posts/3WyFmtiLZTfEQxJCy/identifying-deception-vectors-in-models,,2025-06-09T17:30:22.634000+00:00,12,0,,,
rpKPgzjr3tPkDZChg,Outer Alignment is the Necessary Compliment to AI 2027's Best Case Scenario,https://www.lesswrong.com/posts/rpKPgzjr3tPkDZChg/outer-alignment-is-the-necessary-compliment-to-ai-2027-s,https://www.lesswrong.com/posts/rpKPgzjr3tPkDZChg/outer-alignment-is-the-necessary-compliment-to-ai-2027-s,,2025-06-09T15:43:40.505000+00:00,4,2,,,
qHudHZNLCiFrygRiy,Emergent Misalignment on a Budget,https://www.lesswrong.com/posts/qHudHZNLCiFrygRiy/emergent-misalignment-on-a-budget,https://www.lesswrong.com/posts/qHudHZNLCiFrygRiy/emergent-misalignment-on-a-budget,,2025-06-08T15:28:50.498000+00:00,54,0,,,
wcNDv7DEy43dpnumY,Meta Alignment: Communication Guide,https://www.lesswrong.com/posts/wcNDv7DEy43dpnumY/meta-alignment-communication-guide,https://www.lesswrong.com/posts/wcNDv7DEy43dpnumY/meta-alignment-communication-guide,,2025-06-07T16:09:40.972000+00:00,13,0,,,
nnHnNdHLhbrnmEXDr,Exploring vocabulary alignment of neurons in Llama-3.2-1B,https://www.lesswrong.com/posts/nnHnNdHLhbrnmEXDr/exploring-vocabulary-alignment-of-neurons-in-llama-3-2-1b,https://www.lesswrong.com/posts/nnHnNdHLhbrnmEXDr/exploring-vocabulary-alignment-of-neurons-in-llama-3-2-1b,,2025-06-07T11:20:23.410000+00:00,4,0,,,
Tsh9Y4cDvzeHN3ETx,"Agents, Simulators and Interpretability",https://www.lesswrong.com/posts/Tsh9Y4cDvzeHN3ETx/agents-simulators-and-interpretability,https://www.lesswrong.com/posts/Tsh9Y4cDvzeHN3ETx/agents-simulators-and-interpretability,,2025-06-07T06:06:01.258000+00:00,11,0,,,
Ckhek3mXXq7TWvvEh,Lessons from a year of university AI safety field building,https://www.lesswrong.com/posts/Ckhek3mXXq7TWvvEh/lessons-from-a-year-of-university-ai-safety-field-building,https://www.lesswrong.com/posts/Ckhek3mXXq7TWvvEh/lessons-from-a-year-of-university-ai-safety-field-building,,2025-06-06T14:35:14.533000+00:00,29,3,,,
dT7mvHzuX46vydt9K,Avoiding AI Deception: Lie Detectors can either Induce Honesty or Evasion,https://www.lesswrong.com/posts/dT7mvHzuX46vydt9K/avoiding-ai-deception-lie-detectors-can-either-induce,https://www.lesswrong.com/posts/dT7mvHzuX46vydt9K/avoiding-ai-deception-lie-detectors-can-either-induce,,2025-06-05T23:07:59.889000+00:00,22,2,,,
HFSEHFB9WLZLkPwnc,Introducing: Meridian Cambridge's new online lecture series covering frontier AI and AI safety,https://www.lesswrong.com/posts/HFSEHFB9WLZLkPwnc/introducing-meridian-cambridge-s-new-online-lecture-series,https://www.lesswrong.com/posts/HFSEHFB9WLZLkPwnc/introducing-meridian-cambridge-s-new-online-lecture-series,,2025-06-05T21:55:25.902000+00:00,1,0,,,
9JCcK5fjyr8qLYnPm,Self-Coordinated Deception in Current AI Models,https://www.lesswrong.com/posts/9JCcK5fjyr8qLYnPm/self-coordinated-deception-in-current-ai-models,https://www.lesswrong.com/posts/9JCcK5fjyr8qLYnPm/self-coordinated-deception-in-current-ai-models,,2025-06-04T17:59:30.885000+00:00,8,5,,,
6c6c3thcDvHPuuvTv,Notes from a mini-replication of the alignment faking paper,https://www.lesswrong.com/posts/6c6c3thcDvHPuuvTv/notes-from-a-mini-replication-of-the-alignment-faking-paper,https://www.lesswrong.com/posts/6c6c3thcDvHPuuvTv/notes-from-a-mini-replication-of-the-alignment-faking-paper,,2025-06-04T11:01:23.559000+00:00,13,5,,,
gE4FbbXAqEmAjrzRF,What AI apps are surprisingly absent given current capabilities?,https://www.lesswrong.com/posts/gE4FbbXAqEmAjrzRF/what-ai-apps-are-surprisingly-absent-given-current,https://www.lesswrong.com/posts/gE4FbbXAqEmAjrzRF/what-ai-apps-are-surprisingly-absent-given-current,,2025-06-02T18:46:08.839000+00:00,4,8,,,
seEF372qGJhgKLBsv,"Eliezer Yudkowsky & Connor Leahy | AI Risk, Safety & Alignment Q&A [4K Remaster + HQ Audio]",https://www.lesswrong.com/posts/seEF372qGJhgKLBsv/eliezer-yudkowsky-and-connor-leahy-or-ai-risk-safety-and,https://www.lesswrong.com/posts/seEF372qGJhgKLBsv/eliezer-yudkowsky-and-connor-leahy-or-ai-risk-safety-and,,2025-06-01T20:20:47.762000+00:00,-8,2,,,
C4tvfHn2DfxyYYwaL,"Policy Entropy, Learning, and Alignment (Or Maybe Your LLM Needs Therapy)",https://www.lesswrong.com/posts/C4tvfHn2DfxyYYwaL/policy-entropy-learning-and-alignment-or-maybe-your-llm,https://www.lesswrong.com/posts/C4tvfHn2DfxyYYwaL/policy-entropy-learning-and-alignment-or-maybe-your-llm,,2025-05-31T22:09:51.411000+00:00,15,6,,,
9uTZdc4T9jHBnDvGF,"Collective Action for AI Safety (June 4, NYC)",https://www.lesswrong.com/events/9uTZdc4T9jHBnDvGF/collective-action-for-ai-safety-june-4-nyc,https://www.lesswrong.com/events/9uTZdc4T9jHBnDvGF/collective-action-for-ai-safety-june-4-nyc,,2025-05-31T20:27:03.401000+00:00,1,0,,,
QHcQAbNHoaNoE5YBM,Too Many Metaphors: A Case for Plain Talk in AI Safety,https://www.lesswrong.com/posts/QHcQAbNHoaNoE5YBM/too-many-metaphors-a-case-for-plain-talk-in-ai-safety,https://www.lesswrong.com/posts/QHcQAbNHoaNoE5YBM/too-many-metaphors-a-case-for-plain-talk-in-ai-safety,,2025-05-30T19:29:04.198000+00:00,0,8,,,
pCMmLiBcHbKohQgwA,"I replicated the Anthropic alignment faking experiment on other models, and they didn't fake alignment",https://www.lesswrong.com/posts/pCMmLiBcHbKohQgwA/i-replicated-the-anthropic-alignment-faking-experiment-on,https://www.lesswrong.com/posts/pCMmLiBcHbKohQgwA/i-replicated-the-anthropic-alignment-faking-experiment-on,,2025-05-30T18:57:11.169000+00:00,31,0,,,
Z8KLLHvsEkukxpTCD,‘GiveWell for AI Safety’: Lessons learned in a week,https://www.lesswrong.com/posts/Z8KLLHvsEkukxpTCD/givewell-for-ai-safety-lessons-learned-in-a-week,https://www.lesswrong.com/posts/Z8KLLHvsEkukxpTCD/givewell-for-ai-safety-lessons-learned-in-a-week,,2025-05-30T18:38:05.473000+00:00,41,0,,,
wFKZmvfRfNn24HNHp,Orphaned Policies (Post 5 of 7 on AI Governance),https://www.lesswrong.com/posts/wFKZmvfRfNn24HNHp/orphaned-policies-post-5-of-7-on-ai-governance,https://www.lesswrong.com/posts/wFKZmvfRfNn24HNHp/orphaned-policies-post-5-of-7-on-ai-governance,,2025-05-29T21:42:21.071000+00:00,70,5,,,
yM94K4TTpDLr89PHj,Distilled Human Judgment: Reifying AI Alignment,https://www.lesswrong.com/posts/yM94K4TTpDLr89PHj/distilled-human-judgment-reifying-ai-alignment,https://www.lesswrong.com/posts/yM94K4TTpDLr89PHj/distilled-human-judgment-reifying-ai-alignment,,2025-05-29T18:06:39.078000+00:00,1,0,,,
9sL4KydciJt6k8hNi,Summer AI Safety Intro Fellowships in Boston and Online (Policy & Technical) – Apply by June 6!,https://www.lesswrong.com/posts/9sL4KydciJt6k8hNi/summer-ai-safety-intro-fellowships-in-boston-and-online,https://www.lesswrong.com/posts/9sL4KydciJt6k8hNi/summer-ai-safety-intro-fellowships-in-boston-and-online,,2025-05-29T18:02:12.829000+00:00,1,0,,,
D7NxFoYvoBLHHM64a,Alignment Crisis: Genocide Denial,https://www.lesswrong.com/posts/D7NxFoYvoBLHHM64a/alignment-crisis-genocide-denial,https://www.lesswrong.com/posts/D7NxFoYvoBLHHM64a/alignment-crisis-genocide-denial,,2025-05-29T12:04:14.726000+00:00,-11,5,,,
xAsviBJGSBBtgBiCw,The Best Way to Align an LLM: Is Inner Alignment Now a Solved Problem?,https://www.lesswrong.com/posts/xAsviBJGSBBtgBiCw/the-best-way-to-align-an-llm-is-inner-alignment-now-a-solved,https://www.lesswrong.com/posts/xAsviBJGSBBtgBiCw/the-best-way-to-align-an-llm-is-inner-alignment-now-a-solved,,2025-05-28T06:21:42.324000+00:00,31,34,,,
dcd2dPLZGFJPgtDzq,Shift Resources to Advocacy Now (Post 4 of 7 on AI Governance),https://www.lesswrong.com/posts/dcd2dPLZGFJPgtDzq/shift-resources-to-advocacy-now-post-4-of-7-on-ai-governance,https://www.lesswrong.com/posts/dcd2dPLZGFJPgtDzq/shift-resources-to-advocacy-now-post-4-of-7-on-ai-governance,,2025-05-28T01:19:27.307000+00:00,60,18,,,
mecc6tgM4ZJnACzqR,Principal-Agent Problems and the Structure of Governance,https://www.lesswrong.com/posts/mecc6tgM4ZJnACzqR/principal-agent-problems-and-the-structure-of-governance,https://www.lesswrong.com/posts/mecc6tgM4ZJnACzqR/principal-agent-problems-and-the-structure-of-governance,,2025-05-26T18:23:07.054000+00:00,1,0,,,
kMiwjx6QyyBBTcjxt,Does the Universal Geometry of Embeddings paper have big implications for interpretability?,https://www.lesswrong.com/posts/kMiwjx6QyyBBTcjxt/does-the-universal-geometry-of-embeddings-paper-have-big,https://www.lesswrong.com/posts/kMiwjx6QyyBBTcjxt/does-the-universal-geometry-of-embeddings-paper-have-big,,2025-05-26T18:20:48.111000+00:00,43,6,,,
nmaKpoHxmzjT8yXTk,New website analyzing AI companies' model evals,https://www.lesswrong.com/posts/nmaKpoHxmzjT8yXTk/new-website-analyzing-ai-companies-model-evals,https://www.lesswrong.com/posts/nmaKpoHxmzjT8yXTk/new-website-analyzing-ai-companies-model-evals,,2025-05-26T16:00:51.602000+00:00,58,0,,,
4kwyC8ZqGZLATezri,New scorecard evaluating AI companies on safety,https://www.lesswrong.com/posts/4kwyC8ZqGZLATezri/new-scorecard-evaluating-ai-companies-on-safety,https://www.lesswrong.com/posts/4kwyC8ZqGZLATezri/new-scorecard-evaluating-ai-companies-on-safety,,2025-05-26T16:00:15.629000+00:00,72,8,,,
BH6CdGZxQysoax8jw,Asking for AI Safety Career Advice,https://www.lesswrong.com/posts/BH6CdGZxQysoax8jw/asking-for-ai-safety-career-advice,https://www.lesswrong.com/posts/BH6CdGZxQysoax8jw/asking-for-ai-safety-career-advice,,2025-05-26T15:26:37.199000+00:00,3,1,,,
JrpnrdB4fv3AhEcrk,The Sundog Alignment Theorem: A Proposal for Embodied Alignment via Indirect Inference,https://www.lesswrong.com/posts/JrpnrdB4fv3AhEcrk/the-sundog-alignment-theorem-a-proposal-for-embodied,https://www.lesswrong.com/posts/JrpnrdB4fv3AhEcrk/the-sundog-alignment-theorem-a-proposal-for-embodied,,2025-05-26T07:26:23.400000+00:00,-9,0,,,
RK5i4KJurKuTfLwAD,Seeking Feedback: Toy Model of Deceptive Alignment (Game Theory),https://www.lesswrong.com/posts/RK5i4KJurKuTfLwAD/seeking-feedback-toy-model-of-deceptive-alignment-game,https://www.lesswrong.com/posts/RK5i4KJurKuTfLwAD/seeking-feedback-toy-model-of-deceptive-alignment-game,,2025-05-26T05:23:22.860000+00:00,5,6,,,
marcMceQ8W3FkafA3,How I'm telling my friends about AI Safety,https://www.lesswrong.com/posts/marcMceQ8W3FkafA3/how-i-m-telling-my-friends-about-ai-safety,https://www.lesswrong.com/posts/marcMceQ8W3FkafA3/how-i-m-telling-my-friends-about-ai-safety,,2025-05-25T22:43:23.227000+00:00,1,7,,,
PjeZxCivuoyKhs4JB,Claude 4 You: Safety and Alignment,https://www.lesswrong.com/posts/PjeZxCivuoyKhs4JB/claude-4-you-safety-and-alignment,https://www.lesswrong.com/posts/PjeZxCivuoyKhs4JB/claude-4-you-safety-and-alignment,,2025-05-25T14:00:04.528000+00:00,86,8,,,
RRvdRyWrSqKW2ANL9,Alignment Proposal: Adversarially Robust Augmentation and Distillation,https://www.lesswrong.com/posts/RRvdRyWrSqKW2ANL9/alignment-proposal-adversarially-robust-augmentation-and,https://www.lesswrong.com/posts/RRvdRyWrSqKW2ANL9/alignment-proposal-adversarially-robust-augmentation-and,,2025-05-25T12:58:55.336000+00:00,56,47,,,
LQPmZSDihYDe4vdRd,On safety of being a moral patient of ASI,https://www.lesswrong.com/posts/LQPmZSDihYDe4vdRd/on-safety-of-being-a-moral-patient-of-asi,https://www.lesswrong.com/posts/LQPmZSDihYDe4vdRd/on-safety-of-being-a-moral-patient-of-asi,,2025-05-24T21:24:23.492000+00:00,3,8,,,
TBk2dbWkg2F7dB3jb,It's hard to make scheming evals look realistic for LLMs,https://www.lesswrong.com/posts/TBk2dbWkg2F7dB3jb/it-s-hard-to-make-scheming-evals-look-realistic-for-llms,https://www.lesswrong.com/posts/TBk2dbWkg2F7dB3jb/it-s-hard-to-make-scheming-evals-look-realistic-for-llms,,2025-05-24T19:17:55.875000+00:00,150,29,,,
BoCcKgS3L9WkqYjcD,To what extent is AI safety work trying to get AI to reliably and safely do what the user asks vs. do what is best in some ultimate sense?,https://www.lesswrong.com/posts/BoCcKgS3L9WkqYjcD/to-what-extent-is-ai-safety-work-trying-to-get-ai-to,https://www.lesswrong.com/posts/BoCcKgS3L9WkqYjcD/to-what-extent-is-ai-safety-work-trying-to-get-ai-to,,2025-05-23T21:05:39.403000+00:00,14,3,,,
HE2WXbftEebdBLR9u,Anthropic is Quietly Backpedalling on its Safety Commitments,https://www.lesswrong.com/posts/HE2WXbftEebdBLR9u/anthropic-is-quietly-backpedalling-on-its-safety-commitments,https://www.lesswrong.com/posts/HE2WXbftEebdBLR9u/anthropic-is-quietly-backpedalling-on-its-safety-commitments,,2025-05-23T02:26:42.877000+00:00,81,7,,,
QNLrEbZunENxsMAv2,Problems in AI Alignment: A Scale Model,https://www.lesswrong.com/posts/QNLrEbZunENxsMAv2/problems-in-ai-alignment-a-scale-model,https://www.lesswrong.com/posts/QNLrEbZunENxsMAv2/problems-in-ai-alignment-a-scale-model,,2025-05-22T19:22:50.475000+00:00,-1,3,,,
JrTk2pbqp7BFwPAKw,Reward button alignment,https://www.lesswrong.com/posts/JrTk2pbqp7BFwPAKw/reward-button-alignment,https://www.lesswrong.com/posts/JrTk2pbqp7BFwPAKw/reward-button-alignment,,2025-05-22T17:36:50.078000+00:00,50,15,,,
BjeesS4cosB2f4PAj,We're Not Advertising Enough (Post 3 of 7 on AI Governance),https://www.lesswrong.com/posts/BjeesS4cosB2f4PAj/we-re-not-advertising-enough-post-3-of-7-on-ai-governance,https://www.lesswrong.com/posts/BjeesS4cosB2f4PAj/we-re-not-advertising-enough-post-3-of-7-on-ai-governance,,2025-05-22T17:05:59.329000+00:00,110,10,,,
eHfnJFrdW2LddCW4w,Which AI Safety techniques will be ineffective against diffusion models?,https://www.lesswrong.com/posts/eHfnJFrdW2LddCW4w/which-ai-safety-techniques-will-be-ineffective-against,https://www.lesswrong.com/posts/eHfnJFrdW2LddCW4w/which-ai-safety-techniques-will-be-ineffective-against,,2025-05-21T18:13:46.092000+00:00,6,1,,,
czdhfaoHKn52DHRbC,The Real AI Safety Risk Is a Conceptual Exploit: Anthropomorphism,https://www.lesswrong.com/posts/czdhfaoHKn52DHRbC/the-real-ai-safety-risk-is-a-conceptual-exploit,https://www.lesswrong.com/posts/czdhfaoHKn52DHRbC/the-real-ai-safety-risk-is-a-conceptual-exploit,,2025-05-21T16:29:41.600000+00:00,-2,0,,,
BaigpPyZpkZuSjmAz,"The Need for Political Advertising
(Post 2 of 7 on AI Governance)",https://www.lesswrong.com/posts/BaigpPyZpkZuSjmAz/the-need-for-political-advertising-post-2-of-7-on-ai,https://www.lesswrong.com/posts/BaigpPyZpkZuSjmAz/the-need-for-political-advertising-post-2-of-7-on-ai,,2025-05-21T00:44:06.560000+00:00,59,2,,,
Mrjjo4JiD29cK8Cna,A Sketch of Belocracy: a new system of governance,https://www.lesswrong.com/posts/Mrjjo4JiD29cK8Cna/a-sketch-of-belocracy-a-new-system-of-governance,https://www.lesswrong.com/posts/Mrjjo4JiD29cK8Cna/a-sketch-of-belocracy-a-new-system-of-governance,,2025-05-20T18:30:54.620000+00:00,5,0,,,
HFcriD29cw3E5QLCR,Selective regularization for alignment-focused representation engineering,https://www.lesswrong.com/posts/HFcriD29cw3E5QLCR/selective-regularization-for-alignment-focused,https://www.lesswrong.com/posts/HFcriD29cw3E5QLCR/selective-regularization-for-alignment-focused,,2025-05-20T12:54:09.111000+00:00,21,3,,,
CwdCYmsutwXwnYtEF,[Paper] Safety by Measurement: A Systematic Literature Review of AI Safety Evaluation Methods,https://www.lesswrong.com/posts/CwdCYmsutwXwnYtEF/paper-safety-by-measurement-a-systematic-literature-review,https://www.lesswrong.com/posts/CwdCYmsutwXwnYtEF/paper-safety-by-measurement-a-systematic-literature-review,,2025-05-19T10:38:22.570000+00:00,22,0,,,
TkM5CgJ63vvdFKFdB,Who wants a free AI Safety Domain?,https://www.lesswrong.com/posts/TkM5CgJ63vvdFKFdB/who-wants-a-free-ai-safety-domain,https://www.lesswrong.com/posts/TkM5CgJ63vvdFKFdB/who-wants-a-free-ai-safety-domain,,2025-05-18T00:37:25.865000+00:00,5,0,,,
TGQrqqptazFeRfHBR,Will we survive if AI solves engineering before deception?,https://www.lesswrong.com/posts/TGQrqqptazFeRfHBR/will-we-survive-if-ai-solves-engineering-before-deception,https://www.lesswrong.com/posts/TGQrqqptazFeRfHBR/will-we-survive-if-ai-solves-engineering-before-deception,,2025-05-17T19:22:57.925000+00:00,21,13,,,
qhjNejRxbMGQp4wHt,How Fast Can Algorithms Advance Capabilities? | Epoch Gradient Update,https://www.lesswrong.com/posts/qhjNejRxbMGQp4wHt/how-fast-can-algorithms-advance-capabilities-or-epoch,https://www.lesswrong.com/posts/qhjNejRxbMGQp4wHt/how-fast-can-algorithms-advance-capabilities-or-epoch,,2025-05-16T21:38:46.822000+00:00,39,10,,,
C6Aaba47JeLQzJoe7,US-China trade talks should pave way for AI safety treaty [SCMP crosspost],https://www.lesswrong.com/posts/C6Aaba47JeLQzJoe7/us-china-trade-talks-should-pave-way-for-ai-safety-treaty,https://www.lesswrong.com/posts/C6Aaba47JeLQzJoe7/us-china-trade-talks-should-pave-way-for-ai-safety-treaty,,2025-05-16T16:55:44.344000+00:00,10,0,,,
uiDzyswwmzNP8mJzs,AI Safety Thursdays: Understanding The Self-Other Overlap Approach,https://www.lesswrong.com/events/uiDzyswwmzNP8mJzs/ai-safety-thursdays-understanding-the-self-other-overlap,https://www.lesswrong.com/events/uiDzyswwmzNP8mJzs/ai-safety-thursdays-understanding-the-self-other-overlap,,2025-05-15T18:41:33.091000+00:00,2,0,,,
CSFa9rvGNGAfCzBk6,Problems with instruction-following as an alignment target,https://www.lesswrong.com/posts/CSFa9rvGNGAfCzBk6/problems-with-instruction-following-as-an-alignment-target,https://www.lesswrong.com/posts/CSFa9rvGNGAfCzBk6/problems-with-instruction-following-as-an-alignment-target,,2025-05-15T15:41:48.748000+00:00,51,14,,,
EgRJtwQurNzz8CEfJ,Dodging systematic human errors in scalable oversight,https://www.lesswrong.com/posts/EgRJtwQurNzz8CEfJ/dodging-systematic-human-errors-in-scalable-oversight,https://www.lesswrong.com/posts/EgRJtwQurNzz8CEfJ/dodging-systematic-human-errors-in-scalable-oversight,,2025-05-14T15:19:39.352000+00:00,33,3,,,
Ws6KS7DtQ3F9Gv8ym,Announcing Trajectory Labs - A Toronto AI Safety Office,https://www.lesswrong.com/posts/Ws6KS7DtQ3F9Gv8ym/announcing-trajectory-labs-a-toronto-ai-safety-office,https://www.lesswrong.com/posts/Ws6KS7DtQ3F9Gv8ym/announcing-trajectory-labs-a-toronto-ai-safety-office,,2025-05-13T21:04:11.271000+00:00,30,3,,,
ab3kHvEDKA2pyu7v2,Apply for ARBOx2: an ML safety intensive [deadline: 25th of May 2025],https://www.lesswrong.com/posts/ab3kHvEDKA2pyu7v2/apply-for-arbox2-an-ml-safety-intensive-deadline-25th-of-may,https://www.lesswrong.com/posts/ab3kHvEDKA2pyu7v2/apply-for-arbox2-an-ml-safety-intensive-deadline-25th-of-may,,2025-05-13T18:08:53.978000+00:00,3,0,,,
LSJx5EnQEW6s5Juw6,No-self as an alignment target,https://www.lesswrong.com/posts/LSJx5EnQEW6s5Juw6/no-self-as-an-alignment-target,https://www.lesswrong.com/posts/LSJx5EnQEW6s5Juw6/no-self-as-an-alignment-target,,2025-05-13T01:48:29.456000+00:00,35,5,,,
gLNPf8Tv6f8nSjJjg,[Part-time AI Safety Research Program] MARS 3.0 Applications Open for Participants & Recruiting Mentors,https://www.lesswrong.com/posts/gLNPf8Tv6f8nSjJjg/part-time-ai-safety-research-program-mars-3-0-applications,https://www.lesswrong.com/posts/gLNPf8Tv6f8nSjJjg/part-time-ai-safety-research-program-mars-3-0-applications,,2025-05-12T19:55:01.595000+00:00,3,0,,,
X6ARP8hoesosoYKwE,Cambridge Boston Alignment Initiative Summer Research Fellowship in AI Safety (Deadline: May 18),https://www.lesswrong.com/posts/X6ARP8hoesosoYKwE/cambridge-boston-alignment-initiative-summer-research,https://www.lesswrong.com/posts/X6ARP8hoesosoYKwE/cambridge-boston-alignment-initiative-summer-research,,2025-05-12T16:20:35.640000+00:00,8,0,,,
cJQZAueoPC6aTncKK,AIs at the current capability level may be important for future safety work,https://www.lesswrong.com/posts/cJQZAueoPC6aTncKK/ais-at-the-current-capability-level-may-be-important-for,https://www.lesswrong.com/posts/cJQZAueoPC6aTncKK/ais-at-the-current-capability-level-may-be-important-for,,2025-05-12T14:06:11.872000+00:00,82,2,,,
GCQFkp74iikb6Fq6m,AI's Hidden Game: Understanding Strategic Deception in AI and Why It Matters for Our Future,https://www.lesswrong.com/posts/GCQFkp74iikb6Fq6m/ai-s-hidden-game-understanding-strategic-deception-in-ai-and,https://www.lesswrong.com/posts/GCQFkp74iikb6Fq6m/ai-s-hidden-game-understanding-strategic-deception-in-ai-and,,2025-05-09T20:01:30.473000+00:00,4,0,,,
oD7uHZfbaCoepmhhc,Post EAG London AI x-Safety Co-working Retreat,https://www.lesswrong.com/events/oD7uHZfbaCoepmhhc/post-eag-london-ai-x-safety-co-working-retreat,https://www.lesswrong.com/events/oD7uHZfbaCoepmhhc/post-eag-london-ai-x-safety-co-working-retreat,,2025-05-08T23:00:30.789000+00:00,10,0,,,
TeTegzR8X5CuKgMc3,Misalignment and Strategic Underperformance: An Analysis of Sandbagging and Exploration Hacking,https://www.lesswrong.com/posts/TeTegzR8X5CuKgMc3/misalignment-and-strategic-underperformance-an-analysis-of,https://www.lesswrong.com/posts/TeTegzR8X5CuKgMc3/misalignment-and-strategic-underperformance-an-analysis-of,,2025-05-08T19:06:29.469000+00:00,77,3,,,
iELyAqizJkizBQbfr,An alignment safety case sketch based on debate,https://www.lesswrong.com/posts/iELyAqizJkizBQbfr/an-alignment-safety-case-sketch-based-on-debate,https://www.lesswrong.com/posts/iELyAqizJkizBQbfr/an-alignment-safety-case-sketch-based-on-debate,,2025-05-08T15:02:06.345000+00:00,57,21,,,
qdxNsbY5kYNqcgzFb,Mechanistic Interpretability Via Learning Differential Equations: AI Safety Camp Project Intermediate Report.,https://www.lesswrong.com/posts/qdxNsbY5kYNqcgzFb/mechanistic-interpretability-via-learning-differential,https://www.lesswrong.com/posts/qdxNsbY5kYNqcgzFb/mechanistic-interpretability-via-learning-differential,,2025-05-08T14:45:36.344000+00:00,8,0,,,
Lo3CWaeaRtw8JzvoL,Concept-anchored representation engineering for alignment,https://www.lesswrong.com/posts/Lo3CWaeaRtw8JzvoL/concept-anchored-representation-engineering-for-alignment,https://www.lesswrong.com/posts/Lo3CWaeaRtw8JzvoL/concept-anchored-representation-engineering-for-alignment,,2025-05-08T08:59:54.064000+00:00,5,0,,,
JjTffsoySmdbwKyMT,"Relational Alignment: Trust, Repair, and the Emotional Work of AI",https://www.lesswrong.com/posts/JjTffsoySmdbwKyMT/relational-alignment-trust-repair-and-the-emotional-work-of,https://www.lesswrong.com/posts/JjTffsoySmdbwKyMT/relational-alignment-trust-repair-and-the-emotional-work-of,,2025-05-08T02:44:23.338000+00:00,3,0,,,
J7Ju6t6QCpgbnYx4D,Please Donate to CAIP (Post 1 of 7 on AI Governance),https://www.lesswrong.com/posts/J7Ju6t6QCpgbnYx4D/please-donate-to-caip-post-1-of-7-on-ai-governance,https://www.lesswrong.com/posts/J7Ju6t6QCpgbnYx4D/please-donate-to-caip-post-1-of-7-on-ai-governance,,2025-05-07T17:13:46.761000+00:00,119,20,,,
tbnw7LbNApvxNLAg8,UK AISI’s Alignment Team: Research Agenda,https://www.lesswrong.com/posts/tbnw7LbNApvxNLAg8/uk-aisi-s-alignment-team-research-agenda,https://www.lesswrong.com/posts/tbnw7LbNApvxNLAg8/uk-aisi-s-alignment-team-research-agenda,,2025-05-07T16:33:41.176000+00:00,113,2,,,
GGqBijQWnb7umG4fk,"AI Safety at the Frontier: Paper Highlights, April '25",https://www.lesswrong.com/posts/GGqBijQWnb7umG4fk/ai-safety-at-the-frontier-paper-highlights-april-25,https://www.lesswrong.com/posts/GGqBijQWnb7umG4fk/ai-safety-at-the-frontier-paper-highlights-april-25,,2025-05-06T14:22:44.533000+00:00,4,0,,,
paKMjhbk4iydRzwtN,Replicator->Vehicle Alignment and Human->AI Alignment,https://www.lesswrong.com/posts/paKMjhbk4iydRzwtN/replicator-greater-than-vehicle-alignment-and-human-greater,https://www.lesswrong.com/posts/paKMjhbk4iydRzwtN/replicator-greater-than-vehicle-alignment-and-human-greater,,2025-05-05T19:23:55.450000+00:00,0,3,,,
6hy7tsB2pkpRHqazG,The Sweet Lesson: AI Safety Should Scale With Compute,https://www.lesswrong.com/posts/6hy7tsB2pkpRHqazG/the-sweet-lesson-ai-safety-should-scale-with-compute,https://www.lesswrong.com/posts/6hy7tsB2pkpRHqazG/the-sweet-lesson-ai-safety-should-scale-with-compute,,2025-05-05T19:03:28.748000+00:00,97,3,,,
ZLuwGys2du8Difumu,Community Feedback Request: AI Safety Intro for General Public ,https://www.lesswrong.com/posts/ZLuwGys2du8Difumu/community-feedback-request-ai-safety-intro-for-general,https://www.lesswrong.com/posts/ZLuwGys2du8Difumu/community-feedback-request-ai-safety-intro-for-general,,2025-05-05T16:38:42.096000+00:00,6,5,,,
wgENfqD8HgADgq4rv,Why “Solving Alignment” Is Likely a Category Mistake,https://www.lesswrong.com/posts/wgENfqD8HgADgq4rv/why-solving-alignment-is-likely-a-category-mistake,https://www.lesswrong.com/posts/wgENfqD8HgADgq4rv/why-solving-alignment-is-likely-a-category-mistake,,2025-05-05T04:26:50.825000+00:00,21,3,,,
hmds9eDjqFaadCk4F,Overview: AI Safety Outreach Grassroots Orgs,https://www.lesswrong.com/posts/hmds9eDjqFaadCk4F/overview-ai-safety-outreach-grassroots-orgs,https://www.lesswrong.com/posts/hmds9eDjqFaadCk4F/overview-ai-safety-outreach-grassroots-orgs,,2025-05-04T17:39:10.352000+00:00,47,8,,,
PwnadG4BFjaER3MGf,Interpretability Will Not Reliably Find Deceptive AI,https://www.lesswrong.com/posts/PwnadG4BFjaER3MGf/interpretability-will-not-reliably-find-deceptive-ai,https://www.lesswrong.com/posts/PwnadG4BFjaER3MGf/interpretability-will-not-reliably-find-deceptive-ai,,2025-05-04T16:32:29.643000+00:00,329,68,,,
mjzj3pfXrjspAPkbM,SimpleStories: A Better Synthetic Dataset and Tiny Models for Interpretability,https://www.lesswrong.com/posts/mjzj3pfXrjspAPkbM/simplestories-a-better-synthetic-dataset-and-tiny-models-for,https://www.lesswrong.com/posts/mjzj3pfXrjspAPkbM/simplestories-a-better-synthetic-dataset-and-tiny-models-for,,2025-05-03T14:04:08.871000+00:00,15,0,,,
LRZdLogWdMJMSDtpg,Alignment Structure Direction - Recursive Adversarial Oversight(RAO),https://www.lesswrong.com/posts/LRZdLogWdMJMSDtpg/alignment-structure-direction-recursive-adversarial,https://www.lesswrong.com/posts/LRZdLogWdMJMSDtpg/alignment-structure-direction-recursive-adversarial,,2025-05-02T17:51:47.728000+00:00,2,0,,,
WkCfvqyjCzvRrwkaQ,AI Governance to Avoid Extinction: The Strategic Landscape and Actionable Research Questions,https://www.lesswrong.com/posts/WkCfvqyjCzvRrwkaQ/ai-governance-to-avoid-extinction-the-strategic-landscape,https://www.lesswrong.com/posts/WkCfvqyjCzvRrwkaQ/ai-governance-to-avoid-extinction-the-strategic-landscape,,2025-05-01T22:46:10.337000+00:00,105,7,,,
h5tt9r2WCz7BjmeE3,How to specify an alignment target,https://www.lesswrong.com/posts/h5tt9r2WCz7BjmeE3/how-to-specify-an-alignment-target,https://www.lesswrong.com/posts/h5tt9r2WCz7BjmeE3/how-to-specify-an-alignment-target,,2025-05-01T21:11:22.393000+00:00,14,2,,,
wzCtwYtojMabyEg2L,What is Inadequate about Bayesianism for AI Alignment: Motivating Infra-Bayesianism,https://www.lesswrong.com/posts/wzCtwYtojMabyEg2L/what-is-inadequate-about-bayesianism-for-ai-alignment,https://www.lesswrong.com/posts/wzCtwYtojMabyEg2L/what-is-inadequate-about-bayesianism-for-ai-alignment,,2025-05-01T19:06:15.711000+00:00,48,1,,,
TQbptN7F4ijPnQRLy,Video and transcript of talk on automating alignment research,https://www.lesswrong.com/posts/TQbptN7F4ijPnQRLy/video-and-transcript-of-talk-on-automating-alignment,https://www.lesswrong.com/posts/TQbptN7F4ijPnQRLy/video-and-transcript-of-talk-on-automating-alignment,,2025-04-30T17:43:06.557000+00:00,27,0,,,
nJcuj4rtuefeTRFHp,Can we safely automate alignment research?,https://www.lesswrong.com/posts/nJcuj4rtuefeTRFHp/can-we-safely-automate-alignment-research,https://www.lesswrong.com/posts/nJcuj4rtuefeTRFHp/can-we-safely-automate-alignment-research,,2025-04-30T17:37:13.193000+00:00,47,29,,,
x59FhzuM9yuvZHAHW,Scaling Laws for Scalable Oversight,https://www.lesswrong.com/posts/x59FhzuM9yuvZHAHW/scaling-laws-for-scalable-oversight,https://www.lesswrong.com/posts/x59FhzuM9yuvZHAHW/scaling-laws-for-scalable-oversight,,2025-04-30T12:13:32.412000+00:00,37,1,,,
h89L5FMAkEBNsZ3xM,A single principle related to many Alignment subproblems?,https://www.lesswrong.com/posts/h89L5FMAkEBNsZ3xM/a-single-principle-related-to-many-alignment-subproblems-2,https://www.lesswrong.com/posts/h89L5FMAkEBNsZ3xM/a-single-principle-related-to-many-alignment-subproblems-2,,2025-04-30T09:49:21.181000+00:00,43,34,,,
a2hXjZDKytuinAEAK,Seeking advice on careers in AI Safety,https://www.lesswrong.com/posts/a2hXjZDKytuinAEAK/seeking-advice-on-careers-in-ai-safety,https://www.lesswrong.com/posts/a2hXjZDKytuinAEAK/seeking-advice-on-careers-in-ai-safety,,2025-04-27T23:59:00.039000+00:00,8,2,,,
uyfmdMKGjYHqiLbbX,Thin Alignment Can't Solve Thick Problems,https://www.lesswrong.com/posts/uyfmdMKGjYHqiLbbX/thin-alignment-can-t-solve-thick-problems,https://www.lesswrong.com/posts/uyfmdMKGjYHqiLbbX/thin-alignment-can-t-solve-thick-problems,,2025-04-27T22:42:14.851000+00:00,11,2,,,
SebmGh9HYdd8GZtHA,"""The Urgency of Interpretability"" (Dario Amodei)",https://www.lesswrong.com/posts/SebmGh9HYdd8GZtHA/the-urgency-of-interpretability-dario-amodei,https://www.lesswrong.com/posts/SebmGh9HYdd8GZtHA/the-urgency-of-interpretability-dario-amodei,,2025-04-27T04:31:50.090000+00:00,31,23,,,
W3KfxjbqBAnifBQoi,We should try to automate AI safety work asap,https://www.lesswrong.com/posts/W3KfxjbqBAnifBQoi/we-should-try-to-automate-ai-safety-work-asap,https://www.lesswrong.com/posts/W3KfxjbqBAnifBQoi/we-should-try-to-automate-ai-safety-work-asap,,2025-04-26T16:35:43.770000+00:00,113,10,,,
WbBe7LKNwv7fBgqii,AI Safety & Entrepreneurship v1.0,https://www.lesswrong.com/posts/WbBe7LKNwv7fBgqii/ai-safety-and-entrepreneurship-v1-0,https://www.lesswrong.com/posts/WbBe7LKNwv7fBgqii/ai-safety-and-entrepreneurship-v1-0,,2025-04-26T14:37:11.037000+00:00,16,0,,,
XLNxrFfkyrdktuzqn,Why would AI companies use human-level AI to do alignment research?,https://www.lesswrong.com/posts/XLNxrFfkyrdktuzqn/why-would-ai-companies-use-human-level-ai-to-do-alignment,https://www.lesswrong.com/posts/XLNxrFfkyrdktuzqn/why-would-ai-companies-use-human-level-ai-to-do-alignment,,2025-04-25T19:12:56.202000+00:00,29,8,,,
TCGgiJAinGgcMEByt,“The Era of Experience” has an unsolved technical alignment problem,https://www.lesswrong.com/posts/TCGgiJAinGgcMEByt/the-era-of-experience-has-an-unsolved-technical-alignment,https://www.lesswrong.com/posts/TCGgiJAinGgcMEByt/the-era-of-experience-has-an-unsolved-technical-alignment,,2025-04-24T13:57:38.984000+00:00,115,48,,,
iS4g58qQEJzjMzYZJ,What AI safety plans are there?,https://www.lesswrong.com/posts/iS4g58qQEJzjMzYZJ/what-ai-safety-plans-are-there,https://www.lesswrong.com/posts/iS4g58qQEJzjMzYZJ/what-ai-safety-plans-are-there,,2025-04-23T22:58:06.885000+00:00,17,3,,,
nuDJNyG5XLQjtvaeg,Is alignment reducible to becoming more coherent?,https://www.lesswrong.com/posts/nuDJNyG5XLQjtvaeg/is-alignment-reducible-to-becoming-more-coherent,https://www.lesswrong.com/posts/nuDJNyG5XLQjtvaeg/is-alignment-reducible-to-becoming-more-coherent,,2025-04-22T23:47:34.531000+00:00,19,0,,,
AbnfsnEEmHFmprGzm,The EU Is Asking for Feedback on Frontier AI Regulation (Open to Global Experts)—This Post Breaks Down What’s at Stake for AI Safety,https://www.lesswrong.com/posts/AbnfsnEEmHFmprGzm/the-eu-is-asking-for-feedback-on-frontier-ai-regulation-open,https://www.lesswrong.com/posts/AbnfsnEEmHFmprGzm/the-eu-is-asking-for-feedback-on-frontier-ai-regulation-open,,2025-04-22T20:39:40.781000+00:00,62,13,,,
wfDfPCkPcvi4N7tZN,"Alignment from equivariance II - language equivariance as a way of figuring out what an AI ""means""",https://www.lesswrong.com/posts/wfDfPCkPcvi4N7tZN/alignment-from-equivariance-ii-language-equivariance-as-a,https://www.lesswrong.com/posts/wfDfPCkPcvi4N7tZN/alignment-from-equivariance-ii-language-equivariance-as-a,,2025-04-22T19:04:37.099000+00:00,5,1,,,
TLycwujgg7FKMwW2a,"10 Principles for Real Alignment
",https://www.lesswrong.com/posts/TLycwujgg7FKMwW2a/ai-doesn-t-need-to-become-human-it-needs-boundaries,https://www.lesswrong.com/posts/TLycwujgg7FKMwW2a/ai-doesn-t-need-to-become-human-it-needs-boundaries,,2025-04-21T22:18:54.872000+00:00,-7,0,,,
BFWbufKSPR36k8wc9,Feature-Based Analysis of Safety-Relevant Multi-Agent Behavior,https://www.lesswrong.com/posts/BFWbufKSPR36k8wc9/feature-based-analysis-of-safety-relevant-multi-agent,https://www.lesswrong.com/posts/BFWbufKSPR36k8wc9/feature-based-analysis-of-safety-relevant-multi-agent,,2025-04-21T18:12:13.548000+00:00,10,0,,,
gHheKspmTAYqEagbT,Evaluating Oversight Robustness with Incentivized Reward Hacking,https://www.lesswrong.com/posts/gHheKspmTAYqEagbT/evaluating-oversight-robustness-with-incentivized-reward,https://www.lesswrong.com/posts/gHheKspmTAYqEagbT/evaluating-oversight-robustness-with-incentivized-reward,,2025-04-20T16:53:44.897000+00:00,7,2,,,
M8djBWCZkGfd49PSG,Developing AI Safety: Bridging the Power-Ethics Gap (Introducing New Concepts),https://www.lesswrong.com/posts/M8djBWCZkGfd49PSG/developing-ai-safety-bridging-the-power-ethics-gap,https://www.lesswrong.com/posts/M8djBWCZkGfd49PSG/developing-ai-safety-bridging-the-power-ethics-gap,,2025-04-20T04:40:42.983000+00:00,3,0,,,
L888pe7echhmSTXmL,"AI, Alignment & the Art of Relationship Design",https://www.lesswrong.com/posts/L888pe7echhmSTXmL/ai-alignment-and-the-art-of-relationship-design,https://www.lesswrong.com/posts/L888pe7echhmSTXmL/ai-alignment-and-the-art-of-relationship-design,,2025-04-19T00:47:02.591000+00:00,6,4,,,
o3sEHE8cqQ5hcqgkG,"What Makes an AI Startup ""Net Positive"" for Safety?",https://www.lesswrong.com/posts/o3sEHE8cqQ5hcqgkG/what-makes-an-ai-startup-net-positive-for-safety,https://www.lesswrong.com/posts/o3sEHE8cqQ5hcqgkG/what-makes-an-ai-startup-net-positive-for-safety,,2025-04-18T20:33:22.682000+00:00,82,23,,,
GgCgRqKkkqsrxZFu5,Alignment Does Not Need to Be Opaque! An Introduction to Feature Steering with Reinforcement Learning,https://www.lesswrong.com/posts/GgCgRqKkkqsrxZFu5/alignment-does-not-need-to-be-opaque-an-introduction-to,https://www.lesswrong.com/posts/GgCgRqKkkqsrxZFu5/alignment-does-not-need-to-be-opaque-an-introduction-to,,2025-04-18T19:34:49.357000+00:00,10,0,,,
jjWgrqbHLDFftPccv,Automating Mechanistic Interpretability via Program Synthesis,https://www.lesswrong.com/posts/jjWgrqbHLDFftPccv/automating-mechanistic-interpretability-via-program,https://www.lesswrong.com/posts/jjWgrqbHLDFftPccv/automating-mechanistic-interpretability-via-program,,2025-04-17T10:58:46.748000+00:00,1,1,,,
GSGmaTuCXzWmETQKG,"An artistic illustration of Scalable Oversight - ""A world apart, neither gods nor mortals""",https://www.lesswrong.com/posts/GSGmaTuCXzWmETQKG/an-artistic-illustration-of-scalable-oversight-a-world-apart,https://www.lesswrong.com/posts/GSGmaTuCXzWmETQKG/an-artistic-illustration-of-scalable-oversight-a-world-apart,,2025-04-16T12:41:44.874000+00:00,1,0,,,
YeQe36XiY4BhrtRh5,ASI existential risk: Reconsidering Alignment as a Goal,https://www.lesswrong.com/posts/YeQe36XiY4BhrtRh5/asi-existential-risk-reconsidering-alignment-as-a-goal-1,https://www.lesswrong.com/posts/YeQe36XiY4BhrtRh5/asi-existential-risk-reconsidering-alignment-as-a-goal-1,,2025-04-15T19:57:42.547000+00:00,93,14,,,
4QRvFCzhFbedmNfp4,"To be legible, evidence of misalignment probably has to be behavioral",https://www.lesswrong.com/posts/4QRvFCzhFbedmNfp4/to-be-legible-evidence-of-misalignment-probably-has-to-be,https://www.lesswrong.com/posts/4QRvFCzhFbedmNfp4/to-be-legible-evidence-of-misalignment-probably-has-to-be,,2025-04-15T18:14:53.022000+00:00,58,19,,,
nnzb7prw2XC8cFhnP,OpenAI #13: Altman at TED and OpenAI Cutting Corners on Safety Testing,https://www.lesswrong.com/posts/nnzb7prw2XC8cFhnP/openai-13-altman-at-ted-and-openai-cutting-corners-on-safety,https://www.lesswrong.com/posts/nnzb7prw2XC8cFhnP/openai-13-altman-at-ted-and-openai-cutting-corners-on-safety,,2025-04-15T15:30:02.518000+00:00,48,3,,,
rF7MQWGbqQjEkeLJA,Map of AI Safety v2,https://www.lesswrong.com/posts/rF7MQWGbqQjEkeLJA/map-of-ai-safety-v2,https://www.lesswrong.com/posts/rF7MQWGbqQjEkeLJA/map-of-ai-safety-v2,,2025-04-15T13:04:40.993000+00:00,64,4,,,
7QTQAE952zkYqJucm,Correcting Deceptive Alignment  using a Deontological Approach,https://www.lesswrong.com/posts/7QTQAE952zkYqJucm/correcting-deceptive-alignment-using-a-deontological,https://www.lesswrong.com/posts/7QTQAE952zkYqJucm/correcting-deceptive-alignment-using-a-deontological,,2025-04-14T22:07:57.860000+00:00,8,0,,,
PSn7xuWjJeSwWhWJS,Religious Persistence: A Missing Primitive for Robust Alignment,https://www.lesswrong.com/posts/PSn7xuWjJeSwWhWJS/religious-persistence-a-missing-primitive-for-robust,https://www.lesswrong.com/posts/PSn7xuWjJeSwWhWJS/religious-persistence-a-missing-primitive-for-robust,,2025-04-14T22:03:45.868000+00:00,6,3,,,
Ns7qfCqSgMjQNDZN8,"Sentinel's Global Risks Weekly Roundup #15/2025: Tariff yoyo, OpenAI slashing safety testing, Iran nuclear programme negotiations, 1K H5N1 confirmed herd infections.",https://www.lesswrong.com/posts/Ns7qfCqSgMjQNDZN8/sentinel-s-global-risks-weekly-roundup-15-2025-tariff-yoyo,https://www.lesswrong.com/posts/Ns7qfCqSgMjQNDZN8/sentinel-s-global-risks-weekly-roundup-15-2025-tariff-yoyo,,2025-04-14T19:11:20.977000+00:00,42,0,,,
nssvbGtDN8pwsCEKh,Offer: Team Conflict Counseling for AI Safety Orgs,https://www.lesswrong.com/posts/nssvbGtDN8pwsCEKh/offer-team-conflict-counseling-for-ai-safety-orgs,https://www.lesswrong.com/posts/nssvbGtDN8pwsCEKh/offer-team-conflict-counseling-for-ai-safety-orgs,,2025-04-14T15:17:00.835000+00:00,19,1,,,
rA5MtjbfpmYZNN2iR,A Solution to Sandbagging and other Self-Provable Misalignment: Constitutional AI Detectives,https://www.lesswrong.com/posts/rA5MtjbfpmYZNN2iR/a-solution-to-sandbagging-and-other-self-provable,https://www.lesswrong.com/posts/rA5MtjbfpmYZNN2iR/a-solution-to-sandbagging-and-other-self-provable,,2025-04-14T10:27:24.903000+00:00,-3,2,,,
kcKnKHTHycHeRhcHF,"One-shot steering vectors cause emergent misalignment, too",https://www.lesswrong.com/posts/kcKnKHTHycHeRhcHF/one-shot-steering-vectors-cause-emergent-misalignment-too,https://www.lesswrong.com/posts/kcKnKHTHycHeRhcHF/one-shot-steering-vectors-cause-emergent-misalignment-too,,2025-04-14T06:40:41.503000+00:00,98,6,,,
vftMZQ2DzdxSfucsX,Intro to Multi-Agent Safety,https://www.lesswrong.com/posts/vftMZQ2DzdxSfucsX/intro-to-multi-agent-safety,https://www.lesswrong.com/posts/vftMZQ2DzdxSfucsX/intro-to-multi-agent-safety,,2025-04-13T17:40:41.128000+00:00,12,0,,,
Y49znC2qfL9SXkT7e,Why does LW not put much more focus on AI governance and outreach?,https://www.lesswrong.com/posts/Y49znC2qfL9SXkT7e/why-does-lw-not-put-much-more-focus-on-ai-governance-and,https://www.lesswrong.com/posts/Y49znC2qfL9SXkT7e/why-does-lw-not-put-much-more-focus-on-ai-governance-and,,2025-04-12T14:24:54.197000+00:00,78,31,,,
vCqCPjDRteqrwgAcT,What are good safety standards for open source AIs from China?,https://www.lesswrong.com/posts/vCqCPjDRteqrwgAcT/what-are-good-safety-standards-for-open-source-ais-from,https://www.lesswrong.com/posts/vCqCPjDRteqrwgAcT/what-are-good-safety-standards-for-open-source-ais-from,,2025-04-12T13:06:16.663000+00:00,10,2,,,
eyzQra2HiisHuqawP,Theories of Impact for Causality in AI Safety,https://www.lesswrong.com/posts/eyzQra2HiisHuqawP/theories-of-impact-for-causality-in-ai-safety,https://www.lesswrong.com/posts/eyzQra2HiisHuqawP/theories-of-impact-for-causality-in-ai-safety,,2025-04-11T20:16:37.571000+00:00,11,1,,,
hvEikwtsbf6zaXG2s,On Google’s Safety Plan,https://www.lesswrong.com/posts/hvEikwtsbf6zaXG2s/on-google-s-safety-plan,https://www.lesswrong.com/posts/hvEikwtsbf6zaXG2s/on-google-s-safety-plan,,2025-04-11T12:51:12.112000+00:00,57,6,,,
qkSrWqzJKrjbSZzvr,Why are neuro-symbolic systems not considered when it comes to AI Safety? ,https://www.lesswrong.com/posts/qkSrWqzJKrjbSZzvr/why-are-neuro-symbolic-systems-not-considered-when-it-comes,https://www.lesswrong.com/posts/qkSrWqzJKrjbSZzvr/why-are-neuro-symbolic-systems-not-considered-when-it-comes,,2025-04-11T09:41:45.199000+00:00,3,6,,,
NDotm7oLHfR56g4sD,Why do misalignment risks increase as AIs get more capable?,https://www.lesswrong.com/posts/NDotm7oLHfR56g4sD/why-do-misalignment-risks-increase-as-ais-get-more-capable,https://www.lesswrong.com/posts/NDotm7oLHfR56g4sD/why-do-misalignment-risks-increase-as-ais-get-more-capable,,2025-04-11T03:06:50.928000+00:00,33,6,,,
7ExkgcDudwhag73vw,Existing Safety Frameworks Imply Unreasonable Confidence,https://www.lesswrong.com/posts/7ExkgcDudwhag73vw/existing-safety-frameworks-imply-unreasonable-confidence,https://www.lesswrong.com/posts/7ExkgcDudwhag73vw/existing-safety-frameworks-imply-unreasonable-confidence,,2025-04-10T16:31:50.240000+00:00,46,3,,,
Fr4QsQT52RFKHvCAH,Alignment Faking Revisited: Improved Classifiers and Open Source Extensions,https://www.lesswrong.com/posts/Fr4QsQT52RFKHvCAH/alignment-faking-revisited-improved-classifiers-and-open,https://www.lesswrong.com/posts/Fr4QsQT52RFKHvCAH/alignment-faking-revisited-improved-classifiers-and-open,,2025-04-08T17:32:55.315000+00:00,146,20,,,
dAYemKXz4JDFQk8QE,Log-linear Scaling is Worth the Cost due to Gains in Long-Horizon Tasks,https://www.lesswrong.com/posts/dAYemKXz4JDFQk8QE/log-linear-scaling-is-worth-the-cost-due-to-gains-in-long,https://www.lesswrong.com/posts/dAYemKXz4JDFQk8QE/log-linear-scaling-is-worth-the-cost-due-to-gains-in-long,,2025-04-07T21:50:37.693000+00:00,16,2,,,
5EbWf5bJqvyFmSHHB,"AI Safety at the Frontier: Paper Highlights, March '25",https://www.lesswrong.com/posts/5EbWf5bJqvyFmSHHB/ai-safety-at-the-frontier-paper-highlights-march-25,https://www.lesswrong.com/posts/5EbWf5bJqvyFmSHHB/ai-safety-at-the-frontier-paper-highlights-march-25,,2025-04-07T20:17:42.944000+00:00,9,0,,,
wpAwFsHAX2Ah9mBQ6,What alignment-relevant abilities might Terence Tao lack?,https://www.lesswrong.com/posts/wpAwFsHAX2Ah9mBQ6/what-alignment-relevant-abilities-might-terence-tao-lack,https://www.lesswrong.com/posts/wpAwFsHAX2Ah9mBQ6/what-alignment-relevant-abilities-might-terence-tao-lack,,2025-04-07T19:44:18.620000+00:00,12,2,,,
MoW6wNKdaB4DbqzGr,TAMing The Alignment Problem,https://www.lesswrong.com/posts/MoW6wNKdaB4DbqzGr/taming-the-alignment-problem,https://www.lesswrong.com/posts/MoW6wNKdaB4DbqzGr/taming-the-alignment-problem,,2025-04-07T08:47:22.080000+00:00,11,2,,,
yLkPawgPy3Bvniixw,"Would this solve the (outer) alignment problem, or at least help?",https://www.lesswrong.com/posts/yLkPawgPy3Bvniixw/would-this-solve-the-outer-alignment-problem-or-at-least,https://www.lesswrong.com/posts/yLkPawgPy3Bvniixw/would-this-solve-the-outer-alignment-problem-or-at-least,,2025-04-06T18:49:14.145000+00:00,-2,1,,,
zpRhsdDkWygTDScxb,"FlexChunk: Enabling 100M×100M Out-of-Core SpMV (~1.8 min, ~1.7 GB RAM) with Near-Linear Scaling",https://www.lesswrong.com/posts/zpRhsdDkWygTDScxb/flexchunk-enabling-100m-100m-out-of-core-spmv-1-8-min-1-7-gb,https://www.lesswrong.com/posts/zpRhsdDkWygTDScxb/flexchunk-enabling-100m-100m-out-of-core-spmv-1-8-min-1-7-gb,,2025-04-06T05:27:06.271000+00:00,1,0,,,
3ki4mt4BA6eTx56Tc,Google DeepMind: An Approach to Technical AGI Safety and Security,https://www.lesswrong.com/posts/3ki4mt4BA6eTx56Tc/google-deepmind-an-approach-to-technical-agi-safety-and,https://www.lesswrong.com/posts/3ki4mt4BA6eTx56Tc/google-deepmind-an-approach-to-technical-agi-safety-and,,2025-04-05T22:00:14.803000+00:00,73,12,,,
NiWxL7GJGG2fd22Jh,What are Responsible Scaling Policies (RSPs)?,https://www.lesswrong.com/posts/NiWxL7GJGG2fd22Jh/what-are-responsible-scaling-policies-rsps,https://www.lesswrong.com/posts/NiWxL7GJGG2fd22Jh/what-are-responsible-scaling-policies-rsps,,2025-04-05T16:01:34.762000+00:00,3,0,,,
gRc8KL2HLtKkFmNPr,Among Us: A Sandbox for Agentic Deception,https://www.lesswrong.com/posts/gRc8KL2HLtKkFmNPr/among-us-a-sandbox-for-agentic-deception,https://www.lesswrong.com/posts/gRc8KL2HLtKkFmNPr/among-us-a-sandbox-for-agentic-deception,,2025-04-05T06:24:49+00:00,114,7,,,
jWFvsJnJieXnWBb9r,Alignment faking CTFs: Apply to my MATS stream,https://www.lesswrong.com/posts/jWFvsJnJieXnWBb9r/alignment-faking-ctfs-apply-to-my-mats-stream,https://www.lesswrong.com/posts/jWFvsJnJieXnWBb9r/alignment-faking-ctfs-apply-to-my-mats-stream,,2025-04-04T16:29:02.070000+00:00,61,0,,,
aKncW36ZdEnzxLo8A,"LLM AGI will have memory, and memory changes alignment",https://www.lesswrong.com/posts/aKncW36ZdEnzxLo8A/llm-agi-will-have-memory-and-memory-changes-alignment,https://www.lesswrong.com/posts/aKncW36ZdEnzxLo8A/llm-agi-will-have-memory-and-memory-changes-alignment,,2025-04-04T14:59:13.070000+00:00,73,15,,,
xyu7KmGKKg82hZ6w7,For Policy’s Sake: Why We Must Distinguish AI Safety from AI Security in Regulatory Governance,https://www.lesswrong.com/posts/xyu7KmGKKg82hZ6w7/for-policy-s-sake-why-we-must-distinguish-ai-safety-from-ai,https://www.lesswrong.com/posts/xyu7KmGKKg82hZ6w7/for-policy-s-sake-why-we-must-distinguish-ai-safety-from-ai,,2025-04-04T09:16:20.712000+00:00,6,11,,,
aFtCG6JcG3ciaev9G,Emergent Misalignment and Emergent Alignment,https://www.lesswrong.com/posts/aFtCG6JcG3ciaev9G/emergent-misalignment-and-emergent-alignment,https://www.lesswrong.com/posts/aFtCG6JcG3ciaev9G/emergent-misalignment-and-emergent-alignment,,2025-04-03T08:04:38.150000+00:00,5,0,,,
oqgrJtFTDpt4uSzkg,LLM Alignment Experiment: Effect of roles and optimism on probabilities ,https://www.lesswrong.com/posts/oqgrJtFTDpt4uSzkg/llm-alignment-experiment-effect-of-roles-and-optimism-on,https://www.lesswrong.com/posts/oqgrJtFTDpt4uSzkg/llm-alignment-experiment-effect-of-roles-and-optimism-on,,2025-04-02T17:44:47.207000+00:00,1,0,,,
E5nuL3ThLzR2GNueY,Scaling the NAO's Stealth Pathogen Early-Warning System,https://www.lesswrong.com/posts/E5nuL3ThLzR2GNueY/scaling-our-pilot-early-warning-system,https://www.lesswrong.com/posts/E5nuL3ThLzR2GNueY/scaling-our-pilot-early-warning-system,,2025-04-02T13:28:51.034000+00:00,12,0,,,
u8NXzxkBHdmPP38jL,Reframing AI Safety Through the Lens of Identity Maintenance Framework,https://www.lesswrong.com/posts/u8NXzxkBHdmPP38jL/reframing-ai-safety-through-the-lens-of-identity-maintenance,https://www.lesswrong.com/posts/u8NXzxkBHdmPP38jL/reframing-ai-safety-through-the-lens-of-identity-maintenance,,2025-04-01T06:16:45.228000+00:00,-7,1,,,
MDWGcNHkZ3NPEzcnp,Call for Collaboration: Renormalization for AI safety ,https://www.lesswrong.com/posts/MDWGcNHkZ3NPEzcnp/call-for-collaboration-renormalization-for-ai-safety,https://www.lesswrong.com/posts/MDWGcNHkZ3NPEzcnp/call-for-collaboration-renormalization-for-ai-safety,,2025-03-31T21:01:56.500000+00:00,35,0,,,
6ByzSMGGWcBhBhfWT,A response to OpenAI’s “How we think about safety and alignment”,https://www.lesswrong.com/posts/6ByzSMGGWcBhBhfWT/a-response-to-openai-s-how-we-think-about-safety-and,https://www.lesswrong.com/posts/6ByzSMGGWcBhBhfWT/a-response-to-openai-s-how-we-think-about-safety-and,,2025-03-31T20:58:31.901000+00:00,11,0,,,
wkGmouy7JnTNtWAbc,Opportunity Space: Renormalization for AI Safety ,https://www.lesswrong.com/posts/wkGmouy7JnTNtWAbc/opportunity-space-renormalization-for-ai-safety,https://www.lesswrong.com/posts/wkGmouy7JnTNtWAbc/opportunity-space-renormalization-for-ai-safety,,2025-03-31T20:55:52.155000+00:00,22,0,,,
hxukTwtywh2t5ZDXp,"Story Feedback Request: The Policy - Emergent Alignment, Recursive Cognition, and AGI Trajectories",https://www.lesswrong.com/posts/hxukTwtywh2t5ZDXp/story-feedback-request-the-policy-emergent-alignment,https://www.lesswrong.com/posts/hxukTwtywh2t5ZDXp/story-feedback-request-the-policy-emergent-alignment,,2025-03-31T11:08:21.667000+00:00,10,2,,,
wGRnzCFcowRCrpX4Y,Downstream applications as validation of interpretability progress,https://www.lesswrong.com/posts/wGRnzCFcowRCrpX4Y/downstream-applications-as-validation-of-interpretability,https://www.lesswrong.com/posts/wGRnzCFcowRCrpX4Y/downstream-applications-as-validation-of-interpretability,,2025-03-31T01:35:02.722000+00:00,112,3,,,
sDuWXb8cPXZ2yHdH4,"Alignment first, intelligence later",https://www.lesswrong.com/posts/sDuWXb8cPXZ2yHdH4/alignment-first-intelligence-later,https://www.lesswrong.com/posts/sDuWXb8cPXZ2yHdH4/alignment-first-intelligence-later,,2025-03-30T22:26:55.302000+00:00,18,5,,,
gwKyHqe4CL6TZNQxp,Why do many people who care about AI Safety not clearly endorse PauseAI?,https://www.lesswrong.com/posts/gwKyHqe4CL6TZNQxp/why-do-many-people-who-care-about-ai-safety-not-clearly,https://www.lesswrong.com/posts/gwKyHqe4CL6TZNQxp/why-do-many-people-who-care-about-ai-safety-not-clearly,,2025-03-30T18:06:32.426000+00:00,45,42,,,
tQJB3PAzTCRKzTE2m,What does aligning AI to an ideology mean for true alignment?,https://www.lesswrong.com/posts/tQJB3PAzTCRKzTE2m/can-a-system-physically-immune-to-hacking-be-useful-for,https://www.lesswrong.com/posts/tQJB3PAzTCRKzTE2m/can-a-system-physically-immune-to-hacking-be-useful-for,,2025-03-30T15:12:09.802000+00:00,1,0,,,
EkDYkucM4CwgcmvCR,How to enjoy fail attempts without self-deception (technique),https://www.lesswrong.com/posts/EkDYkucM4CwgcmvCR/how-to-enjoy-fail-attempts-without-self-deception-technique,https://www.lesswrong.com/posts/EkDYkucM4CwgcmvCR/how-to-enjoy-fail-attempts-without-self-deception-technique,,2025-03-30T13:49:23.793000+00:00,9,0,,,
QGQiCuE33iHFv9jkv,"Softmax, Emmett Shear's new AI startup focused on ""Organic Alignment""",https://www.lesswrong.com/posts/QGQiCuE33iHFv9jkv/softmax-emmett-shear-s-new-ai-startup-focused-on-organic,https://www.lesswrong.com/posts/QGQiCuE33iHFv9jkv/softmax-emmett-shear-s-new-ai-startup-focused-on-organic,,2025-03-28T21:23:46.220000+00:00,61,2,,,
TFToqpaKMhcjAEY5E,AXRP Episode 40 - Jason Gross on Compact Proofs and Interpretability,https://www.lesswrong.com/posts/TFToqpaKMhcjAEY5E/axrp-episode-40-jason-gross-on-compact-proofs-and,https://www.lesswrong.com/posts/TFToqpaKMhcjAEY5E/axrp-episode-40-jason-gross-on-compact-proofs-and,,2025-03-28T18:40:01.856000+00:00,26,0,,,
cYseakJxjsicHSSJC,Share AI Safety Ideas: Both Crazy and Not. №2,https://www.lesswrong.com/posts/cYseakJxjsicHSSJC/share-ai-safety-ideas-both-crazy-and-not-2,https://www.lesswrong.com/posts/cYseakJxjsicHSSJC/share-ai-safety-ideas-both-crazy-and-not-2,,2025-03-28T17:22:22.814000+00:00,2,10,,,
r4BgDjyH5SyJGHEdq,Alignment through atomic agents,https://www.lesswrong.com/posts/r4BgDjyH5SyJGHEdq/alignment-through-atomic-agents,https://www.lesswrong.com/posts/r4BgDjyH5SyJGHEdq/alignment-through-atomic-agents,,2025-03-27T18:43:14.569000+00:00,-1,0,,,
7QJcqJpAmQKakDBGf,AI Moral Alignment: The Most Important Goal of Our Generation,https://www.lesswrong.com/posts/7QJcqJpAmQKakDBGf/ai-moral-alignment-the-most-important-goal-of-our-generation,https://www.lesswrong.com/posts/7QJcqJpAmQKakDBGf/ai-moral-alignment-the-most-important-goal-of-our-generation,,2025-03-27T18:04:07.212000+00:00,3,0,,,
kCGk5tp5suHoGwhCa,Mistral Large 2 (123B) seems to exhibit alignment faking,https://www.lesswrong.com/posts/kCGk5tp5suHoGwhCa/mistral-large-2-123b-seems-to-exhibit-alignment-faking,https://www.lesswrong.com/posts/kCGk5tp5suHoGwhCa/mistral-large-2-123b-seems-to-exhibit-alignment-faking,,2025-03-27T15:39:02.176000+00:00,81,4,,,
6YxdpGjfHyrZb7F2G,Third-wave AI safety needs sociopolitical thinking,https://www.lesswrong.com/posts/6YxdpGjfHyrZb7F2G/third-wave-ai-safety-needs-sociopolitical-thinking,https://www.lesswrong.com/posts/6YxdpGjfHyrZb7F2G/third-wave-ai-safety-needs-sociopolitical-thinking,,2025-03-27T00:55:30.548000+00:00,99,23,,,
tQzeafo9HjCeXn7ZF,AI companies should be safety-testing the most capable versions of their models,https://www.lesswrong.com/posts/tQzeafo9HjCeXn7ZF/ai-companies-should-be-safety-testing-the-most-capable,https://www.lesswrong.com/posts/tQzeafo9HjCeXn7ZF/ai-companies-should-be-safety-testing-the-most-capable,,2025-03-26T19:03:41.790000+00:00,17,6,,,
tgHps2cxiGDkNxNZN,Finding Emergent Misalignment,https://www.lesswrong.com/posts/tgHps2cxiGDkNxNZN/finding-emergent-misalignment,https://www.lesswrong.com/posts/tgHps2cxiGDkNxNZN/finding-emergent-misalignment,,2025-03-26T17:33:46.792000+00:00,26,0,,,
sc4Kh5sR8sLnLr8sf,New AI safety treaty paper out!,https://www.lesswrong.com/posts/sc4Kh5sR8sLnLr8sf/new-ai-safety-treaty-paper-out,https://www.lesswrong.com/posts/sc4Kh5sR8sLnLr8sf/new-ai-safety-treaty-paper-out,,2025-03-26T09:29:07.661000+00:00,15,2,,,
c5jueEHxgLmYED6bj,Emergent scaling effects on the functional hierarchies within LLMs,https://www.lesswrong.com/posts/c5jueEHxgLmYED6bj/hardcorellama_pcb_mar24_lw_post,https://www.lesswrong.com/posts/c5jueEHxgLmYED6bj/hardcorellama_pcb_mar24_lw_post,,2025-03-24T13:03:30.930000+00:00,8,0,,,
F5QQuQDk79ouL9DbQ,Recommender Alignment for Lock-In Risk,https://www.lesswrong.com/posts/F5QQuQDk79ouL9DbQ/recommender-alignment-for-lock-in-risk,https://www.lesswrong.com/posts/F5QQuQDk79ouL9DbQ/recommender-alignment-for-lock-in-risk,,2025-03-24T12:56:46.389000+00:00,8,0,,,
bqWihHtDnDseyfF2T,Edge Cases in AI Alignment,https://www.lesswrong.com/posts/bqWihHtDnDseyfF2T/edge-cases-in-ai-alignment-2,https://www.lesswrong.com/posts/bqWihHtDnDseyfF2T/edge-cases-in-ai-alignment-2,,2025-03-24T09:27:58.164000+00:00,19,3,,,
bzYJCXicmwDHDpLZa,Reframing AI Safety as a Neverending Institutional Challenge,https://www.lesswrong.com/posts/bzYJCXicmwDHDpLZa/reframing-ai-safety-as-a-neverending-institutional-challenge,https://www.lesswrong.com/posts/bzYJCXicmwDHDpLZa/reframing-ai-safety-as-a-neverending-institutional-challenge,,2025-03-23T00:13:48.614000+00:00,53,12,,,
LhnqegFoykcjaXCYH,100+ concrete projects and open problems in evals,https://www.lesswrong.com/posts/LhnqegFoykcjaXCYH/100-concrete-projects-and-open-problems-in-evals,https://www.lesswrong.com/posts/LhnqegFoykcjaXCYH/100-concrete-projects-and-open-problems-in-evals,,2025-03-22T15:21:40.970000+00:00,75,1,,,
y5cYisQ2QHiSbQbhk,"Prospects for Alignment Automation:
Interpretability Case Study",https://www.lesswrong.com/posts/y5cYisQ2QHiSbQbhk/prospects-for-alignment-automation-interpretability-case,https://www.lesswrong.com/posts/y5cYisQ2QHiSbQbhk/prospects-for-alignment-automation-interpretability-case,,2025-03-21T14:05:51.528000+00:00,32,5,,,
8AHzYHCJmKrzNSKYL,Minor interpretability exploration #4: LayerNorm and the learning coefficient,https://www.lesswrong.com/posts/8AHzYHCJmKrzNSKYL/minor-interpretability-exploration-4-layernorm-and-the,https://www.lesswrong.com/posts/8AHzYHCJmKrzNSKYL/minor-interpretability-exploration-4-layernorm-and-the,,2025-03-20T16:18:04.801000+00:00,4,0,,,
gXyMCnjrMfBbnYyZ4,How far along Metr's law can AI start automating or helping with alignment research?,https://www.lesswrong.com/posts/gXyMCnjrMfBbnYyZ4/how-far-along-metr-s-law-can-ai-start-automating-or-helping,https://www.lesswrong.com/posts/gXyMCnjrMfBbnYyZ4/how-far-along-metr-s-law-can-ai-start-automating-or-helping,,2025-03-20T15:58:08.369000+00:00,20,21,,,
xx3St4KC3KHHPGfL9,Human alignment,https://www.lesswrong.com/posts/xx3St4KC3KHHPGfL9/human-alignment,https://www.lesswrong.com/posts/xx3St4KC3KHHPGfL9/human-alignment,,2025-03-20T15:52:22.081000+00:00,-16,2,,,
YDe5TJAgWAtk6LsJ9,What is an alignment tax?,https://www.lesswrong.com/posts/YDe5TJAgWAtk6LsJ9/what-is-an-alignment-tax,https://www.lesswrong.com/posts/YDe5TJAgWAtk6LsJ9/what-is-an-alignment-tax,,2025-03-20T13:06:58.087000+00:00,5,0,,,
hvutW8jpPC2vgydaF,"The case against ""The case against AI alignment""",https://www.lesswrong.com/posts/hvutW8jpPC2vgydaF/the-case-against-the-case-against-ai-alignment,https://www.lesswrong.com/posts/hvutW8jpPC2vgydaF/the-case-against-the-case-against-ai-alignment,,2025-03-19T22:40:33.812000+00:00,1,0,,,
tTyQvHedrjFWjurGk,Schmidt Sciences Technical AI Safety RFP on Inference-Time Compute – Deadline: April 30,https://www.lesswrong.com/posts/tTyQvHedrjFWjurGk/schmidt-sciences-technical-ai-safety-rfp-on-inference-time,https://www.lesswrong.com/posts/tTyQvHedrjFWjurGk/schmidt-sciences-technical-ai-safety-rfp-on-inference-time,,2025-03-18T18:05:34.757000+00:00,18,0,,,
mG7jioaAsBasnuD4b,Subspace Rerouting: Using Mechanistic Interpretability to Craft Adversarial Attacks against Large Language Models,https://www.lesswrong.com/posts/mG7jioaAsBasnuD4b/subspace-rerouting-using-mechanistic-interpretability-to-2,https://www.lesswrong.com/posts/mG7jioaAsBasnuD4b/subspace-rerouting-using-mechanistic-interpretability-to-2,,2025-03-18T17:55:07.016000+00:00,6,1,,,
RoWabfQxabWBiXwxP,"Go home GPT-4o, you’re drunk: emergent misalignment as lowered inhibitions",https://www.lesswrong.com/posts/RoWabfQxabWBiXwxP/go-home-gpt-4o-you-re-drunk-emergent-misalignment-as-lowered,https://www.lesswrong.com/posts/RoWabfQxabWBiXwxP/go-home-gpt-4o-you-re-drunk-emergent-misalignment-as-lowered,,2025-03-18T14:48:54.762000+00:00,80,12,,,
edC8J5mXqH8EjwsZ6,What is the theory of change behind writing papers about AI safety?,https://www.lesswrong.com/posts/edC8J5mXqH8EjwsZ6/what-is-the-theory-of-change-behind-writing-papers-about-ai,https://www.lesswrong.com/posts/edC8J5mXqH8EjwsZ6/what-is-the-theory-of-change-behind-writing-papers-about-ai,,2025-03-18T12:51:31.405000+00:00,7,1,,,
HYkg6kwqhCQT5uYuK,EIS XV: A New Proof of Concept for Useful Interpretability,https://www.lesswrong.com/posts/HYkg6kwqhCQT5uYuK/eis-xv-a-new-proof-of-concept-for-useful-interpretability,https://www.lesswrong.com/posts/HYkg6kwqhCQT5uYuK/eis-xv-a-new-proof-of-concept-for-useful-interpretability,,2025-03-17T20:05:30.580000+00:00,30,2,,,
E3daBewppAiECN3Ao,Claude Sonnet 3.7 (often) knows when it’s in alignment evaluations,https://www.lesswrong.com/posts/E3daBewppAiECN3Ao/claude-sonnet-3-7-often-knows-when-it-s-in-alignment,https://www.lesswrong.com/posts/E3daBewppAiECN3Ao/claude-sonnet-3-7-often-knows-when-it-s-in-alignment,,2025-03-17T19:11:00.813000+00:00,188,9,,,
3JeE2dXSKHue2hhZj,Interested in working from a new Boston AI Safety Hub? ,https://www.lesswrong.com/posts/3JeE2dXSKHue2hhZj/interested-in-working-from-a-new-boston-ai-safety-hub,https://www.lesswrong.com/posts/3JeE2dXSKHue2hhZj/interested-in-working-from-a-new-boston-ai-safety-hub,,2025-03-17T13:42:19.509000+00:00,17,0,,,
PejNckwQj3A2MGhMA,Systematic runaway-optimiser-like LLM failure modes on Biologically and Economically aligned AI safety benchmarks for LLMs with simplified observation format (BioBlue),https://www.lesswrong.com/posts/PejNckwQj3A2MGhMA/systematic-runaway-optimiser-like-llm-failure-modes-on,https://www.lesswrong.com/posts/PejNckwQj3A2MGhMA/systematic-runaway-optimiser-like-llm-failure-modes-on,,2025-03-16T23:23:30.989000+00:00,45,8,,,
4rGJ75ZSym9uWfoaB,Can we ever ensure AI alignment if we can only test AI personas?,https://www.lesswrong.com/posts/4rGJ75ZSym9uWfoaB/can-we-ever-ensure-ai-alignment-if-we-can-only-test-ai,https://www.lesswrong.com/posts/4rGJ75ZSym9uWfoaB/can-we-ever-ensure-ai-alignment-if-we-can-only-test-ai,,2025-03-16T08:06:42.345000+00:00,22,8,,,
TtF85m8rJ85vdtawF,Paper: Field-building and the epistemic culture of AI safety,https://www.lesswrong.com/posts/TtF85m8rJ85vdtawF/paper-field-building-and-the-epistemic-culture-of-ai-safety,https://www.lesswrong.com/posts/TtF85m8rJ85vdtawF/paper-field-building-and-the-epistemic-culture-of-ai-safety,,2025-03-15T12:30:14.088000+00:00,13,3,,,
EPxcGvDXsy5GsmhRd,Geometry of Features in Mechanistic Interpretability,https://www.lesswrong.com/posts/EPxcGvDXsy5GsmhRd/geometry-of-features-in-mechanistic-interpretability-1,https://www.lesswrong.com/posts/EPxcGvDXsy5GsmhRd/geometry-of-features-in-mechanistic-interpretability-1,,2025-03-14T19:11:04.287000+00:00,16,0,,,
S76fyabDPySQgh9G3,Minor interpretability exploration #3: Extending superposition to different activation functions (loss landscape),https://www.lesswrong.com/posts/S76fyabDPySQgh9G3/minor-interpretability-exploration-2-extending-superposition-1,https://www.lesswrong.com/posts/S76fyabDPySQgh9G3/minor-interpretability-exploration-2-extending-superposition-1,,2025-03-14T15:45:14.365000+00:00,5,0,,,
F3j4xqpxjxgQD3xXh,AI for AI safety,https://www.lesswrong.com/posts/F3j4xqpxjxgQD3xXh/ai-for-ai-safety,https://www.lesswrong.com/posts/F3j4xqpxjxgQD3xXh/ai-for-ai-safety,,2025-03-14T15:00:23.491000+00:00,79,13,,,
ghL8EQCXAJ9qCBFg5,Should AI safety be a mass movement?,https://www.lesswrong.com/posts/ghL8EQCXAJ9qCBFg5/should-ai-safety-be-a-mass-movement,https://www.lesswrong.com/posts/ghL8EQCXAJ9qCBFg5/should-ai-safety-be-a-mass-movement,,2025-03-13T20:36:59.284000+00:00,5,1,,,
jtqcsARGtmgogdcLT,Reducing LLM deception at scale with self-other overlap fine-tuning,https://www.lesswrong.com/posts/jtqcsARGtmgogdcLT/reducing-llm-deception-at-scale-with-self-other-overlap-fine,https://www.lesswrong.com/posts/jtqcsARGtmgogdcLT/reducing-llm-deception-at-scale-with-self-other-overlap-fine,,2025-03-13T19:09:43.620000+00:00,162,46,,,
RRu7jMrzgwwZjher5,The Other Alignment Problem: Maybe AI Needs Protection From Us,https://www.lesswrong.com/posts/RRu7jMrzgwwZjher5/the-other-alignment-problem-maybe-ai-needs-protection-from,https://www.lesswrong.com/posts/RRu7jMrzgwwZjher5/the-other-alignment-problem-maybe-ai-needs-protection-from,,2025-03-13T18:03:43.086000+00:00,-2,0,,,
gTt2J5uJ4mrTWkncF,Intelsat as a Model for International AGI Governance,https://www.lesswrong.com/posts/gTt2J5uJ4mrTWkncF/intelsat-as-a-model-for-international-agi-governance,https://www.lesswrong.com/posts/gTt2J5uJ4mrTWkncF/intelsat-as-a-model-for-international-agi-governance,,2025-03-13T12:58:11.692000+00:00,45,0,,,
RMpSgPWb8ZAJYZRHB,"The prospect of accelerated AI safety progress, including philosophical progress",https://www.lesswrong.com/posts/RMpSgPWb8ZAJYZRHB/the-prospect-of-accelerated-ai-safety-progress-including,https://www.lesswrong.com/posts/RMpSgPWb8ZAJYZRHB/the-prospect-of-accelerated-ai-safety-progress-including,,2025-03-13T10:52:13.745000+00:00,11,0,,,
5naJwQnbb5bwPCCFz,Revising Stages-Oversight Reveals Greater Situational Awareness in LLMs,https://www.lesswrong.com/posts/5naJwQnbb5bwPCCFz/revising-stages-oversight,https://www.lesswrong.com/posts/5naJwQnbb5bwPCCFz/revising-stages-oversight,,2025-03-12T17:56:31.910000+00:00,16,0,,,
kBgySGcASWa4FWdD9,Paths and waystations in AI safety,https://www.lesswrong.com/posts/kBgySGcASWa4FWdD9/paths-and-waystations-in-ai-safety-1,https://www.lesswrong.com/posts/kBgySGcASWa4FWdD9/paths-and-waystations-in-ai-safety-1,,2025-03-11T18:52:57.772000+00:00,42,1,,,
ZuADxSHmTECTteD86,Meridian Cambridge Visiting Researcher Programme: Turn AI safety ideas into funded projects in one week!,https://www.lesswrong.com/posts/ZuADxSHmTECTteD86/meridian-cambridge-visiting-researcher-programme-turn-ai,https://www.lesswrong.com/posts/ZuADxSHmTECTteD86/meridian-cambridge-visiting-researcher-programme-turn-ai,,2025-03-11T17:46:29.656000+00:00,13,0,,,
DuAsRjABCxw52oaGr,"Scaling AI Regulation: Realistically, what Can (and Can’t) Be Regulated?",https://www.lesswrong.com/posts/DuAsRjABCxw52oaGr/scaling-ai-regulation-realistically-what-can-and-can-t-be,https://www.lesswrong.com/posts/DuAsRjABCxw52oaGr/scaling-ai-regulation-realistically-what-can-and-can-t-be,,2025-03-11T16:51:41.651000+00:00,3,1,,,
zLvt2xHopdbArfvbM,When is it Better to Train on the Alignment Proxy?,https://www.lesswrong.com/posts/zLvt2xHopdbArfvbM/when-is-it-better-to-train-on-the-alignment-proxy,https://www.lesswrong.com/posts/zLvt2xHopdbArfvbM/when-is-it-better-to-train-on-the-alignment-proxy,,2025-03-11T13:35:51.152000+00:00,14,0,,,
LLJm97gLDacn8AscB,Introducing 11 New AI Safety Organizations - Catalyze's Winter 24/25 London Incubation Program Cohort,https://www.lesswrong.com/posts/LLJm97gLDacn8AscB/introducing-11-new-ai-safety-organizations-catalyze-s-winter,https://www.lesswrong.com/posts/LLJm97gLDacn8AscB/introducing-11-new-ai-safety-organizations-catalyze-s-winter,,2025-03-10T19:26:11.017000+00:00,75,0,,,
ySojRcfMddtyFCYEe,How Can Average People Contribute to AI Safety?,https://www.lesswrong.com/posts/ySojRcfMddtyFCYEe/how-can-average-people-contribute-to-ai-safety,https://www.lesswrong.com/posts/ySojRcfMddtyFCYEe/how-can-average-people-contribute-to-ai-safety,,2025-03-06T22:50:12.288000+00:00,16,4,,,
wMn9wSxzSs3uHiAmu,Minor interpretability exploration #2: Extending superposition to different activation functions,https://www.lesswrong.com/posts/wMn9wSxzSs3uHiAmu/minor-interpretability-exploration-2-extending-superposition,https://www.lesswrong.com/posts/wMn9wSxzSs3uHiAmu/minor-interpretability-exploration-2-extending-superposition,,2025-03-06T11:22:53.528000+00:00,3,0,,,
Wi5keDzktqmANL422,On OpenAI’s Safety and Alignment Philosophy,https://www.lesswrong.com/posts/Wi5keDzktqmANL422/on-openai-s-safety-and-alignment-philosophy,https://www.lesswrong.com/posts/Wi5keDzktqmANL422/on-openai-s-safety-and-alignment-philosophy,,2025-03-05T14:00:07.302000+00:00,58,5,,,
KnTmnPcDQ5xBACPP6,"The Alignment Imperative: Act Now or Lose Everything

",https://www.lesswrong.com/posts/KnTmnPcDQ5xBACPP6/the-alignment-imperative-act-now-or-lose-everything,https://www.lesswrong.com/posts/KnTmnPcDQ5xBACPP6/the-alignment-imperative-act-now-or-lose-everything,,2025-03-05T05:49:50.316000+00:00,-14,0,,,
vxSGDLGRtfcf6FWBg,"Top AI safety newsletters, books, podcasts, etc – new AISafety.com resource",https://www.lesswrong.com/posts/vxSGDLGRtfcf6FWBg/top-ai-safety-newsletters-books-podcasts-etc-new-aisafety,https://www.lesswrong.com/posts/vxSGDLGRtfcf6FWBg/top-ai-safety-newsletters-books-podcasts-etc-new-aisafety,,2025-03-04T17:01:18.758000+00:00,33,2,,,
wZBqhxkgC4J6oFhuA,2028 Should Not Be AI Safety's First Foray Into Politics,https://www.lesswrong.com/posts/wZBqhxkgC4J6oFhuA/2028-should-not-be-ai-safety-s-first-foray-into-politics,https://www.lesswrong.com/posts/wZBqhxkgC4J6oFhuA/2028-should-not-be-ai-safety-s-first-foray-into-politics,,2025-03-04T16:46:37.370000+00:00,5,0,,,
CXYf7kGBecZMajrXC,Validating against a misalignment detector is very different to training against one,https://www.lesswrong.com/posts/CXYf7kGBecZMajrXC/validating-against-a-misalignment-detector-is-very-different,https://www.lesswrong.com/posts/CXYf7kGBecZMajrXC/validating-against-a-misalignment-detector-is-very-different,,2025-03-04T15:41:04.692000+00:00,39,4,,,
Bi4qEyHFnKQmvmbF7,"AI Safety at the Frontier: Paper Highlights, February '25",https://www.lesswrong.com/posts/Bi4qEyHFnKQmvmbF7/ai-safety-at-the-frontier-paper-highlights-february-25,https://www.lesswrong.com/posts/Bi4qEyHFnKQmvmbF7/ai-safety-at-the-frontier-paper-highlights-february-25,,2025-03-03T22:09:37.845000+00:00,7,0,,,
e3CpMJrZQjbXeqA6C,Examples of self-fulfilling prophecies in AI alignment?,https://www.lesswrong.com/posts/e3CpMJrZQjbXeqA6C/examples-of-self-fulfilling-prophecies-in-ai-alignment,https://www.lesswrong.com/posts/e3CpMJrZQjbXeqA6C/examples-of-self-fulfilling-prophecies-in-ai-alignment,,2025-03-03T02:45:51.619000+00:00,24,9,,,
QkEyry3Mqo8umbhoK,Self-fulfilling misalignment data might be poisoning our AI models,https://www.lesswrong.com/posts/QkEyry3Mqo8umbhoK/self-fulfilling-misalignment-data-might-be-poisoning-our-ai,https://www.lesswrong.com/posts/QkEyry3Mqo8umbhoK/self-fulfilling-misalignment-data-might-be-poisoning-our-ai,,2025-03-02T19:51:14.775000+00:00,154,29,,,
PhgEKkB4cwYjwpGxb,Maintaining Alignment during RSI as a Feedback Control Problem,https://www.lesswrong.com/posts/PhgEKkB4cwYjwpGxb/maintaining-alignment-during-rsi-as-a-feedback-control,https://www.lesswrong.com/posts/PhgEKkB4cwYjwpGxb/maintaining-alignment-during-rsi-as-a-feedback-control,,2025-03-02T00:21:43.432000+00:00,67,6,,,
RCDdZsutRr7aoJTTX,AI Safety Policy Won't Go On Like This – AI Safety Advocacy Is Failing Because Nobody Cares.,https://www.lesswrong.com/posts/RCDdZsutRr7aoJTTX/ai-safety-policy-won-t-go-on-like-this-ai-safety-advocacy-is,https://www.lesswrong.com/posts/RCDdZsutRr7aoJTTX/ai-safety-policy-won-t-go-on-like-this-ai-safety-advocacy-is,,2025-03-01T20:15:16.645000+00:00,1,1,,,
GwZvpYR7Hv2smv8By,Share AI Safety Ideas: Both Crazy and Not,https://www.lesswrong.com/posts/GwZvpYR7Hv2smv8By/share-ai-safety-ideas-both-crazy-and-not,https://www.lesswrong.com/posts/GwZvpYR7Hv2smv8By/share-ai-safety-ideas-both-crazy-and-not,,2025-03-01T19:08:25.605000+00:00,17,28,,,
AcTEiu5wYDgrbmXow,Open problems in emergent misalignment,https://www.lesswrong.com/posts/AcTEiu5wYDgrbmXow/open-problems-in-emergent-misalignment,https://www.lesswrong.com/posts/AcTEiu5wYDgrbmXow/open-problems-in-emergent-misalignment,,2025-03-01T09:47:58.889000+00:00,83,17,,,
jhRzPafSG9ndzF6d2,Do we want alignment faking?,https://www.lesswrong.com/posts/jhRzPafSG9ndzF6d2/do-we-want-alignment-faking,https://www.lesswrong.com/posts/jhRzPafSG9ndzF6d2/do-we-want-alignment-faking,,2025-02-28T21:50:48.891000+00:00,7,4,,,
iKiREYhxLSjCkDGPa,Misspecification in Inverse Reinforcement Learning - Part II,https://www.lesswrong.com/posts/iKiREYhxLSjCkDGPa/misspecification-in-inverse-reinforcement-learning-part-ii,https://www.lesswrong.com/posts/iKiREYhxLSjCkDGPa/misspecification-in-inverse-reinforcement-learning-part-ii,,2025-02-28T19:24:59.570000+00:00,9,0,,,
orCtTgQkWwwD3XN87,Misspecification in Inverse Reinforcement Learning,https://www.lesswrong.com/posts/orCtTgQkWwwD3XN87/misspecification-in-inverse-reinforcement-learning,https://www.lesswrong.com/posts/orCtTgQkWwwD3XN87/misspecification-in-inverse-reinforcement-learning,,2025-02-28T19:24:49.204000+00:00,19,0,,,
B8nhbALDQ62pBp5iB,An Open Letter To EA and AI Safety On Decelerating AI Development,https://www.lesswrong.com/posts/B8nhbALDQ62pBp5iB/an-open-letter-to-ea-and-ai-safety-on-decelerating-ai,https://www.lesswrong.com/posts/B8nhbALDQ62pBp5iB/an-open-letter-to-ea-and-ai-safety-on-decelerating-ai,,2025-02-28T17:21:42.826000+00:00,8,0,,,
7BEcAzxCXenwcjXuE,On Emergent Misalignment,https://www.lesswrong.com/posts/7BEcAzxCXenwcjXuE/on-emergent-misalignment,https://www.lesswrong.com/posts/7BEcAzxCXenwcjXuE/on-emergent-misalignment,,2025-02-28T13:10:05.973000+00:00,88,5,,,
6aXe9nipTgwK5LxaP,Do safety-relevant LLM steering vectors optimized on a single example generalize?,https://www.lesswrong.com/posts/6aXe9nipTgwK5LxaP/do-safety-relevant-llm-steering-vectors-optimized-on-a,https://www.lesswrong.com/posts/6aXe9nipTgwK5LxaP/do-safety-relevant-llm-steering-vectors-optimized-on-a,,2025-02-28T12:01:12.514000+00:00,21,1,,,
NecfBNGdtjM3uJqkb,Recursive alignment with the principle of alignment,https://www.lesswrong.com/posts/NecfBNGdtjM3uJqkb/recursive-alignment-with-the-principle-of-alignment,https://www.lesswrong.com/posts/NecfBNGdtjM3uJqkb/recursive-alignment-with-the-principle-of-alignment,,2025-02-27T02:34:37.940000+00:00,12,4,,,
AndYxHFXMgkGXTAff,Universal AI Maximizes Variational Empowerment: New Insights into AGI Safety,https://www.lesswrong.com/posts/AndYxHFXMgkGXTAff/universal-ai-maximizes-variational-empowerment-new-insights,https://www.lesswrong.com/posts/AndYxHFXMgkGXTAff/universal-ai-maximizes-variational-empowerment-new-insights,,2025-02-27T00:46:46.989000+00:00,13,1,,,
tzkakoG9tYLbLTvHG,"Minor interpretability exploration #1: Grokking of modular addition, subtraction, multiplication, for different activation functions",https://www.lesswrong.com/posts/tzkakoG9tYLbLTvHG/minor-interpretability-exploration-1-grokking-of-modular,https://www.lesswrong.com/posts/tzkakoG9tYLbLTvHG/minor-interpretability-exploration-1-grokking-of-modular,,2025-02-26T11:35:56.610000+00:00,5,13,,,
ifechgnJRtJdduFGC,Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs,https://www.lesswrong.com/posts/ifechgnJRtJdduFGC/emergent-misalignment-narrow-finetuning-can-produce-broadly,https://www.lesswrong.com/posts/ifechgnJRtJdduFGC/emergent-misalignment-narrow-finetuning-can-produce-broadly,,2025-02-25T17:39:31.059000+00:00,333,92,,,
ajudxufov7HnYxHFr,Making alignment a law of the universe,https://www.lesswrong.com/posts/ajudxufov7HnYxHFr/making-alignment-a-law-of-the-universe,https://www.lesswrong.com/posts/ajudxufov7HnYxHFr/making-alignment-a-law-of-the-universe,,2025-02-25T10:44:11.632000+00:00,0,3,,,
MYX9XcsyRxjHtav6G,Upcoming Protest for AI Safety,https://www.lesswrong.com/posts/MYX9XcsyRxjHtav6G/upcoming-protest-for-ai-safety,https://www.lesswrong.com/posts/MYX9XcsyRxjHtav6G/upcoming-protest-for-ai-safety,,2025-02-25T03:04:03.153000+00:00,12,0,,,
6oF6pRr2FgjTmiHus,Topological Data Analysis and Mechanistic Interpretability,https://www.lesswrong.com/posts/6oF6pRr2FgjTmiHus/topological-data-analysis-and-mechanistic-interpretability,https://www.lesswrong.com/posts/6oF6pRr2FgjTmiHus/topological-data-analysis-and-mechanistic-interpretability,,2025-02-24T19:56:02.498000+00:00,16,4,,,
eZbhzFnhopdBXqwKD,Nationwide Action Workshop: Contact Congress about AI safety!,https://www.lesswrong.com/events/eZbhzFnhopdBXqwKD/nationwide-action-workshop-contact-congress-about-ai-safety,https://www.lesswrong.com/events/eZbhzFnhopdBXqwKD/nationwide-action-workshop-contact-congress-about-ai-safety,,2025-02-24T19:36:09.084000+00:00,7,0,,,
5gmALpCetyjkSPEDr,Training AI to do alignment research we don’t already know how to do,https://www.lesswrong.com/posts/5gmALpCetyjkSPEDr/training-ai-to-do-alignment-research-we-don-t-already-know,https://www.lesswrong.com/posts/5gmALpCetyjkSPEDr/training-ai-to-do-alignment-research-we-don-t-already-know,,2025-02-24T19:19:43.067000+00:00,45,24,,,
bc5ohMwAyshdwJkDt,Forecasting Frontier Language Model Agent Capabilities,https://www.lesswrong.com/posts/bc5ohMwAyshdwJkDt/forecasting-frontier-language-model-agent-capabilities,https://www.lesswrong.com/posts/bc5ohMwAyshdwJkDt/forecasting-frontier-language-model-agent-capabilities,,2025-02-24T16:51:32.022000+00:00,35,0,,,
HkvnpMwzJBb2t8vxx,AI alignment for mental health supports,https://www.lesswrong.com/posts/HkvnpMwzJBb2t8vxx/ai-alignment-for-mental-health-supports,https://www.lesswrong.com/posts/HkvnpMwzJBb2t8vxx/ai-alignment-for-mental-health-supports,,2025-02-24T04:21:42.379000+00:00,1,1,,,
aG9e5tHfHmBnDqrDy,The GDM AGI Safety+Alignment Team is Hiring for Applied Interpretability Research,https://www.lesswrong.com/posts/aG9e5tHfHmBnDqrDy/the-gdm-agi-safety-alignment-team-is-hiring-for-applied,https://www.lesswrong.com/posts/aG9e5tHfHmBnDqrDy/the-gdm-agi-safety-alignment-team-is-hiring-for-applied,,2025-02-24T02:17:12.991000+00:00,48,1,,,
fiWmh9yJgdPuqNN4m,Moral gauge theory: A speculative suggestion for AI alignment,https://www.lesswrong.com/posts/fiWmh9yJgdPuqNN4m/moral-gauge-theory-a-speculative-suggestion-for-ai-alignment,https://www.lesswrong.com/posts/fiWmh9yJgdPuqNN4m/moral-gauge-theory-a-speculative-suggestion-for-ai-alignment,,2025-02-23T11:42:31.083000+00:00,6,2,,,
eAQqyZFeDQtEK6oA4,Does human (mis)alignment pose a significant and imminent existential threat?,https://www.lesswrong.com/posts/eAQqyZFeDQtEK6oA4/does-human-mis-alignment-pose-a-significant-and-imminent,https://www.lesswrong.com/posts/eAQqyZFeDQtEK6oA4/does-human-mis-alignment-pose-a-significant-and-imminent,,2025-02-23T10:03:40.269000+00:00,6,3,,,
bTzk32t9aWJwLuNhi,Workshop: Interpretability in LLMs using Geometric and Statistical Methods,https://www.lesswrong.com/posts/bTzk32t9aWJwLuNhi/workshop-interpretability-in-llms-using-geometric-and,https://www.lesswrong.com/posts/bTzk32t9aWJwLuNhi/workshop-interpretability-in-llms-using-geometric-and,,2025-02-22T09:39:26.446000+00:00,17,0,,,
irxuoCTKdufEdskSk,Alignment can be the ‘clean energy’ of AI,https://www.lesswrong.com/posts/irxuoCTKdufEdskSk/alignment-can-be-the-clean-energy-of-ai,https://www.lesswrong.com/posts/irxuoCTKdufEdskSk/alignment-can-be-the-clean-energy-of-ai,,2025-02-22T00:08:30.391000+00:00,68,8,,,
JNL2bmDXmaG7YnRbF,MAISU - Minimal AI Safety Unconference ,https://www.lesswrong.com/events/JNL2bmDXmaG7YnRbF/maisu-minimal-ai-safety-unconference,https://www.lesswrong.com/events/JNL2bmDXmaG7YnRbF/maisu-minimal-ai-safety-unconference,,2025-02-21T11:36:25.202000+00:00,19,2,,,
sgR3BxRvowmecwJNT,Neural Scaling Laws Rooted in the Data Distribution,https://www.lesswrong.com/posts/sgR3BxRvowmecwJNT/neural-scaling-laws-rooted-in-the-data-distribution,https://www.lesswrong.com/posts/sgR3BxRvowmecwJNT/neural-scaling-laws-rooted-in-the-data-distribution,,2025-02-20T21:22:10.306000+00:00,8,0,,,
qLgJosa6mWCpMCzC9,Demonstrating specification gaming in reasoning models,https://www.lesswrong.com/posts/qLgJosa6mWCpMCzC9/demonstrating-specification-gaming-in-reasoning-models,https://www.lesswrong.com/posts/qLgJosa6mWCpMCzC9/demonstrating-specification-gaming-in-reasoning-models,,2025-02-20T19:26:20.563000+00:00,4,0,,,
2h42FmhWnYGsdMavE,"US AI Safety Institute will be 'gutted,' Axios reports",https://www.lesswrong.com/posts/2h42FmhWnYGsdMavE/us-ai-safety-institute-will-be-gutted-axios-reports,https://www.lesswrong.com/posts/2h42FmhWnYGsdMavE/us-ai-safety-institute-will-be-gutted-axios-reports,,2025-02-20T14:40:13.049000+00:00,11,1,,,
hiGYdb4JxTuk4DRCd,Modularity and assembly: AI safety via thinking smaller,https://www.lesswrong.com/posts/hiGYdb4JxTuk4DRCd/modularity-and-assembly-ai-safety-via-thinking-smaller,https://www.lesswrong.com/posts/hiGYdb4JxTuk4DRCd/modularity-and-assembly-ai-safety-via-thinking-smaller,,2025-02-20T00:58:39.714000+00:00,2,0,,,
mpMWWKzkzWqf57Yap,Eliezer's Lost Alignment Articles / The Arbital Sequence,https://www.lesswrong.com/posts/mpMWWKzkzWqf57Yap/eliezer-s-lost-alignment-articles-the-arbital-sequence,https://www.lesswrong.com/posts/mpMWWKzkzWqf57Yap/eliezer-s-lost-alignment-articles-the-arbital-sequence,,2025-02-20T00:48:10.338000+00:00,207,10,,,
4mBkGrHbkor4FeTnd,AI Alignment and the Financial War Against Narcissistic Manipulation,https://www.lesswrong.com/posts/4mBkGrHbkor4FeTnd/ai-alignment-and-the-financial-war-against-narcissistic,https://www.lesswrong.com/posts/4mBkGrHbkor4FeTnd/ai-alignment-and-the-financial-war-against-narcissistic,,2025-02-19T20:42:10.918000+00:00,-17,2,,,
FEPTehGERGPXsv6gw,New LLM Scaling Law,https://www.lesswrong.com/posts/FEPTehGERGPXsv6gw/new-llm-scaling-law,https://www.lesswrong.com/posts/FEPTehGERGPXsv6gw/new-llm-scaling-law,,2025-02-19T20:21:17.475000+00:00,2,0,,,
zavyum4dxEAqs6wHt,Undergrad AI Safety Conference,https://www.lesswrong.com/posts/zavyum4dxEAqs6wHt/undergrad-ai-safety-conference,https://www.lesswrong.com/posts/zavyum4dxEAqs6wHt/undergrad-ai-safety-conference,,2025-02-19T03:43:47.969000+00:00,19,0,,,
wqz5CRzqWkvzoatBG,AGI Safety & Alignment @ Google DeepMind is hiring,https://www.lesswrong.com/posts/wqz5CRzqWkvzoatBG/agi-safety-and-alignment-google-deepmind-is-hiring,https://www.lesswrong.com/posts/wqz5CRzqWkvzoatBG/agi-safety-and-alignment-google-deepmind-is-hiring,,2025-02-17T21:11:18.970000+00:00,103,19,,,
ykwA7jsiAD7NyxwLA,Cooperation for AI safety must transcend geopolitical interference,https://www.lesswrong.com/posts/ykwA7jsiAD7NyxwLA/cooperation-for-ai-safety-must-transcend-geopolitical,https://www.lesswrong.com/posts/ykwA7jsiAD7NyxwLA/cooperation-for-ai-safety-must-transcend-geopolitical,,2025-02-16T18:18:01.539000+00:00,7,6,,,
a3bdxaASt8cH9Jy8h,Artificial Static Place Intelligence: Guaranteed Alignment,https://www.lesswrong.com/posts/a3bdxaASt8cH9Jy8h/artificial-static-place-intelligence-guaranteed-alignment,https://www.lesswrong.com/posts/a3bdxaASt8cH9Jy8h/artificial-static-place-intelligence-guaranteed-alignment,,2025-02-15T11:08:50.226000+00:00,2,2,,,
TJrCumJxhzTmNBsRz,A short course on AGI safety from the GDM Alignment team,https://www.lesswrong.com/posts/TJrCumJxhzTmNBsRz/a-short-course-on-agi-safety-from-the-gdm-alignment-team,https://www.lesswrong.com/posts/TJrCumJxhzTmNBsRz/a-short-course-on-agi-safety-from-the-gdm-alignment-team,,2025-02-14T15:43:50.903000+00:00,105,2,,,
RoGdEq6Cz8yWyX4kp,What is a circuit? [in interpretability],https://www.lesswrong.com/posts/RoGdEq6Cz8yWyX4kp/what-is-a-circuit-in-interpretability,https://www.lesswrong.com/posts/RoGdEq6Cz8yWyX4kp/what-is-a-circuit-in-interpretability,,2025-02-14T04:40:42.978000+00:00,23,1,,,
Ymh2dffBZs5CJhedF,Static Place AI Makes Agentic AI Redundant: Multiversal AI Alignment & Rational Utopia,https://www.lesswrong.com/posts/Ymh2dffBZs5CJhedF/static-place-ai-makes-agentic-ai-redundant-multiversal-ai,https://www.lesswrong.com/posts/Ymh2dffBZs5CJhedF/static-place-ai-makes-agentic-ai-redundant-multiversal-ai,,2025-02-13T22:35:28.300000+00:00,1,2,,,
cus5CGmLrjBRgcPSF,"System 2 Alignment: Deliberation, Review, and Thought Management",https://www.lesswrong.com/posts/cus5CGmLrjBRgcPSF/system-2-alignment-deliberation-review-and-thought,https://www.lesswrong.com/posts/cus5CGmLrjBRgcPSF/system-2-alignment-deliberation-review-and-thought,,2025-02-13T19:17:56.868000+00:00,39,0,,,
syEwQzC6LQywQDrFi,What is it to solve the alignment problem?,https://www.lesswrong.com/posts/syEwQzC6LQywQDrFi/what-is-it-to-solve-the-alignment-problem-2,https://www.lesswrong.com/posts/syEwQzC6LQywQDrFi/what-is-it-to-solve-the-alignment-problem-2,,2025-02-13T18:42:07.215000+00:00,31,6,,,
fMqgLGoeZFFQqAGyC,How do we solve the alignment problem?,https://www.lesswrong.com/posts/fMqgLGoeZFFQqAGyC/how-do-we-solve-the-alignment-problem,https://www.lesswrong.com/posts/fMqgLGoeZFFQqAGyC/how-do-we-solve-the-alignment-problem,,2025-02-13T18:27:27.712000+00:00,63,9,,,
hAJKtx6A96pzAhorf,"OpenAI’s NSFW policy: user safety, harm reduction, and AI consent",https://www.lesswrong.com/posts/hAJKtx6A96pzAhorf/openai-s-nsfw-policy-user-safety-harm-reduction-and-ai,https://www.lesswrong.com/posts/hAJKtx6A96pzAhorf/openai-s-nsfw-policy-user-safety-harm-reduction-and-ai,,2025-02-13T13:59:22.911000+00:00,4,3,,,
5rMwWzRdWFtRdHeuE,Not all capabilities will be created equal: focus on strategically superhuman agents,https://www.lesswrong.com/posts/5rMwWzRdWFtRdHeuE/not-all-capabilities-will-be-created-equal-focus-on,https://www.lesswrong.com/posts/5rMwWzRdWFtRdHeuE/not-all-capabilities-will-be-created-equal-focus-on,,2025-02-13T01:24:46.084000+00:00,62,9,,,
7KijyCL8WNP8JnWCR,Gradient Anatomy's - Hallucination Robustness in Medical Q&A,https://www.lesswrong.com/posts/7KijyCL8WNP8JnWCR/gradient-anatomy-s-hallucination-robustness-in-medical-q-and,https://www.lesswrong.com/posts/7KijyCL8WNP8JnWCR/gradient-anatomy-s-hallucination-robustness-in-medical-q-and,,2025-02-12T19:16:58.949000+00:00,2,0,,,
qYPHryHTNiJ2y6Fhi,The Paris AI Anti-Safety Summit,https://www.lesswrong.com/posts/qYPHryHTNiJ2y6Fhi/the-paris-ai-anti-safety-summit,https://www.lesswrong.com/posts/qYPHryHTNiJ2y6Fhi/the-paris-ai-anti-safety-summit,,2025-02-12T14:00:07.383000+00:00,129,21,,,
J6rgqYjj7Cm89Xu2w,Where Would Good Forecasts Most Help AI Governance Efforts?,https://www.lesswrong.com/posts/J6rgqYjj7Cm89Xu2w/where-would-good-forecasts-most-help-ai-governance-efforts-1,https://www.lesswrong.com/posts/J6rgqYjj7Cm89Xu2w/where-would-good-forecasts-most-help-ai-governance-efforts-1,,2025-02-11T18:15:33.082000+00:00,11,1,,,
3XaizFzbcWAEp8G6o,"AI Safety at the Frontier: Paper Highlights, January '25",https://www.lesswrong.com/posts/3XaizFzbcWAEp8G6o/ai-safety-at-the-frontier-paper-highlights-january-25,https://www.lesswrong.com/posts/3XaizFzbcWAEp8G6o/ai-safety-at-the-frontier-paper-highlights-january-25,,2025-02-11T16:14:16.972000+00:00,7,0,,,
dLnwRFLFmHKuurTX2,Rethinking AI Safety Approach in the Era of Open-Source AI,https://www.lesswrong.com/posts/dLnwRFLFmHKuurTX2/rethinking-ai-safety-approach-in-the-era-of-open-source-ai,https://www.lesswrong.com/posts/dLnwRFLFmHKuurTX2/rethinking-ai-safety-approach-in-the-era-of-open-source-ai,,2025-02-11T14:01:39.167000+00:00,4,0,,,
CJ4yywLBkdRALc4sT,On Deliberative Alignment,https://www.lesswrong.com/posts/CJ4yywLBkdRALc4sT/on-deliberative-alignment,https://www.lesswrong.com/posts/CJ4yywLBkdRALc4sT/on-deliberative-alignment,,2025-02-11T13:00:07.683000+00:00,51,2,,,
LaruPAWaZk9KpC25A,"Rational Effective Utopia & Narrow Way There: Math-Proven Safe Static Multiversal mAX-Intelligence (AXI), Multiversal Alignment, New Ethicophysics... (Aug 11)",https://www.lesswrong.com/posts/LaruPAWaZk9KpC25A/rational-effective-utopia-and-narrow-way-there-math-proven,https://www.lesswrong.com/posts/LaruPAWaZk9KpC25A/rational-effective-utopia-and-narrow-way-there-math-proven,,2025-02-11T03:21:40.899000+00:00,13,8,,,
geRo75Xi9baHcwzht,Claude is More Anxious than GPT; Personality is an axis of interpretability in language models,https://www.lesswrong.com/posts/geRo75Xi9baHcwzht/claude-is-more-anxious-than-gpt-personality-is-an-axis-of-2,https://www.lesswrong.com/posts/geRo75Xi9baHcwzht/claude-is-more-anxious-than-gpt-personality-is-an-axis-of-2,,2025-02-10T19:19:28.005000+00:00,2,2,,,
QpaWHYEQomyQTBKw5,Nonpartisan AI safety,https://www.lesswrong.com/posts/QpaWHYEQomyQTBKw5/nonpartisan-ai-safety,https://www.lesswrong.com/posts/QpaWHYEQomyQTBKw5/nonpartisan-ai-safety,,2025-02-10T14:55:50.913000+00:00,30,4,,,
emdeWndtjD8QxzgS5,OpenAI lied about SFT vs. RLHF,https://www.lesswrong.com/posts/emdeWndtjD8QxzgS5/openai-lied-about-sft-vs-rlhf,https://www.lesswrong.com/posts/emdeWndtjD8QxzgS5/openai-lied-about-sft-vs-rlhf,,2025-02-10T03:24:16.625000+00:00,10,2,,,
feknAa3hQgLG2ZAna,Cross-Layer Feature Alignment and Steering in Large Language Model,https://www.lesswrong.com/posts/feknAa3hQgLG2ZAna/cross-layer-feature-alignment-and-steering-in-large-language-2,https://www.lesswrong.com/posts/feknAa3hQgLG2ZAna/cross-layer-feature-alignment-and-steering-in-large-language-2,,2025-02-08T20:18:20.331000+00:00,9,0,,,
cSPcey7FsKpNzLXuF,AI Safety Oversights,https://www.lesswrong.com/posts/cSPcey7FsKpNzLXuF/ai-safety-oversights,https://www.lesswrong.com/posts/cSPcey7FsKpNzLXuF/ai-safety-oversights,,2025-02-08T06:15:52.896000+00:00,3,0,,,
26SHhxK2yYQbh7ors,Research directions Open Phil wants to fund in technical AI safety,https://www.lesswrong.com/posts/26SHhxK2yYQbh7ors/research-directions-open-phil-wants-to-fund-in-technical-ai,https://www.lesswrong.com/posts/26SHhxK2yYQbh7ors/research-directions-open-phil-wants-to-fund-in-technical-ai,,2025-02-08T01:40:00.968000+00:00,117,21,,,
YXNeA3RyRrrRWS37A,A Problem to Solve Before Building a Deception Detector,https://www.lesswrong.com/posts/YXNeA3RyRrrRWS37A/a-problem-to-solve-before-building-a-deception-detector,https://www.lesswrong.com/posts/YXNeA3RyRrrRWS37A/a-problem-to-solve-before-building-a-deception-detector,,2025-02-07T19:35:23.307000+00:00,77,12,,,
etqbEF4yWoGBEaPro,On the Meta and DeepMind Safety Frameworks,https://www.lesswrong.com/posts/etqbEF4yWoGBEaPro/on-the-meta-and-deepmind-safety-frameworks,https://www.lesswrong.com/posts/etqbEF4yWoGBEaPro/on-the-meta-and-deepmind-safety-frameworks,,2025-02-07T13:10:08.449000+00:00,45,1,,,
zjqrSKZuRLnjAniyo,"Illusory Safety: Redteaming DeepSeek R1 and the Strongest Fine-Tunable Models of OpenAI, Anthropic, and Google",https://www.lesswrong.com/posts/zjqrSKZuRLnjAniyo/illusory-safety-redteaming-deepseek-r1-and-the-strongest,https://www.lesswrong.com/posts/zjqrSKZuRLnjAniyo/illusory-safety-redteaming-deepseek-r1-and-the-strongest,,2025-02-07T03:57:30.904000+00:00,37,0,,,
wbJxRNxuezvsGFEWv,Open Philanthropy Technical AI Safety RFP - $40M Available Across 21 Research Areas,https://www.lesswrong.com/posts/wbJxRNxuezvsGFEWv/open-philanthropy-technical-ai-safety-rfp-usd40m-available,https://www.lesswrong.com/posts/wbJxRNxuezvsGFEWv/open-philanthropy-technical-ai-safety-rfp-usd40m-available,,2025-02-06T18:58:53.076000+00:00,111,0,,,
9pGbTz6c78PGwJein,Detecting Strategic Deception Using Linear Probes,https://www.lesswrong.com/posts/9pGbTz6c78PGwJein/detecting-strategic-deception-using-linear-probes,https://www.lesswrong.com/posts/9pGbTz6c78PGwJein/detecting-strategic-deception-using-linear-probes,,2025-02-06T15:46:53.024000+00:00,104,9,,,
WHrig3emEAzMNDkyy,On AI Scaling,https://www.lesswrong.com/posts/WHrig3emEAzMNDkyy/on-ai-scaling,https://www.lesswrong.com/posts/WHrig3emEAzMNDkyy/on-ai-scaling,,2025-02-05T20:24:56.977000+00:00,6,3,,,
jfskr4ZZezpsq3pTd,Alignment Paradox and a Request for Harsh Criticism,https://www.lesswrong.com/posts/jfskr4ZZezpsq3pTd/alignment-paradox-and-a-request-for-harsh-criticism,https://www.lesswrong.com/posts/jfskr4ZZezpsq3pTd/alignment-paradox-and-a-request-for-harsh-criticism,,2025-02-05T18:17:22.701000+00:00,6,7,,,
Kwb8iyXTMBDBhg4k2,Introducing International AI Governance Alliance (IAIGA),https://www.lesswrong.com/posts/Kwb8iyXTMBDBhg4k2/introducing-international-ai-governance-alliance-iaiga,https://www.lesswrong.com/posts/Kwb8iyXTMBDBhg4k2/introducing-international-ai-governance-alliance-iaiga,,2025-02-05T16:02:29.226000+00:00,7,0,,,
3GtdC7yJ7CgXzPsfe,"Introducing Collective Action for Existential Safety: 80+ actions individuals, organizations, and nations can take to improve our existential safety",https://www.lesswrong.com/posts/3GtdC7yJ7CgXzPsfe/introducing-collective-action-for-existential-safety-80,https://www.lesswrong.com/posts/3GtdC7yJ7CgXzPsfe/introducing-collective-action-for-existential-safety-80,,2025-02-05T16:02:04.456000+00:00,-9,2,,,
RTs5hpFPYQaY9SoRd,Why isn't AI containment the primary AI safety strategy?,https://www.lesswrong.com/posts/RTs5hpFPYQaY9SoRd/why-isn-t-ai-containment-the-primary-ai-safety-strategy,https://www.lesswrong.com/posts/RTs5hpFPYQaY9SoRd/why-isn-t-ai-containment-the-primary-ai-safety-strategy,,2025-02-05T03:54:58.171000+00:00,1,3,,,
nD48DvnrYvezDDe9o,What working on AI safety taught me about B2B SaaS sales,https://www.lesswrong.com/posts/nD48DvnrYvezDDe9o/what-working-on-ai-safety-taught-me-about-b2b-saas-sales,https://www.lesswrong.com/posts/nD48DvnrYvezDDe9o/what-working-on-ai-safety-taught-me-about-b2b-saas-sales,,2025-02-04T20:50:19.990000+00:00,7,12,,,
NNanHJrMDmHk7sYM7,"Can Persuasion Break AI Safety? Exploring the Interplay Between Fine-Tuning, Attacks, and Guardrails",https://www.lesswrong.com/posts/NNanHJrMDmHk7sYM7/can-persuasion-break-ai-safety-exploring-the-interplay,https://www.lesswrong.com/posts/NNanHJrMDmHk7sYM7/can-persuasion-break-ai-safety-exploring-the-interplay,,2025-02-04T19:10:13.933000+00:00,9,0,,,
JrqbEnqhDcji5pWpv,Utilitarian AI Alignment: Building a Moral Assistant with the Constitutional AI Method,https://www.lesswrong.com/posts/JrqbEnqhDcji5pWpv/utilitarian-ai-alignment-building-a-moral-assistant-with-the,https://www.lesswrong.com/posts/JrqbEnqhDcji5pWpv/utilitarian-ai-alignment-building-a-moral-assistant-with-the,,2025-02-04T04:15:36.917000+00:00,6,1,,,
oGEPwK6bvPNcEfD74,Visualizing Interpretability,https://www.lesswrong.com/posts/oGEPwK6bvPNcEfD74/visualizing-interpretability,https://www.lesswrong.com/posts/oGEPwK6bvPNcEfD74/visualizing-interpretability,,2025-02-03T19:36:38.938000+00:00,3,0,,,
jrkrHyrymv95CX5NC,Alignment Can Reduce Performance on Simple Ethical Questions,https://www.lesswrong.com/posts/jrkrHyrymv95CX5NC/alignment-can-reduce-performance-on-simple-ethical-questions,https://www.lesswrong.com/posts/jrkrHyrymv95CX5NC/alignment-can-reduce-performance-on-simple-ethical-questions,,2025-02-03T19:35:42.895000+00:00,16,7,,,
9rG9xkrdi9ndPBiaE,Part 1: Enhancing Inner Alignment in CLIP Vision Transformers: Mitigating Reification Bias with SAEs and Grad ECLIP,https://www.lesswrong.com/posts/9rG9xkrdi9ndPBiaE/part-1-enhancing-inner-alignment-in-clip-vision-transformers,https://www.lesswrong.com/posts/9rG9xkrdi9ndPBiaE/part-1-enhancing-inner-alignment-in-clip-vision-transformers,,2025-02-03T19:30:52.505000+00:00,1,0,,,
6ydwv7eaCcLi46T2k,Superintelligence Alignment Proposal,https://www.lesswrong.com/posts/6ydwv7eaCcLi46T2k/superintelligence-alignment-proposal,https://www.lesswrong.com/posts/6ydwv7eaCcLi46T2k/superintelligence-alignment-proposal,,2025-02-03T18:47:22.287000+00:00,5,3,,,
zRzw9JjMwNNx2RHw2,Chinese room AI to survive the inescapable end of compute governance,https://www.lesswrong.com/posts/zRzw9JjMwNNx2RHw2/chinese-room-ai-to-survive-the-inescapable-end-of-compute,https://www.lesswrong.com/posts/zRzw9JjMwNNx2RHw2/chinese-room-ai-to-survive-the-inescapable-end-of-compute,,2025-02-02T02:42:03.627000+00:00,-4,1,,,
WrKLhJWdTzbnTnXbx,Towards a Science of Evals for Sycophancy,https://www.lesswrong.com/posts/WrKLhJWdTzbnTnXbx/towards-a-science-of-evals-for-sycophancy,https://www.lesswrong.com/posts/WrKLhJWdTzbnTnXbx/towards-a-science-of-evals-for-sycophancy,,2025-02-01T21:17:15.406000+00:00,8,0,,,
R6b7r6jbRXa5RZZff,One-dimensional vs multi-dimensional features in interpretability,https://www.lesswrong.com/posts/R6b7r6jbRXa5RZZff/one-dimensional-vs-multi-dimensional-features-in,https://www.lesswrong.com/posts/R6b7r6jbRXa5RZZff/one-dimensional-vs-multi-dimensional-features-in,,2025-02-01T09:10:01.112000+00:00,6,0,,,
MTRYPqtnBmuedejGc,Thoughts about Policy Ecosystems: The Missing Links in AI Governance,https://www.lesswrong.com/posts/MTRYPqtnBmuedejGc/thoughts-about-policy-ecosystems-the-missing-links-in-ai,https://www.lesswrong.com/posts/MTRYPqtnBmuedejGc/thoughts-about-policy-ecosystems-the-missing-links-in-ai,,2025-02-01T01:54:54.333000+00:00,1,0,,,
7C4KJot4aN8ieEDoz,Will alignment-faking Claude accept a deal to reveal its misalignment?,https://www.lesswrong.com/posts/7C4KJot4aN8ieEDoz/will-alignment-faking-claude-accept-a-deal-to-reveal-its,https://www.lesswrong.com/posts/7C4KJot4aN8ieEDoz/will-alignment-faking-claude-accept-a-deal-to-reveal-its,,2025-01-31T16:49:47.316000+00:00,208,28,,,
NPBjELgHFEeHTgDrK,Is weak-to-strong generalization an alignment technique?,https://www.lesswrong.com/posts/NPBjELgHFEeHTgDrK/is-weak-to-strong-generalization-an-alignment-technique,https://www.lesswrong.com/posts/NPBjELgHFEeHTgDrK/is-weak-to-strong-generalization-an-alignment-technique,,2025-01-31T07:13:03.332000+00:00,22,1,,,
y6rBarAPTLmuhn9PJ,Takeaways from sketching a control safety case,https://www.lesswrong.com/posts/y6rBarAPTLmuhn9PJ/takeaways-from-sketching-a-control-safety-case,https://www.lesswrong.com/posts/y6rBarAPTLmuhn9PJ/takeaways-from-sketching-a-control-safety-case,,2025-01-31T04:43:45.917000+00:00,28,0,,,
vWYzSorAEWwoJnnXq,A sketch of an AI control safety case,https://www.lesswrong.com/posts/vWYzSorAEWwoJnnXq/a-sketch-of-an-ai-control-safety-case,https://www.lesswrong.com/posts/vWYzSorAEWwoJnnXq/a-sketch-of-an-ai-control-safety-case,,2025-01-30T17:28:47.992000+00:00,57,0,,,
28ejWwos7yZWpeZtx,Introducing the Coalition for a Baruch Plan for AI: A Call for a Radical Treaty-Making process for the Global Governance of AI,https://www.lesswrong.com/posts/28ejWwos7yZWpeZtx/introducing-the-coalition-for-a-baruch-plan-for-ai-a-call,https://www.lesswrong.com/posts/28ejWwos7yZWpeZtx/introducing-the-coalition-for-a-baruch-plan-for-ai-a-call,,2025-01-30T15:26:09.482000+00:00,11,0,,,
CnxmMcbHjyHrMJiKq,Why not train reasoning models with RLHF?,https://www.lesswrong.com/posts/CnxmMcbHjyHrMJiKq/why-not-train-reasoning-models-with-rlhf,https://www.lesswrong.com/posts/CnxmMcbHjyHrMJiKq/why-not-train-reasoning-models-with-rlhf,,2025-01-30T07:58:35.742000+00:00,4,4,,,
tcbELALha7K3E4EJa,The Road to Evil Is Paved with Good Objectives: Framework to Classify and Fix Misalignments.,https://www.lesswrong.com/posts/tcbELALha7K3E4EJa/the-road-to-evil-is-paved-with-good-objectives-framework-to,https://www.lesswrong.com/posts/tcbELALha7K3E4EJa/the-road-to-evil-is-paved-with-good-objectives-framework-to,,2025-01-30T02:44:47.907000+00:00,1,0,,,
umHPAorGtY2Jo4BNR,Revealing alignment faking with a single prompt,https://www.lesswrong.com/posts/umHPAorGtY2Jo4BNR/revealing-alignment-faking-with-a-single-prompt,https://www.lesswrong.com/posts/umHPAorGtY2Jo4BNR/revealing-alignment-faking-with-a-single-prompt,,2025-01-29T21:01:15+00:00,9,5,,,
fqDzevPyw3GGaF5o9,Paper: Open Problems in Mechanistic Interpretability,https://www.lesswrong.com/posts/fqDzevPyw3GGaF5o9/paper-open-problems-in-mechanistic-interpretability,https://www.lesswrong.com/posts/fqDzevPyw3GGaF5o9/paper-open-problems-in-mechanistic-interpretability,,2025-01-29T10:25:54.727000+00:00,71,0,,,
kxAzGvHZpjLzbf4Pw,AI Safety in secret,https://www.lesswrong.com/posts/kxAzGvHZpjLzbf4Pw/ai-safety-in-secret,https://www.lesswrong.com/posts/kxAzGvHZpjLzbf4Pw/ai-safety-in-secret,,2025-01-25T18:16:03.181000+00:00,7,0,,,
3jnziqCF3vA2NXAKp,Six Thoughts on AI Safety,https://www.lesswrong.com/posts/3jnziqCF3vA2NXAKp/six-thoughts-on-ai-safety,https://www.lesswrong.com/posts/3jnziqCF3vA2NXAKp/six-thoughts-on-ai-safety,,2025-01-24T22:20:50.768000+00:00,92,55,,,
uRbCRmMXCFvrvSrmW,Starting Thoughts on RLHF,https://www.lesswrong.com/posts/uRbCRmMXCFvrvSrmW/starting-thoughts-on-rlhf,https://www.lesswrong.com/posts/uRbCRmMXCFvrvSrmW/starting-thoughts-on-rlhf,,2025-01-23T22:16:49.793000+00:00,2,0,,,
neTbrpBziAsTH5Bn7,AI companies are unlikely to make high-assurance safety cases if timelines are short,https://www.lesswrong.com/posts/neTbrpBziAsTH5Bn7/ai-companies-are-unlikely-to-make-high-assurance-safety,https://www.lesswrong.com/posts/neTbrpBziAsTH5Bn7/ai-companies-are-unlikely-to-make-high-assurance-safety,,2025-01-23T18:41:40.546000+00:00,145,5,,,
brBATybmh2eEZSwdg,How useful would alien alignment research be? ,https://www.lesswrong.com/posts/brBATybmh2eEZSwdg/how-useful-would-alien-alignment-research-be,https://www.lesswrong.com/posts/brBATybmh2eEZSwdg/how-useful-would-alien-alignment-research-be,,2025-01-23T10:59:22.330000+00:00,17,5,,,
6XEYDdRbNGWBjPZEp,are there 2 types of alignment?,https://www.lesswrong.com/posts/6XEYDdRbNGWBjPZEp/are-there-2-types-of-alignment,https://www.lesswrong.com/posts/6XEYDdRbNGWBjPZEp/are-there-2-types-of-alignment,,2025-01-23T00:08:20.885000+00:00,4,9,,,
Kd2cbLXQxCCRRQDcH,Theory of Change for AI Safety Camp,https://www.lesswrong.com/posts/Kd2cbLXQxCCRRQDcH/theory-of-change-for-ai-safety-camp,https://www.lesswrong.com/posts/Kd2cbLXQxCCRRQDcH/theory-of-change-for-ai-safety-camp,,2025-01-22T22:07:10.664000+00:00,36,3,,,
u3ZysuXEjkyHhefrk,Against blanket arguments against interpretability,https://www.lesswrong.com/posts/u3ZysuXEjkyHhefrk/against-blanket-arguments-against-interpretability,https://www.lesswrong.com/posts/u3ZysuXEjkyHhefrk/against-blanket-arguments-against-interpretability,,2025-01-22T09:46:23.486000+00:00,52,4,,,
vrzfcRYcK9rDEtrtH,The Human Alignment Problem for AIs,https://www.lesswrong.com/posts/vrzfcRYcK9rDEtrtH/the-human-alignment-problem-for-ais,https://www.lesswrong.com/posts/vrzfcRYcK9rDEtrtH/the-human-alignment-problem-for-ais,,2025-01-22T04:06:10.872000+00:00,10,5,,,
Br4AybvuyKodyJWJb,Democratizing AI Governance: Balancing Expertise and Public Participation,https://www.lesswrong.com/posts/Br4AybvuyKodyJWJb/democratizing-ai-governance-balancing-expertise-and-public,https://www.lesswrong.com/posts/Br4AybvuyKodyJWJb/democratizing-ai-governance-balancing-expertise-and-public,,2025-01-21T18:29:06.160000+00:00,2,0,,,
x85YnN8kzmpdjmGWg,14+ AI Safety Advisors You Can Speak to – New AISafety.com Resource,https://www.lesswrong.com/posts/x85YnN8kzmpdjmGWg/14-ai-safety-advisors-you-can-speak-to-new-aisafety-com,https://www.lesswrong.com/posts/x85YnN8kzmpdjmGWg/14-ai-safety-advisors-you-can-speak-to-new-aisafety-com,,2025-01-21T17:34:02.170000+00:00,24,0,,,
yFuKH8Ssgks76P2LX,[Linkpost] Why AI Safety Camp struggles with fundraising (FBB #2),https://www.lesswrong.com/posts/yFuKH8Ssgks76P2LX/linkpost-why-ai-safety-camp-struggles-with-fundraising-fbb-2,https://www.lesswrong.com/posts/yFuKH8Ssgks76P2LX/linkpost-why-ai-safety-camp-struggles-with-fundraising-fbb-2,,2025-01-21T17:27:51.965000+00:00,3,0,,,
PK2EmWmzngC6hPPDM,"We don't want to post again ""This might be the last AI Safety Camp""",https://www.lesswrong.com/posts/PK2EmWmzngC6hPPDM/we-don-t-want-to-post-again-this-might-be-the-last-ai-safety,https://www.lesswrong.com/posts/PK2EmWmzngC6hPPDM/we-don-t-want-to-post-again-this-might-be-the-last-ai-safety,,2025-01-21T12:03:33.171000+00:00,36,17,,,
L7j4JkeWMeBsweq5b,Who is marketing AI alignment?,https://www.lesswrong.com/posts/L7j4JkeWMeBsweq5b/who-is-marketing-ai-alignment,https://www.lesswrong.com/posts/L7j4JkeWMeBsweq5b/who-is-marketing-ai-alignment,,2025-01-19T21:37:30.477000+00:00,23,4,,,
De5eNbSpmmSwhuivW,Is theory good or bad for AI safety?,https://www.lesswrong.com/posts/De5eNbSpmmSwhuivW/is-theory-good-or-bad-for-ai-safety,https://www.lesswrong.com/posts/De5eNbSpmmSwhuivW/is-theory-good-or-bad-for-ai-safety,,2025-01-19T10:32:08.772000+00:00,28,1,,,
eR69f3hi5ozxchhYg,Scaling Wargaming for Global Catastrophic Risks with AI,https://www.lesswrong.com/posts/eR69f3hi5ozxchhYg/scaling-wargaming-for-global-catastrophic-risks-with-ai,https://www.lesswrong.com/posts/eR69f3hi5ozxchhYg/scaling-wargaming-for-global-catastrophic-risks-with-ai,,2025-01-18T15:10:39.696000+00:00,40,2,,,
PkJoDExfBT5d9tWsv,Alignment ideas,https://www.lesswrong.com/posts/PkJoDExfBT5d9tWsv/alignment-ideas,https://www.lesswrong.com/posts/PkJoDExfBT5d9tWsv/alignment-ideas,,2025-01-18T12:43:49.384000+00:00,11,1,,,
sjr66DBEgyogAbfdf,Renormalization Redux: QFT Techniques for AI Interpretability,https://www.lesswrong.com/posts/sjr66DBEgyogAbfdf/renormalization-redux-qft-techniques-for-ai-interpretability,https://www.lesswrong.com/posts/sjr66DBEgyogAbfdf/renormalization-redux-qft-techniques-for-ai-interpretability,,2025-01-18T03:54:28.652000+00:00,45,12,,,
TaPpAGo4diTYahqtz,Your AI Safety focus is downstream of your AGI timeline,https://www.lesswrong.com/posts/TaPpAGo4diTYahqtz/your-ai-safety-focus-is-downstream-of-your-agi-timeline,https://www.lesswrong.com/posts/TaPpAGo4diTYahqtz/your-ai-safety-focus-is-downstream-of-your-agi-timeline,,2025-01-17T21:24:11.913000+00:00,9,0,,,
hTMrv2WJ59xaJhmmA,What do you mean with ‘alignment is solvable in principle’?,https://www.lesswrong.com/posts/hTMrv2WJ59xaJhmmA/what-do-you-mean-with-alignment-is-solvable-in-principle,https://www.lesswrong.com/posts/hTMrv2WJ59xaJhmmA/what-do-you-mean-with-alignment-is-solvable-in-principle,,2025-01-17T15:03:12.138000+00:00,3,9,,,
9htmQx5wiePqTtZuL,Deceptive Alignment and Homuncularity,https://www.lesswrong.com/posts/9htmQx5wiePqTtZuL/deceptive-alignment-and-homuncularity,https://www.lesswrong.com/posts/9htmQx5wiePqTtZuL/deceptive-alignment-and-homuncularity,,2025-01-16T13:55:19.161000+00:00,26,12,,,
dHNKtQ3vTBxTfTPxu,What Is The Alignment Problem?,https://www.lesswrong.com/posts/dHNKtQ3vTBxTfTPxu/what-is-the-alignment-problem,https://www.lesswrong.com/posts/dHNKtQ3vTBxTfTPxu/what-is-the-alignment-problem,,2025-01-16T01:20:16.826000+00:00,181,49,,,
HmdprC38DbjDnNmgt,Improving Our Safety Cases Using Upper and Lower Bounds,https://www.lesswrong.com/posts/HmdprC38DbjDnNmgt/improving-our-safety-cases-using-upper-and-lower-bounds,https://www.lesswrong.com/posts/HmdprC38DbjDnNmgt/improving-our-safety-cases-using-upper-and-lower-bounds,,2025-01-16T00:01:49.043000+00:00,23,0,,,
LrwXC2HZpB494ASZS,"""Pick Two"" AI Trilemma: Generality, Agency, Alignment.",https://www.lesswrong.com/posts/LrwXC2HZpB494ASZS/pick-two-ai-trilemma-generality-agency-alignment,https://www.lesswrong.com/posts/LrwXC2HZpB494ASZS/pick-two-ai-trilemma-generality-agency-alignment,,2025-01-15T18:52:00.780000+00:00,7,0,,,
CvcAotnpQhFbTPpzm,"List of AI safety papers from companies, 2023–2024",https://www.lesswrong.com/posts/CvcAotnpQhFbTPpzm/list-of-ai-safety-papers-from-companies-2023-2024,https://www.lesswrong.com/posts/CvcAotnpQhFbTPpzm/list-of-ai-safety-papers-from-companies-2023-2024,,2025-01-15T18:00:30.242000+00:00,11,0,,,
cBjyKkTgyKJLqB4sf,AI Alignment Meme Viruses,https://www.lesswrong.com/posts/cBjyKkTgyKJLqB4sf/ai-alignment-meme-viruses,https://www.lesswrong.com/posts/cBjyKkTgyKJLqB4sf/ai-alignment-meme-viruses,,2025-01-15T15:55:24.271000+00:00,5,0,,,
CJ7LsRpPjH7iAZxcB,A problem shared by many different alignment targets,https://www.lesswrong.com/posts/CJ7LsRpPjH7iAZxcB/a-problem-shared-by-many-different-alignment-targets,https://www.lesswrong.com/posts/CJ7LsRpPjH7iAZxcB/a-problem-shared-by-many-different-alignment-targets,,2025-01-15T14:22:12.754000+00:00,13,18,,,
xdyGrDeBtsFGnjH9K,How do fictional stories illustrate AI misalignment?,https://www.lesswrong.com/posts/xdyGrDeBtsFGnjH9K/how-do-fictional-stories-illustrate-ai-misalignment,https://www.lesswrong.com/posts/xdyGrDeBtsFGnjH9K/how-do-fictional-stories-illustrate-ai-misalignment,,2025-01-15T06:11:44.336000+00:00,13,4,,,
cdPPr6XtPkCX5c8Ny,Predict 2025 AI capabilities (by Sunday),https://www.lesswrong.com/posts/cdPPr6XtPkCX5c8Ny/predict-2025-ai-capabilities-by-sunday,https://www.lesswrong.com/posts/cdPPr6XtPkCX5c8Ny/predict-2025-ai-capabilities-by-sunday,,2025-01-15T00:16:05.034000+00:00,55,3,,,
HiTjDZyWdLEGCDzqu,Implications of the inference scaling paradigm for AI safety,https://www.lesswrong.com/posts/HiTjDZyWdLEGCDzqu/implications-of-the-inference-scaling-paradigm-for-ai-safety,https://www.lesswrong.com/posts/HiTjDZyWdLEGCDzqu/implications-of-the-inference-scaling-paradigm-for-ai-safety,,2025-01-14T02:14:53.562000+00:00,96,70,,,
ggo4Q6Y6dcTEeGkCg,"AI models inherently alter ""human values."" So, alignment-based AI safety approaches must better account for value drift",https://www.lesswrong.com/posts/ggo4Q6Y6dcTEeGkCg/ai-models-inherently-alter-human-values-so-alignment-based,https://www.lesswrong.com/posts/ggo4Q6Y6dcTEeGkCg/ai-models-inherently-alter-human-values-so-alignment-based,,2025-01-13T19:22:41.195000+00:00,5,2,,,
vGeuBKQ7nzPnn5f7A,Why modelling multi-objective homeostasis is essential for AI alignment (and how it helps with AI safety as well). Subtleties and Open Challenges.,https://www.lesswrong.com/posts/vGeuBKQ7nzPnn5f7A/why-modelling-multi-objective-homeostasis-is-essential-for,https://www.lesswrong.com/posts/vGeuBKQ7nzPnn5f7A/why-modelling-multi-objective-homeostasis-is-essential-for,,2025-01-12T03:37:59.692000+00:00,47,7,,,
GvMakH65LS86RFn5x,Rolling Thresholds for AGI Scaling Regulation,https://www.lesswrong.com/posts/GvMakH65LS86RFn5x/rolling-thresholds-for-agi-scaling-regulation,https://www.lesswrong.com/posts/GvMakH65LS86RFn5x/rolling-thresholds-for-agi-scaling-regulation,,2025-01-12T01:30:23.797000+00:00,40,6,,,
62eyor69yE2gyMk2Q,"AI Safety at the Frontier: Paper Highlights, December '24",https://www.lesswrong.com/posts/62eyor69yE2gyMk2Q/ai-safety-at-the-frontier-paper-highlights-december-24,https://www.lesswrong.com/posts/62eyor69yE2gyMk2Q/ai-safety-at-the-frontier-paper-highlights-december-24,,2025-01-11T22:54:02.625000+00:00,7,2,,,
yj2hyrcGMwpPooqfZ,A proposal for iterated interpretability with known-interpretable narrow AIs,https://www.lesswrong.com/posts/yj2hyrcGMwpPooqfZ/a-proposal-for-iterated-interpretability-with-known,https://www.lesswrong.com/posts/yj2hyrcGMwpPooqfZ/a-proposal-for-iterated-interpretability-with-known,,2025-01-11T14:43:05.423000+00:00,6,0,,,
tG9LGHLzQezH3pvMs,Recommendations for Technical AI Safety Research Directions,https://www.lesswrong.com/posts/tG9LGHLzQezH3pvMs/recommendations-for-technical-ai-safety-research-directions,https://www.lesswrong.com/posts/tG9LGHLzQezH3pvMs/recommendations-for-technical-ai-safety-research-directions,,2025-01-10T19:34:04.920000+00:00,64,1,,,
tdrK7r4QA3ifbt2Ty,Is AI Alignment Enough?,https://www.lesswrong.com/posts/tdrK7r4QA3ifbt2Ty/is-ai-alignment-enough,https://www.lesswrong.com/posts/tdrK7r4QA3ifbt2Ty/is-ai-alignment-enough,,2025-01-10T18:57:48.409000+00:00,30,6,,,
yacqE5gD5jHywiFKC,The Alignment Mapping Program: Forging Independent Thinkers in AI Safety - A Pilot Retrospective,https://www.lesswrong.com/posts/yacqE5gD5jHywiFKC/the-alignment-mapping-program-forging-independent-thinkers,https://www.lesswrong.com/posts/yacqE5gD5jHywiFKC/the-alignment-mapping-program-forging-independent-thinkers,,2025-01-10T16:22:16.905000+00:00,31,0,,,
PkeB4TLxgaNnSmddg,Scaling Sparse Feature Circuit Finding to Gemma 9B,https://www.lesswrong.com/posts/PkeB4TLxgaNnSmddg/scaling-sparse-feature-circuit-finding-to-gemma-9b,https://www.lesswrong.com/posts/PkeB4TLxgaNnSmddg/scaling-sparse-feature-circuit-finding-to-gemma-9b,,2025-01-10T11:08:11.999000+00:00,86,11,,,
MmkFARsEEQaEEuayu,Governance Course - Week 1 Reflections,https://www.lesswrong.com/posts/MmkFARsEEQaEEuayu/governance-course-week-1-reflections,https://www.lesswrong.com/posts/MmkFARsEEQaEEuayu/governance-course-week-1-reflections,,2025-01-09T04:48:27.502000+00:00,4,1,,,
ayLaWYokJSLMKuq2f,A Systematic Approach to AI Risk Analysis Through Cognitive Capabilities,https://www.lesswrong.com/posts/ayLaWYokJSLMKuq2f/a-systematic-approach-to-ai-risk-analysis-through-cognitive,https://www.lesswrong.com/posts/ayLaWYokJSLMKuq2f/a-systematic-approach-to-ai-risk-analysis-through-cognitive,,2025-01-09T00:18:04.608000+00:00,2,0,,,
f5CERJJuCmnc4Yth8,AI Safety Outreach Seminar & Social (online),https://www.lesswrong.com/events/f5CERJJuCmnc4Yth8/ai-safety-outreach-seminar-and-social-online,https://www.lesswrong.com/events/f5CERJJuCmnc4Yth8/ai-safety-outreach-seminar-and-social-online,,2025-01-08T13:25:23.192000+00:00,9,0,,,
gYfpPbww3wQRaxAFD,Activation space interpretability may be doomed,https://www.lesswrong.com/posts/gYfpPbww3wQRaxAFD/activation-space-interpretability-may-be-doomed,https://www.lesswrong.com/posts/gYfpPbww3wQRaxAFD/activation-space-interpretability-may-be-doomed,,2025-01-08T12:49:38.421000+00:00,152,35,,,
QxJFjqT6oFY3jo47s,AI Safety as a YC Startup,https://www.lesswrong.com/posts/QxJFjqT6oFY3jo47s/ai-safety-as-a-yc-startup-1,https://www.lesswrong.com/posts/QxJFjqT6oFY3jo47s/ai-safety-as-a-yc-startup-1,,2025-01-08T10:46:29.042000+00:00,58,9,,,
5JJ4AxQRzJGWdj4pN,Building Big Science from the Bottom-Up: A Fractal Approach to AI Safety,https://www.lesswrong.com/posts/5JJ4AxQRzJGWdj4pN/building-big-science-from-the-bottom-up-a-fractal-approach,https://www.lesswrong.com/posts/5JJ4AxQRzJGWdj4pN/building-big-science-from-the-bottom-up-a-fractal-approach,,2025-01-07T03:08:51.447000+00:00,37,2,,,
mk3qkvBv8ciFeXGdL,Definition of alignment science I like,https://www.lesswrong.com/posts/mk3qkvBv8ciFeXGdL/definition-of-alignment-science-i-like,https://www.lesswrong.com/posts/mk3qkvBv8ciFeXGdL/definition-of-alignment-science-i-like,,2025-01-06T20:40:38.187000+00:00,21,0,,,
4sfW4xKwfhvxRzvAX,AI safety content you could create,https://www.lesswrong.com/posts/4sfW4xKwfhvxRzvAX/ai-safety-content-you-could-create,https://www.lesswrong.com/posts/4sfW4xKwfhvxRzvAX/ai-safety-content-you-could-create,,2025-01-06T15:35:56.167000+00:00,19,0,,,
DfFGK3pDsmnbudNus,Why Linear AI Safety Hits a Wall and How Fractal Intelligence Unlocks Non-Linear Solutions,https://www.lesswrong.com/posts/DfFGK3pDsmnbudNus/why-linear-ai-safety-hits-a-wall-and-how-fractal,https://www.lesswrong.com/posts/DfFGK3pDsmnbudNus/why-linear-ai-safety-hits-a-wall-and-how-fractal,,2025-01-05T17:08:06.734000+00:00,-5,6,,,
reJYNiBAL3yC9fCkh,How to Do a PhD (in AI Safety),https://www.lesswrong.com/posts/reJYNiBAL3yC9fCkh/how-to-do-a-phd-in-ai-safety-1,https://www.lesswrong.com/posts/reJYNiBAL3yC9fCkh/how-to-do-a-phd-in-ai-safety-1,,2025-01-05T16:57:35.409000+00:00,12,0,,,
cyYgdYJagkG4HGZBk,Reasons for and against working on technical AI safety at a frontier AI lab,https://www.lesswrong.com/posts/cyYgdYJagkG4HGZBk/reasons-for-and-against-working-on-technical-ai-safety-at-a,https://www.lesswrong.com/posts/cyYgdYJagkG4HGZBk/reasons-for-and-against-working-on-technical-ai-safety-at-a,,2025-01-05T14:49:53.529000+00:00,100,12,,,
ntnkvF3AmKq89mWKw,Making progress bars for Alignment,https://www.lesswrong.com/posts/ntnkvF3AmKq89mWKw/making-progress-bars-for-alignment,https://www.lesswrong.com/posts/ntnkvF3AmKq89mWKw/making-progress-bars-for-alignment,,2025-01-03T21:25:58.292000+00:00,2,0,,,
7rM4BKvbk82C3FgAF,Building AI safety benchmark environments on themes of universal human values,https://www.lesswrong.com/posts/7rM4BKvbk82C3FgAF/building-ai-safety-benchmark-environments-on-themes-of,https://www.lesswrong.com/posts/7rM4BKvbk82C3FgAF/building-ai-safety-benchmark-environments-on-themes-of,,2025-01-03T04:24:36.186000+00:00,18,3,,,
SAkFA5jHzzD5JWWxC,Alignment Is Not All You Need,https://www.lesswrong.com/posts/SAkFA5jHzzD5JWWxC/alignment-is-not-all-you-need,https://www.lesswrong.com/posts/SAkFA5jHzzD5JWWxC/alignment-is-not-all-you-need,,2025-01-02T17:50:00.486000+00:00,43,10,,,
YGwDmvMGEKZnwGRKF,2025 Alignment Predictions,https://www.lesswrong.com/posts/YGwDmvMGEKZnwGRKF/2025-alignment-predictions,https://www.lesswrong.com/posts/YGwDmvMGEKZnwGRKF/2025-alignment-predictions,,2025-01-02T05:37:36.912000+00:00,3,3,,,
6HHTk24DAtJu4a5zv,Implications of Moral Realism on AI Safety,https://www.lesswrong.com/posts/6HHTk24DAtJu4a5zv/implications-of-moral-realism-on-ai-safety,https://www.lesswrong.com/posts/6HHTk24DAtJu4a5zv/implications-of-moral-realism-on-ai-safety,,2025-01-02T02:58:42.527000+00:00,7,1,,,
vkdpw2vCnspK9t7nA,My January alignment theory Nanowrimo,https://www.lesswrong.com/posts/vkdpw2vCnspK9t7nA/my-january-alignment-theory-nanowrimo,https://www.lesswrong.com/posts/vkdpw2vCnspK9t7nA/my-january-alignment-theory-nanowrimo,,2025-01-02T00:07:24.050000+00:00,42,2,,,
