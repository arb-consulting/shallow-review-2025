## High-level taxonomy posts and research agendas

Request for Proposals: Technical AI Safety Research
- https://www.openphilanthropy.org/request-for-proposals-technical-ai-safety-research/

Research directions Open Phil wants to fund in technical AI safety
- https://www.alignmentforum.org/posts/26SHhxK2yYQbh7ors/research-directions-open-phil-wants-to-fund-in-technical-ai

UK AISI’s Alignment Team: Research Agenda
- https://www.alignmentforum.org/posts/tbnw7LbNApvxNLAg8/uk-aisi-s-alignment-team-research-agenda

The Alignment Project Research Agenda (AISI)
- https://www.alignmentforum.org/s/wvLzDiWQWBC9b5HGa

Agent Foundations 2025 at CMU
- https://www.agentfoundations2025atcmu.org/workshop-papers

What’s the short timeline plan?
- https://www.alignmentforum.org/posts/bb5Tnjdrptu89rcyY/what-s-the-short-timeline-plan

AGI Safety & Alignment @ Google DeepMind is hiring
- https://www.alignmentforum.org/posts/wqz5CRzqWkvzoatBG/agi-safety-and-alignment-google-deepmind-is-hiring

MATS 8.0 Research Projects - Summer 2025
- https://substack.com/home/post/p-171758976

# New advancements

## Emergent misalignment
Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs
- https://www.alignmentforum.org/posts/ifechgnJRtJdduFGC/emergent-misalignment-narrow-finetuning-can-produce-broadly

Subliminal Learning: LLMs Transmit Behavioral Traits via Hidden Signals in Data
- https://www.alignmentforum.org/posts/cGcwQDKAKbQ68BGuR/subliminal-learning-llms-transmit-behavioral-traits-via

Convergent Linear Representations of Emergent Misalignment
- https://arxiv.org/abs/2506.11618

## Interpretability
Tracing the Thoughts of a Large Language Model
- https://www.alignmentforum.org/posts/zsr4rWRASxwmgXfmq/tracing-the-thoughts-of-a-large-language-model

Attribution-based parameter decomposition
- https://www.alignmentforum.org/posts/EPefYWjuHNcNH4C7E/attribution-based-parameter-decomposition

Stochastic Parameter Decomposition (improvement on attribution-based parameter decomposition)
- https://www.alignmentforum.org/posts/yjrpmCmqurDmbMztW/paper-stochastic-parameter-decomposition

Auditing language models for hidden objectives
- https://www.anthropic.com/research/auditing-hidden-objectives
- Use SAEs to find hidden objectives in language models

Paper: Open Problems in Mechanistic Interpretability
- https://www.alignmentforum.org/posts/fqDzevPyw3GGaF5o9/paper-open-problems-in-mechanistic-interpretability

## Evals
METR: Measuring AI Ability to Complete Long Tasks
- https://www.alignmentforum.org/posts/deesrjitvXM4xYGZd/metr-measuring-ai-ability-to-complete-long-tasks

Stress Testing Deliberative Alignment for Anti-Scheming Training
- https://www.alignmentforum.org/posts/JmRfgNYCrYogCq7ny/stress-testing-deliberative-alignment-for-anti-scheming

## CoT monitoring and faithfulness
OpenAI: Detecting misbehavior in frontier reasoning models
- https://www.alignmentforum.org/posts/7wFdXj9oR8M9AiFht/openai-detecting-misbehavior-in-frontier-reasoning-models

Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety
- https://www.alignmentforum.org/posts/7xneDbsgj6yJDJMjK/chain-of-thought-monitorability-a-new-and-fragile

Reasoning models don't always say what they think
- https://www.anthropic.com/research/reasoning-models-dont-say-think

SHADE-Arena: Evaluating sabotage and monitoring in LLM agents
- https://www.anthropic.com/research/shade-arena-sabotage-monitoring

Mitigating Deceptive Alignment via Self-Monitoring
- https://arxiv.org/abs/2505.18807

Deceptive Automated Interpretability: Language Models Coordinating to Fool Oversight Systems
- https://arxiv.org/pdf/2504.07831

Detecting Strategic Deception Using Linear Probes
- https://www.alignmentforum.org/posts/9pGbTz6c78PGwJein/detecting-strategic-deception-using-linear-probes

When Chain of Thought is Necessary, Language Models Struggle to Evade Monitors
- https://arxiv.org/abs/2507.05246

## Representation engineering
Taxonomy, Opportunities, and Challenges of Representation Engineering for Large Language Models
- https://arxiv.org/pdf/2502.19649

## New threat models
Gradual Disempowerment: Systemic Existential Risks from Incremental AI Development
- https://www.alignmentforum.org/posts/pZhEQieM9otKXhxmd/gradual-disempowerment-systemic-existential-risks-from

## RL safety
MONA: Managed Myopia with Approval Feedback
- https://www.alignmentforum.org/posts/zWySWKuXnhMDhgwc3/mona-managed-myopia-with-approval-feedback-2

“Behaviorist” RL reward functions lead to scheming
- https://www.alignmentforum.org/posts/FNJF3SoNiwceAQ69W/behaviorist-rl-reward-functions-lead-to-scheming

Reward button alignment
- https://www.alignmentforum.org/posts/JrTk2pbqp7BFwPAKw/reward-button-alignment

## Alignment Stress Testing
Agentic Misalignment: How LLMs could be insider threats
- https://www.anthropic.com/research/agentic-misalignment

## Strategy
On the Rationality of Deterring ASI
- https://www.alignmentforum.org/posts/XsYQyBgm8eKjd3Sqw/on-the-rationality-of-deterring-asi

## Critiques
Interpretability Will Not Reliably Find Deceptive AI
- https://www.alignmentforum.org/posts/PwnadG4BFjaER3MGf/interpretability-will-not-reliably-find-deceptive-ai

Activation space interpretability may be doomed
- https://www.alignmentforum.org/posts/gYfpPbww3wQRaxAFD/activation-space-interpretability-may-be-doomed

## Position posts
Mech interp is not pre-paradigmatic
- https://www.alignmentforum.org/posts/beREnXhBnzxbJtr8k/mech-interp-is-not-pre-paradigmatic

What Is The Alignment Problem?
- https://www.alignmentforum.org/posts/dHNKtQ3vTBxTfTPxu/what-is-the-alignment-problem
