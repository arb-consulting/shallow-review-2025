First I'll read through all the posts to understand what kinds of categories of posts are being made.

I will probably write down some categories each post could go into and read the WIP draft to understand where each post could fit.

Categories I'm thinking of already:
- Interpretability
- Criticism
- Agent foundations
- RL safety

Basically my goal right now is to go through the list of posts and identify posts that seem like an advancement in the field of AI alignment.

AI prompt:

You should go through the list of posts and identify posts that seem like an advancement in the field of AI alignment.

Here are categories of posts you should keep:
- Posts that have unusually high karma
- Posts that are high-quality critiques of a specific research agenda
- Literature reviews
- Posts that link to a paper or introduce an advancement in the field of AI alignment
- High quality position posts

Out of approximately 400 posts, you should aim to keep the 40 post interesting posts.

# Posts to keep (aim for 40)

## High-level taxonomy posts and research agendas

Request for Proposals: Technical AI Safety Research
- https://www.openphilanthropy.org/request-for-proposals-technical-ai-safety-research/

Research directions Open Phil wants to fund in technical AI safety
- https://www.alignmentforum.org/posts/26SHhxK2yYQbh7ors/research-directions-open-phil-wants-to-fund-in-technical-ai

UK AISI’s Alignment Team: Research Agenda
- https://www.alignmentforum.org/posts/tbnw7LbNApvxNLAg8/uk-aisi-s-alignment-team-research-agenda

The Alignment Project Research Agenda
- https://www.alignmentforum.org/s/wvLzDiWQWBC9b5HGa

Agent Foundations 2025 at CMU
- https://www.agentfoundations2025atcmu.org/workshop-papers

What’s the short timeline plan?
- https://www.alignmentforum.org/posts/bb5Tnjdrptu89rcyY/what-s-the-short-timeline-plan

AGI Safety & Alignment @ Google DeepMind is hiring
- https://www.alignmentforum.org/posts/wqz5CRzqWkvzoatBG/agi-safety-and-alignment-google-deepmind-is-hiring

Half a year ago, we published an overview of our recent research. This should give you a decent sense of the type of work we plan to do in the future as well. The biggest change relative to that post is that we’re planning to work a lot on monitoring, particularly chain-of-thought monitoring, which we think of as a near-term example of AI control.

An Approach to Technical AGI Safety and Security (Google DeepMind)
- https://arxiv.org/pdf/2504.01849

MATS 8.0 Research Projects - Summer 2025
- https://substack.com/home/post/p-171758976

The Singapore Consensus onGlobal AI Safety Research Priorities
- https://arxiv.org/pdf/2506.20702

A comprehensive survey in llm (-agent) full stack safety: Data, training and deployment
- https://arxiv.org/abs/2504.15585

# New advancements

## Emergent misalignment
Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs
- https://www.alignmentforum.org/posts/ifechgnJRtJdduFGC/emergent-misalignment-narrow-finetuning-can-produce-broadly

Subliminal Learning: LLMs Transmit Behavioral Traits via Hidden Signals in Data
- https://www.alignmentforum.org/posts/cGcwQDKAKbQ68BGuR/subliminal-learning-llms-transmit-behavioral-traits-via

Convergent Linear Representations of Emergent Misalignment
- https://arxiv.org/abs/2506.11618

## Interpretability
Tracing the Thoughts of a Large Language Model
- https://www.alignmentforum.org/posts/zsr4rWRASxwmgXfmq/tracing-the-thoughts-of-a-large-language-model

Attribution-based parameter decomposition
- https://www.alignmentforum.org/posts/EPefYWjuHNcNH4C7E/attribution-based-parameter-decomposition

Stochastic Parameter Decomposition (improvement on attribution-based parameter decomposition)
- https://www.alignmentforum.org/posts/yjrpmCmqurDmbMztW/paper-stochastic-parameter-decomposition

Auditing language models for hidden objectives
- https://www.anthropic.com/research/auditing-hidden-objectives
- Use SAEs to find hidden objectives in language models

Paper: Open Problems in Mechanistic Interpretability
- https://www.alignmentforum.org/posts/fqDzevPyw3GGaF5o9/paper-open-problems-in-mechanistic-interpretability

Dense SAE Latents Are Features, Not Bugs
- https://arxiv.org/abs/2506.15679

## Evals
METR: Measuring AI Ability to Complete Long Tasks
- https://www.alignmentforum.org/posts/deesrjitvXM4xYGZd/metr-measuring-ai-ability-to-complete-long-tasks

RepliBench: Evaluating the Autonomous Replication Capabilities of Language Model Agents
- https://arxiv.org/pdf/2504.18565

Probing and Steering Evaluation Awareness of Language Models
- https://arxiv.org/pdf/2507.01786

Large language model-powered AI systems achieve self-replication with no human intervention
- https://arxiv.org/pdf/2503.17378

Technical Report: Evaluating Goal Drift in Language Model Agents
- https://arxiv.org/pdf/2505.02709

Large Language Models Often Know When They Are Being Evaluated
- https://arxiv.org/pdf/2505.23836

## CoT monitoring and faithfulness
OpenAI: Detecting misbehavior in frontier reasoning models
- https://www.alignmentforum.org/posts/7wFdXj9oR8M9AiFht/openai-detecting-misbehavior-in-frontier-reasoning-models

Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety
- https://www.alignmentforum.org/posts/7xneDbsgj6yJDJMjK/chain-of-thought-monitorability-a-new-and-fragile

Reasoning models don't always say what they think
- https://www.anthropic.com/research/reasoning-models-dont-say-think

SHADE-Arena: Evaluating sabotage and monitoring in LLM agents
- https://www.anthropic.com/research/shade-arena-sabotage-monitoring

Mitigating Deceptive Alignment via Self-Monitoring
- https://arxiv.org/abs/2505.18807

Deceptive Automated Interpretability: Language Models Coordinating to Fool Oversight Systems
- https://arxiv.org/pdf/2504.07831

RL-Obfuscation: Can Language Models Learn to Evade Latent-Space Monitors?
- https://arxiv.org/pdf/2506.14261

Detecting Strategic Deception Using Linear Probes
- https://arxiv.org/pdf/2502.03407

Mitigating Deceptive Alignment via Self-Monitoring
- https://arxiv.org/pdf/2505.18807

When Chain of Thought is Necessary, Language Models Struggle to Evade Monitors
- https://arxiv.org/abs/2507.05246

## Representation engineering
Taxonomy, Opportunities, and Challenges of Representation Engineering for Large Language Models
- https://arxiv.org/pdf/2502.19649

## Debate
Avoiding Obfuscation with Prover-Estimator Debate

## New threat models
Gradual Disempowerment: Systemic Existential Risks from Incremental AI Development
- https://www.alignmentforum.org/posts/pZhEQieM9otKXhxmd/gradual-disempowerment-systemic-existential-risks-from

## RL safety
MONA: Managed Myopia with Approval Feedback
- https://www.alignmentforum.org/posts/zWySWKuXnhMDhgwc3/mona-managed-myopia-with-approval-feedback-2

“Behaviorist” RL reward functions lead to scheming
- https://www.alignmentforum.org/posts/FNJF3SoNiwceAQ69W/behaviorist-rl-reward-functions-lead-to-scheming

Reward button alignment
- https://www.alignmentforum.org/posts/JrTk2pbqp7BFwPAKw/reward-button-alignment

SafeWork-R1: Coevolving Safety and Intelligence under the AI-45◦ Law
- https://arxiv.org/pdf/2507.18576

Mitigating Goal Misgeneralization via Minimax Regret
- https://arxiv.org/pdf/2507.03068

AssistanceZero: Scalably Solving Assistance Games
- https://arxiv.org/abs/2504.07091


## Alignment Stress Testing
Agentic Misalignment: How LLMs could be insider threats
- https://www.anthropic.com/research/agentic-misalignment

## RLHF
Generative RLHF-V: Learning Principles from Multi-modal Human Preference
-https://arxiv.org/abs/2505.18531

Align Anything: Training All-Modality Models to Follow Instructions with
Language Feedback
- https://arxiv.org/abs/2412.15838

Mitigating Reward Over-Optimization in RLHF via Behavior-Supported Regularization
- https://arxiv.org/abs/2503.18130

Safe RLHF-V: Safe Reinforcement Learning from Multi-modal Human Feedback
https://arxiv.org/abs/2503.18130


## Strategy
On the Rationality of Deterring ASI
- https://www.alignmentforum.org/posts/XsYQyBgm8eKjd3Sqw/on-the-rationality-of-deterring-asi

## Critiques
Interpretability Will Not Reliably Find Deceptive AI
- https://www.alignmentforum.org/posts/PwnadG4BFjaER3MGf/interpretability-will-not-reliably-find-deceptive-ai

Activation space interpretability may be doomed
- https://www.alignmentforum.org/posts/gYfpPbww3wQRaxAFD/activation-space-interpretability-may-be-doomed
- Interpreting neural networks by decomposing layer activations alone is risky: you may recover statistical features of activation distributions that seem meaningful but are not functionally relevant to the model’s computations. To reliably interpret models, you need methods that tie activation structure with how the model actually uses it (weights, gradients, circuit structure).
- When you study a model's activations, you might see geometric or statistical patterns that come from the inputs, not from the model's internal knowledge.
Activation-only analysis shows what patterns exist, but not what the model does with them.
- To solve that, you need methods that bring in: causal testing (ablation, activation patching), mechanistic tracing (following circuits through weights), gradient or training dynamics (what gets optimized), joint analyses that connect activations and weights. These tell you which structures the model is actually using — not just what’s floating around in the activations.

## Position posts
Mech interp is not pre-paradigmatic
- https://www.alignmentforum.org/posts/beREnXhBnzxbJtr8k/mech-interp-is-not-pre-paradigmatic

What Is The Alignment Problem?
- https://www.alignmentforum.org/posts/dHNKtQ3vTBxTfTPxu/what-is-the-alignment-problem

Towards AI-45◦ Law: A Roadmap to Trustworthy AGI
- https://arxiv.org/pdf/2412.14186

## Other
- https://www.alignmentforum.org/posts/TJrCumJxhzTmNBsRz/a-short-course-on-agi-safety-from-the-gdm-alignment-team

- Safety Alignment Can Be Not Superficial With Explicit Safety Signals



# Thoughts from reading the posts