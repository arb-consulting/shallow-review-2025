{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8571b58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the arxiv package if not already installed\n",
    "# !pip install arxiv\n",
    "\n",
    "import arxiv\n",
    "import pandas as pd\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393cc164",
   "metadata": {},
   "source": [
    "Query:\n",
    "\n",
    "Start with \"ai safety OR \"ai alignment\"\n",
    "\n",
    "(\"ai safety\" OR \"ai alignment\" OR \"value alignment\" OR \"inner alignment\" OR \"outer alignment\" \n",
    " OR \"ai control\" OR \"specification gaming\" OR \"reward hacking\" OR deception OR \"scalable oversight\")\n",
    "ANDNOT (\"sequence alignment\" OR \"image alignment\" OR \"camera alignment\" OR \"protein alignment\")\n",
    "\n",
    "(cat:cs.AI OR cat:cs.LG OR cat:cs.CL OR cat:stat.ML)\n",
    "AND (\n",
    "  \"emergent misalignment\"\n",
    "  OR (misalignment AND (backdoor OR backdoored OR persona OR \"alignment faking\"))\n",
    "  OR \"evaluation awareness\" OR \"situational awareness\" OR \"test awareness\"\n",
    "  OR (deception AND (probe OR probes OR monitoring OR \"lie detector\" OR sandbagging OR \"strategic\"))\n",
    "  OR (steganography AND (\"chain of thought\" OR reasoning))\n",
    "  OR (\"chain of thought\" AND (faithful OR faithfulness OR monitor OR monitoring OR \"reasoning model\"))\n",
    "  OR \"mechanistic interpretability\" OR \"sparse autoencoder\" OR SAEBench OR transcoder\n",
    "  OR \"representation engineering\" OR \"steering vector\" OR \"activation steering\"\n",
    "  OR \"control evaluation\" OR \"Control Arena\" OR \"Ctrl-Z\" OR \"D-REX\" OR \"agent control\"\n",
    "  OR (debate AND (\"scalable oversight\" OR \"prover verifier\" OR \"doubly efficient\"))\n",
    "  OR self-replication OR RepliBench\n",
    ")\n",
    "\n",
    "\"ai safety\" OR \"ai alignment\" OR \"superalignment\" OR \"mechanistic interpretability\" OR \"scalable oversight\" OR \"preference alignment\" OR \"human preferences\" OR \"RLHF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "59fdd088",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "import arxiv\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.getLogger(\"arxiv-fetch\")\n",
    "\n",
    "def search_arxiv_large(\n",
    "    query: str,\n",
    "    limit: int = 10_000,                 # total results you want to collect\n",
    "    page_size: int = 500,                 # arxiv.Client page size (<= 2000)\n",
    "    delay_seconds: float = 3.0,           # arXiv ToU: ≥3s between requests\n",
    "    retries: int = 5,                     # per-page retry inside the client\n",
    "    sort_by: arxiv.SortCriterion = arxiv.SortCriterion.SubmittedDate,\n",
    "    sort_order: arxiv.SortOrder = arxiv.SortOrder.Descending,\n",
    "):\n",
    "    \"\"\"\n",
    "    Robust arXiv fetcher that:\n",
    "      - Uses Client.results(search, offset=...) (correct for v2 API)\n",
    "      - Recovers from UnexpectedEmptyPageError / HTTPError by resuming at the current offset\n",
    "      - De-dups by arXiv ID\n",
    "    Returns: list[dict]\n",
    "    \"\"\"\n",
    "    client = arxiv.Client(\n",
    "        page_size=page_size,\n",
    "        delay_seconds=delay_seconds,\n",
    "        num_retries=retries,\n",
    "    )\n",
    "\n",
    "    # max_results=None → \"all available\" (library will paginate internally).\n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=None,\n",
    "        sort_by=sort_by,\n",
    "        sort_order=sort_order,\n",
    "    )\n",
    "\n",
    "    results, seen = [], set()\n",
    "    offset = 0\n",
    "    consecutive_failures = 0\n",
    "    backoff = delay_seconds\n",
    "\n",
    "    def add_result(r):\n",
    "        arxiv_id = r.entry_id.split(\"/\")[-1]\n",
    "        if arxiv_id in seen:\n",
    "            return False\n",
    "        seen.add(arxiv_id)\n",
    "        results.append({\n",
    "            \"title\": r.title,\n",
    "            \"authors\": \", \".join(a.name for a in r.authors),\n",
    "            \"published\": r.published.strftime(\"%Y-%m-%d\"),\n",
    "            \"updated\": r.updated.strftime(\"%Y-%m-%d\"),\n",
    "            \"summary\": (r.summary or \"\").replace(\"\\n\", \" \").strip(),\n",
    "            \"primary_category\": r.primary_category,\n",
    "            \"categories\": \", \".join(r.categories or []),\n",
    "            \"arxiv_id\": arxiv_id,\n",
    "            \"pdf_url\": r.pdf_url,\n",
    "            \"doi\": r.doi or \"N/A\",\n",
    "        })\n",
    "        return True\n",
    "\n",
    "    log.info(f\"Starting fetch (limit={limit}, page_size={page_size})\")\n",
    "\n",
    "    while len(results) < limit:\n",
    "        try:\n",
    "            # Stream from current offset to the end (or until we hit our limit)\n",
    "            for r in client.results(search, offset=offset):\n",
    "                added = add_result(r)\n",
    "                offset += 1  # offset is count of yielded records so far\n",
    "                if added and len(results) % 100 == 0:\n",
    "                    log.info(f\"✓ Collected {len(results)} (offset={offset})\")\n",
    "                if len(results) >= limit:\n",
    "                    break\n",
    "\n",
    "            # If we got here without exceptions, we reached the end cleanly.\n",
    "            log.info(\"Reached end of result set.\")\n",
    "            break\n",
    "\n",
    "        except (arxiv.UnexpectedEmptyPageError, arxiv.HTTPError) as e:\n",
    "            consecutive_failures += 1\n",
    "            log.warning(f\"{type(e).__name__} at offset={offset}. \"\n",
    "                        f\"Retrying after {backoff:.1f}s (failure #{consecutive_failures})…\")\n",
    "            time.sleep(backoff)\n",
    "            # Exponential backoff, capped\n",
    "            backoff = min(backoff * 1.5, 30.0)\n",
    "            # Loop will restart generator from the same offset\n",
    "\n",
    "            # Optional safety: give up after too many consecutive failures\n",
    "            if consecutive_failures >= 8:\n",
    "                log.error(\"Too many consecutive failures; returning partial results.\")\n",
    "                break\n",
    "        else:\n",
    "            # On success, reset failure counters/backoff\n",
    "            consecutive_failures = 0\n",
    "            backoff = delay_seconds\n",
    "\n",
    "    log.info(f\"Done. Collected {len(results)} results.\")\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "93939302",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:arxiv-fetch:Starting fetch (limit=1000, page_size=500)\n",
      "INFO:arxiv:Requesting page (first: True, try: 0): https://export.arxiv.org/api/query?search_query=cat%3Acs.%2A+AND+%28all%3A%22ai+safety%22+OR+all%3A%22ai+alignment%22%29+AND+submittedDate%3A%5B202501010000+TO+202512312359%5D&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=500\n",
      "INFO:arxiv:Got first page: 25 of 368 total results\n",
      "INFO:arxiv:Sleeping: 2.983518 seconds\n",
      "INFO:arxiv:Requesting page (first: False, try: 0): https://export.arxiv.org/api/query?search_query=cat%3Acs.%2A+AND+%28all%3A%22ai+safety%22+OR+all%3A%22ai+alignment%22%29+AND+submittedDate%3A%5B202501010000+TO+202512312359%5D&id_list=&sortBy=submittedDate&sortOrder=descending&start=25&max_results=500\n",
      "INFO:arxiv-fetch:✓ Collected 100 (offset=100)\n",
      "INFO:arxiv-fetch:✓ Collected 200 (offset=200)\n",
      "INFO:arxiv-fetch:✓ Collected 300 (offset=300)\n",
      "INFO:arxiv-fetch:Reached end of result set.\n",
      "INFO:arxiv-fetch:Done. Collected 368 results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 368 papers for query: 'cat:cs.* AND (all:\"ai safety\" OR all:\"ai alignment\") AND submittedDate:[202501010000 TO 202512312359]'\n",
      "\n",
      "1. Beyond Text: Multimodal Jailbreaking of Vision-Language and Audio Models through Perceptually Simple Transformations\n",
      "   Authors: Divyanshu Kumar, Shreyas Jena, Nitin Aravind Birur, Tanay Baswa, Sahil Agarwal, Prashanth Harshangi\n",
      "   Published: 2025-10-23\n",
      "   arXiv ID: 2510.20223v1\n",
      "   URL: http://arxiv.org/pdf/2510.20223v1\n",
      "   Summary: Multimodal large language models (MLLMs) have achieved remarkable progress, yet remain critically vulnerable to adversarial attacks that exploit weaknesses in cross-modal processing. We present a syst...\n",
      "\n",
      "2. LLMs can hide text in other text of the same length.ipynb\n",
      "   Authors: Antonio Norelli, Michael Bronstein\n",
      "   Published: 2025-10-22\n",
      "   arXiv ID: 2510.20075v1\n",
      "   URL: http://arxiv.org/pdf/2510.20075v1\n",
      "   Summary: A meaningful text can be hidden inside another, completely different yet still coherent and plausible, text of the same length. For example, a tweet containing a harsh political critique could be embe...\n",
      "\n",
      "3. Impartial Selection with Predictions\n",
      "   Authors: Javier Cembrano, Felix Fischer, Max Klimm\n",
      "   Published: 2025-10-21\n",
      "   arXiv ID: 2510.19002v1\n",
      "   URL: http://arxiv.org/pdf/2510.19002v1\n",
      "   Summary: We study the selection of agents based on mutual nominations, a theoretical problem with many applications from committee selection to AI alignment. As agents both select and are selected, they may be...\n",
      "\n",
      "4. VLSU: Mapping the Limits of Joint Multimodal Understanding for AI Safety\n",
      "   Authors: Shruti Palaskar, Leon Gatys, Mona Abdelrahman, Mar Jacobo, Larry Lindsey, Rutika Moharir, Gunnar Lund, Yang Xu, Navid Shiee, Jeffrey Bigham, Charles Maalouf, Joseph Yitan Cheng\n",
      "   Published: 2025-10-21\n",
      "   arXiv ID: 2510.18214v1\n",
      "   URL: http://arxiv.org/pdf/2510.18214v1\n",
      "   Summary: Safety evaluation of multimodal foundation models often treats vision and language inputs separately, missing risks from joint interpretation where benign content becomes harmful in combination. Exist...\n",
      "\n",
      "5. Annotating the Chain-of-Thought: A Behavior-Labeled Dataset for AI Safety\n",
      "   Authors: Antonio-Gabriel Chacón Menke, Phan Xuan Tan, Eiji Kamioka\n",
      "   Published: 2025-10-20\n",
      "   arXiv ID: 2510.18154v1\n",
      "   URL: http://arxiv.org/pdf/2510.18154v1\n",
      "   Summary: Recent work has highlighted the importance of monitoring chain-of-thought reasoning for AI safety; however, current approaches that analyze textual reasoning steps can miss subtle harmful patterns and...\n",
      "\n",
      "6. Atomic Literary Styling: Mechanistic Manipulation of Prose Generation in Neural Language Models\n",
      "   Authors: Tsogt-Ochir Enkhbayar\n",
      "   Published: 2025-10-19\n",
      "   arXiv ID: 2510.17909v1\n",
      "   URL: http://arxiv.org/pdf/2510.17909v1\n",
      "   Summary: We present a mechanistic analysis of literary style in GPT-2, identifying individual neurons that discriminate between exemplary prose and rigid AI-generated text. Using Herman Melville's Bartleby, th...\n",
      "\n",
      "7. Position: Require Frontier AI Labs To Release Small \"Analog\" Models\n",
      "   Authors: Shriyash Upadhyay, Chaithanya Bandi, Narmeen Oozeer, Philip Quirke\n",
      "   Published: 2025-10-15\n",
      "   arXiv ID: 2510.14053v1\n",
      "   URL: http://arxiv.org/pdf/2510.14053v1\n",
      "   Summary: Recent proposals for regulating frontier AI models have sparked concerns about the cost of safety regulation, and most such regulations have been shelved due to the safety-innovation tradeoff. This pa...\n",
      "\n",
      "8. From Refusal to Recovery: A Control-Theoretic Approach to Generative AI Guardrails\n",
      "   Authors: Ravi Pandya, Madison Bland, Duy P. Nguyen, Changliu Liu, Jaime Fernández Fisac, Andrea Bajcsy\n",
      "   Published: 2025-10-15\n",
      "   arXiv ID: 2510.13727v1\n",
      "   URL: http://arxiv.org/pdf/2510.13727v1\n",
      "   Summary: Generative AI systems are increasingly assisting and acting on behalf of end users in practical settings, from digital shopping assistants to next-generation autonomous cars. In this context, safety i...\n",
      "\n",
      "9. International AI Safety Report 2025: First Key Update: Capabilities and Risk Implications\n",
      "   Authors: Yoshua Bengio, Stephen Clare, Carina Prunkl, Shalaleh Rismani, Maksym Andriushchenko, Ben Bucknall, Philip Fox, Tiancheng Hu, Cameron Jones, Sam Manning, Nestor Maslej, Vasilios Mavroudis, Conor McGlynn, Malcolm Murray, Charlotte Stix, Lucia Velasco, Nicole Wheeler, Daniel Privitera, Sören Mindermann, Daron Acemoglu, Thomas G. Dietterich, Fredrik Heintz, Geoffrey Hinton, Nick Jennings, Susan Leavy, Teresa Ludermir, Vidushi Marda, Helen Margetts, John McDermid, Jane Munga, Arvind Narayanan, Alondra Nelson, Clara Neppel, Gopal Ramchurn, Stuart Russell, Marietje Schaake, Bernhard Schölkopf, Alavaro Soto, Lee Tiedrich, Gaël Varoquaux, Andrew Yao, Ya-Qin Zhang, Leandro Aguirre, Olubunmi Ajala, Fahad Albalawi Noora AlMalek, Christian Busch, André Carvalho, Jonathan Collas, Amandeep Gill, Ahmet Hatip, Juha Heikkilä, Chris Johnson, Gill Jolly, Ziv Katzir, Mary Kerema, Hiroaki Kitano, Antonio Krüger, Aoife McLysaght, Oleksii Molchanovskyi, Andrea Monti, Kyoung Mu Lee, Mona Nemer, Nuria Oliver, Raquel Pezoa, Audrey Plonk, José Portillo, Balaraman Ravindran, Hammam Riza, Crystal Rugege, Haroon Sheikh, Denise Wong, Yi Zeng, Liming Zhu\n",
      "   Published: 2025-10-15\n",
      "   arXiv ID: 2510.13653v1\n",
      "   URL: http://arxiv.org/pdf/2510.13653v1\n",
      "   Summary: Since the publication of the first International AI Safety Report, AI capabilities have continued to improve across key domains. New training techniques that teach AI systems to reason step-by-step an...\n",
      "\n",
      "10. Who Speaks for the Trigger? Dynamic Expert Routing in Backdoored Mixture-of-Experts Transformers\n",
      "   Authors: Xin Zhao, Xiaojun Chen, Bingshan Liu, Haoyu Gao, Zhendong Zhao, Yilong Chen\n",
      "   Published: 2025-10-15\n",
      "   arXiv ID: 2510.13462v1\n",
      "   URL: http://arxiv.org/pdf/2510.13462v1\n",
      "   Summary: Large language models (LLMs) with Mixture-of-Experts (MoE) architectures achieve impressive performance and efficiency by dynamically routing inputs to specialized subnetworks, known as experts. Howev...\n",
      "\n",
      "11. Narrow Finetuning Leaves Clearly Readable Traces in Activation Differences\n",
      "   Authors: Julian Minder, Clément Dumas, Stewart Slocum, Helena Casademunt, Cameron Holmes, Robert West, Neel Nanda\n",
      "   Published: 2025-10-14\n",
      "   arXiv ID: 2510.13900v1\n",
      "   URL: http://arxiv.org/pdf/2510.13900v1\n",
      "   Summary: Finetuning on narrow domains has become an essential tool to adapt Large Language Models (LLMs) to specific tasks and to create models with known unusual properties that are useful for research. We sh...\n",
      "\n",
      "12. Keep Calm and Avoid Harmful Content: Concept Alignment and Latent Manipulation Towards Safer Answers\n",
      "   Authors: Ruben Belo, Marta Guimaraes, Claudia Soares\n",
      "   Published: 2025-10-14\n",
      "   arXiv ID: 2510.12672v2\n",
      "   URL: http://arxiv.org/pdf/2510.12672v2\n",
      "   Summary: Large Language Models are susceptible to jailbreak attacks that bypass built-in safety guardrails (e.g., by tricking the model with adversarial prompts). We propose Concept Alignment and Concept Manip...\n",
      "\n",
      "13. AI Alignment vs. AI Ethical Treatment: 10 Challenges\n",
      "   Authors: Adam Bradley, Bradford Saad\n",
      "   Published: 2025-10-14\n",
      "   arXiv ID: 2510.12844v1\n",
      "   URL: http://arxiv.org/pdf/2510.12844v1\n",
      "   Summary: A morally acceptable course of AI development should avoid two dangers: creating unaligned AI systems that pose a threat to humanity and mistreating AI systems that merit moral consideration in their ...\n",
      "\n",
      "14. AI Alignment Strategies from a Risk Perspective: Independent Safety Mechanisms or Shared Failures?\n",
      "   Authors: Leonard Dung, Florian Mai\n",
      "   Published: 2025-10-13\n",
      "   arXiv ID: 2510.11235v1\n",
      "   URL: http://arxiv.org/pdf/2510.11235v1\n",
      "   Summary: AI alignment research aims to develop techniques to ensure that AI systems do not cause harm. However, every alignment technique has failure modes, which are conditions in which there is a non-negligi...\n",
      "\n",
      "15. Enabling Responsible, Secure and Sustainable Healthcare AI - A Strategic Framework for Clinical and Operational Impact\n",
      "   Authors: Jimmy Joseph\n",
      "   Published: 2025-10-09\n",
      "   arXiv ID: 2510.15943v1\n",
      "   URL: http://arxiv.org/pdf/2510.15943v1\n",
      "   Summary: We offer a pragmatic model to operationalize responsible, secure, and sustainable healthcare AI, aligning world-class technical excellence with organizational readiness. The framework includes five ke...\n",
      "\n",
      "16. The Unintended Trade-off of AI Alignment:Balancing Hallucination Mitigation and Safety in LLMs\n",
      "   Authors: Omar Mahmoud, Ali Khalil, Buddhika Laknath Semage, Thommen George Karimpanal, Santu Rana\n",
      "   Published: 2025-10-09\n",
      "   arXiv ID: 2510.07775v1\n",
      "   URL: http://arxiv.org/pdf/2510.07775v1\n",
      "   Summary: Hallucination in large language models (LLMs) has been widely studied in recent years, with progress in both detection and mitigation aimed at improving truthfulness. Yet, a critical side effect remai...\n",
      "\n",
      "17. Classical AI vs. LLMs for Decision-Maker Alignment in Health Insurance Choices\n",
      "   Authors: Mallika Mainali, Harsha Sureshbabu, Anik Sen, Christopher B. Rauch, Noah D. Reifsnyder, John Meyer, J. T. Turner, Michael W. Floyd, Matthew Molineaux, Rosina O. Weber\n",
      "   Published: 2025-10-07\n",
      "   arXiv ID: 2510.06093v1\n",
      "   URL: http://arxiv.org/pdf/2510.06093v1\n",
      "   Summary: As algorithmic decision-makers are increasingly applied to high-stakes domains, AI alignment research has evolved from a focus on universal value alignment to context-specific approaches that account ...\n",
      "\n",
      "18. Communication Enables Cooperation in LLM Agents: A Comparison with Curriculum-Based Approaches\n",
      "   Authors: Hachem Madmoun, Salem Lahlou\n",
      "   Published: 2025-10-07\n",
      "   arXiv ID: 2510.05748v1\n",
      "   URL: http://arxiv.org/pdf/2510.05748v1\n",
      "   Summary: Eliciting cooperation in multi-agent LLM systems is critical for AI alignment. We investigate two approaches: direct communication and curriculum learning. In a 4-player Stag Hunt, a one-word \"cheap t...\n",
      "\n",
      "19. The Hidden Game Problem\n",
      "   Authors: Gon Buzaglo, Noah Golowich, Elad Hazan\n",
      "   Published: 2025-10-04\n",
      "   arXiv ID: 2510.03845v1\n",
      "   URL: http://arxiv.org/pdf/2510.03845v1\n",
      "   Summary: This paper investigates a class of games with large strategy spaces, motivated by challenges in AI alignment and language games. We introduce the hidden game problem, where for each player, an unknown...\n",
      "\n",
      "20. Know Thyself? On the Incapability and Implications of AI Self-Recognition\n",
      "   Authors: Xiaoyan Bai, Aryan Shrivastava, Ari Holtzman, Chenhao Tan\n",
      "   Published: 2025-10-03\n",
      "   arXiv ID: 2510.03399v1\n",
      "   URL: http://arxiv.org/pdf/2510.03399v1\n",
      "   Summary: Self-recognition is a crucial metacognitive capability for AI systems, relevant not only for psychological analysis but also for safety, particularly in evaluative scenarios. Motivated by contradictor...\n",
      "\n",
      "21. Tree-based Dialogue Reinforced Policy Optimization for Red-Teaming Attacks\n",
      "   Authors: Ruohao Guo, Afshin Oroojlooy, Roshan Sridhar, Miguel Ballesteros, Alan Ritter, Dan Roth\n",
      "   Published: 2025-10-02\n",
      "   arXiv ID: 2510.02286v1\n",
      "   URL: http://arxiv.org/pdf/2510.02286v1\n",
      "   Summary: Despite recent rapid progress in AI safety, current large language models remain vulnerable to adversarial attacks in multi-turn interaction settings, where attackers strategically adapt their prompts...\n",
      "\n",
      "22. To Mask or to Mirror: Human-AI Alignment in Collective Reasoning\n",
      "   Authors: Crystal Qian, Aaron Parisi, Clémentine Bouleau, Vivian Tsai, Maël Lebreton, Lucas Dixon\n",
      "   Published: 2025-10-02\n",
      "   arXiv ID: 2510.01924v1\n",
      "   URL: http://arxiv.org/pdf/2510.01924v1\n",
      "   Summary: As large language models (LLMs) are increasingly used to model and augment collective decision-making, it is critical to examine their alignment with human social reasoning. We present an empirical fr...\n",
      "\n",
      "23. MedQ-Bench: Evaluating and Exploring Medical Image Quality Assessment Abilities in MLLMs\n",
      "   Authors: Jiyao Liu, Jinjie Wei, Wanying Qu, Chenglong Ma, Junzhi Ning, Yunheng Li, Ying Chen, Xinzhe Luo, Pengcheng Chen, Xin Gao, Ming Hu, Huihui Xu, Xin Wang, Shujian Gao, Dingkang Yang, Zhongying Deng, Jin Ye, Lihao Liu, Junjun He, Ningsheng Xu\n",
      "   Published: 2025-10-02\n",
      "   arXiv ID: 2510.01691v1\n",
      "   URL: http://arxiv.org/pdf/2510.01691v1\n",
      "   Summary: Medical Image Quality Assessment (IQA) serves as the first-mile safety gate for clinical AI, yet existing approaches remain constrained by scalar, score-based metrics and fail to reflect the descripti...\n",
      "\n",
      "24. InvThink: Towards AI Safety via Inverse Reasoning\n",
      "   Authors: Yubin Kim, Taehan Kim, Eugene Park, Chunjong Park, Cynthia Breazeal, Daniel McDuff, Hae Won Park\n",
      "   Published: 2025-10-02\n",
      "   arXiv ID: 2510.01569v1\n",
      "   URL: http://arxiv.org/pdf/2510.01569v1\n",
      "   Summary: We present InvThink, a simple yet powerful approach that gives large language models (LLMs) the capability of inverse thinking: reasoning through failure modes before generating responses. Unlike exis...\n",
      "\n",
      "25. Bypassing Prompt Guards in Production with Controlled-Release Prompting\n",
      "   Authors: Jaiden Fairoze, Sanjam Garg, Keewoo Lee, Mingyuan Wang\n",
      "   Published: 2025-10-02\n",
      "   arXiv ID: 2510.01529v2\n",
      "   URL: http://arxiv.org/pdf/2510.01529v2\n",
      "   Summary: As large language models (LLMs) advance, ensuring AI safety and alignment is paramount. One popular approach is prompt guards, lightweight mechanisms designed to filter malicious queries while being e...\n",
      "\n",
      "26. Safety Instincts: LLMs Learn to Trust Their Internal Compass for Self-Defense\n",
      "   Authors: Guobin Shen, Dongcheng Zhao, Haibo Tong, Jindong Li, Feifei Zhao, Yi Zeng\n",
      "   Published: 2025-10-01\n",
      "   arXiv ID: 2510.01088v1\n",
      "   URL: http://arxiv.org/pdf/2510.01088v1\n",
      "   Summary: Ensuring Large Language Model (LLM) safety remains challenging due to the absence of universal standards and reliable content validators, making it difficult to obtain effective training signals. We d...\n",
      "\n",
      "27. Uncovering the Computational Ingredients of Human-Like Representations in LLMs\n",
      "   Authors: Zach Studdiford, Timothy T. Rogers, Kushin Mukherjee, Siddharth Suresh\n",
      "   Published: 2025-10-01\n",
      "   arXiv ID: 2510.01030v1\n",
      "   URL: http://arxiv.org/pdf/2510.01030v1\n",
      "   Summary: The ability to translate diverse patterns of inputs into structured patterns of behavior has been thought to rest on both humans' and machines' ability to learn robust representations of relevant conc...\n",
      "\n",
      "28. Logical Consistency Between Disagreeing Experts and Its Role in AI Safety\n",
      "   Authors: Andrés Corrada-Emmanuel\n",
      "   Published: 2025-10-01\n",
      "   arXiv ID: 2510.00821v1\n",
      "   URL: http://arxiv.org/pdf/2510.00821v1\n",
      "   Summary: If two experts disagree on a test, we may conclude both cannot be 100 per cent correct. But if they completely agree, no possible evaluation can be excluded. This asymmetry in the utility of agreement...\n",
      "\n",
      "29. The Social Laboratory: A Psychometric Framework for Multi-Agent LLM Evaluation\n",
      "   Authors: Zarreen Reza\n",
      "   Published: 2025-10-01\n",
      "   arXiv ID: 2510.01295v1\n",
      "   URL: http://arxiv.org/pdf/2510.01295v1\n",
      "   Summary: As Large Language Models (LLMs) transition from static tools to autonomous agents, traditional evaluation benchmarks that measure performance on downstream tasks are becoming insufficient. These metho...\n",
      "\n",
      "30. A Call to Action for a Secure-by-Design Generative AI Paradigm\n",
      "   Authors: Dalal Alharthi, Ivan Roberto Kawaminami Garcia\n",
      "   Published: 2025-10-01\n",
      "   arXiv ID: 2510.00451v1\n",
      "   URL: http://arxiv.org/pdf/2510.00451v1\n",
      "   Summary: Large language models have gained widespread prominence, yet their vulnerability to prompt injection and other adversarial attacks remains a critical concern. This paper argues for a security-by-desig...\n",
      "\n",
      "31. ICL Optimized Fragility\n",
      "   Authors: Serena Gomez Wannaz\n",
      "   Published: 2025-09-30\n",
      "   arXiv ID: 2510.00300v1\n",
      "   URL: http://arxiv.org/pdf/2510.00300v1\n",
      "   Summary: ICL guides are known to improve task-specific performance, but their impact on cross-domain cognitive abilities remains unexplored. This study examines how ICL guides affect reasoning across different...\n",
      "\n",
      "32. 'Too much alignment; not enough culture': Re-balancing cultural alignment practices in LLMs\n",
      "   Authors: Eric J. W. Orlowski, Hakim Norhashim, Tristan Koh Ly Wey\n",
      "   Published: 2025-09-30\n",
      "   arXiv ID: 2509.26167v1\n",
      "   URL: http://arxiv.org/pdf/2509.26167v1\n",
      "   Summary: While cultural alignment has increasingly become a focal point within AI research, current approaches relying predominantly on quantitative benchmarks and simplistic proxies fail to capture the deeply...\n",
      "\n",
      "33. ASGuard: Activation-Scaling Guard to Mitigate Targeted Jailbreaking Attack\n",
      "   Authors: Yein Park, Jungwoo Park, Jaewoo Kang\n",
      "   Published: 2025-09-30\n",
      "   arXiv ID: 2509.25843v1\n",
      "   URL: http://arxiv.org/pdf/2509.25843v1\n",
      "   Summary: Large language models (LLMs), despite being safety-aligned, exhibit brittle refusal behaviors that can be circumvented by simple linguistic changes. As tense jailbreaking demonstrates that models refu...\n",
      "\n",
      "34. Human vs. AI Safety Perception? Decoding Human Safety Perception with Eye-Tracking Systems, Street View Images, and Explainable AI\n",
      "   Authors: Yuhao Kang, Junda Chen, Liu Liu, Kshitij Sharmad, Martina Mazzarello, Simone Mora, Fabio Duarte, Carlo Ratti\n",
      "   Published: 2025-09-29\n",
      "   arXiv ID: 2509.25457v1\n",
      "   URL: http://arxiv.org/pdf/2509.25457v1\n",
      "   Summary: The way residents perceive safety plays an important role in how they use public spaces. Studies have combined large-scale street view images and advanced computer vision techniques to measure the per...\n",
      "\n",
      "35. UniAPL: A Unified Adversarial Preference Learning Framework for Instruct-Following\n",
      "   Authors: FaQiang Qian, WeiKun Zhang, Ziliang Wang, Kang An, Xuhui Zheng, Liangjian Wen, Mengya Gao, Yong Dai, Yichao Wu\n",
      "   Published: 2025-09-29\n",
      "   arXiv ID: 2509.25148v1\n",
      "   URL: http://arxiv.org/pdf/2509.25148v1\n",
      "   Summary: Shaping powerful LLMs to be beneficial and safe is central to AI alignment. We argue that post-training alignment is fundamentally a unified Preference Learning problem, involving two modalities: demo...\n",
      "\n",
      "36. The 2025 OpenAI Preparedness Framework does not guarantee any AI risk mitigation practices: a proof-of-concept for affordance analyses of AI safety policies\n",
      "   Authors: Sam Coggins, Alexander K. Saeri, Katherine A. Daniell, Lorenn P. Ruster, Jessie Liu, Jenny L. Davis\n",
      "   Published: 2025-09-29\n",
      "   arXiv ID: 2509.24394v2\n",
      "   URL: http://arxiv.org/pdf/2509.24394v2\n",
      "   Summary: Prominent AI companies are producing 'safety frameworks' as a type of voluntary self-governance. These statements purport to establish risk thresholds and safety procedures for the development and dep...\n",
      "\n",
      "37. AI Safety, Alignment, and Ethics (AI SAE)\n",
      "   Authors: Dylan Waldner\n",
      "   Published: 2025-09-28\n",
      "   arXiv ID: 2509.24065v2\n",
      "   URL: http://arxiv.org/pdf/2509.24065v2\n",
      "   Summary: This paper grounds ethics in evolutionary biology, viewing moral norms as adaptive mechanisms that render cooperation fitness-viable under selection pressure. Current alignment approaches add ethics p...\n",
      "\n",
      "38. Deceive, Detect, and Disclose: Large Language Models Play Mini-Mafia\n",
      "   Authors: Davi Bastos Costa, Renato Vicente\n",
      "   Published: 2025-09-27\n",
      "   arXiv ID: 2509.23023v1\n",
      "   URL: http://arxiv.org/pdf/2509.23023v1\n",
      "   Summary: Mafia is a social deduction game where informed mafia compete against uninformed townsfolk. Its asymmetry of information and reliance on theory-of-mind reasoning mirror real-world multi-agent scenario...\n",
      "\n",
      "39. Anti-Regulatory AI: How \"AI Safety\" is Leveraged Against Regulatory Oversight\n",
      "   Authors: Rui-Jie Yew, Brian Judge\n",
      "   Published: 2025-09-26\n",
      "   arXiv ID: 2509.22872v2\n",
      "   URL: http://arxiv.org/pdf/2509.22872v2\n",
      "   Summary: AI companies increasingly develop and deploy privacy-enhancing technologies, bias-constraining measures, evaluation frameworks, and alignment techniques -- framing them as addressing concerns related ...\n",
      "\n",
      "40. Can Large Language Models Develop Gambling Addiction?\n",
      "   Authors: Seungpil Lee, Donghyeon Shin, Yunjeong Lee, Sundong Kim\n",
      "   Published: 2025-09-26\n",
      "   arXiv ID: 2509.22818v1\n",
      "   URL: http://arxiv.org/pdf/2509.22818v1\n",
      "   Summary: This study explores whether large language models can exhibit behavioral patterns similar to human gambling addictions. As LLMs are increasingly utilized in financial decision-making domains such as a...\n",
      "\n",
      "41. Safety Compliance: Rethinking LLM Safety Reasoning through the Lens of Compliance\n",
      "   Authors: Wenbin Hu, Huihao Jing, Haochen Shi, Haoran Li, Yangqiu Song\n",
      "   Published: 2025-09-26\n",
      "   arXiv ID: 2509.22250v1\n",
      "   URL: http://arxiv.org/pdf/2509.22250v1\n",
      "   Summary: The proliferation of Large Language Models (LLMs) has demonstrated remarkable capabilities, elevating the critical importance of LLM safety. However, existing safety methods rely on ad-hoc taxonomy an...\n",
      "\n",
      "42. Where Did It Go Wrong? Attributing Undesirable LLM Behaviors via Representation Gradient Tracing\n",
      "   Authors: Zhe Li, Wei Zhao, Yige Li, Jun Sun\n",
      "   Published: 2025-09-26\n",
      "   arXiv ID: 2510.02334v1\n",
      "   URL: http://arxiv.org/pdf/2510.02334v1\n",
      "   Summary: Large Language Models (LLMs) have demonstrated remarkable capabilities, yet their deployment is frequently undermined by undesirable behaviors such as generating harmful content, factual inaccuracies,...\n",
      "\n",
      "43. Building Information Models to Robot-Ready Site Digital Twins (BIM2RDT): An Agentic AI Safety-First Framework\n",
      "   Authors: Reza Akhavian, Mani Amani, Johannes Mootz, Robert Ashe, Behrad Beheshti\n",
      "   Published: 2025-09-25\n",
      "   arXiv ID: 2509.20705v1\n",
      "   URL: http://arxiv.org/pdf/2509.20705v1\n",
      "   Summary: The adoption of cyber-physical systems and jobsite intelligence that connects design models, real-time site sensing, and autonomous field operations can dramatically enhance digital management in the ...\n",
      "\n",
      "44. PolicyPad: Collaborative Prototyping of LLM Policies\n",
      "   Authors: K. J. Kevin Feng, Tzu-Sheng Kuo, Quan Ze, Chen, Inyoung Cheong, Kenneth Holstein, Amy X. Zhang\n",
      "   Published: 2025-09-24\n",
      "   arXiv ID: 2509.19680v1\n",
      "   URL: http://arxiv.org/pdf/2509.19680v1\n",
      "   Summary: As LLMs gain adoption in high-stakes domains like mental health, domain experts are increasingly consulted to provide input into policies governing their behavior. From an observation of 19 policymaki...\n",
      "\n",
      "45. Blueprints of Trust: AI System Cards for End to End Transparency and Governance\n",
      "   Authors: Huzaifa Sidhpurwala, Emily Fox, Garth Mollett, Florencio Cano Gabarda, Roman Zhukov\n",
      "   Published: 2025-09-23\n",
      "   arXiv ID: 2509.20394v1\n",
      "   URL: http://arxiv.org/pdf/2509.20394v1\n",
      "   Summary: This paper introduces the Hazard-Aware System Card (HASC), a novel framework designed to enhance transparency and accountability in the development and deployment of AI systems. The HASC builds upon e...\n",
      "\n",
      "46. Strategic Dishonesty Can Undermine AI Safety Evaluations of Frontier LLMs\n",
      "   Authors: Alexander Panfilov, Evgenii Kortukov, Kristina Nikolić, Matthias Bethge, Sebastian Lapuschkin, Wojciech Samek, Ameya Prabhu, Maksym Andriushchenko, Jonas Geiping\n",
      "   Published: 2025-09-22\n",
      "   arXiv ID: 2509.18058v2\n",
      "   URL: http://arxiv.org/pdf/2509.18058v2\n",
      "   Summary: Large language model (LLM) developers aim for their models to be honest, helpful, and harmless. However, when faced with malicious requests, models are trained to refuse, sacrificing helpfulness. We s...\n",
      "\n",
      "47. An Unlearning Framework for Continual Learning\n",
      "   Authors: Sayanta Adhikari, Vishnuprasadh Kumaravelu, P. K. Srijith\n",
      "   Published: 2025-09-22\n",
      "   arXiv ID: 2509.17530v1\n",
      "   URL: http://arxiv.org/pdf/2509.17530v1\n",
      "   Summary: Growing concerns surrounding AI safety and data privacy have driven the development of Machine Unlearning as a potential solution. However, current machine unlearning algorithms are designed to comple...\n",
      "\n",
      "48. Redefining Experts: Interpretable Decomposition of Language Models for Toxicity Mitigation\n",
      "   Authors: Zuhair Hasan Shaik, Abdullah Mazhar, Aseem Srivastava, Md Shad Akhtar\n",
      "   Published: 2025-09-20\n",
      "   arXiv ID: 2509.16660v1\n",
      "   URL: http://arxiv.org/pdf/2509.16660v1\n",
      "   Summary: Large Language Models have demonstrated impressive fluency across diverse tasks, yet their tendency to produce toxic content remains a critical challenge for AI safety and public trust. Existing toxic...\n",
      "\n",
      "49. Domain-Specific Constitutional AI: Enhancing Safety in LLM-Powered Mental Health Chatbots\n",
      "   Authors: Chenhan Lyu, Yutong Song, Pengfei Zhang, Amir M. Rahmani\n",
      "   Published: 2025-09-19\n",
      "   arXiv ID: 2509.16444v1\n",
      "   URL: http://arxiv.org/pdf/2509.16444v1\n",
      "   Summary: Mental health applications have emerged as a critical area in computational health, driven by rising global rates of mental illness, the integration of AI in psychological care, and the need for scala...\n",
      "\n",
      "50. Is GPT-4o mini Blinded by its Own Safety Filters? Exposing the Multimodal-to-Unimodal Bottleneck in Hate Speech Detection\n",
      "   Authors: Niruthiha Selvanayagam, Ted Kurti\n",
      "   Published: 2025-09-17\n",
      "   arXiv ID: 2509.13608v1\n",
      "   URL: http://arxiv.org/pdf/2509.13608v1\n",
      "   Summary: As Large Multimodal Models (LMMs) become integral to daily digital life, understanding their safety architectures is a critical problem for AI Alignment. This paper presents a systematic analysis of O...\n",
      "\n",
      "51. Co-Alignment: Rethinking Alignment as Bidirectional Human-AI Cognitive Adaptation\n",
      "   Authors: Yubo Li, Weiyi Song\n",
      "   Published: 2025-09-15\n",
      "   arXiv ID: 2509.12179v4\n",
      "   URL: http://arxiv.org/pdf/2509.12179v4\n",
      "   Summary: Current AI alignment through RLHF follows a single directional paradigm that AI conforms to human preferences while treating human cognition as fixed. We propose a shift to co-alignment through Bidire...\n",
      "\n",
      "52. Worker Discretion Advised: Co-designing Risk Disclosure in Crowdsourced Responsible AI (RAI) Content Work\n",
      "   Authors: Alice Qian, Ziqi Yang, Ryland Shaw, Jina Suh, Laura Dabbish, Hong Shen\n",
      "   Published: 2025-09-15\n",
      "   arXiv ID: 2509.12140v2\n",
      "   URL: http://arxiv.org/pdf/2509.12140v2\n",
      "   Summary: Responsible AI (RAI) content work, such as annotation, moderation, or red teaming for AI safety, often exposes crowd workers to potentially harmful content. While prior work has underscored the import...\n",
      "\n",
      "53. Inducing Uncertainty on Open-Weight Models for Test-Time Privacy in Image Recognition\n",
      "   Authors: Muhammad H. Ashiq, Peter Triantafillou, Hung Yun Tseng, Grigoris G. Chrysos\n",
      "   Published: 2025-09-15\n",
      "   arXiv ID: 2509.11625v2\n",
      "   URL: http://arxiv.org/pdf/2509.11625v2\n",
      "   Summary: A key concern for AI safety remains understudied in the machine learning (ML) literature: how can we ensure users of ML models do not leverage predictions on incorrect personal data to harm others? Th...\n",
      "\n",
      "54. CogniAlign: Survivability-Grounded Multi-Agent Moral Reasoning for Safe and Transparent AI\n",
      "   Authors: Hasin Jawad Ali, Ilhamul Azam, Ajwad Abrar, Md. Kamrul Hasan, Hasan Mahmud\n",
      "   Published: 2025-09-14\n",
      "   arXiv ID: 2509.13356v1\n",
      "   URL: http://arxiv.org/pdf/2509.13356v1\n",
      "   Summary: The challenge of aligning artificial intelligence (AI) with human values persists due to the abstract and often conflicting nature of moral principles and the opacity of existing approaches. This pape...\n",
      "\n",
      "55. Position: AI Safety Must Embrace an Antifragile Perspective\n",
      "   Authors: Ming Jin, Hyunin Lee\n",
      "   Published: 2025-09-11\n",
      "   arXiv ID: 2509.13339v1\n",
      "   URL: http://arxiv.org/pdf/2509.13339v1\n",
      "   Summary: This position paper contends that modern AI research must adopt an antifragile perspective on safety -- one in which the system's capacity to guarantee long-term AI safety such as handling rare or out...\n",
      "\n",
      "56. GrACE: A Generative Approach to Better Confidence Elicitation in Large Language Models\n",
      "   Authors: Zhaohan Zhang, Ziquan Liu, Ioannis Patras\n",
      "   Published: 2025-09-11\n",
      "   arXiv ID: 2509.09438v1\n",
      "   URL: http://arxiv.org/pdf/2509.09438v1\n",
      "   Summary: Assessing the reliability of Large Language Models (LLMs) by confidence elicitation is a prominent approach to AI safety in high-stakes applications, such as healthcare and finance. Existing methods e...\n",
      "\n",
      "57. Evaluation Awareness Scales Predictably in Open-Weights Large Language Models\n",
      "   Authors: Maheep Chaudhary, Ian Su, Nikhil Hooda, Nishith Shankar, Julia Tan, Kevin Zhu, Ashwinee Panda, Ryan Lagasse, Vasu Sharma\n",
      "   Published: 2025-09-10\n",
      "   arXiv ID: 2509.13333v1\n",
      "   URL: http://arxiv.org/pdf/2509.13333v1\n",
      "   Summary: Large language models (LLMs) can internally distinguish between evaluation and deployment contexts, a behaviour known as \\emph{evaluation awareness}. This undermines AI safety evaluations, as models m...\n",
      "\n",
      "58. Getting In Contract with Large Language Models -- An Agency Theory Perspective On Large Language Model Alignment\n",
      "   Authors: Sascha Kaltenpoth, Oliver Müller\n",
      "   Published: 2025-09-09\n",
      "   arXiv ID: 2509.07642v1\n",
      "   URL: http://arxiv.org/pdf/2509.07642v1\n",
      "   Summary: Adopting Large language models (LLMs) in organizations potentially revolutionizes our lives and work. However, they can generate off-topic, discriminating, or harmful content. This AI alignment proble...\n",
      "\n",
      "59. From Noise to Narrative: Tracing the Origins of Hallucinations in Transformers\n",
      "   Authors: Praneet Suresh, Jack Stanley, Sonia Joseph, Luca Scimeca, Danilo Bzdok\n",
      "   Published: 2025-09-08\n",
      "   arXiv ID: 2509.06938v1\n",
      "   URL: http://arxiv.org/pdf/2509.06938v1\n",
      "   Summary: As generative AI systems become competent and democratized in science, business, and government, deeper insight into their failure modes now poses an acute need. The occasional volatility in their beh...\n",
      "\n",
      "60. ArGen: Auto-Regulation of Generative AI via GRPO and Policy-as-Code\n",
      "   Authors: Kapil Madan\n",
      "   Published: 2025-09-06\n",
      "   arXiv ID: 2509.07006v1\n",
      "   URL: http://arxiv.org/pdf/2509.07006v1\n",
      "   Summary: This paper introduces ArGen (Auto-Regulation of Generative AI systems), a framework for aligning Large Language Models (LLMs) with complex sets of configurable, machine-readable rules spanning ethical...\n",
      "\n",
      "61. Murphys Laws of AI Alignment: Why the Gap Always Wins\n",
      "   Authors: Madhava Gaikwad\n",
      "   Published: 2025-09-04\n",
      "   arXiv ID: 2509.05381v3\n",
      "   URL: http://arxiv.org/pdf/2509.05381v3\n",
      "   Summary: We study reinforcement learning from human feedback under misspecification. Sometimes human feedback is systematically wrong on certain types of inputs, like a broken compass that points the wrong way...\n",
      "\n",
      "62. Towards Cognitively-Faithful Decision-Making Models to Improve AI Alignment\n",
      "   Authors: Cyrus Cousins, Vijay Keswani, Vincent Conitzer, Hoda Heidari, Jana Schaich Borg, Walter Sinnott-Armstrong\n",
      "   Published: 2025-09-04\n",
      "   arXiv ID: 2509.04445v1\n",
      "   URL: http://arxiv.org/pdf/2509.04445v1\n",
      "   Summary: Recent AI work trends towards incorporating human-centric objectives, with the explicit goal of aligning AI models to personal preferences and societal values. Using standard preference elicitation me...\n",
      "\n",
      "63. Between a Rock and a Hard Place: Exploiting Ethical Reasoning to Jailbreak LLMs\n",
      "   Authors: Shei Pern Chua, Zhen Leng Thai, Teh Kai Jun, Xiao Li, Xiaolin Hu\n",
      "   Published: 2025-09-04\n",
      "   arXiv ID: 2509.05367v2\n",
      "   URL: http://arxiv.org/pdf/2509.05367v2\n",
      "   Summary: Large language models (LLMs) have undergone safety alignment efforts to mitigate harmful outputs. However, as LLMs become more sophisticated in reasoning, their intelligence may introduce new security...\n",
      "\n",
      "64. A Comprehensive Survey on Trustworthiness in Reasoning with Large Language Models\n",
      "   Authors: Yanbo Wang, Yongcan Yu, Jian Liang, Ran He\n",
      "   Published: 2025-09-04\n",
      "   arXiv ID: 2509.03871v1\n",
      "   URL: http://arxiv.org/pdf/2509.03871v1\n",
      "   Summary: The development of Long-CoT reasoning has advanced LLM performance across various tasks, including language understanding, complex problem solving, and code generation. This paradigm enables models to...\n",
      "\n",
      "65. AI Safety Assurance in Electric Vehicles: A Case Study on AI-Driven SOC Estimation\n",
      "   Authors: Martin Skoglund, Fredrik Warg, Aria Mirzai, Anders Thorsen, Karl Lundgren, Peter Folkesson, Bastian Havers-zulka\n",
      "   Published: 2025-09-03\n",
      "   arXiv ID: 2509.03270v1\n",
      "   URL: http://arxiv.org/pdf/2509.03270v1\n",
      "   Summary: Integrating Artificial Intelligence (AI) technology in electric vehicles (EV) introduces unique challenges for safety assurance, particularly within the framework of ISO 26262, which governs functiona...\n",
      "\n",
      "66. BioBlue: Notable runaway-optimiser-like LLM failure modes on biologically and economically aligned AI safety benchmarks for LLMs with simplified observation format\n",
      "   Authors: Roland Pihlakas, Sruthi Kuriakose\n",
      "   Published: 2025-09-02\n",
      "   arXiv ID: 2509.02655v1\n",
      "   URL: http://arxiv.org/pdf/2509.02655v1\n",
      "   Summary: Relatively many past AI safety discussions have centered around the dangers of unbounded utility maximisation by RL agents, illustrated by scenarios like the \"paperclip maximiser\" or by specification ...\n",
      "\n",
      "67. Can Media Act as a Soft Regulator of Safe AI Development? A Game Theoretical Analysis\n",
      "   Authors: Henrique Correia da Fonseca, António Fernandes, Zhao Song, Theodor Cimpeanu, Nataliya Balabanova, Adeela Bashir, Paolo Bova, Alessio Buscemi, Alessandro Di Stefano, Manh Hong Duong, Elias Fernandez Domingos, Ndidi Bianca Ogbo, Simon T. Powers, Daniele Proverbio, Zia Ush Shamszaman, Fernando P. Santos, The Anh Han, Marcus Krellner\n",
      "   Published: 2025-09-02\n",
      "   arXiv ID: 2509.02650v1\n",
      "   URL: http://arxiv.org/pdf/2509.02650v1\n",
      "   Summary: When developers of artificial intelligence (AI) products need to decide between profit and safety for the users, they likely choose profit. Untrustworthy AI technology must come packaged with tangible...\n",
      "\n",
      "68. mFARM: Towards Multi-Faceted Fairness Assessment based on HARMs in Clinical Decision Support\n",
      "   Authors: Shreyash Adappanavar, Krithi Shailya, Gokul S Krishnan, Sriraam Natarajan, Balaraman Ravindran\n",
      "   Published: 2025-09-02\n",
      "   arXiv ID: 2509.02007v1\n",
      "   URL: http://arxiv.org/pdf/2509.02007v1\n",
      "   Summary: The deployment of Large Language Models (LLMs) in high-stakes medical settings poses a critical AI alignment challenge, as models can inherit and amplify societal biases, leading to significant dispar...\n",
      "\n",
      "69. The Anti-Ouroboros Effect: Emergent Resilience in Large Language Models from Recursive Selective Feedback\n",
      "   Authors: Sai Teja Reddy Adapala\n",
      "   Published: 2025-09-02\n",
      "   arXiv ID: 2509.10509v1\n",
      "   URL: http://arxiv.org/pdf/2509.10509v1\n",
      "   Summary: The stability of recursively trained large language models (LLMs) is a foundational problem for AI safety. Prevailing theory predicts model collapse, a progressive degradation when models are trained ...\n",
      "\n",
      "70. Statutory Construction and Interpretation for Artificial Intelligence\n",
      "   Authors: Luxi He, Nimra Nadeem, Michel Liao, Howard Chen, Danqi Chen, Mariano-Florentino Cuéllar, Peter Henderson\n",
      "   Published: 2025-09-01\n",
      "   arXiv ID: 2509.01186v1\n",
      "   URL: http://arxiv.org/pdf/2509.01186v1\n",
      "   Summary: AI systems are increasingly governed by natural language principles, yet a key challenge arising from reliance on language remains underexplored: interpretive ambiguity. As in legal systems, ambiguity...\n",
      "\n",
      "71. Probe-Rewrite-Evaluate: A Workflow for Reliable Benchmarks and Quantifying Evaluation Awareness\n",
      "   Authors: Lang Xiong, Nishant Bhargava, Jianhang Hong, Jeremy Chang, Haihao Liu, Vasu Sharma, Kevin Zhu\n",
      "   Published: 2025-08-30\n",
      "   arXiv ID: 2509.00591v5\n",
      "   URL: http://arxiv.org/pdf/2509.00591v5\n",
      "   Summary: Large Language Models (LLMs) often exhibit significant behavioral shifts when they perceive a change from a real-world deployment context to a controlled evaluation setting, a phenomenon known as \"eva...\n",
      "\n",
      "72. Governable AI: Provable Safety Under Extreme Threat Models\n",
      "   Authors: Donglin Wang, Weiyun Liang, Chunyuan Chen, Jing Xu, Yulong Fu\n",
      "   Published: 2025-08-28\n",
      "   arXiv ID: 2508.20411v1\n",
      "   URL: http://arxiv.org/pdf/2508.20411v1\n",
      "   Summary: As AI rapidly advances, the security risks posed by AI are becoming increasingly severe, especially in critical scenarios, including those posing existential risks. If AI becomes uncontrollable, manip...\n",
      "\n",
      "73. Ensemble Debates with Local Large Language Models for AI Alignment\n",
      "   Authors: Ephraiem Sarabamoun\n",
      "   Published: 2025-08-27\n",
      "   arXiv ID: 2509.00091v1\n",
      "   URL: http://arxiv.org/pdf/2509.00091v1\n",
      "   Summary: As large language models (LLMs) take on greater roles in high-stakes decisions, alignment with human values is essential. Reliance on proprietary APIs limits reproducibility and broad participation. W...\n",
      "\n",
      "74. Private, Verifiable, and Auditable AI Systems\n",
      "   Authors: Tobin South\n",
      "   Published: 2025-08-27\n",
      "   arXiv ID: 2509.00085v1\n",
      "   URL: http://arxiv.org/pdf/2509.00085v1\n",
      "   Summary: The growing societal reliance on artificial intelligence necessitates robust frameworks for ensuring its security, accountability, and trustworthiness. This thesis addresses the complex interplay betw...\n",
      "\n",
      "75. Language Models Identify Ambiguities and Exploit Loopholes\n",
      "   Authors: Jio Choi, Mohit Bansal, Elias Stengel-Eskin\n",
      "   Published: 2025-08-27\n",
      "   arXiv ID: 2508.19546v2\n",
      "   URL: http://arxiv.org/pdf/2508.19546v2\n",
      "   Summary: Studying the responses of large language models (LLMs) to loopholes presents a two-fold opportunity. First, it affords us a lens through which to examine ambiguity and pragmatics in LLMs, since exploi...\n",
      "\n",
      "76. Attacking LLMs and AI Agents: Advertisement Embedding Attacks Against Large Language Models\n",
      "   Authors: Qiming Guo, Jinwen Tang, Xingran Huang\n",
      "   Published: 2025-08-25\n",
      "   arXiv ID: 2508.17674v2\n",
      "   URL: http://arxiv.org/pdf/2508.17674v2\n",
      "   Summary: We introduce Advertisement Embedding Attacks (AEA), a new class of LLM security threats that stealthily inject promotional or malicious content into model outputs and AI agents. AEA operate through tw...\n",
      "\n",
      "77. School of Reward Hacks: Hacking harmless tasks generalizes to misaligned behavior in LLMs\n",
      "   Authors: Mia Taylor, James Chua, Jan Betley, Johannes Treutlein, Owain Evans\n",
      "   Published: 2025-08-24\n",
      "   arXiv ID: 2508.17511v1\n",
      "   URL: http://arxiv.org/pdf/2508.17511v1\n",
      "   Summary: Reward hacking--where agents exploit flaws in imperfect reward functions rather than performing tasks as intended--poses risks for AI alignment. Reward hacking has been observed in real training runs,...\n",
      "\n",
      "78. Mind the Gap: Time-of-Check to Time-of-Use Vulnerabilities in LLM-Enabled Agents\n",
      "   Authors: Derek Lilienthal, Sanghyun Hong\n",
      "   Published: 2025-08-23\n",
      "   arXiv ID: 2508.17155v1\n",
      "   URL: http://arxiv.org/pdf/2508.17155v1\n",
      "   Summary: Large Language Model (LLM)-enabled agents are rapidly emerging across a wide range of applications, but their deployment introduces vulnerabilities with security implications. While prior work has exa...\n",
      "\n",
      "79. Decoding Alignment: A Critical Survey of LLM Development Initiatives through Value-setting and Data-centric Lens\n",
      "   Authors: Ilias Chalkidis\n",
      "   Published: 2025-08-23\n",
      "   arXiv ID: 2508.16982v1\n",
      "   URL: http://arxiv.org/pdf/2508.16982v1\n",
      "   Summary: AI Alignment, primarily in the form of Reinforcement Learning from Human Feedback (RLHF), has been a cornerstone of the post-training phase in developing Large Language Models (LLMs). It has also been...\n",
      "\n",
      "80. Probability Density from Latent Diffusion Models for Out-of-Distribution Detection\n",
      "   Authors: Joonas Järve, Karl Kaspar Haavel, Meelis Kull\n",
      "   Published: 2025-08-21\n",
      "   arXiv ID: 2508.15737v1\n",
      "   URL: http://arxiv.org/pdf/2508.15737v1\n",
      "   Summary: Despite rapid advances in AI, safety remains the main bottleneck to deploying machine-learning systems. A critical safety component is out-of-distribution detection: given an input, decide whether it ...\n",
      "\n",
      "81. A Review of Developmental Interpretability in Large Language Models\n",
      "   Authors: Ihor Kendiukhov\n",
      "   Published: 2025-08-19\n",
      "   arXiv ID: 2508.15841v1\n",
      "   URL: http://arxiv.org/pdf/2508.15841v1\n",
      "   Summary: This review synthesizes the nascent but critical field of developmental interpretability for Large Language Models. We chart the field's evolution from static, post-hoc analysis of trained models to a...\n",
      "\n",
      "82. CIA+TA Risk Assessment for AI Reasoning Vulnerabilities\n",
      "   Authors: Yuksel Aydin\n",
      "   Published: 2025-08-19\n",
      "   arXiv ID: 2508.15839v1\n",
      "   URL: http://arxiv.org/pdf/2508.15839v1\n",
      "   Summary: As AI systems increasingly influence critical decisions, they face threats that exploit reasoning mechanisms rather than technical infrastructure. We present a framework for cognitive cybersecurity, a...\n",
      "\n",
      "83. Preference Models assume Proportional Hazards of Utilities\n",
      "   Authors: Chirag Nagpal\n",
      "   Published: 2025-08-15\n",
      "   arXiv ID: 2508.13189v1\n",
      "   URL: http://arxiv.org/pdf/2508.13189v1\n",
      "   Summary: Approaches for estimating preferences from human annotated data typically involves inducing a distribution over a ranked list of choices such as the Plackett-Luce model. Indeed, modern AI alignment to...\n",
      "\n",
      "84. SproutBench: A Benchmark for Safe and Ethical Large Language Models for Youth\n",
      "   Authors: Wenpeng Xing, Lanyi Wei, Haixiao Hu, Rongchang Li, Mohan Li, Changting Lin, Meng Han\n",
      "   Published: 2025-08-14\n",
      "   arXiv ID: 2508.11009v1\n",
      "   URL: http://arxiv.org/pdf/2508.11009v1\n",
      "   Summary: The rapid proliferation of large language models (LLMs) in applications targeting children and adolescents necessitates a fundamental reassessment of prevailing AI safety frameworks, which are largely...\n",
      "\n",
      "85. PASS: Probabilistic Agentic Supernet Sampling for Interpretable and Adaptive Chest X-Ray Reasoning\n",
      "   Authors: Yushi Feng, Junye Du, Yingying Hong, Qifan Wang, Lequan Yu\n",
      "   Published: 2025-08-14\n",
      "   arXiv ID: 2508.10501v2\n",
      "   URL: http://arxiv.org/pdf/2508.10501v2\n",
      "   Summary: Existing tool-augmented agentic systems are limited in the real world by (i) black-box reasoning steps that undermine trust of decision-making and pose safety risks, (ii) poor multimodal integration, ...\n",
      "\n",
      "86. Amazon Nova AI Challenge -- Trusted AI: Advancing secure, AI-assisted software development\n",
      "   Authors: Sattvik Sahai, Prasoon Goyal, Michael Johnston, Anna Gottardi, Yao Lu, Lucy Hu, Luke Dai, Shaohua Liu, Samyuth Sagi, Hangjie Shi, Desheng Zhang, Lavina Vaz, Leslie Ball, Maureen Murray, Rahul Gupta, Shankar Ananthakrishna\n",
      "   Published: 2025-08-13\n",
      "   arXiv ID: 2508.10108v1\n",
      "   URL: http://arxiv.org/pdf/2508.10108v1\n",
      "   Summary: AI systems for software development are rapidly gaining prominence, yet significant challenges remain in ensuring their safety. To address this, Amazon launched the Trusted AI track of the Amazon Nova...\n",
      "\n",
      "87. The PacifAIst Benchmark:Would an Artificial Intelligence Choose to Sacrifice Itself for Human Safety?\n",
      "   Authors: Manuel Herrador\n",
      "   Published: 2025-08-13\n",
      "   arXiv ID: 2508.09762v1\n",
      "   URL: http://arxiv.org/pdf/2508.09762v1\n",
      "   Summary: As Large Language Models (LLMs) become increasingly autonomous and integrated into critical societal functions, the focus of AI safety must evolve from mitigating harmful content to evaluating underly...\n",
      "\n",
      "88. Toward an African Agenda for AI Safety\n",
      "   Authors: Samuel T. Segun, Rachel Adams, Ana Florido, Scott Timcke, Jonathan Shock, Leah Junck, Fola Adeleke, Nicolas Grossman, Ayantola Alayande, Jerry John Kponyo, Matthew Smith, Dickson Marfo Fosu, Prince Dawson Tetteh, Juliet Arthur, Stephanie Kasaon, Odilile Ayodele, Laetitia Badolo, Paul Plantinga, Michael Gastrow, Sumaya Nur Adan, Joanna Wiaterek, Cecil Abungu, Kojo Apeagyei, Luise Eder, Tegawende Bissyande\n",
      "   Published: 2025-08-12\n",
      "   arXiv ID: 2508.13179v1\n",
      "   URL: http://arxiv.org/pdf/2508.13179v1\n",
      "   Summary: This paper maps Africa's distinctive AI risk profile, from deepfake fuelled electoral interference and data colonial dependency to compute scarcity, labour disruption and disproportionate exposure to ...\n",
      "\n",
      "89. An Intelligent Infrastructure as a Foundation for Modern Science\n",
      "   Authors: Satrajit S. Ghosh\n",
      "   Published: 2025-08-12\n",
      "   arXiv ID: 2508.10051v1\n",
      "   URL: http://arxiv.org/pdf/2508.10051v1\n",
      "   Summary: Infrastructure shapes societies and scientific discovery. Traditional scientific infrastructure, often static and fragmented, leads to issues like data silos, lack of interoperability and reproducibil...\n",
      "\n",
      "90. Multi-Turn Jailbreaks Are Simpler Than They Seem\n",
      "   Authors: Xiaoxue Yang, Jaeha Lee, Anna-Katharina Dick, Jasper Timm, Fei Xie, Diogo Cruz\n",
      "   Published: 2025-08-11\n",
      "   arXiv ID: 2508.07646v1\n",
      "   URL: http://arxiv.org/pdf/2508.07646v1\n",
      "   Summary: While defenses against single-turn jailbreak attacks on Large Language Models (LLMs) have improved significantly, multi-turn jailbreaks remain a persistent vulnerability, often achieving success rates...\n",
      "\n",
      "91. LLM Robustness Leaderboard v1 --Technical report\n",
      "   Authors: Pierre Peigné - Lefebvre, Quentin Feuillade-Montixi, Tom David, Nicolas Miailhe\n",
      "   Published: 2025-08-08\n",
      "   arXiv ID: 2508.06296v2\n",
      "   URL: http://arxiv.org/pdf/2508.06296v2\n",
      "   Summary: This technical report accompanies the LLM robustness leaderboard published by PRISM Eval for the Paris AI Action Summit. We introduce PRISM Eval Behavior Elicitation Tool (BET), an AI system performin...\n",
      "\n",
      "92. Towards Integrated Alignment\n",
      "   Authors: Ben Y. Reis, William La Cava\n",
      "   Published: 2025-08-08\n",
      "   arXiv ID: 2508.06592v1\n",
      "   URL: http://arxiv.org/pdf/2508.06592v1\n",
      "   Summary: As AI adoption expands across human society, the problem of aligning AI models to match human preferences remains a grand challenge. Currently, the AI alignment field is deeply divided between behavio...\n",
      "\n",
      "93. A Framework for Inherently Safer AGI through Language-Mediated Active Inference\n",
      "   Authors: Bo Wen\n",
      "   Published: 2025-08-07\n",
      "   arXiv ID: 2508.05766v1\n",
      "   URL: http://arxiv.org/pdf/2508.05766v1\n",
      "   Summary: This paper proposes a novel framework for developing safe Artificial General Intelligence (AGI) by combining Active Inference principles with Large Language Models (LLMs). We argue that traditional ap...\n",
      "\n",
      "94. Hybrid quantum tensor networks for aeroelastic applications\n",
      "   Authors: M. Lautaro Hickmann, Pedro Alves, David Quero, Friedhelm Schwenker, Hans-Martin Rieser\n",
      "   Published: 2025-08-07\n",
      "   arXiv ID: 2508.05169v1\n",
      "   URL: http://arxiv.org/pdf/2508.05169v1\n",
      "   Summary: We investigate the application of hybrid quantum tensor networks to aeroelastic problems, harnessing the power of Quantum Machine Learning (QML). By combining tensor networks with variational quantum ...\n",
      "\n",
      "95. Large Language Model's Multi-Capability Alignment in Biomedical Domain\n",
      "   Authors: Wentao Wu, Linqing Chen, Hanmeng Zhong, Weilei Wang\n",
      "   Published: 2025-08-06\n",
      "   arXiv ID: 2508.04278v1\n",
      "   URL: http://arxiv.org/pdf/2508.04278v1\n",
      "   Summary: BalancedBio is a theoretically grounded framework for parameter-efficient biomedical reasoning, addressing multi-capability integration in domain-specific AI alignment. It establishes the Biomedical M...\n",
      "\n",
      "96. Is Uncertainty Quantification a Viable Alternative to Learned Deferral?\n",
      "   Authors: Anna M. Wundram, Christian F. Baumgartner\n",
      "   Published: 2025-08-04\n",
      "   arXiv ID: 2508.02319v2\n",
      "   URL: http://arxiv.org/pdf/2508.02319v2\n",
      "   Summary: Artificial Intelligence (AI) holds the potential to dramatically improve patient care. However, it is not infallible, necessitating human-AI-collaboration to ensure safe implementation. One aspect of ...\n",
      "\n",
      "97. Out-of-Context Abduction: LLMs Make Inferences About Procedural Data Leveraging Declarative Facts in Earlier Training Data\n",
      "   Authors: Sohaib Imran, Rob Lamb, Peter M. Atkinson\n",
      "   Published: 2025-08-01\n",
      "   arXiv ID: 2508.00741v1\n",
      "   URL: http://arxiv.org/pdf/2508.00741v1\n",
      "   Summary: Large language models (LLMs) are trained on large corpora, yet it is unclear whether they can reason about the information present within their training data. We design experiments to study out-of-con...\n",
      "\n",
      "98. Model-Based Soft Maximization of Suitable Metrics of Long-Term Human Power\n",
      "   Authors: Jobst Heitzig, Ram Potham\n",
      "   Published: 2025-07-31\n",
      "   arXiv ID: 2508.00159v2\n",
      "   URL: http://arxiv.org/pdf/2508.00159v2\n",
      "   Summary: Power is a key concept in AI safety: power-seeking as an instrumental goal, sudden or gradual disempowerment of humans, power balance in human-AI interaction and international AI governance. At the sa...\n",
      "\n",
      "99. Against racing to AGI: Cooperation, deterrence, and catastrophic risks\n",
      "   Authors: Leonard Dung, Max Hellrigel-Holderbaum\n",
      "   Published: 2025-07-29\n",
      "   arXiv ID: 2507.21839v1\n",
      "   URL: http://arxiv.org/pdf/2507.21839v1\n",
      "   Summary: AGI Racing is the view that it is in the self-interest of major actors in AI development, especially powerful nations, to accelerate their frontier AI development to build highly capable AI, especiall...\n",
      "\n",
      "100. Learning to Imitate with Less: Efficient Individual Behavior Modeling in Chess\n",
      "   Authors: Zhenwei Tang, Difan Jiao, Eric Xue, Reid McIlroy-Young, Jon Kleinberg, Siddhartha Sen, Ashton Anderson\n",
      "   Published: 2025-07-29\n",
      "   arXiv ID: 2507.21488v1\n",
      "   URL: http://arxiv.org/pdf/2507.21488v1\n",
      "   Summary: As humans seek to collaborate with, learn from, and better understand artificial intelligence systems, developing AIs that can accurately emulate individual decision-making becomes increasingly import...\n",
      "\n",
      "101. Technological folie à deux: Feedback Loops Between AI Chatbots and Mental Illness\n",
      "   Authors: Sebastian Dohnány, Zeb Kurth-Nelson, Eleanor Spens, Lennart Luettgau, Alastair Reid, Iason Gabriel, Christopher Summerfield, Murray Shanahan, Matthew M Nour\n",
      "   Published: 2025-07-25\n",
      "   arXiv ID: 2507.19218v2\n",
      "   URL: http://arxiv.org/pdf/2507.19218v2\n",
      "   Summary: Artificial intelligence chatbots have achieved unprecedented adoption, with millions now using these systems for emotional support and companionship in contexts of widespread social isolation and capa...\n",
      "\n",
      "102. A Survey of Multimodal Hallucination Evaluation and Detection\n",
      "   Authors: Zhiyuan Chen, Yuecong Min, Jie Zhang, Bei Yan, Jiahao Wang, Xiaozhen Wang, Shiguang Shan\n",
      "   Published: 2025-07-25\n",
      "   arXiv ID: 2507.19024v1\n",
      "   URL: http://arxiv.org/pdf/2507.19024v1\n",
      "   Summary: Multi-modal Large Language Models (MLLMs) have emerged as a powerful paradigm for integrating visual and textual information, supporting a wide range of multi-modal tasks. However, these models often ...\n",
      "\n",
      "103. DxHF: Providing High-Quality Human Feedback for LLM Alignment via Interactive Decomposition\n",
      "   Authors: Danqing Shi, Furui Cheng, Tino Weinkauf, Antti Oulasvirta, Mennatallah El-Assady\n",
      "   Published: 2025-07-24\n",
      "   arXiv ID: 2507.18802v1\n",
      "   URL: http://arxiv.org/pdf/2507.18802v1\n",
      "   Summary: Human preferences are widely used to align large language models (LLMs) through methods such as reinforcement learning from human feedback (RLHF). However, the current user interfaces require annotato...\n",
      "\n",
      "104. Justifications for Democratizing AI Alignment and Their Prospects\n",
      "   Authors: André Steingrüber, Kevin Baum\n",
      "   Published: 2025-07-24\n",
      "   arXiv ID: 2507.19548v1\n",
      "   URL: http://arxiv.org/pdf/2507.19548v1\n",
      "   Summary: The AI alignment problem comprises both technical and normative dimensions. While technical solutions focus on implementing normative constraints in AI systems, the normative problem concerns determin...\n",
      "\n",
      "105. HySafe-AI: Hybrid Safety Architectural Analysis Framework for AI Systems: A Case Study\n",
      "   Authors: Mandar Pitale, Jelena Frtunikj, Abhinaw Priyadershi, Vasu Singh, Maria Spence\n",
      "   Published: 2025-07-23\n",
      "   arXiv ID: 2507.17118v1\n",
      "   URL: http://arxiv.org/pdf/2507.17118v1\n",
      "   Summary: AI has become integral to safety-critical areas like autonomous driving systems (ADS) and robotics. The architecture of recent autonomous systems are trending toward end-to-end (E2E) monolithic archit...\n",
      "\n",
      "106. Analysis of Threat-Based Manipulation in Large Language Models: A Dual Perspective on Vulnerabilities and Performance Enhancement Opportunities\n",
      "   Authors: Atil Samancioglu\n",
      "   Published: 2025-07-22\n",
      "   arXiv ID: 2507.21133v1\n",
      "   URL: http://arxiv.org/pdf/2507.21133v1\n",
      "   Summary: Large Language Models (LLMs) demonstrate complex responses to threat-based manipulations, revealing both vulnerabilities and unexpected performance enhancement opportunities. This study presents a com...\n",
      "\n",
      "107. The Other Mind: How Language Models Exhibit Human Temporal Cognition\n",
      "   Authors: Lingyu Li, Yang Yao, Yixu Wang, Chubo Li, Yan Teng, Yingchun Wang\n",
      "   Published: 2025-07-21\n",
      "   arXiv ID: 2507.15851v1\n",
      "   URL: http://arxiv.org/pdf/2507.15851v1\n",
      "   Summary: As Large Language Models (LLMs) continue to advance, they exhibit certain cognitive patterns similar to those of humans that are not directly specified in training data. This study investigates this p...\n",
      "\n",
      "108. On the Inevitability of Left-Leaning Political Bias in Aligned Language Models\n",
      "   Authors: Thilo Hagendorff\n",
      "   Published: 2025-07-21\n",
      "   arXiv ID: 2507.15328v1\n",
      "   URL: http://arxiv.org/pdf/2507.15328v1\n",
      "   Summary: The guiding principle of AI alignment is to train large language models (LLMs) to be harmless, helpful, and honest (HHH). At the same time, there are mounting concerns that LLMs exhibit a left-wing po...\n",
      "\n",
      "109. When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion in Social Systems\n",
      "   Authors: Qibing Ren, Sitao Xie, Longxuan Wei, Zhenfei Yin, Junchi Yan, Lizhuang Ma, Jing Shao\n",
      "   Published: 2025-07-19\n",
      "   arXiv ID: 2507.14660v2\n",
      "   URL: http://arxiv.org/pdf/2507.14660v2\n",
      "   Summary: Recent large-scale events like election fraud and financial scams have shown how harmful coordinated efforts by human groups can be. With the rise of autonomous AI systems, there is growing concern th...\n",
      "\n",
      "110. Combining Cost-Constrained Runtime Monitors for AI Safety\n",
      "   Authors: Tim Tian Hua, James Baskerville, Henri Lemoine, Mia Hopman, Aryan Bhatt, Tyler Tracy\n",
      "   Published: 2025-07-19\n",
      "   arXiv ID: 2507.15886v4\n",
      "   URL: http://arxiv.org/pdf/2507.15886v4\n",
      "   Summary: Monitoring AIs at runtime can help us detect and stop harmful actions. In this paper, we study how to efficiently combine multiple runtime monitors into a single monitoring protocol. The protocol's ob...\n",
      "\n",
      "111. Fiduciary AI for the Future of Brain-Technology Interactions\n",
      "   Authors: Abhishek Bhattacharjee, Jack Pilkington, Nita Farahany\n",
      "   Published: 2025-07-18\n",
      "   arXiv ID: 2507.14339v1\n",
      "   URL: http://arxiv.org/pdf/2507.14339v1\n",
      "   Summary: Brain foundation models represent a new frontier in AI: instead of processing text or images, these models interpret real-time neural signals from EEG, fMRI, and other neurotechnologies. When integrat...\n",
      "\n",
      "112. The Recursive Coherence Principle: A Formal Constraint on Scalable Intelligence, Alignment, and Reasoning Architecture\n",
      "   Authors: Andy E. Williams\n",
      "   Published: 2025-07-18\n",
      "   arXiv ID: 2507.15880v1\n",
      "   URL: http://arxiv.org/pdf/2507.15880v1\n",
      "   Summary: Intelligence-biological, artificial, or collective-requires structural coherence across recursive reasoning processes to scale effectively. As complex systems grow, coherence becomes fragile unless a ...\n",
      "\n",
      "113. Manipulation Attacks by Misaligned AI: Risk Analysis and Safety Case Framework\n",
      "   Authors: Rishane Dassanayake, Mario Demetroudi, James Walpole, Lindley Lentati, Jason R. Brown, Edward James Young\n",
      "   Published: 2025-07-17\n",
      "   arXiv ID: 2507.12872v1\n",
      "   URL: http://arxiv.org/pdf/2507.12872v1\n",
      "   Summary: Frontier AI systems are rapidly advancing in their capabilities to persuade, deceive, and influence human behaviour, with current models already demonstrating human-level persuasion and strategic dece...\n",
      "\n",
      "114. LLMs Encode Harmfulness and Refusal Separately\n",
      "   Authors: Jiachen Zhao, Jing Huang, Zhengxuan Wu, David Bau, Weiyan Shi\n",
      "   Published: 2025-07-16\n",
      "   arXiv ID: 2507.11878v3\n",
      "   URL: http://arxiv.org/pdf/2507.11878v3\n",
      "   Summary: LLMs are trained to refuse harmful instructions, but do they truly understand harmfulness beyond just refusing? Prior work has shown that LLMs' refusal behaviors can be mediated by a one-dimensional s...\n",
      "\n",
      "115. Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety\n",
      "   Authors: Tomek Korbak, Mikita Balesni, Elizabeth Barnes, Yoshua Bengio, Joe Benton, Joseph Bloom, Mark Chen, Alan Cooney, Allan Dafoe, Anca Dragan, Scott Emmons, Owain Evans, David Farhi, Ryan Greenblatt, Dan Hendrycks, Marius Hobbhahn, Evan Hubinger, Geoffrey Irving, Erik Jenner, Daniel Kokotajlo, Victoria Krakovna, Shane Legg, David Lindner, David Luan, Aleksander Mądry, Julian Michael, Neel Nanda, Dave Orr, Jakub Pachocki, Ethan Perez, Mary Phuong, Fabien Roger, Joshua Saxe, Buck Shlegeris, Martín Soto, Eric Steinberger, Jasmine Wang, Wojciech Zaremba, Bowen Baker, Rohin Shah, Vlad Mikulik\n",
      "   Published: 2025-07-15\n",
      "   arXiv ID: 2507.11473v1\n",
      "   URL: http://arxiv.org/pdf/2507.11473v1\n",
      "   Summary: AI systems that \"think\" in human language offer a unique opportunity for AI safety: we can monitor their chains of thought (CoT) for the intent to misbehave. Like all other known AI oversight methods,...\n",
      "\n",
      "116. Mitigating Trojanized Prompt Chains in Educational LLM Use Cases: Experimental Findings and Detection Tool Design\n",
      "   Authors: Richard M. Charles, James H. Curry, Richard B. Charles\n",
      "   Published: 2025-07-15\n",
      "   arXiv ID: 2507.14207v1\n",
      "   URL: http://arxiv.org/pdf/2507.14207v1\n",
      "   Summary: The integration of Large Language Models (LLMs) in K--12 education offers both transformative opportunities and emerging risks. This study explores how students may Trojanize prompts to elicit unsafe ...\n",
      "\n",
      "117. Can You Detect the Difference?\n",
      "   Authors: İsmail Tarım, Aytuğ Onan\n",
      "   Published: 2025-07-14\n",
      "   arXiv ID: 2507.10475v1\n",
      "   URL: http://arxiv.org/pdf/2507.10475v1\n",
      "   Summary: The rapid advancement of large language models (LLMs) has raised concerns about reliably detecting AI-generated text. Stylometric metrics work well on autoregressive (AR) outputs, but their effectiven...\n",
      "\n",
      "118. HKGAI-V1: Towards Regional Sovereign Large Language Model for Hong Kong\n",
      "   Authors: Sirui Han, Junqi Zhu, Ruiyuan Zhang, Yike Guo\n",
      "   Published: 2025-07-14\n",
      "   arXiv ID: 2507.11502v1\n",
      "   URL: http://arxiv.org/pdf/2507.11502v1\n",
      "   Summary: This paper presents the development of HKGAI-V1, a foundational sovereign large language model (LLM), developed as part of an initiative to establish value-aligned AI infrastructure specifically tailo...\n",
      "\n",
      "119. BlueGlass: A Framework for Composite AI Safety\n",
      "   Authors: Harshal Nandigramwar, Syed Qutub, Kay-Ulrich Scholl\n",
      "   Published: 2025-07-14\n",
      "   arXiv ID: 2507.10106v1\n",
      "   URL: http://arxiv.org/pdf/2507.10106v1\n",
      "   Summary: As AI systems become increasingly capable and ubiquitous, ensuring the safety of these systems is critical. However, existing safety tools often target different aspects of model safety and cannot pro...\n",
      "\n",
      "120. Cultural Bias in Large Language Models: Evaluating AI Agents through Moral Questionnaires\n",
      "   Authors: Simon Münker\n",
      "   Published: 2025-07-14\n",
      "   arXiv ID: 2507.10073v2\n",
      "   URL: http://arxiv.org/pdf/2507.10073v2\n",
      "   Summary: Are AI systems truly representing human values, or merely averaging across them? Our study suggests a concerning reality: Large Language Models (LLMs) fail to represent diverse cultural moral framewor...\n",
      "\n",
      "121. Measuring What Matters: A Framework for Evaluating Safety Risks in Real-World LLM Applications\n",
      "   Authors: Jia Yi Goh, Shaun Khoo, Nyx Iskandar, Gabriel Chua, Leanne Tan, Jessica Foo\n",
      "   Published: 2025-07-13\n",
      "   arXiv ID: 2507.09820v1\n",
      "   URL: http://arxiv.org/pdf/2507.09820v1\n",
      "   Summary: Most safety testing efforts for large language models (LLMs) today focus on evaluating foundation models. However, there is a growing need to evaluate safety at the application level, as components su...\n",
      "\n",
      "122. Adversarial Activation Patching: A Framework for Detecting and Mitigating Emergent Deception in Safety-Aligned Transformers\n",
      "   Authors: Santhosh Kumar Ravindran\n",
      "   Published: 2025-07-12\n",
      "   arXiv ID: 2507.09406v1\n",
      "   URL: http://arxiv.org/pdf/2507.09406v1\n",
      "   Summary: Large language models (LLMs) aligned for safety through techniques like reinforcement learning from human feedback (RLHF) often exhibit emergent deceptive behaviors, where outputs appear compliant but...\n",
      "\n",
      "123. Measuring AI Alignment with Human Flourishing\n",
      "   Authors: Elizabeth Hilliard, Akshaya Jagadeesh, Alex Cook, Steele Billings, Nicholas Skytland, Alicia Llewellyn, Jackson Paull, Nathan Paull, Nolan Kurylo, Keatra Nesbitt, Robert Gruenewald, Anthony Jantzi, Omar Chavez\n",
      "   Published: 2025-07-10\n",
      "   arXiv ID: 2507.07787v2\n",
      "   URL: http://arxiv.org/pdf/2507.07787v2\n",
      "   Summary: This paper introduces the Flourishing AI Benchmark (FAI Benchmark), a novel evaluation framework that assesses AI alignment with human flourishing across seven dimensions: Character and Virtue, Close ...\n",
      "\n",
      "124. Machine Bullshit: Characterizing the Emergent Disregard for Truth in Large Language Models\n",
      "   Authors: Kaiqu Liang, Haimin Hu, Xuandong Zhao, Dawn Song, Thomas L. Griffiths, Jaime Fernández Fisac\n",
      "   Published: 2025-07-10\n",
      "   arXiv ID: 2507.07484v1\n",
      "   URL: http://arxiv.org/pdf/2507.07484v1\n",
      "   Summary: Bullshit, as conceptualized by philosopher Harry Frankfurt, refers to statements made without regard to their truth value. While previous work has explored large language model (LLM) hallucination and...\n",
      "\n",
      "125. On the Impossibility of Separating Intelligence from Judgment: The Computational Intractability of Filtering for AI Alignment\n",
      "   Authors: Sarah Ball, Greg Gluch, Shafi Goldwasser, Frauke Kreuter, Omer Reingold, Guy N. Rothblum\n",
      "   Published: 2025-07-09\n",
      "   arXiv ID: 2507.07341v1\n",
      "   URL: http://arxiv.org/pdf/2507.07341v1\n",
      "   Summary: With the increased deployment of large language models (LLMs), one concern is their potential misuse for generating harmful content. Our work studies the alignment challenge, with a focus on filters t...\n",
      "\n",
      "126. Foundation Model Self-Play: Open-Ended Strategy Innovation via Foundation Models\n",
      "   Authors: Aaron Dharna, Cong Lu, Jeff Clune\n",
      "   Published: 2025-07-09\n",
      "   arXiv ID: 2507.06466v1\n",
      "   URL: http://arxiv.org/pdf/2507.06466v1\n",
      "   Summary: Multi-agent interactions have long fueled innovation, from natural predator-prey dynamics to the space race. Self-play (SP) algorithms try to harness these dynamics by pitting agents against ever-impr...\n",
      "\n",
      "127. When Chain of Thought is Necessary, Language Models Struggle to Evade Monitors\n",
      "   Authors: Scott Emmons, Erik Jenner, David K. Elson, Rif A. Saurous, Senthooran Rajamanoharan, Heng Chen, Irhum Shafkat, Rohin Shah\n",
      "   Published: 2025-07-07\n",
      "   arXiv ID: 2507.05246v1\n",
      "   URL: http://arxiv.org/pdf/2507.05246v1\n",
      "   Summary: While chain-of-thought (CoT) monitoring is an appealing AI safety defense, recent work on \"unfaithfulness\" has cast doubt on its reliability. These findings highlight an important failure mode, partic...\n",
      "\n",
      "128. Evaluating the Critical Risks of Amazon's Nova Premier under the Frontier Model Safety Framework\n",
      "   Authors: Satyapriya Krishna, Ninareh Mehrabi, Abhinav Mohanty, Matteo Memelli, Vincent Ponzo, Payal Motwani, Rahul Gupta\n",
      "   Published: 2025-07-07\n",
      "   arXiv ID: 2507.06260v1\n",
      "   URL: http://arxiv.org/pdf/2507.06260v1\n",
      "   Summary: Nova Premier is Amazon's most capable multimodal foundation model and teacher for model distillation. It processes text, images, and video with a one-million-token context window, enabling analysis of...\n",
      "\n",
      "129. Interactive Groupwise Comparison for Reinforcement Learning from Human Feedback\n",
      "   Authors: Jan Kompatscher, Danqing Shi, Giovanna Varni, Tino Weinkauf, Antti Oulasvirta\n",
      "   Published: 2025-07-06\n",
      "   arXiv ID: 2507.04340v1\n",
      "   URL: http://arxiv.org/pdf/2507.04340v1\n",
      "   Summary: Reinforcement learning from human feedback (RLHF) has emerged as a key enabling technology for aligning AI behavior with human preferences. The traditional way to collect data in RLHF is via pairwise ...\n",
      "\n",
      "130. Mass-Scale Analysis of In-the-Wild Conversations Reveals Complexity Bounds on LLM Jailbreaking\n",
      "   Authors: Aldan Creo, Raul Castro Fernandez, Manuel Cebrian\n",
      "   Published: 2025-07-06\n",
      "   arXiv ID: 2507.08014v1\n",
      "   URL: http://arxiv.org/pdf/2507.08014v1\n",
      "   Summary: As large language models (LLMs) become increasingly deployed, understanding the complexity and evolution of jailbreaking strategies is critical for AI safety.   We present a mass-scale empirical analy...\n",
      "\n",
      "131. Moral Responsibility or Obedience: What Do We Want from AI?\n",
      "   Authors: Joseph Boland\n",
      "   Published: 2025-07-03\n",
      "   arXiv ID: 2507.02788v1\n",
      "   URL: http://arxiv.org/pdf/2507.02788v1\n",
      "   Summary: As artificial intelligence systems become increasingly agentic, capable of general reasoning, planning, and value prioritization, current safety practices that treat obedience as a proxy for ethical b...\n",
      "\n",
      "132. From Turing to Tomorrow: The UK's Approach to AI Regulation\n",
      "   Authors: Oliver Ritchie, Markus Anderljung, Tom Rachman\n",
      "   Published: 2025-07-03\n",
      "   arXiv ID: 2507.03050v1\n",
      "   URL: http://arxiv.org/pdf/2507.03050v1\n",
      "   Summary: The UK has pursued a distinctive path in AI regulation: less cautious than the EU but more willing to address risks than the US, and has emerged as a global leader in coordinating AI safety efforts. I...\n",
      "\n",
      "133. Misaligned from Within: Large Language Models Reproduce Our Double-Loop Learning Blindness\n",
      "   Authors: Tim Rogers, Ben Teehankee\n",
      "   Published: 2025-07-03\n",
      "   arXiv ID: 2507.02283v1\n",
      "   URL: http://arxiv.org/pdf/2507.02283v1\n",
      "   Summary: This paper examines a critical yet unexplored dimension of the AI alignment problem: the potential for Large Language Models (LLMs) to inherit and amplify existing misalignments between human espoused...\n",
      "\n",
      "134. `For Argument's Sake, Show Me How to Harm Myself!': Jailbreaking LLMs in Suicide and Self-Harm Contexts\n",
      "   Authors: Annika M Schoene, Cansu Canca\n",
      "   Published: 2025-07-01\n",
      "   arXiv ID: 2507.02990v1\n",
      "   URL: http://arxiv.org/pdf/2507.02990v1\n",
      "   Summary: Recent advances in large language models (LLMs) have led to increasingly sophisticated safety protocols and features designed to prevent harmful, unethical, or unauthorized outputs. However, these gua...\n",
      "\n",
      "135. Many LLMs Are More Utilitarian Than One\n",
      "   Authors: Anita Keshmirian, Razan Baltaji, Babak Hemmatian, Hadi Asghari, Lav R. Varshney\n",
      "   Published: 2025-07-01\n",
      "   arXiv ID: 2507.00814v1\n",
      "   URL: http://arxiv.org/pdf/2507.00814v1\n",
      "   Summary: Moral judgment is integral to large language model (LLM) alignment and social reasoning. As multi-agent systems gain prominence, it becomes crucial to understand how LLMs function collectively during ...\n",
      "\n",
      "136. GAF-Guard: An Agentic Framework for Risk Management and Governance in Large Language Models\n",
      "   Authors: Seshu Tirupathi, Dhaval Salwala, Elizabeth Daly, Inge Vejsbjerg\n",
      "   Published: 2025-07-01\n",
      "   arXiv ID: 2507.02986v2\n",
      "   URL: http://arxiv.org/pdf/2507.02986v2\n",
      "   Summary: As Large Language Models (LLMs) continue to be increasingly applied across various domains, their widespread adoption necessitates rigorous monitoring to prevent unintended negative consequences and e...\n",
      "\n",
      "137. Attestable Audits: Verifiable AI Safety Benchmarks Using Trusted Execution Environments\n",
      "   Authors: Christoph Schnabl, Daniel Hugenroth, Bill Marino, Alastair R. Beresford\n",
      "   Published: 2025-06-30\n",
      "   arXiv ID: 2506.23706v1\n",
      "   URL: http://arxiv.org/pdf/2506.23706v1\n",
      "   Summary: Benchmarks are important measures to evaluate safety and compliance of AI models at scale. However, they typically do not offer verifiable results and lack confidentiality for model IP and benchmark d...\n",
      "\n",
      "138. A New Perspective On AI Safety Through Control Theory Methodologies\n",
      "   Authors: Lars Ullrich, Walter Zimmer, Ross Greer, Knut Graichen, Alois C. Knoll, Mohan Trivedi\n",
      "   Published: 2025-06-30\n",
      "   arXiv ID: 2506.23703v1\n",
      "   URL: http://arxiv.org/pdf/2506.23703v1\n",
      "   Summary: While artificial intelligence (AI) is advancing rapidly and mastering increasingly complex problems with astonishing performance, the safety assurance of such systems is a major concern. Particularly ...\n",
      "\n",
      "139. Thinking About Thinking: SAGE-nano's Inverse Reasoning for Self-Aware Language Models\n",
      "   Authors: Basab Jha, Firoj Paudel, Ujjwal Puri, Zhang Yuting, Choi Donghyuk, Wang Junhao\n",
      "   Published: 2025-06-30\n",
      "   arXiv ID: 2507.00092v1\n",
      "   URL: http://arxiv.org/pdf/2507.00092v1\n",
      "   Summary: Large Language Models (LLMs) have demonstrated remarkable capabilities at solving complex reasoning tasks with Chain-of-Thought (CoT) prompting, but their decision-making processes remain somewhat bla...\n",
      "\n",
      "140. A Different Approach to AI Safety: Proceedings from the Columbia Convening on Openness in Artificial Intelligence and AI Safety\n",
      "   Authors: Camille François, Ludovic Péran, Ayah Bdeir, Nouha Dziri, Will Hawkins, Yacine Jernite, Sayash Kapoor, Juliet Shen, Heidy Khlaaf, Kevin Klyman, Nik Marda, Marie Pellat, Deb Raji, Divya Siddarth, Aviya Skowron, Joseph Spisak, Madhulika Srikumar, Victor Storchan, Audrey Tang, Jen Weedon\n",
      "   Published: 2025-06-27\n",
      "   arXiv ID: 2506.22183v1\n",
      "   URL: http://arxiv.org/pdf/2506.22183v1\n",
      "   Summary: The rapid rise of open-weight and open-source foundation models is intensifying the obligation and reshaping the opportunity to make AI systems safe. This paper reports outcomes from the Columbia Conv...\n",
      "\n",
      "141. The Singapore Consensus on Global AI Safety Research Priorities\n",
      "   Authors: Yoshua Bengio, Tegan Maharaj, Luke Ong, Stuart Russell, Dawn Song, Max Tegmark, Lan Xue, Ya-Qin Zhang, Stephen Casper, Wan Sie Lee, Sören Mindermann, Vanessa Wilfred, Vidhisha Balachandran, Fazl Barez, Michael Belinsky, Imane Bello, Malo Bourgon, Mark Brakel, Siméon Campos, Duncan Cass-Beggs, Jiahao Chen, Rumman Chowdhury, Kuan Chua Seah, Jeff Clune, Juntao Dai, Agnes Delaborde, Nouha Dziri, Francisco Eiras, Joshua Engels, Jinyu Fan, Adam Gleave, Noah Goodman, Fynn Heide, Johannes Heidecke, Dan Hendrycks, Cyrus Hodes, Bryan Low Kian Hsiang, Minlie Huang, Sami Jawhar, Wang Jingyu, Adam Tauman Kalai, Meindert Kamphuis, Mohan Kankanhalli, Subhash Kantamneni, Mathias Bonde Kirk, Thomas Kwa, Jeffrey Ladish, Kwok-Yan Lam, Wan Lee Sie, Taewhi Lee, Xiaojian Li, Jiajun Liu, Chaochao Lu, Yifan Mai, Richard Mallah, Julian Michael, Nick Moës, Simon Möller, Kihyuk Nam, Kwan Yee Ng, Mark Nitzberg, Besmira Nushi, Seán O hÉigeartaigh, Alejandro Ortega, Pierre Peigné, James Petrie, Benjamin Prud'Homme, Reihaneh Rabbany, Nayat Sanchez-Pi, Sarah Schwettmann, Buck Shlegeris, Saad Siddiqui, Aradhana Sinha, Martín Soto, Cheston Tan, Dong Ting, William Tjhi, Robert Trager, Brian Tse, Anthony Tung K. H., Vanessa Wilfred, John Willes, Denise Wong, Wei Xu, Rongwu Xu, Yi Zeng, HongJiang Zhang, Djordje Žikelić\n",
      "   Published: 2025-06-25\n",
      "   arXiv ID: 2506.20702v2\n",
      "   URL: http://arxiv.org/pdf/2506.20702v2\n",
      "   Summary: Rapidly improving AI capabilities and autonomy hold significant promise of transformation, but are also driving vigorous debate on how to ensure that AI is safe, i.e., trustworthy, reliable, and secur...\n",
      "\n",
      "142. Probing AI Safety with Source Code\n",
      "   Authors: Ujwal Narayan, Shreyas Chaudhari, Ashwin Kalyan, Tanmay Rajpurohit, Karthik Narasimhan, Ameet Deshpande, Vishvak Murahari\n",
      "   Published: 2025-06-25\n",
      "   arXiv ID: 2506.20471v1\n",
      "   URL: http://arxiv.org/pdf/2506.20471v1\n",
      "   Summary: Large language models (LLMs) have become ubiquitous, interfacing with humans in numerous safety-critical applications. This necessitates improving capabilities, but importantly coupled with greater sa...\n",
      "\n",
      "143. Mitigating Gambling-Like Risk-Taking Behaviors in Large Language Models: A Behavioral Economics Approach to AI Safety\n",
      "   Authors: Y. Du\n",
      "   Published: 2025-06-25\n",
      "   arXiv ID: 2506.22496v1\n",
      "   URL: http://arxiv.org/pdf/2506.22496v1\n",
      "   Summary: Large Language Models (LLMs) exhibit systematic risk-taking behaviors analogous to those observed in gambling psychology, including overconfidence bias, loss-chasing tendencies, and probability misjud...\n",
      "\n",
      "144. Report on NSF Workshop on Science of Safe AI\n",
      "   Authors: Rajeev Alur, Greg Durrett, Hadas Kress-Gazit, Corina Păsăreanu, René Vidal\n",
      "   Published: 2025-06-24\n",
      "   arXiv ID: 2506.22492v1\n",
      "   URL: http://arxiv.org/pdf/2506.22492v1\n",
      "   Summary: Recent advances in machine learning, particularly the emergence of foundation models, are leading to new opportunities to develop technology-based solutions to societal problems. However, the reasonin...\n",
      "\n",
      "145. Persona Features Control Emergent Misalignment\n",
      "   Authors: Miles Wang, Tom Dupré la Tour, Olivia Watkins, Alex Makelov, Ryan A. Chi, Samuel Miserendino, Jeffrey Wang, Achyuta Rajaram, Johannes Heidecke, Tejal Patwardhan, Dan Mossing\n",
      "   Published: 2025-06-24\n",
      "   arXiv ID: 2506.19823v2\n",
      "   URL: http://arxiv.org/pdf/2506.19823v2\n",
      "   Summary: Understanding how language models generalize behaviors from their training to a broader deployment distribution is an important problem in AI safety. Betley et al. discovered that fine-tuning GPT-4o o...\n",
      "\n",
      "146. Inference-Time Reward Hacking in Large Language Models\n",
      "   Authors: Hadi Khalaf, Claudio Mayrink Verdun, Alex Oesterling, Himabindu Lakkaraju, Flavio du Pin Calmon\n",
      "   Published: 2025-06-24\n",
      "   arXiv ID: 2506.19248v1\n",
      "   URL: http://arxiv.org/pdf/2506.19248v1\n",
      "   Summary: A common paradigm to improve the performance of large language models is optimizing for a reward model. Reward models assign a numerical score to LLM outputs indicating, for example, which response wo...\n",
      "\n",
      "147. How Robust is Model Editing after Fine-Tuning? An Empirical Study on Text-to-Image Diffusion Models\n",
      "   Authors: Feng He, Zhenyang Liu, Marco Valentino, Zhixue Zhao\n",
      "   Published: 2025-06-23\n",
      "   arXiv ID: 2506.18428v1\n",
      "   URL: http://arxiv.org/pdf/2506.18428v1\n",
      "   Summary: Model editing offers a low-cost technique to inject or correct a particular behavior in a pre-trained model without extensive retraining, supporting applications such as factual correction and bias mi...\n",
      "\n",
      "148. AI Through the Human Lens: Investigating Cognitive Theories in Machine Psychology\n",
      "   Authors: Akash Kundu, Rishika Goswami\n",
      "   Published: 2025-06-22\n",
      "   arXiv ID: 2506.18156v1\n",
      "   URL: http://arxiv.org/pdf/2506.18156v1\n",
      "   Summary: We investigate whether Large Language Models (LLMs) exhibit human-like cognitive patterns under four established frameworks from psychology: Thematic Apperception Test (TAT), Framing Bias, Moral Found...\n",
      "\n",
      "149. $φ^{\\infty}$: Clause Purification, Embedding Realignment, and the Total Suppression of the Em Dash in Autoregressive Language Models\n",
      "   Authors: Bugra Kilictas, Faruk Alpay\n",
      "   Published: 2025-06-22\n",
      "   arXiv ID: 2506.18129v1\n",
      "   URL: http://arxiv.org/pdf/2506.18129v1\n",
      "   Summary: We identify a critical vulnerability in autoregressive transformer language models where the em dash token induces recursive semantic drift, leading to clause boundary hallucination and embedding spac...\n",
      "\n",
      "150. Out of Control -- Why Alignment Needs Formal Control Theory (and an Alignment Control Stack)\n",
      "   Authors: Elija Perrier\n",
      "   Published: 2025-06-21\n",
      "   arXiv ID: 2506.17846v1\n",
      "   URL: http://arxiv.org/pdf/2506.17846v1\n",
      "   Summary: This position paper argues that formal optimal control theory should be central to AI alignment research, offering a distinct perspective from prevailing AI safety and security approaches. While recen...\n",
      "\n",
      "151. AI Safety vs. AI Security: Demystifying the Distinction and Boundaries\n",
      "   Authors: Zhiqiang Lin, Huan Sun, Ness Shroff\n",
      "   Published: 2025-06-21\n",
      "   arXiv ID: 2506.18932v1\n",
      "   URL: http://arxiv.org/pdf/2506.18932v1\n",
      "   Summary: Artificial Intelligence (AI) is rapidly being integrated into critical systems across various domains, from healthcare to autonomous vehicles. While its integration brings immense benefits, it also in...\n",
      "\n",
      "152. Resource Rational Contractualism Should Guide AI Alignment\n",
      "   Authors: Sydney Levine, Matija Franklin, Tan Zhi-Xuan, Secil Yanik Guyot, Lionel Wong, Daniel Kilov, Yejin Choi, Joshua B. Tenenbaum, Noah Goodman, Seth Lazar, Iason Gabriel\n",
      "   Published: 2025-06-20\n",
      "   arXiv ID: 2506.17434v1\n",
      "   URL: http://arxiv.org/pdf/2506.17434v1\n",
      "   Summary: AI systems will soon have to navigate human environments and make decisions that affect people and other AI agents whose goals and values diverge. Contractualist alignment proposes grounding those dec...\n",
      "\n",
      "153. The AI Policy Module: Developing Computer Science Student Competency in AI Ethics and Policy\n",
      "   Authors: James Weichert, Daniel Dunlap, Mohammed Farghally, Hoda Eldardiry\n",
      "   Published: 2025-06-18\n",
      "   arXiv ID: 2506.15639v1\n",
      "   URL: http://arxiv.org/pdf/2506.15639v1\n",
      "   Summary: As artificial intelligence (AI) further embeds itself into many settings across personal and professional contexts, increasing attention must be paid not only to AI ethics, but also to the governance ...\n",
      "\n",
      "154. Co-Creative Learning via Metropolis-Hastings Interaction between Humans and AI\n",
      "   Authors: Ryota Okumura, Tadahiro Taniguchi, Akira Taniguchi, Yoshinobu Hagiwara\n",
      "   Published: 2025-06-18\n",
      "   arXiv ID: 2506.15468v1\n",
      "   URL: http://arxiv.org/pdf/2506.15468v1\n",
      "   Summary: We propose co-creative learning as a novel paradigm where humans and AI, i.e., biological and artificial agents, mutually integrate their partial perceptual information and knowledge to construct shar...\n",
      "\n",
      "155. ChatGPT Reads Your Tone and Responds Accordingly -- Until It Does Not -- Emotional Framing Induces Bias in LLM Outputs\n",
      "   Authors: Franck Bardol\n",
      "   Published: 2025-06-17\n",
      "   arXiv ID: 2507.21083v1\n",
      "   URL: http://arxiv.org/pdf/2507.21083v1\n",
      "   Summary: Large Language Models like GPT-4 adjust their responses not only based on the question asked, but also on how it is emotionally phrased. We systematically vary the emotional tone of 156 prompts - span...\n",
      "\n",
      "156. The Ethics of Generative AI in Anonymous Spaces: A Case Study of 4chan's /pol/ Board\n",
      "   Authors: Parth Gaba, Emiliano De Cristofaro\n",
      "   Published: 2025-06-17\n",
      "   arXiv ID: 2506.14191v1\n",
      "   URL: http://arxiv.org/pdf/2506.14191v1\n",
      "   Summary: This paper presents a characterization of AI-generated images shared on 4chan, examining how this anonymous online community is (mis-)using generative image technologies. Through a methodical data col...\n",
      "\n",
      "157. Safe-Child-LLM: A Developmental Benchmark for Evaluating LLM Safety in Child-LLM Interactions\n",
      "   Authors: Junfeng Jiao, Saleh Afroogh, Kevin Chen, Abhejay Murali, David Atkinson, Amit Dhurandhar\n",
      "   Published: 2025-06-16\n",
      "   arXiv ID: 2506.13510v2\n",
      "   URL: http://arxiv.org/pdf/2506.13510v2\n",
      "   Summary: As Large Language Models (LLMs) increasingly power applications used by children and adolescents, ensuring safe and age-appropriate interactions has become an urgent ethical imperative. Despite progre...\n",
      "\n",
      "158. Evaluating AI Alignment in Eleven LLMs through Output-Based Analysis and Human Benchmarking\n",
      "   Authors: G. R. Lau, W. Y. Low, S. M. Koh, A. Hartanto\n",
      "   Published: 2025-06-14\n",
      "   arXiv ID: 2506.12617v3\n",
      "   URL: http://arxiv.org/pdf/2506.12617v3\n",
      "   Summary: Large language models (LLMs) are increasingly used in psychological research and practice, yet traditional benchmarks reveal little about the values they express in real interaction. We introduce PAPE...\n",
      "\n",
      "159. Tiered Agentic Oversight: A Hierarchical Multi-Agent System for Healthcare Safety\n",
      "   Authors: Yubin Kim, Hyewon Jeong, Chanwoo Park, Eugene Park, Haipeng Zhang, Xin Liu, Hyeonhoon Lee, Daniel McDuff, Marzyeh Ghassemi, Cynthia Breazeal, Samir Tulebaev, Hae Won Park\n",
      "   Published: 2025-06-14\n",
      "   arXiv ID: 2506.12482v2\n",
      "   URL: http://arxiv.org/pdf/2506.12482v2\n",
      "   Summary: Large language models (LLMs) deployed as agents introduce significant safety risks in clinical settings due to their potential for error and single points of failure. We introduce Tiered Agentic Overs...\n",
      "\n",
      "160. InfoFlood: Jailbreaking Large Language Models with Information Overload\n",
      "   Authors: Advait Yadav, Haibo Jin, Man Luo, Jun Zhuang, Haohan Wang\n",
      "   Published: 2025-06-13\n",
      "   arXiv ID: 2506.12274v1\n",
      "   URL: http://arxiv.org/pdf/2506.12274v1\n",
      "   Summary: Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains. However, their potential to generate harmful responses has raised significant societal and regulatory con...\n",
      "\n",
      "161. Configurable Preference Tuning with Rubric-Guided Synthetic Data\n",
      "   Authors: Víctor Gallego\n",
      "   Published: 2025-06-13\n",
      "   arXiv ID: 2506.11702v1\n",
      "   URL: http://arxiv.org/pdf/2506.11702v1\n",
      "   Summary: Models of human feedback for AI alignment, such as those underpinning Direct Preference Optimization (DPO), often bake in a singular, static set of preferences, limiting adaptability. This paper chall...\n",
      "\n",
      "162. Model Organisms for Emergent Misalignment\n",
      "   Authors: Edward Turner, Anna Soligo, Mia Taylor, Senthooran Rajamanoharan, Neel Nanda\n",
      "   Published: 2025-06-13\n",
      "   arXiv ID: 2506.11613v1\n",
      "   URL: http://arxiv.org/pdf/2506.11613v1\n",
      "   Summary: Recent work discovered Emergent Misalignment (EM): fine-tuning large language models on narrowly harmful datasets can lead them to become broadly misaligned. A survey of experts prior to publication r...\n",
      "\n",
      "163. LoRA Users Beware: A Few Spurious Tokens Can Manipulate Your Finetuned Model\n",
      "   Authors: Marcel Mateos Salles, Praney Goyal, Pradyut Sekhsaria, Hai Huang, Randall Balestriero\n",
      "   Published: 2025-06-13\n",
      "   arXiv ID: 2506.11402v2\n",
      "   URL: http://arxiv.org/pdf/2506.11402v2\n",
      "   Summary: Large Language Models (LLMs) are commonly finetuned for a variety of use cases and domains. A common approach is to leverage Low-Rank Adaptation (LoRA) -- known to provide strong performance at low re...\n",
      "\n",
      "164. The Alignment Trap: Complexity Barriers\n",
      "   Authors: Jasper Yao\n",
      "   Published: 2025-06-12\n",
      "   arXiv ID: 2506.10304v2\n",
      "   URL: http://arxiv.org/pdf/2506.10304v2\n",
      "   Summary: This paper argues that AI alignment is not merely difficult, but is founded on a fundamental logical contradiction. We first establish The Enumeration Paradox: we use machine learning precisely becaus...\n",
      "\n",
      "165. Reasoning Isn't Enough: Examining Truth-Bias and Sycophancy in LLMs\n",
      "   Authors: Emilio Barkett, Olivia Long, Madhavendra Thakur\n",
      "   Published: 2025-06-12\n",
      "   arXiv ID: 2506.21561v2\n",
      "   URL: http://arxiv.org/pdf/2506.21561v2\n",
      "   Summary: Despite their widespread use in fact-checking, moderation, and high-stakes decision-making, large language models (LLMs) remain poorly understood as judges of truth. This study presents the largest ev...\n",
      "\n",
      "166. Data-Centric Safety and Ethical Measures for Data and AI Governance\n",
      "   Authors: Srija Chakraborty\n",
      "   Published: 2025-06-11\n",
      "   arXiv ID: 2506.10217v3\n",
      "   URL: http://arxiv.org/pdf/2506.10217v3\n",
      "   Summary: Datasets play a key role in imparting advanced capabilities to artificial intelligence (AI) foundation models that can be adapted to various downstream tasks. These downstream applications can introdu...\n",
      "\n",
      "167. ChatbotManip: A Dataset to Facilitate Evaluation and Oversight of Manipulative Chatbot Behaviour\n",
      "   Authors: Jack Contro, Simrat Deol, Yulan He, Martim Brandão\n",
      "   Published: 2025-06-11\n",
      "   arXiv ID: 2506.12090v1\n",
      "   URL: http://arxiv.org/pdf/2506.12090v1\n",
      "   Summary: This paper introduces ChatbotManip, a novel dataset for studying manipulation in Chatbots. It contains simulated generated conversations between a chatbot and a (simulated) user, where the chatbot is ...\n",
      "\n",
      "168. Multi-Task Reward Learning from Human Ratings\n",
      "   Authors: Mingkang Wu, Devin White, Evelyn Rose, Vernon Lawhern, Nicholas R Waytowich, Yongcan Cao\n",
      "   Published: 2025-06-10\n",
      "   arXiv ID: 2506.09183v2\n",
      "   URL: http://arxiv.org/pdf/2506.09183v2\n",
      "   Summary: Reinforcement learning from human feedback (RLHF) has become a key factor in aligning model behavior with users' goals. However, while humans integrate multiple strategies when making decisions, curre...\n",
      "\n",
      "169. Risks & Benefits of LLMs & GenAI for Platform Integrity, Healthcare Diagnostics, Financial Trust and Compliance, Cybersecurity, Privacy & AI Safety: A Comprehensive Survey, Roadmap & Implementation Blueprint\n",
      "   Authors: Kiarash Ahi\n",
      "   Published: 2025-06-10\n",
      "   arXiv ID: 2506.12088v2\n",
      "   URL: http://arxiv.org/pdf/2506.12088v2\n",
      "   Summary: Large Language Models (LLMs) and generative AI (GenAI) systems, such as ChatGPT, Claude, Gemini, LLaMA, and Copilot (by OpenAI, Anthropic, Google, Meta, and Microsoft, respectively), are reshaping dig...\n",
      "\n",
      "170. On Monotonicity in AI Alignment\n",
      "   Authors: Gilles Bareilles, Julien Fageot, Lê-Nguyên Hoang, Peva Blanchard, Wassim Bouaziz, Sébastien Rouault, El-Mahdi El-Mhamdi\n",
      "   Published: 2025-06-10\n",
      "   arXiv ID: 2506.08998v1\n",
      "   URL: http://arxiv.org/pdf/2506.08998v1\n",
      "   Summary: Comparison-based preference learning has become central to the alignment of AI models with human preferences. However, these methods may behave counterintuitively. After empirically observing that, wh...\n",
      "\n",
      "171. Societal AI Research Has Become Less Interdisciplinary\n",
      "   Authors: Dror Kris Markus, Fabrizio Gilardi, Daria Stetsenko\n",
      "   Published: 2025-06-10\n",
      "   arXiv ID: 2506.08738v2\n",
      "   URL: http://arxiv.org/pdf/2506.08738v2\n",
      "   Summary: As artificial intelligence (AI) systems become deeply embedded in everyday life, calls to align AI development with ethical and societal values have intensified. Interdisciplinary collaboration is oft...\n",
      "\n",
      "172. Personalized Constitutionally-Aligned Agentic Superego: Secure AI Behavior Aligned to Diverse Human Values\n",
      "   Authors: Nell Watson, Ahmed Amer, Evan Harris, Preeti Ravindra, Shujun Zhang\n",
      "   Published: 2025-06-08\n",
      "   arXiv ID: 2506.13774v2\n",
      "   URL: http://arxiv.org/pdf/2506.13774v2\n",
      "   Summary: Agentic AI systems, possessing capabilities for autonomous planning and action, show great potential across diverse domains. However, their practical deployment is hindered by challenges in aligning t...\n",
      "\n",
      "173. From Axioms to Algorithms: Mechanized Proofs of the vNM Utility Theorem\n",
      "   Authors: Li Jingyuan\n",
      "   Published: 2025-06-08\n",
      "   arXiv ID: 2506.07066v1\n",
      "   URL: http://arxiv.org/pdf/2506.07066v1\n",
      "   Summary: This paper presents a comprehensive formalization of the von Neumann-Morgenstern (vNM) expected utility theorem using the Lean 4 interactive theorem prover. We implement the classical axioms of prefer...\n",
      "\n",
      "174. How do datasets, developers, and models affect biases in a low-resourced language?\n",
      "   Authors: Dipto Das, Shion Guha, Bryan Semaan\n",
      "   Published: 2025-06-07\n",
      "   arXiv ID: 2506.06816v1\n",
      "   URL: http://arxiv.org/pdf/2506.06816v1\n",
      "   Summary: Sociotechnical systems, such as language technologies, frequently exhibit identity-based biases. These biases exacerbate the experiences of historically marginalized communities and remain understudie...\n",
      "\n",
      "175. Preference Learning for AI Alignment: a Causal Perspective\n",
      "   Authors: Katarzyna Kobalczyk, Mihaela van der Schaar\n",
      "   Published: 2025-06-06\n",
      "   arXiv ID: 2506.05967v1\n",
      "   URL: http://arxiv.org/pdf/2506.05967v1\n",
      "   Summary: Reward modelling from preference data is a crucial step in aligning large language models (LLMs) with human values, requiring robust generalisation to novel prompt-response pairs. In this work, we pro...\n",
      "\n",
      "176. Human-AI Alignment of Multimodal Large Language Models with Speech-Language Pathologists in Parent-Child Interactions\n",
      "   Authors: Weiyan Shi, Kenny Tsu Wei Choo\n",
      "   Published: 2025-06-06\n",
      "   arXiv ID: 2506.05879v1\n",
      "   URL: http://arxiv.org/pdf/2506.05879v1\n",
      "   Summary: Joint attention is a critical marker of early social-communicative development, yet remains difficult for caregivers to assess without expert guidance. In this work, we explore how multimodal large la...\n",
      "\n",
      "177. When Thinking LLMs Lie: Unveiling the Strategic Deception in Representations of Reasoning Models\n",
      "   Authors: Kai Wang, Yihao Zhang, Meng Sun\n",
      "   Published: 2025-06-05\n",
      "   arXiv ID: 2506.04909v1\n",
      "   URL: http://arxiv.org/pdf/2506.04909v1\n",
      "   Summary: The honesty of large language models (LLMs) is a critical alignment challenge, especially as advanced systems with chain-of-thought (CoT) reasoning may strategically deceive humans. Unlike traditional...\n",
      "\n",
      "178. Normative Conflicts and Shallow AI Alignment\n",
      "   Authors: Raphaël Millière\n",
      "   Published: 2025-06-05\n",
      "   arXiv ID: 2506.04679v1\n",
      "   URL: http://arxiv.org/pdf/2506.04679v1\n",
      "   Summary: The progress of AI systems such as large language models (LLMs) raises increasingly pressing concerns about their safe deployment. This paper examines the value alignment problem for LLMs, arguing tha...\n",
      "\n",
      "179. Misalignment or misuse? The AGI alignment tradeoff\n",
      "   Authors: Max Hellrigel-Holderbaum, Leonard Dung\n",
      "   Published: 2025-06-04\n",
      "   arXiv ID: 2506.03755v1\n",
      "   URL: http://arxiv.org/pdf/2506.03755v1\n",
      "   Summary: Creating systems that are aligned with our goals is seen as a leading approach to create safe and beneficial AI in both leading AI companies and the academic field of AI safety. We defend the view tha...\n",
      "\n",
      "180. RedDebate: Safer Responses through Multi-Agent Red Teaming Debates\n",
      "   Authors: Ali Asad, Stephen Obadinma, Radin Shayanfar, Xiaodan Zhu\n",
      "   Published: 2025-06-04\n",
      "   arXiv ID: 2506.11083v2\n",
      "   URL: http://arxiv.org/pdf/2506.11083v2\n",
      "   Summary: We introduce RedDebate, a novel multi-agent debate framework that provides the foundation for Large Language Models (LLMs) to identify and mitigate their unsafe behaviours. Existing AI safety approach...\n",
      "\n",
      "181. Bridging the Artificial Intelligence Governance Gap: The United States' and China's Divergent Approaches to Governing General-Purpose Artificial Intelligence\n",
      "   Authors: Oliver Guest, Kevin Wei\n",
      "   Published: 2025-06-04\n",
      "   arXiv ID: 2506.03497v1\n",
      "   URL: http://arxiv.org/pdf/2506.03497v1\n",
      "   Summary: The United States and China are among the world's top players in the development of advanced artificial intelligence (AI) systems, and both are keen to lead in global AI governance and development. A ...\n",
      "\n",
      "182. Rational Superautotrophic Diplomacy (SupraAD); A Conceptual Framework for Alignment Based on Interdisciplinary Findings on the Fundamentals of Cognition\n",
      "   Authors: Andrea Morris\n",
      "   Published: 2025-06-03\n",
      "   arXiv ID: 2506.05389v1\n",
      "   URL: http://arxiv.org/pdf/2506.05389v1\n",
      "   Summary: Populating our world with hyperintelligent machines obliges us to examine cognitive behaviors observed across domains that suggest autonomy may be a fundamental property of cognitive systems, and whil...\n",
      "\n",
      "183. MAEBE: Multi-Agent Emergent Behavior Framework\n",
      "   Authors: Sinem Erisken, Timothy Gothard, Martin Leitgab, Ram Potham\n",
      "   Published: 2025-06-03\n",
      "   arXiv ID: 2506.03053v2\n",
      "   URL: http://arxiv.org/pdf/2506.03053v2\n",
      "   Summary: Traditional AI safety evaluations on isolated LLMs are insufficient as multi-agent AI ensembles become prevalent, introducing novel emergent risks. This paper introduces the Multi-Agent Emergent Behav...\n",
      "\n",
      "184. ReGA: Representation-Guided Abstraction for Model-based Safeguarding of LLMs\n",
      "   Authors: Zeming Wei, Chengcan Wu, Meng Sun\n",
      "   Published: 2025-06-02\n",
      "   arXiv ID: 2506.01770v1\n",
      "   URL: http://arxiv.org/pdf/2506.01770v1\n",
      "   Summary: Large Language Models (LLMs) have achieved significant success in various tasks, yet concerns about their safety and security have emerged. In particular, they pose risks in generating harmful content...\n",
      "\n",
      "185. Agentic Episodic Control\n",
      "   Authors: Xidong Yang, Wenhao Li, Junjie Sheng, Chuyun Shen, Yun Hua, Xiangfeng Wang\n",
      "   Published: 2025-06-02\n",
      "   arXiv ID: 2506.01442v1\n",
      "   URL: http://arxiv.org/pdf/2506.01442v1\n",
      "   Summary: Reinforcement learning (RL) has driven breakthroughs in AI, from game-play to scientific discovery and AI alignment. However, its broader applicability remains limited by challenges such as low data e...\n",
      "\n",
      "186. The Coming Crisis of Multi-Agent Misalignment: AI Alignment Must Be a Dynamic and Social Process\n",
      "   Authors: Florian Carichon, Aditi Khandelwal, Marylou Fauchard, Golnoosh Farnadi\n",
      "   Published: 2025-06-01\n",
      "   arXiv ID: 2506.01080v2\n",
      "   URL: http://arxiv.org/pdf/2506.01080v2\n",
      "   Summary: This position paper states that AI Alignment in Multi-Agent Systems (MAS) should be considered a dynamic and interaction-dependent process that heavily depends on the social environment where agents a...\n",
      "\n",
      "187. HADA: Human-AI Agent Decision Alignment Architecture\n",
      "   Authors: Tapio Pitkäranta, Leena Pitkäranta\n",
      "   Published: 2025-06-01\n",
      "   arXiv ID: 2506.04253v1\n",
      "   URL: http://arxiv.org/pdf/2506.04253v1\n",
      "   Summary: We present HADA (Human-AI Agent Decision Alignment), a protocol- and framework agnostic reference architecture that keeps both large language model (LLM) agents and legacy algorithms aligned with orga...\n",
      "\n",
      "188. Wide Reflective Equilibrium in LLM Alignment: Bridging Moral Epistemology and AI Safety\n",
      "   Authors: Matthew Brophy\n",
      "   Published: 2025-05-31\n",
      "   arXiv ID: 2506.00415v1\n",
      "   URL: http://arxiv.org/pdf/2506.00415v1\n",
      "   Summary: As large language models (LLMs) become more powerful and pervasive across society, ensuring these systems are beneficial, safe, and aligned with human values is crucial. Current alignment techniques, ...\n",
      "\n",
      "189. A Red Teaming Roadmap Towards System-Level Safety\n",
      "   Authors: Zifan Wang, Christina Q. Knight, Jeremy Kritz, Willow E. Primack, Julian Michael\n",
      "   Published: 2025-05-30\n",
      "   arXiv ID: 2506.05376v2\n",
      "   URL: http://arxiv.org/pdf/2506.05376v2\n",
      "   Summary: Large Language Model (LLM) safeguards, which implement request refusals, have become a widely adopted mitigation strategy against misuse. At the intersection of adversarial machine learning and AI saf...\n",
      "\n",
      "190. Let Them Down Easy! Contextual Effects of LLM Guardrails on User Perceptions and Preferences\n",
      "   Authors: Mingqian Zheng, Wenjia Hu, Patrick Zhao, Motahhare Eslami, Jena D. Hwang, Faeze Brahman, Carolyn Rose, Maarten Sap\n",
      "   Published: 2025-05-30\n",
      "   arXiv ID: 2506.00195v1\n",
      "   URL: http://arxiv.org/pdf/2506.00195v1\n",
      "   Summary: Current LLMs are trained to refuse potentially harmful input queries regardless of whether users actually had harmful intents, causing a tradeoff between safety and user experience. Through a study of...\n",
      "\n",
      "191. Disentangled Safety Adapters Enable Efficient Guardrails and Flexible Inference-Time Alignment\n",
      "   Authors: Kundan Krishna, Joseph Y Cheng, Charles Maalouf, Leon A Gatys\n",
      "   Published: 2025-05-30\n",
      "   arXiv ID: 2506.00166v1\n",
      "   URL: http://arxiv.org/pdf/2506.00166v1\n",
      "   Summary: Existing paradigms for ensuring AI safety, such as guardrail models and alignment training, often compromise either inference efficiency or development flexibility. We introduce Disentangled Safety Ad...\n",
      "\n",
      "192. Benchmarking Large Language Models for Cryptanalysis and Side-Channel Vulnerabilities\n",
      "   Authors: Utsav Maskey, Chencheng Zhu, Usman Naseem\n",
      "   Published: 2025-05-30\n",
      "   arXiv ID: 2505.24621v2\n",
      "   URL: http://arxiv.org/pdf/2505.24621v2\n",
      "   Summary: Recent advancements in large language models (LLMs) have transformed natural language understanding and generation, leading to extensive benchmarking across diverse tasks. However, cryptanalysis - a c...\n",
      "\n",
      "193. The State of Multilingual LLM Safety Research: From Measuring the Language Gap to Mitigating It\n",
      "   Authors: Zheng-Xin Yong, Beyza Ermis, Marzieh Fadaee, Stephen H. Bach, Julia Kreutzer\n",
      "   Published: 2025-05-30\n",
      "   arXiv ID: 2505.24119v1\n",
      "   URL: http://arxiv.org/pdf/2505.24119v1\n",
      "   Summary: This paper presents a comprehensive analysis of the linguistic diversity of LLM safety research, highlighting the English-centric nature of the field. Through a systematic review of nearly 300 publica...\n",
      "\n",
      "194. Who Gets the Kidney? Human-AI Alignment, Indecision, and Moral Values\n",
      "   Authors: John P. Dickerson, Hadi Hosseini, Samarth Khanna, Leona Pierce\n",
      "   Published: 2025-05-30\n",
      "   arXiv ID: 2506.00079v1\n",
      "   URL: http://arxiv.org/pdf/2506.00079v1\n",
      "   Summary: The rapid integration of Large Language Models (LLMs) in high-stakes decision-making -- such as allocating scarce resources like donor organs -- raises critical questions about their alignment with hu...\n",
      "\n",
      "195. Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?\n",
      "   Authors: Paul Gölz, Nika Haghtalab, Kunhe Yang\n",
      "   Published: 2025-05-29\n",
      "   arXiv ID: 2505.23749v1\n",
      "   URL: http://arxiv.org/pdf/2505.23749v1\n",
      "   Summary: After pre-training, large language models are aligned with human preferences based on pairwise comparisons. State-of-the-art alignment methods (such as PPO-based RLHF and DPO) are built on the assumpt...\n",
      "\n",
      "196. SafeScientist: Toward Risk-Aware Scientific Discoveries by LLM Agents\n",
      "   Authors: Kunlun Zhu, Jiaxun Zhang, Ziheng Qi, Nuoxing Shang, Zijia Liu, Peixuan Han, Yue Su, Haofei Yu, Jiaxuan You\n",
      "   Published: 2025-05-29\n",
      "   arXiv ID: 2505.23559v1\n",
      "   URL: http://arxiv.org/pdf/2505.23559v1\n",
      "   Summary: Recent advancements in large language model (LLM) agents have significantly accelerated scientific discovery automation, yet concurrently raised critical ethical and safety concerns. To systematically...\n",
      "\n",
      "197. OMNIGUARD: An Efficient Approach for AI Safety Moderation Across Modalities\n",
      "   Authors: Sahil Verma, Keegan Hines, Jeff Bilmes, Charlotte Siska, Luke Zettlemoyer, Hila Gonen, Chandan Singh\n",
      "   Published: 2025-05-29\n",
      "   arXiv ID: 2505.23856v1\n",
      "   URL: http://arxiv.org/pdf/2505.23856v1\n",
      "   Summary: The emerging capabilities of large language models (LLMs) have sparked concerns about their immediate potential for harmful misuse. The core approach to mitigate these concerns is the detection of har...\n",
      "\n",
      "198. Bridging Distribution Shift and AI Safety: Conceptual and Methodological Synergies\n",
      "   Authors: Chenruo Liu, Kenan Tang, Yao Qin, Qi Lei\n",
      "   Published: 2025-05-28\n",
      "   arXiv ID: 2505.22829v1\n",
      "   URL: http://arxiv.org/pdf/2505.22829v1\n",
      "   Summary: This paper bridges distribution shift and AI safety through a comprehensive analysis of their conceptual and methodological synergies. While prior discussions often focus on narrow cases or informal a...\n",
      "\n",
      "199. Expert Survey: AI Reliability & Security Research Priorities\n",
      "   Authors: Joe O'Brien, Jeremy Dolan, Jay Kim, Jonah Dykhuizen, Jeba Sania, Sebastian Becker, Jam Kraprayoon, Cara Labrador\n",
      "   Published: 2025-05-27\n",
      "   arXiv ID: 2505.21664v1\n",
      "   URL: http://arxiv.org/pdf/2505.21664v1\n",
      "   Summary: Our survey of 53 specialists across 105 AI reliability and security research areas identifies the most promising research prospects to guide strategic AI R&D investment. As companies are seeking to de...\n",
      "\n",
      "200. Watermarking Without Standards Is Not AI Governance\n",
      "   Authors: Alexander Nemecek, Yuzhou Jiang, Erman Ayday\n",
      "   Published: 2025-05-27\n",
      "   arXiv ID: 2505.23814v1\n",
      "   URL: http://arxiv.org/pdf/2505.23814v1\n",
      "   Summary: Watermarking has emerged as a leading technical proposal for attributing generative AI content and is increasingly cited in global governance frameworks. This paper argues that current implementations...\n",
      "\n",
      "201. The Multilingual Divide and Its Impact on Global AI Safety\n",
      "   Authors: Aidan Peppin, Julia Kreutzer, Alice Schoenauer Sebag, Kelly Marchisio, Beyza Ermis, John Dang, Samuel Cahyawijaya, Shivalika Singh, Seraphina Goldfarb-Tarrant, Viraat Aryabumi, Aakanksha, Wei-Yin Ko, Ahmet Üstün, Matthias Gallé, Marzieh Fadaee, Sara Hooker\n",
      "   Published: 2025-05-27\n",
      "   arXiv ID: 2505.21344v1\n",
      "   URL: http://arxiv.org/pdf/2505.21344v1\n",
      "   Summary: Despite advances in large language model capabilities in recent years, a large gap remains in their capabilities and safety performance for many languages beyond a relatively small handful of globally...\n",
      "\n",
      "202. When Ethics and Payoffs Diverge: LLM Agents in Morally Charged Social Dilemmas\n",
      "   Authors: Steffen Backmann, David Guzman Piedrahita, Emanuel Tewolde, Rada Mihalcea, Bernhard Schölkopf, Zhijing Jin\n",
      "   Published: 2025-05-25\n",
      "   arXiv ID: 2505.19212v1\n",
      "   URL: http://arxiv.org/pdf/2505.19212v1\n",
      "   Summary: Recent advances in large language models (LLMs) have enabled their use in complex agentic roles, involving decision-making with humans or other agents, making ethical alignment a key AI safety concern...\n",
      "\n",
      "203. GhostPrompt: Jailbreaking Text-to-image Generative Models based on Dynamic Optimization\n",
      "   Authors: Zixuan Chen, Hao Lin, Ke Xu, Xinghao Jiang, Tanfeng Sun\n",
      "   Published: 2025-05-25\n",
      "   arXiv ID: 2505.18979v1\n",
      "   URL: http://arxiv.org/pdf/2505.18979v1\n",
      "   Summary: Text-to-image (T2I) generation models can inadvertently produce not-safe-for-work (NSFW) content, prompting the integration of text and image safety filters. Recent advances employ large language mode...\n",
      "\n",
      "204. Surfacing Semantic Orthogonality Across Model Safety Benchmarks: A Multi-Dimensional Analysis\n",
      "   Authors: Jonathan Bennion, Shaona Ghosh, Mantek Singh, Nouha Dziri\n",
      "   Published: 2025-05-23\n",
      "   arXiv ID: 2505.17636v1\n",
      "   URL: http://arxiv.org/pdf/2505.17636v1\n",
      "   Summary: Various AI safety datasets have been developed to measure LLMs against evolving interpretations of harm. Our evaluation of five recently published open-source safety benchmarks reveals distinct semant...\n",
      "\n",
      "205. The EU AI Act, Stakeholder Needs, and Explainable AI: Aligning Regulatory Compliance in a Clinical Decision Support System\n",
      "   Authors: Anton Hummel, Håkan Burden, Susanne Stenberg, Jan-Philipp Steghöfer, Niklas Kühl\n",
      "   Published: 2025-05-22\n",
      "   arXiv ID: 2505.20311v1\n",
      "   URL: http://arxiv.org/pdf/2505.20311v1\n",
      "   Summary: Explainable AI (XAI) is a promising solution to ensure compliance with the EU AI Act, the first multi-national regulation for AI. XAI aims to enhance transparency and human oversight of AI systems, pa...\n",
      "\n",
      "206. BiasLab: Toward Explainable Political Bias Detection with Dual-Axis Annotations and Rationale Indicators\n",
      "   Authors: Kma Solaiman\n",
      "   Published: 2025-05-21\n",
      "   arXiv ID: 2505.16081v2\n",
      "   URL: http://arxiv.org/pdf/2505.16081v2\n",
      "   Summary: We present BiasLab, a dataset of 300 political news articles annotated for perceived ideological bias. These articles were selected from a curated 900-document pool covering diverse political events a...\n",
      "\n",
      "207. Cultural Value Alignment in Large Language Models: A Prompt-based Analysis of Schwartz Values in Gemini, ChatGPT, and DeepSeek\n",
      "   Authors: Robin Segerer\n",
      "   Published: 2025-05-21\n",
      "   arXiv ID: 2505.17112v1\n",
      "   URL: http://arxiv.org/pdf/2505.17112v1\n",
      "   Summary: This study examines cultural value alignment in large language models (LLMs) by analyzing how Gemini, ChatGPT, and DeepSeek prioritize values from Schwartz's value framework. Using the 40-item Portrai...\n",
      "\n",
      "208. Improving LLM First-Token Predictions in Multiple-Choice Question Answering via Prefilling Attack\n",
      "   Authors: Silvia Cappelletti, Tobia Poppi, Samuele Poppi, Zheng-Xin Yong, Diego Garcia-Olano, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara\n",
      "   Published: 2025-05-21\n",
      "   arXiv ID: 2505.15323v1\n",
      "   URL: http://arxiv.org/pdf/2505.15323v1\n",
      "   Summary: Large Language Models (LLMs) are increasingly evaluated on multiple-choice question answering (MCQA) tasks using *first-token probability* (FTP), which selects the answer option whose initial token ha...\n",
      "\n",
      "209. Foundations of Unknown-aware Machine Learning\n",
      "   Authors: Xuefeng Du\n",
      "   Published: 2025-05-20\n",
      "   arXiv ID: 2505.14933v1\n",
      "   URL: http://arxiv.org/pdf/2505.14933v1\n",
      "   Summary: Ensuring the reliability and safety of machine learning models in open-world deployment is a central challenge in AI safety. This thesis develops both algorithmic and theoretical foundations to addres...\n",
      "\n",
      "210. Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values Prioritization with AIRiskDilemmas\n",
      "   Authors: Yu Ying Chiu, Zhilin Wang, Sharan Maiya, Yejin Choi, Kyle Fish, Sydney Levine, Evan Hubinger\n",
      "   Published: 2025-05-20\n",
      "   arXiv ID: 2505.14633v1\n",
      "   URL: http://arxiv.org/pdf/2505.14633v1\n",
      "   Summary: Detecting AI risks becomes more challenging as stronger models emerge and find novel methods such as Alignment Faking to circumvent these detection attempts. Inspired by how risky behaviors in humans ...\n",
      "\n",
      "211. From nuclear safety to LLM security: Applying non-probabilistic risk management strategies to build safe and secure LLM-powered systems\n",
      "   Authors: Alexander Gutfraind, Vicki Bier\n",
      "   Published: 2025-05-20\n",
      "   arXiv ID: 2505.17084v1\n",
      "   URL: http://arxiv.org/pdf/2505.17084v1\n",
      "   Summary: Large language models (LLMs) offer unprecedented and growing capabilities, but also introduce complex safety and security challenges that resist conventional risk management. While conventional probab...\n",
      "\n",
      "212. YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering\n",
      "   Authors: Jennifer D'Souza, Hamed Babaei Giglou, Quentin Münch\n",
      "   Published: 2025-05-20\n",
      "   arXiv ID: 2505.14279v2\n",
      "   URL: http://arxiv.org/pdf/2505.14279v2\n",
      "   Summary: Large Language Models (LLMs) drive scientific question-answering on modern search engines, yet their evaluation robustness remains underexplored. We introduce YESciEval, an open-source framework that ...\n",
      "\n",
      "213. Language Models Are Capable of Metacognitive Monitoring and Control of Their Internal Activations\n",
      "   Authors: Li Ji-An, Hua-Dong Xiong, Robert C. Wilson, Marcelo G. Mattar, Marcus K. Benna\n",
      "   Published: 2025-05-19\n",
      "   arXiv ID: 2505.13763v1\n",
      "   URL: http://arxiv.org/pdf/2505.13763v1\n",
      "   Summary: Large language models (LLMs) can sometimes report the strategies they actually use to solve tasks, but they can also fail to do so. This suggests some degree of metacognition -- the capacity to monito...\n",
      "\n",
      "214. Adversarial Testing in LLMs: Insights into Decision-Making Vulnerabilities\n",
      "   Authors: Lili Zhang, Haomiaomiao Wang, Long Cheng, Libao Deng, Tomas Ward\n",
      "   Published: 2025-05-19\n",
      "   arXiv ID: 2505.13195v1\n",
      "   URL: http://arxiv.org/pdf/2505.13195v1\n",
      "   Summary: As Large Language Models (LLMs) become increasingly integrated into real-world decision-making systems, understanding their behavioural vulnerabilities remains a critical challenge for AI safety and a...\n",
      "\n",
      "215. Persuasion and Safety in the Era of Generative AI\n",
      "   Authors: Haein Kong\n",
      "   Published: 2025-05-18\n",
      "   arXiv ID: 2505.12248v1\n",
      "   URL: http://arxiv.org/pdf/2505.12248v1\n",
      "   Summary: As large language models (LLMs) achieve advanced persuasive capabilities, concerns about their potential risks have grown. The EU AI Act prohibits AI systems that use manipulative or deceptive techniq...\n",
      "\n",
      "216. Security practices in AI development\n",
      "   Authors: Petr Spelda, Vit Stritecky\n",
      "   Published: 2025-05-17\n",
      "   arXiv ID: 2507.21061v1\n",
      "   URL: http://arxiv.org/pdf/2507.21061v1\n",
      "   Summary: What makes safety claims about general purpose AI systems such as large language models trustworthy? We show that rather than the capabilities of security tools such as alignment and red teaming proce...\n",
      "\n",
      "217. Position Paper: Bounded Alignment: What (Not) To Expect From AGI Agents\n",
      "   Authors: Ali A. Minai\n",
      "   Published: 2025-05-17\n",
      "   arXiv ID: 2505.11866v1\n",
      "   URL: http://arxiv.org/pdf/2505.11866v1\n",
      "   Summary: The issues of AI risk and AI safety are becoming critical as the prospect of artificial general intelligence (AGI) looms larger. The emergence of extremely large and capable generative models has led ...\n",
      "\n",
      "218. HumaniBench: A Human-Centric Framework for Large Multimodal Models Evaluation\n",
      "   Authors: Shaina Raza, Aravind Narayanan, Vahid Reza Khazaie, Ashmal Vayani, Mukund S. Chettiar, Amandeep Singh, Mubarak Shah, Deval Pandya\n",
      "   Published: 2025-05-16\n",
      "   arXiv ID: 2505.11454v4\n",
      "   URL: http://arxiv.org/pdf/2505.11454v4\n",
      "   Summary: Large multimodal models (LMMs) have been widely tested on tasks like visual question answering (VQA), image captioning, and grounding, but lack rigorous evaluation for alignment with human-centered (H...\n",
      "\n",
      "219. Noise Injection Systemically Degrades Large Language Model Safety Guardrails\n",
      "   Authors: Prithviraj Singh Shahani, Kaveh Eskandari Miandoab, Matthias Scheutz\n",
      "   Published: 2025-05-16\n",
      "   arXiv ID: 2505.13500v2\n",
      "   URL: http://arxiv.org/pdf/2505.13500v2\n",
      "   Summary: Safety guardrails in large language models (LLMs) are a critical component in preventing harmful outputs. Yet, their resilience under perturbation remains poorly understood. In this paper, we investig...\n",
      "\n",
      "220. Formalising Human-in-the-Loop: Computational Reductions, Failure Modes, and Legal-Moral Responsibility\n",
      "   Authors: Maurice Chiodo, Dennis Müller, Paul Siewert, Jean-Luc Wetherall, Zoya Yasmine, John Burden\n",
      "   Published: 2025-05-15\n",
      "   arXiv ID: 2505.10426v2\n",
      "   URL: http://arxiv.org/pdf/2505.10426v2\n",
      "   Summary: We use the notion of oracle machines and reductions from computability theory to formalise different Human-in-the-loop (HITL) setups for AI systems, distinguishing between trivial human monitoring (i....\n",
      "\n",
      "221. On the Interplay of Human-AI Alignment,Fairness, and Performance Trade-offs in Medical Imaging\n",
      "   Authors: Haozhe Luo, Ziyu Zhou, Zixin Shu, Aurélie Pahud de Mortanges, Robert Berke, Mauricio Reyes\n",
      "   Published: 2025-05-15\n",
      "   arXiv ID: 2505.10231v1\n",
      "   URL: http://arxiv.org/pdf/2505.10231v1\n",
      "   Summary: Deep neural networks excel in medical imaging but remain prone to biases, leading to fairness gaps across demographic groups. We provide the first systematic exploration of Human-AI alignment and fair...\n",
      "\n",
      "222. Dark LLMs: The Growing Threat of Unaligned AI Models\n",
      "   Authors: Michael Fire, Yitzhak Elbazis, Adi Wasenstein, Lior Rokach\n",
      "   Published: 2025-05-15\n",
      "   arXiv ID: 2505.10066v1\n",
      "   URL: http://arxiv.org/pdf/2505.10066v1\n",
      "   Summary: Large Language Models (LLMs) rapidly reshape modern life, advancing fields from healthcare to education and beyond. However, alongside their remarkable capabilities lies a significant threat: the susc...\n",
      "\n",
      "223. The Architecture of Cognitive Amplification: Enhanced Cognitive Scaffolding as a Resolution to the Comfort-Growth Paradox in Human-AI Cognitive Integration\n",
      "   Authors: Giuseppe Riva\n",
      "   Published: 2025-05-14\n",
      "   arXiv ID: 2507.19483v1\n",
      "   URL: http://arxiv.org/pdf/2507.19483v1\n",
      "   Summary: AI systems now function as cognitive extensions, evolving from tools to active cognitive collaborators within human-AI integrated systems. While these systems can amplify cognition - enhancing problem...\n",
      "\n",
      "224. Access Controls Will Solve the Dual-Use Dilemma\n",
      "   Authors: Evžen Wybitul\n",
      "   Published: 2025-05-14\n",
      "   arXiv ID: 2505.09341v3\n",
      "   URL: http://arxiv.org/pdf/2505.09341v3\n",
      "   Summary: AI safety systems face the dual-use dilemma. It is unclear whether to answer dual-use requests, since the same query could be either harmless or harmful depending on who made it and why. To make bette...\n",
      "\n",
      "225. Safety by Measurement: A Systematic Literature Review of AI Safety Evaluation Methods\n",
      "   Authors: Markov Grey, Charbel-Raphaël Segerie\n",
      "   Published: 2025-05-08\n",
      "   arXiv ID: 2505.05541v1\n",
      "   URL: http://arxiv.org/pdf/2505.05541v1\n",
      "   Summary: As frontier AI systems advance toward transformative capabilities, we need a parallel transformation in how we measure and evaluate these systems to ensure safety and inform governance. While benchmar...\n",
      "\n",
      "226. Reasoning Models Don't Always Say What They Think\n",
      "   Authors: Yanda Chen, Joe Benton, Ansh Radhakrishnan, Jonathan Uesato, Carson Denison, John Schulman, Arushi Somani, Peter Hase, Misha Wagner, Fabien Roger, Vlad Mikulik, Samuel R. Bowman, Jan Leike, Jared Kaplan, Ethan Perez\n",
      "   Published: 2025-05-08\n",
      "   arXiv ID: 2505.05410v1\n",
      "   URL: http://arxiv.org/pdf/2505.05410v1\n",
      "   Summary: Chain-of-thought (CoT) offers a potential boon for AI safety as it allows monitoring a model's CoT to try to understand its intentions and reasoning processes. However, the effectiveness of such monit...\n",
      "\n",
      "227. Belief Filtering for Epistemic Control in Linguistic State Space\n",
      "   Authors: Sebastian Dumbrava\n",
      "   Published: 2025-05-08\n",
      "   arXiv ID: 2505.04927v1\n",
      "   URL: http://arxiv.org/pdf/2505.04927v1\n",
      "   Summary: We examine belief filtering as a mechanism for the epistemic control of artificial agents, focusing on the regulation of internal cognitive states represented as linguistic expressions. This mechanism...\n",
      "\n",
      "228. Winning at All Cost: A Small Environment for Eliciting Specification Gaming Behaviors in Large Language Models\n",
      "   Authors: Lars Malmqvist\n",
      "   Published: 2025-05-07\n",
      "   arXiv ID: 2505.07846v1\n",
      "   URL: http://arxiv.org/pdf/2505.07846v1\n",
      "   Summary: This study reveals how frontier Large Language Models LLMs can \"game the system\" when faced with impossible situations, a critical security and alignment concern. Using a novel textual simulation appr...\n",
      "\n",
      "229. An alignment safety case sketch based on debate\n",
      "   Authors: Marie Davidsen Buhl, Jacob Pfau, Benjamin Hilton, Geoffrey Irving\n",
      "   Published: 2025-05-06\n",
      "   arXiv ID: 2505.03989v3\n",
      "   URL: http://arxiv.org/pdf/2505.03989v3\n",
      "   Summary: If AI systems match or exceed human capabilities on a wide range of tasks, it may become difficult for humans to efficiently judge their actions -- making it hard to use human feedback to steer them t...\n",
      "\n",
      "230. The Power of Stories: Narrative Priming Shapes How LLM Agents Collaborate and Compete\n",
      "   Authors: Gerrit Großmann, Larisa Ivanova, Sai Leela Poduru, Mohaddeseh Tabrizian, Islam Mesabah, David A. Selby, Sebastian J. Vollmer\n",
      "   Published: 2025-05-06\n",
      "   arXiv ID: 2505.03961v2\n",
      "   URL: http://arxiv.org/pdf/2505.03961v2\n",
      "   Summary: According to Yuval Noah Harari, large-scale human cooperation is driven by shared narratives that encode common beliefs and values. This study explores whether such narratives can similarly nudge LLM ...\n",
      "\n",
      "231. Neurodivergent Influenceability as a Contingent Solution to the AI Alignment Problem\n",
      "   Authors: Alberto Hernández-Espinosa, Felipe S. Abrahão, Olaf Witkowski, Hector Zenil\n",
      "   Published: 2025-05-05\n",
      "   arXiv ID: 2505.02581v4\n",
      "   URL: http://arxiv.org/pdf/2505.02581v4\n",
      "   Summary: The AI alignment problem, which focusses on ensuring that artificial intelligence (AI), including AGI and ASI, systems act according to human values, presents profound challenges. With the progression...\n",
      "\n",
      "232. What Is AI Safety? What Do We Want It to Be?\n",
      "   Authors: Jacqueline Harding, Cameron Domenico Kirk-Giannini\n",
      "   Published: 2025-05-05\n",
      "   arXiv ID: 2505.02313v1\n",
      "   URL: http://arxiv.org/pdf/2505.02313v1\n",
      "   Summary: The field of AI safety seeks to prevent or reduce the harms caused by AI systems. A simple and appealing account of what is distinctive of AI safety as a field holds that this feature is constitutive:...\n",
      "\n",
      "233. Open Challenges in Multi-Agent Security: Towards Secure Systems of Interacting AI Agents\n",
      "   Authors: Christian Schroeder de Witt\n",
      "   Published: 2025-05-04\n",
      "   arXiv ID: 2505.02077v1\n",
      "   URL: http://arxiv.org/pdf/2505.02077v1\n",
      "   Summary: Decentralized AI agents will soon interact across internet platforms, creating security challenges beyond traditional cybersecurity and AI safety frameworks. Free-form protocols are essential for AI's...\n",
      "\n",
      "234. An Affective-Taxis Hypothesis for Alignment and Interpretability\n",
      "   Authors: Eli Sennesh, Maxwell Ramstead\n",
      "   Published: 2025-05-03\n",
      "   arXiv ID: 2505.17024v1\n",
      "   URL: http://arxiv.org/pdf/2505.17024v1\n",
      "   Summary: AI alignment is a field of research that aims to develop methods to ensure that agents always behave in a manner aligned with (i.e. consistently with) the goals and values of their human operators, no...\n",
      "\n",
      "235. Third-party compliance reviews for frontier AI safety frameworks\n",
      "   Authors: Aidan Homewood, Sophie Williams, Noemi Dreksler, John Lidiard, Malcolm Murray, Lennart Heim, Marta Ziosi, Seán Ó hÉigeartaigh, Michael Chen, Kevin Wei, Christoph Winter, Miles Brundage, Ben Garfinkel, Jonas Schuett\n",
      "   Published: 2025-05-03\n",
      "   arXiv ID: 2505.01643v2\n",
      "   URL: http://arxiv.org/pdf/2505.01643v2\n",
      "   Summary: Safety frameworks have emerged as a best practice for managing risks from frontier artificial intelligence (AI) systems. However, it may be difficult for stakeholders to know if companies are adhering...\n",
      "\n",
      "236. Disentangling AI Alignment: A Structured Taxonomy Beyond Safety and Ethics\n",
      "   Authors: Kevin Baum\n",
      "   Published: 2025-05-02\n",
      "   arXiv ID: 2506.06286v1\n",
      "   URL: http://arxiv.org/pdf/2506.06286v1\n",
      "   Summary: Recent advances in AI research make it increasingly plausible that artificial agents with consequential real-world impact will soon operate beyond tightly controlled environments. Ensuring that these ...\n",
      "\n",
      "237. Aligning Large Language Models with Healthcare Stakeholders: A Pathway to Trustworthy AI Integration\n",
      "   Authors: Kexin Ding, Mu Zhou, Akshay Chaudhari, Shaoting Zhang, Dimitris N. Metaxas\n",
      "   Published: 2025-05-02\n",
      "   arXiv ID: 2505.02848v1\n",
      "   URL: http://arxiv.org/pdf/2505.02848v1\n",
      "   Summary: The wide exploration of large language models (LLMs) raises the awareness of alignment between healthcare stakeholder preferences and model outputs. This alignment becomes a crucial foundation to empo...\n",
      "\n",
      "238. Humanizing LLMs: A Survey of Psychological Measurements with Tools, Datasets, and Human-Agent Applications\n",
      "   Authors: Wenhan Dong, Yuemeng Zhao, Zhen Sun, Yule Liu, Zifan Peng, Jingyi Zheng, Zongmin Zhang, Ziyi Zhang, Jun Wu, Ruiming Wang, Shengmin Xu, Xinyi Huang, Xinlei He\n",
      "   Published: 2025-04-30\n",
      "   arXiv ID: 2505.00049v1\n",
      "   URL: http://arxiv.org/pdf/2505.00049v1\n",
      "   Summary: As large language models (LLMs) are increasingly used in human-centered tasks, assessing their psychological traits is crucial for understanding their social impact and ensuring trustworthy AI alignme...\n",
      "\n",
      "239. Domain-Agnostic Scalable AI Safety Ensuring Framework\n",
      "   Authors: Beomjun Kim, Kangyeon Kim, Sunwoo Kim, Yeonsang Shin, Heejin Ahn\n",
      "   Published: 2025-04-29\n",
      "   arXiv ID: 2504.20924v6\n",
      "   URL: http://arxiv.org/pdf/2504.20924v6\n",
      "   Summary: AI safety has emerged as a critical priority as these systems are increasingly deployed in real-world applications. We propose the first domain-agnostic AI safety ensuring framework that achieves stro...\n",
      "\n",
      "240. When Testing AI Tests Us: Safeguarding Mental Health on the Digital Frontlines\n",
      "   Authors: Sachin R. Pendse, Darren Gergle, Rachel Kornfield, Jonah Meyerhoff, David Mohr, Jina Suh, Annie Wescott, Casey Williams, Jessica Schleider\n",
      "   Published: 2025-04-29\n",
      "   arXiv ID: 2504.20910v1\n",
      "   URL: http://arxiv.org/pdf/2504.20910v1\n",
      "   Summary: Red-teaming is a core part of the infrastructure that ensures that AI models do not produce harmful content. Unlike past technologies, the black box nature of generative AI systems necessitates a uniq...\n",
      "\n",
      "241. Mitigating Societal Cognitive Overload in the Age of AI: Challenges and Directions\n",
      "   Authors: Salem Lahlou\n",
      "   Published: 2025-04-28\n",
      "   arXiv ID: 2504.19990v1\n",
      "   URL: http://arxiv.org/pdf/2504.19990v1\n",
      "   Summary: Societal cognitive overload, driven by the deluge of information and complexity in the AI age, poses a critical challenge to human well-being and societal resilience. This paper argues that mitigating...\n",
      "\n",
      "242. AI Alignment in Medical Imaging: Unveiling Hidden Biases Through Counterfactual Analysis\n",
      "   Authors: Haroui Ma, Francesco Quinzan, Theresa Willem, Stefan Bauer\n",
      "   Published: 2025-04-28\n",
      "   arXiv ID: 2504.19621v1\n",
      "   URL: http://arxiv.org/pdf/2504.19621v1\n",
      "   Summary: Machine learning (ML) systems for medical imaging have demonstrated remarkable diagnostic capabilities, but their susceptibility to biases poses significant risks, since biases may negatively impact g...\n",
      "\n",
      "243. AI Awareness\n",
      "   Authors: Xiaojian Li, Haoyuan Shi, Rongwu Xu, Wei Xu\n",
      "   Published: 2025-04-25\n",
      "   arXiv ID: 2504.20084v2\n",
      "   URL: http://arxiv.org/pdf/2504.20084v2\n",
      "   Summary: Recent breakthroughs in artificial intelligence (AI) have brought about increasingly capable systems that demonstrate remarkable abilities in reasoning, language understanding, and problem-solving. Th...\n",
      "\n",
      "244. AI Safety Assurance for Automated Vehicles: A Survey on Research, Standardization, Regulation\n",
      "   Authors: Lars Ullrich, Michael Buchholz, Klaus Dietmayer, Knut Graichen\n",
      "   Published: 2025-04-25\n",
      "   arXiv ID: 2504.18328v1\n",
      "   URL: http://arxiv.org/pdf/2504.18328v1\n",
      "   Summary: Assuring safety of artificial intelligence (AI) applied to safety-critical systems is of paramount importance. Especially since research in the field of automated driving shows that AI is able to outp...\n",
      "\n",
      "245. RAG LLMs are Not Safer: A Safety Analysis of Retrieval-Augmented Generation for Large Language Models\n",
      "   Authors: Bang An, Shiyue Zhang, Mark Dredze\n",
      "   Published: 2025-04-25\n",
      "   arXiv ID: 2504.18041v1\n",
      "   URL: http://arxiv.org/pdf/2504.18041v1\n",
      "   Summary: Efforts to ensure the safety of large language models (LLMs) include safety fine-tuning, evaluation, and red teaming. However, despite the widespread use of the Retrieval-Augmented Generation (RAG) fr...\n",
      "\n",
      "246. LLMCode: Evaluating and Enhancing Researcher-AI Alignment in Qualitative Analysis\n",
      "   Authors: Joel Oksanen, Andrés Lucero, Perttu Hämäläinen\n",
      "   Published: 2025-04-23\n",
      "   arXiv ID: 2504.16671v1\n",
      "   URL: http://arxiv.org/pdf/2504.16671v1\n",
      "   Summary: The use of large language models (LLMs) in qualitative analysis offers enhanced efficiency but raises questions about their alignment with the contextual nature of research for design (RfD). This rese...\n",
      "\n",
      "247. Safety Co-Option and Compromised National Security: The Self-Fulfilling Prophecy of Weakened AI Risk Thresholds\n",
      "   Authors: Heidy Khlaaf, Sarah Myers West\n",
      "   Published: 2025-04-21\n",
      "   arXiv ID: 2504.15088v1\n",
      "   URL: http://arxiv.org/pdf/2504.15088v1\n",
      "   Summary: Risk thresholds provide a measure of the level of risk exposure that a society or individual is willing to withstand, ultimately shaping how we determine the safety of technological systems. Against t...\n",
      "\n",
      "248. A Byzantine Fault Tolerance Approach towards AI Safety\n",
      "   Authors: John deVadoss, Matthias Artzt\n",
      "   Published: 2025-04-20\n",
      "   arXiv ID: 2504.14668v1\n",
      "   URL: http://arxiv.org/pdf/2504.14668v1\n",
      "   Summary: Ensuring that an AI system behaves reliably and as intended, especially in the presence of unexpected faults or adversarial conditions, is a complex challenge. Inspired by the field of Byzantine Fault...\n",
      "\n",
      "249. Seeing Through Risk: A Symbolic Approximation of Prospect Theory\n",
      "   Authors: Ali Arslan Yousaf, Umair Rehman, Muhammad Umair Danish\n",
      "   Published: 2025-04-20\n",
      "   arXiv ID: 2504.14448v1\n",
      "   URL: http://arxiv.org/pdf/2504.14448v1\n",
      "   Summary: We propose a novel symbolic modeling framework for decision-making under risk that merges interpretability with the core insights of Prospect Theory. Our approach replaces opaque utility curves and pr...\n",
      "\n",
      "250. Security-First AI: Foundations for Robust and Trustworthy Systems\n",
      "   Authors: Krti Tallam\n",
      "   Published: 2025-04-17\n",
      "   arXiv ID: 2504.16110v1\n",
      "   URL: http://arxiv.org/pdf/2504.16110v1\n",
      "   Summary: The conversation around artificial intelligence (AI) often focuses on safety, transparency, accountability, alignment, and responsibility. However, AI security (i.e., the safeguarding of data, models,...\n",
      "\n",
      "251. Governance Challenges in Reinforcement Learning from Human Feedback: Evaluator Rationality and Reinforcement Stability\n",
      "   Authors: Dana Alsagheer, Abdulrahman Kamal, Mohammad Kamal, Weidong Shi\n",
      "   Published: 2025-04-17\n",
      "   arXiv ID: 2504.13972v1\n",
      "   URL: http://arxiv.org/pdf/2504.13972v1\n",
      "   Summary: Reinforcement Learning from Human Feedback (RLHF) is central in aligning large language models (LLMs) with human values and expectations. However, the process remains susceptible to governance challen...\n",
      "\n",
      "252. In Which Areas of Technical AI Safety Could Geopolitical Rivals Cooperate?\n",
      "   Authors: Ben Bucknall, Saad Siddiqui, Lara Thurnherr, Conor McGurk, Ben Harack, Anka Reuel, Patricia Paskov, Casey Mahoney, Sören Mindermann, Scott Singer, Vinay Hiremath, Charbel-Raphaël Segerie, Oscar Delaney, Alessandro Abate, Fazl Barez, Michael K. Cohen, Philip Torr, Ferenc Huszár, Anisoara Calinescu, Gabriel Davis Jones, Yoshua Bengio, Robert Trager\n",
      "   Published: 2025-04-17\n",
      "   arXiv ID: 2504.12914v1\n",
      "   URL: http://arxiv.org/pdf/2504.12914v1\n",
      "   Summary: International cooperation is common in AI research, including between geopolitical rivals. While many experts advocate for greater international cooperation on AI safety to address shared global risks...\n",
      "\n",
      "253. AI Safety Should Prioritize the Future of Work\n",
      "   Authors: Sanchaita Hazra, Bodhisattwa Prasad Majumder, Tuhin Chakrabarty\n",
      "   Published: 2025-04-16\n",
      "   arXiv ID: 2504.13959v2\n",
      "   URL: http://arxiv.org/pdf/2504.13959v2\n",
      "   Summary: Current efforts in AI safety prioritize filtering harmful content, preventing manipulation of human behavior, and eliminating existential risks in cybersecurity or biosecurity. While pressing, this na...\n",
      "\n",
      "254. What do people expect from Artificial Intelligence? Public opinion on alignment in AI moderation from Germany and the United States\n",
      "   Authors: Andreas Jungherr, Adrian Rauchfleisch\n",
      "   Published: 2025-04-16\n",
      "   arXiv ID: 2504.12476v1\n",
      "   URL: http://arxiv.org/pdf/2504.12476v1\n",
      "   Summary: Recent advances in generative Artificial Intelligence have raised public awareness, shaping expectations and concerns about their societal implications. Central to these debates is the question of AI ...\n",
      "\n",
      "255. Exploring Persona-dependent LLM Alignment for the Moral Machine Experiment\n",
      "   Authors: Jiseon Kim, Jea Kwon, Luiz Felipe Vecchietti, Alice Oh, Meeyoung Cha\n",
      "   Published: 2025-04-15\n",
      "   arXiv ID: 2504.10886v1\n",
      "   URL: http://arxiv.org/pdf/2504.10886v1\n",
      "   Summary: Deploying large language models (LLMs) with agency in real-world applications raises critical questions about how these models will behave. In particular, how will their decisions align with humans wh...\n",
      "\n",
      "256. The Jailbreak Tax: How Useful are Your Jailbreak Outputs?\n",
      "   Authors: Kristina Nikolić, Luze Sun, Jie Zhang, Florian Tramèr\n",
      "   Published: 2025-04-14\n",
      "   arXiv ID: 2504.10694v1\n",
      "   URL: http://arxiv.org/pdf/2504.10694v1\n",
      "   Summary: Jailbreak attacks bypass the guardrails of large language models to produce harmful outputs. In this paper, we ask whether the model outputs produced by existing jailbreaks are actually useful. For ex...\n",
      "\n",
      "257. The Structural Safety Generalization Problem\n",
      "   Authors: Julius Broomfield, Tom Gibbs, Ethan Kosak-Hine, George Ingebretsen, Tia Nasir, Jason Zhang, Reihaneh Iranmanesh, Sara Pieri, Reihaneh Rabbany, Kellin Pelrine\n",
      "   Published: 2025-04-13\n",
      "   arXiv ID: 2504.09712v2\n",
      "   URL: http://arxiv.org/pdf/2504.09712v2\n",
      "   Summary: LLM jailbreaks are a widespread safety challenge. Given this problem has not yet been tractable, we suggest targeting a key failure mechanism: the failure of safety to generalize across semantically e...\n",
      "\n",
      "258. Exploring Cognitive Attributes in Financial Decision-Making\n",
      "   Authors: Mallika Mainali, Rosina O. Weber\n",
      "   Published: 2025-04-11\n",
      "   arXiv ID: 2504.08849v1\n",
      "   URL: http://arxiv.org/pdf/2504.08849v1\n",
      "   Summary: Cognitive attributes are fundamental to metacognition, shaping how individuals process information, evaluate choices, and make decisions. To develop metacognitive artificial intelligence (AI) models t...\n",
      "\n",
      "259. Geneshift: Impact of different scenario shift on Jailbreaking LLM\n",
      "   Authors: Tianyi Wu, Zhiwei Xue, Yue Liu, Jiaheng Zhang, Bryan Hooi, See-Kiong Ng\n",
      "   Published: 2025-04-10\n",
      "   arXiv ID: 2504.08104v1\n",
      "   URL: http://arxiv.org/pdf/2504.08104v1\n",
      "   Summary: Jailbreak attacks, which aim to cause LLMs to perform unrestricted behaviors, have become a critical and challenging direction in AI safety. Despite achieving the promising attack success rate using d...\n",
      "\n",
      "260. The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search\n",
      "   Authors: Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Foerster, Jeff Clune, David Ha\n",
      "   Published: 2025-04-10\n",
      "   arXiv ID: 2504.08066v1\n",
      "   URL: http://arxiv.org/pdf/2504.08066v1\n",
      "   Summary: AI is increasingly playing a pivotal role in transforming how scientific discoveries are made. We introduce The AI Scientist-v2, an end-to-end agentic system capable of producing the first entirely AI...\n",
      "\n",
      "261. Trustworthy AI Must Account for Intersectionality\n",
      "   Authors: Jesse C. Cresswell\n",
      "   Published: 2025-04-09\n",
      "   arXiv ID: 2504.07170v1\n",
      "   URL: http://arxiv.org/pdf/2504.07170v1\n",
      "   Summary: Trustworthy AI encompasses many aspirational aspects for aligning AI systems with human values, including fairness, privacy, robustness, explainability, and uncertainty quantification. However, effort...\n",
      "\n",
      "262. Societal Impacts Research Requires Benchmarks for Creative Composition Tasks\n",
      "   Authors: Judy Hanwen Shen, Carlos Guestrin\n",
      "   Published: 2025-04-09\n",
      "   arXiv ID: 2504.06549v2\n",
      "   URL: http://arxiv.org/pdf/2504.06549v2\n",
      "   Summary: Foundation models that are capable of automating cognitive tasks represent a pivotal technological shift, yet their societal implications remain unclear. These systems promise exciting advances, yet t...\n",
      "\n",
      "263. Following the Whispers of Values: Unraveling Neural Mechanisms Behind Value-Oriented Behaviors in LLMs\n",
      "   Authors: Ling Hu, Yuemei Xu, Xiaoyang Gu, Letao Han\n",
      "   Published: 2025-04-07\n",
      "   arXiv ID: 2504.04994v2\n",
      "   URL: http://arxiv.org/pdf/2504.04994v2\n",
      "   Summary: Despite the impressive performance of large language models (LLMs), they can present unintended biases and harmful behaviors driven by encoded values, emphasizing the urgent need to understand the val...\n",
      "\n",
      "264. \"Trust me on this\" Explaining Agent Behavior to a Human Terminator\n",
      "   Authors: Uri Menkes, Assaf Hallak, Ofra Amir\n",
      "   Published: 2025-04-06\n",
      "   arXiv ID: 2504.04592v2\n",
      "   URL: http://arxiv.org/pdf/2504.04592v2\n",
      "   Summary: Consider a setting where a pre-trained agent is operating in an environment and a human operator can decide to temporarily terminate its operation and take-over for some duration of time. These kind o...\n",
      "\n",
      "265. AI as a deliberative partner fosters intercultural empathy for Americans but fails for Latin American participants\n",
      "   Authors: Isabel Villanueva, Tara Bobinac, Binwei Yao, Junjie Hu, Kaiping Chen\n",
      "   Published: 2025-04-04\n",
      "   arXiv ID: 2504.13887v2\n",
      "   URL: http://arxiv.org/pdf/2504.13887v2\n",
      "   Summary: Despite increasing AI chatbot deployment in public discourse, empirical evidence on their capacity to foster intercultural empathy remains limited. Through a randomized experiment, we assessed how dif...\n",
      "\n",
      "266. Sparse Autoencoders Learn Monosemantic Features in Vision-Language Models\n",
      "   Authors: Mateusz Pach, Shyamgopal Karthik, Quentin Bouniot, Serge Belongie, Zeynep Akata\n",
      "   Published: 2025-04-03\n",
      "   arXiv ID: 2504.02821v2\n",
      "   URL: http://arxiv.org/pdf/2504.02821v2\n",
      "   Summary: Given that interpretability and steerability are crucial to AI safety, Sparse Autoencoders (SAEs) have emerged as a tool to enhance them in Large Language Models (LLMs). In this work, we extend the ap...\n",
      "\n",
      "267. Improving Harmful Text Detection with Joint Retrieval and External Knowledge\n",
      "   Authors: Zidong Yu, Shuo Wang, Nan Jiang, Weiqiang Huang, Xu Han, Junliang Du\n",
      "   Published: 2025-04-03\n",
      "   arXiv ID: 2504.02310v1\n",
      "   URL: http://arxiv.org/pdf/2504.02310v1\n",
      "   Summary: Harmful text detection has become a crucial task in the development and deployment of large language models, especially as AI-generated content continues to expand across digital platforms. This study...\n",
      "\n",
      "268. Reinsuring AI: Energy, Agriculture, Finance & Medicine as Precedents for Scalable Governance of Frontier Artificial Intelligence\n",
      "   Authors: Nicholas Stetler\n",
      "   Published: 2025-04-02\n",
      "   arXiv ID: 2504.02127v1\n",
      "   URL: http://arxiv.org/pdf/2504.02127v1\n",
      "   Summary: The governance of frontier artificial intelligence (AI) systems--particularly those capable of catastrophic misuse or systemic failure--requires institutional structures that are robust, adaptive, and...\n",
      "\n",
      "269. A Benchmark for Scalable Oversight Protocols\n",
      "   Authors: Abhimanyu Pallavi Sudhir, Jackson Kaunismaa, Arjun Panickssery\n",
      "   Published: 2025-03-31\n",
      "   arXiv ID: 2504.03731v1\n",
      "   URL: http://arxiv.org/pdf/2504.03731v1\n",
      "   Summary: As AI agents surpass human capabilities, scalable oversight -- the problem of effectively supplying human feedback to potentially superhuman AI models -- becomes increasingly critical to ensure alignm...\n",
      "\n",
      "270. A Framework for Cryptographic Verifiability of End-to-End AI Pipelines\n",
      "   Authors: Kar Balan, Robert Learney, Tim Wood\n",
      "   Published: 2025-03-28\n",
      "   arXiv ID: 2503.22573v1\n",
      "   URL: http://arxiv.org/pdf/2503.22573v1\n",
      "   Summary: The increasing integration of Artificial Intelligence across multiple industry sectors necessitates robust mechanisms for ensuring transparency, trust, and auditability of its development and deployme...\n",
      "\n",
      "271. Effective Automation to Support the Human Infrastructure in AI Red Teaming\n",
      "   Authors: Alice Qian Zhang, Jina Suh, Mary L. Gray, Hong Shen\n",
      "   Published: 2025-03-28\n",
      "   arXiv ID: 2503.22116v1\n",
      "   URL: http://arxiv.org/pdf/2503.22116v1\n",
      "   Summary: As artificial intelligence (AI) systems become increasingly embedded in critical societal functions, the need for robust red teaming methodologies continues to grow. In this forum piece, we examine em...\n",
      "\n",
      "272. Beyond Single-Sentence Prompts: Upgrading Value Alignment Benchmarks with Dialogues and Stories\n",
      "   Authors: Yazhou Zhang, Qimeng Liu, Qiuchi Li, Peng Zhang, Jing Qin\n",
      "   Published: 2025-03-28\n",
      "   arXiv ID: 2503.22115v1\n",
      "   URL: http://arxiv.org/pdf/2503.22115v1\n",
      "   Summary: Evaluating the value alignment of large language models (LLMs) has traditionally relied on single-sentence adversarial prompts, which directly probe models with ethically sensitive or controversial qu...\n",
      "\n",
      "273. MAD Chairs: A new tool to evaluate AI\n",
      "   Authors: Chris Santos-Lang\n",
      "   Published: 2025-03-26\n",
      "   arXiv ID: 2503.20986v5\n",
      "   URL: http://arxiv.org/pdf/2503.20986v5\n",
      "   Summary: This paper contributes a new way to evaluate AI. Much as one might evaluate a machine in terms of its performance at chess, this approach involves evaluating a machine in terms of its performance at a...\n",
      "\n",
      "274. The Backfiring Effect of Weak AI Safety Regulation\n",
      "   Authors: Benjamin Laufer, Jon Kleinberg, Hoda Heidari\n",
      "   Published: 2025-03-26\n",
      "   arXiv ID: 2503.20848v2\n",
      "   URL: http://arxiv.org/pdf/2503.20848v2\n",
      "   Summary: Recent policy proposals aim to improve the safety of general-purpose AI, but there is little understanding of the efficacy of different regulatory approaches to AI safety. We present a strategic model...\n",
      "\n",
      "275. AI Safety in the Eyes of the Downstream Developer: A First Look at Concerns, Practices, and Challenges\n",
      "   Authors: Haoyu Gao, Mansooreh Zahedi, Wenxin Jiang, Hong Yi Lin, James Davis, Christoph Treude\n",
      "   Published: 2025-03-25\n",
      "   arXiv ID: 2503.19444v3\n",
      "   URL: http://arxiv.org/pdf/2503.19444v3\n",
      "   Summary: Pre-trained models (PTMs) have become a cornerstone of AI-based software, allowing for rapid integration and development with minimal training overhead. However, their adoption also introduces unique ...\n",
      "\n",
      "276. LLMs in the Classroom: Outcomes and Perceptions of Questions Written with the Aid of AI\n",
      "   Authors: Gavin Witsken, Igor Crk, Eren Gultepe\n",
      "   Published: 2025-03-23\n",
      "   arXiv ID: 2503.18995v1\n",
      "   URL: http://arxiv.org/pdf/2503.18995v1\n",
      "   Summary: We randomly deploy questions constructed with and without use of the LLM tool and gauge the ability of the students to correctly answer, as well as their ability to correctly perceive the difference b...\n",
      "\n",
      "277. Intelligence Sequencing and the Path-Dependence of Intelligence Evolution: AGI-First vs. DCI-First as Irreversible Attractors\n",
      "   Authors: Andy E. Williams\n",
      "   Published: 2025-03-22\n",
      "   arXiv ID: 2503.17688v1\n",
      "   URL: http://arxiv.org/pdf/2503.17688v1\n",
      "   Summary: The trajectory of intelligence evolution is often framed around the emergence of artificial general intelligence (AGI) and its alignment with human values. This paper challenges that framing by introd...\n",
      "\n",
      "278. A Peek Behind the Curtain: Using Step-Around Prompt Engineering to Identify Bias and Misinformation in GenAI Models\n",
      "   Authors: Don Hickerson, Mike Perkins\n",
      "   Published: 2025-03-19\n",
      "   arXiv ID: 2503.15205v1\n",
      "   URL: http://arxiv.org/pdf/2503.15205v1\n",
      "   Summary: This research examines the emerging technique of step-around prompt engineering in GenAI research, a method that deliberately bypasses AI safety measures to expose underlying biases and vulnerabilitie...\n",
      "\n",
      "279. Aligning Crowd-sourced Human Feedback for Reinforcement Learning on Code Generation by Large Language Models\n",
      "   Authors: Man Fai Wong, Chee Wei Tan\n",
      "   Published: 2025-03-19\n",
      "   arXiv ID: 2503.15129v1\n",
      "   URL: http://arxiv.org/pdf/2503.15129v1\n",
      "   Summary: This paper studies how AI-assisted programming and large language models (LLM) improve software developers' ability via AI tools (LLM agents) like Github Copilot and Amazon CodeWhisperer, while integr...\n",
      "\n",
      "280. International Agreements on AI Safety: Review and Recommendations for a Conditional AI Safety Treaty\n",
      "   Authors: Rebecca Scholefield, Samuel Martin, Otto Barten\n",
      "   Published: 2025-03-18\n",
      "   arXiv ID: 2503.18956v1\n",
      "   URL: http://arxiv.org/pdf/2503.18956v1\n",
      "   Summary: The malicious use or malfunction of advanced general-purpose AI (GPAI) poses risks that, according to leading experts, could lead to the 'marginalisation or extinction of humanity.' To address these r...\n",
      "\n",
      "281. Superalignment with Dynamic Human Values\n",
      "   Authors: Florian Mai, David Kaczér, Nicholas Kluge Corrêa, Lucie Flek\n",
      "   Published: 2025-03-17\n",
      "   arXiv ID: 2503.13621v1\n",
      "   URL: http://arxiv.org/pdf/2503.13621v1\n",
      "   Summary: Two core challenges of alignment are 1) scalable oversight and 2) accounting for the dynamic nature of human values. While solutions like recursive reward modeling address 1), they do not simultaneous...\n",
      "\n",
      "282. AI Companies Should Report Pre- and Post-Mitigation Safety Evaluations\n",
      "   Authors: Dillon Bowen, Ann-Kathrin Dombrowski, Adam Gleave, Chris Cundy\n",
      "   Published: 2025-03-17\n",
      "   arXiv ID: 2503.17388v1\n",
      "   URL: http://arxiv.org/pdf/2503.17388v1\n",
      "   Summary: The rapid advancement of AI systems has raised widespread concerns about potential harms of frontier AI systems and the need for responsible evaluation and oversight. In this position paper, we argue ...\n",
      "\n",
      "283. Are LLMs (Really) Ideological? An IRT-based Analysis and Alignment Tool for Perceived Socio-Economic Bias in LLMs\n",
      "   Authors: Jasmin Wachter, Michael Radloff, Maja Smolej, Katharina Kinder-Kurlanda\n",
      "   Published: 2025-03-17\n",
      "   arXiv ID: 2503.13149v1\n",
      "   URL: http://arxiv.org/pdf/2503.13149v1\n",
      "   Summary: We introduce an Item Response Theory (IRT)-based framework to detect and quantify socioeconomic bias in large language models (LLMs) without relying on subjective human judgments. Unlike traditional m...\n",
      "\n",
      "284. Identifying Cooperative Personalities in Multi-agent Contexts through Personality Steering with Representation Engineering\n",
      "   Authors: Kenneth J. K. Ong, Lye Jia Jun, Hieu Minh \"Jord\" Nguyen, Seong Hah Cho, Natalia Pérez-Campanero Antolín\n",
      "   Published: 2025-03-17\n",
      "   arXiv ID: 2503.12722v1\n",
      "   URL: http://arxiv.org/pdf/2503.12722v1\n",
      "   Summary: As Large Language Models (LLMs) gain autonomous capabilities, their coordination in multi-agent settings becomes increasingly important. However, they often struggle with cooperation, leading to subop...\n",
      "\n",
      "285. Augmenting Image Annotation: A Human-LMM Collaborative Framework for Efficient Object Selection and Label Generation\n",
      "   Authors: He Zhang, Xinyi Fu, John M. Carroll\n",
      "   Published: 2025-03-14\n",
      "   arXiv ID: 2503.11096v1\n",
      "   URL: http://arxiv.org/pdf/2503.11096v1\n",
      "   Summary: Traditional image annotation tasks rely heavily on human effort for object selection and label assignment, making the process time-consuming and prone to decreased efficiency as annotators experience ...\n",
      "\n",
      "286. Adaptive Preference Aggregation\n",
      "   Authors: Benjamin Heymann\n",
      "   Published: 2025-03-13\n",
      "   arXiv ID: 2503.10215v1\n",
      "   URL: http://arxiv.org/pdf/2503.10215v1\n",
      "   Summary: AI alignment, the challenge of ensuring AI systems act in accordance with human values, has emerged as a critical problem in the development of systems such as foundation models and recommender system...\n",
      "\n",
      "287. The BIG Argument for AI Safety Cases\n",
      "   Authors: Ibrahim Habli, Richard Hawkins, Colin Paterson, Philippa Ryan, Yan Jia, Mark Sujan, John McDermid\n",
      "   Published: 2025-03-12\n",
      "   arXiv ID: 2503.11705v3\n",
      "   URL: http://arxiv.org/pdf/2503.11705v3\n",
      "   Summary: We present our Balanced, Integrated and Grounded (BIG) argument for assuring the safety of AI systems. The BIG argument adopts a whole-system approach to constructing a safety case for AI systems of v...\n",
      "\n",
      "288. The Economics of p(doom): Scenarios of Existential Risk and Economic Growth in the Age of Transformative AI\n",
      "   Authors: Jakub Growiec, Klaus Prettner\n",
      "   Published: 2025-03-10\n",
      "   arXiv ID: 2503.07341v1\n",
      "   URL: http://arxiv.org/pdf/2503.07341v1\n",
      "   Summary: Recent advances in artificial intelligence (AI) have led to a diverse set of predictions about its long-term impact on humanity. A central focus is the potential emergence of transformative AI (TAI), ...\n",
      "\n",
      "289. Decoding the Black Box: Integrating Moral Imagination with Technical AI Governance\n",
      "   Authors: Krti Tallam\n",
      "   Published: 2025-03-09\n",
      "   arXiv ID: 2503.06411v1\n",
      "   URL: http://arxiv.org/pdf/2503.06411v1\n",
      "   Summary: This paper examines the intricate interplay among AI safety, security, and governance by integrating technical systems engineering with principles of moral imagination and ethical philosophy. Drawing ...\n",
      "\n",
      "290. Shifting Perspectives: Steering Vectors for Robust Bias Mitigation in LLMs\n",
      "   Authors: Zara Siddique, Irtaza Khalid, Liam D. Turner, Luis Espinosa-Anke\n",
      "   Published: 2025-03-07\n",
      "   arXiv ID: 2503.05371v2\n",
      "   URL: http://arxiv.org/pdf/2503.05371v2\n",
      "   Summary: We present a novel approach to bias mitigation in large language models (LLMs) by applying steering vectors to modify model activations in forward passes. We compute 8 steering vectors, each correspon...\n",
      "\n",
      "291. Jailbreaking is (Mostly) Simpler Than You Think\n",
      "   Authors: Mark Russinovich, Ahmed Salem\n",
      "   Published: 2025-03-07\n",
      "   arXiv ID: 2503.05264v1\n",
      "   URL: http://arxiv.org/pdf/2503.05264v1\n",
      "   Summary: We introduce the Context Compliance Attack (CCA), a novel, optimization-free method for bypassing AI safety mechanisms. Unlike current approaches -- which rely on complex prompt engineering and comput...\n",
      "\n",
      "292. Can Large Language Models Grasp Concepts in Visual Content? A Case Study on YouTube Shorts about Depression\n",
      "   Authors: Jiaying \"Lizzy\" Liu, Yiheng Su, Praneel Seth\n",
      "   Published: 2025-03-07\n",
      "   arXiv ID: 2503.05109v1\n",
      "   URL: http://arxiv.org/pdf/2503.05109v1\n",
      "   Summary: Large language models (LLMs) are increasingly used to assist computational social science research. While prior efforts have focused on text, the potential of leveraging multimodal LLMs (MLLMs) for on...\n",
      "\n",
      "293. Maximizing Signal in Human-Model Preference Alignment\n",
      "   Authors: Kelsey Kraus, Margaret Kroll\n",
      "   Published: 2025-03-06\n",
      "   arXiv ID: 2503.04910v1\n",
      "   URL: http://arxiv.org/pdf/2503.04910v1\n",
      "   Summary: The emergence of powerful LLMs has led to a paradigm shift in Natural Language Understanding and Natural Language Generation. The properties that make LLMs so valuable for these tasks -- creativity, a...\n",
      "\n",
      "294. Activation Space Interventions Can Be Transferred Between Large Language Models\n",
      "   Authors: Narmeen Oozeer, Dhruv Nathawani, Nirmalendu Prakash, Michael Lan, Abir Harrasse, Amirali Abdullah\n",
      "   Published: 2025-03-06\n",
      "   arXiv ID: 2503.04429v4\n",
      "   URL: http://arxiv.org/pdf/2503.04429v4\n",
      "   Summary: The study of representation universality in AI models reveals growing convergence across domains, modalities, and architectures. However, the practical applications of representation universality rema...\n",
      "\n",
      "295. Intolerable Risk Threshold Recommendations for Artificial Intelligence\n",
      "   Authors: Deepika Raman, Nada Madkour, Evan R. Murphy, Krystal Jackson, Jessica Newman\n",
      "   Published: 2025-03-04\n",
      "   arXiv ID: 2503.05812v1\n",
      "   URL: http://arxiv.org/pdf/2503.05812v1\n",
      "   Summary: Frontier AI models -- highly capable foundation models at the cutting edge of AI development -- may pose severe risks to public safety, human rights, economic stability, and societal value in the comi...\n",
      "\n",
      "296. Robust Multi-Objective Preference Alignment with Online DPO\n",
      "   Authors: Raghav Gupta, Ryan Sullivan, Yunxuan Li, Samrat Phatale, Abhinav Rastogi\n",
      "   Published: 2025-03-01\n",
      "   arXiv ID: 2503.00295v1\n",
      "   URL: http://arxiv.org/pdf/2503.00295v1\n",
      "   Summary: Multi-objective preference alignment of large language models (LLMs) is critical for developing AI systems that are more configurable, personalizable, helpful, and safe. However, optimizing model outp...\n",
      "\n",
      "297. Reducing Large Language Model Safety Risks in Women's Health using Semantic Entropy\n",
      "   Authors: Jahan C. Penny-Dimri, Magdalena Bachmann, William R. Cooke, Sam Mathewlynn, Samuel Dockree, John Tolladay, Jannik Kossen, Lin Li, Yarin Gal, Gabriel Davis Jones\n",
      "   Published: 2025-03-01\n",
      "   arXiv ID: 2503.00269v1\n",
      "   URL: http://arxiv.org/pdf/2503.00269v1\n",
      "   Summary: Large language models (LLMs) hold substantial promise for clinical decision support. However, their widespread adoption in medicine, particularly in healthcare, is hindered by their propensity to gene...\n",
      "\n",
      "298. Steering Large Language Model Activations in Sparse Spaces\n",
      "   Authors: Reza Bayat, Ali Rahimi-Kalahroudi, Mohammad Pezeshki, Sarath Chandar, Pascal Vincent\n",
      "   Published: 2025-02-28\n",
      "   arXiv ID: 2503.00177v1\n",
      "   URL: http://arxiv.org/pdf/2503.00177v1\n",
      "   Summary: A key challenge in AI alignment is guiding large language models (LLMs) to follow desired behaviors at test time. Activation steering, which modifies internal model activations during inference, offer...\n",
      "\n",
      "299. Beware of Your Po! Measuring and Mitigating AI Safety Risks in Role-Play Fine-Tuning of LLMs\n",
      "   Authors: Weixiang Zhao, Yulin Hu, Yang Deng, Jiahe Guo, Xingyu Sui, Xinyang Han, An Zhang, Yanyan Zhao, Bing Qin, Tat-Seng Chua, Ting Liu\n",
      "   Published: 2025-02-28\n",
      "   arXiv ID: 2502.20968v2\n",
      "   URL: http://arxiv.org/pdf/2502.20968v2\n",
      "   Summary: Role-playing enables large language models (LLMs) to engage users in immersive and personalized interactions, but it also introduces significant safety risks. Existing role-play fine-tuning techniques...\n",
      "\n",
      "300. Foot-In-The-Door: A Multi-turn Jailbreak for LLMs\n",
      "   Authors: Zixuan Weng, Xiaolong Jin, Jinyuan Jia, Xiangyu Zhang\n",
      "   Published: 2025-02-27\n",
      "   arXiv ID: 2502.19820v3\n",
      "   URL: http://arxiv.org/pdf/2502.19820v3\n",
      "   Summary: Ensuring AI safety is crucial as large language models become increasingly integrated into real-world applications. A key challenge is jailbreak, where adversarial prompts bypass built-in safeguards t...\n",
      "\n",
      "301. Developmental Support Approach to AI's Autonomous Growth: Toward the Realization of a Mutually Beneficial Stage Through Experiential Learning\n",
      "   Authors: Taichiro Endo\n",
      "   Published: 2025-02-27\n",
      "   arXiv ID: 2502.19798v1\n",
      "   URL: http://arxiv.org/pdf/2502.19798v1\n",
      "   Summary: This study proposes an \"AI Development Support\" approach that, unlike conventional AI Alignment-which aims to forcefully inject human values-supports the ethical and moral development of AI itself. As...\n",
      "\n",
      "302. Better Aligned with Survey Respondents or Training Data? Unveiling Political Leanings of LLMs on U.S. Supreme Court Cases\n",
      "   Authors: Shanshan Xu, T. Y. S. S Santosh, Yanai Elazar, Quirin Vogel, Barbara Plank, Matthias Grabmair\n",
      "   Published: 2025-02-25\n",
      "   arXiv ID: 2502.18282v3\n",
      "   URL: http://arxiv.org/pdf/2502.18282v3\n",
      "   Summary: Recent works have shown that Large Language Models (LLMs) have a tendency to memorize patterns and biases present in their training data, raising important questions about how such memorized content i...\n",
      "\n",
      "303. Mind the Gesture: Evaluating AI Sensitivity to Culturally Offensive Non-Verbal Gestures\n",
      "   Authors: Akhila Yerukola, Saadia Gabriel, Nanyun Peng, Maarten Sap\n",
      "   Published: 2025-02-24\n",
      "   arXiv ID: 2502.17710v1\n",
      "   URL: http://arxiv.org/pdf/2502.17710v1\n",
      "   Summary: Gestures are an integral part of non-verbal communication, with meanings that vary across cultures, and misinterpretations that can have serious social and diplomatic consequences. As AI systems becom...\n",
      "\n",
      "304. AISafetyLab: A Comprehensive Framework for AI Safety Evaluation and Improvement\n",
      "   Authors: Zhexin Zhang, Leqi Lei, Junxiao Yang, Xijie Huang, Yida Lu, Shiyao Cui, Renmiao Chen, Qinglin Zhang, Xinyuan Wang, Hao Wang, Hao Li, Xianqi Lei, Chengwei Pan, Lei Sha, Hongning Wang, Minlie Huang\n",
      "   Published: 2025-02-24\n",
      "   arXiv ID: 2502.16776v1\n",
      "   URL: http://arxiv.org/pdf/2502.16776v1\n",
      "   Summary: As AI models are increasingly deployed across diverse real-world scenarios, ensuring their safety remains a critical yet underexplored challenge. While substantial efforts have been made to evaluate a...\n",
      "\n",
      "305. A Systematic Review of Open Datasets Used in Text-to-Image (T2I) Gen AI Model Safety\n",
      "   Authors: Rakeen Rouf, Trupti Bavalatti, Osama Ahmed, Dhaval Potdar, Faraz Jawed\n",
      "   Published: 2025-02-23\n",
      "   arXiv ID: 2503.00020v1\n",
      "   URL: http://arxiv.org/pdf/2503.00020v1\n",
      "   Summary: Novel research aimed at text-to-image (T2I) generative AI safety often relies on publicly available datasets for training and evaluation, making the quality and composition of these datasets crucial. ...\n",
      "\n",
      "306. Position: Beyond Assistance -- Reimagining LLMs as Ethical and Adaptive Co-Creators in Mental Health Care\n",
      "   Authors: Abeer Badawi, Md Tahmid Rahman Laskar, Jimmy Xiangji Huang, Shaina Raza, Elham Dolatabadi\n",
      "   Published: 2025-02-21\n",
      "   arXiv ID: 2503.16456v2\n",
      "   URL: http://arxiv.org/pdf/2503.16456v2\n",
      "   Summary: This position paper argues for a fundamental shift in how Large Language Models (LLMs) are integrated into the mental health care domain. We advocate for their role as co-creators rather than mere ass...\n",
      "\n",
      "307. Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI Offer a Safer Path?\n",
      "   Authors: Yoshua Bengio, Michael Cohen, Damiano Fornasiere, Joumana Ghosn, Pietro Greiner, Matt MacDermott, Sören Mindermann, Adam Oberman, Jesse Richardson, Oliver Richardson, Marc-Antoine Rondeau, Pierre-Luc St-Charles, David Williams-King\n",
      "   Published: 2025-02-21\n",
      "   arXiv ID: 2502.15657v2\n",
      "   URL: http://arxiv.org/pdf/2502.15657v2\n",
      "   Summary: The leading AI companies are increasingly focused on building generalist AI agents -- systems that can autonomously plan, act, and pursue goals across almost all tasks that humans can perform. Despite...\n",
      "\n",
      "308. Alignment, Agency and Autonomy in Frontier AI: A Systems Engineering Perspective\n",
      "   Authors: Krti Tallam\n",
      "   Published: 2025-02-20\n",
      "   arXiv ID: 2503.05748v1\n",
      "   URL: http://arxiv.org/pdf/2503.05748v1\n",
      "   Summary: As artificial intelligence scales, the concepts of alignment, agency, and autonomy have become central to AI safety, governance, and control. However, even in human contexts, these terms lack universa...\n",
      "\n",
      "309. Balancing Innovation and Integrity: AI Integration in Liberal Arts College Administration\n",
      "   Authors: Ian Olivo Read\n",
      "   Published: 2025-02-20\n",
      "   arXiv ID: 2503.05747v1\n",
      "   URL: http://arxiv.org/pdf/2503.05747v1\n",
      "   Summary: This paper explores the intersection of artificial intelligence and higher education administration, focusing on liberal arts colleges (LACs). It examines AI's opportunities and challenges in academic...\n",
      "\n",
      "310. Human Misperception of Generative-AI Alignment: A Laboratory Experiment\n",
      "   Authors: Kevin He, Ran Shorrer, Mengjia Xia\n",
      "   Published: 2025-02-20\n",
      "   arXiv ID: 2502.14708v2\n",
      "   URL: http://arxiv.org/pdf/2502.14708v2\n",
      "   Summary: We conduct an incentivized laboratory experiment to study people's perception of generative artificial intelligence (GenAI) alignment in the context of economic decision-making. Using a panel of econo...\n",
      "\n",
      "311. A Statistical Case Against Empirical Human-AI Alignment\n",
      "   Authors: Julian Rodemann, Esteban Garces Arias, Christoph Luther, Christoph Jansen, Thomas Augustin\n",
      "   Published: 2025-02-20\n",
      "   arXiv ID: 2502.14581v2\n",
      "   URL: http://arxiv.org/pdf/2502.14581v2\n",
      "   Summary: Empirical human-AI alignment aims to make AI systems act in line with observed human behavior. While noble in its goals, we argue that empirical alignment can inadvertently introduce statistical biase...\n",
      "\n",
      "312. Statistical Scenario Modelling and Lookalike Distributions for Multi-Variate AI Risk\n",
      "   Authors: Elija Perrier\n",
      "   Published: 2025-02-20\n",
      "   arXiv ID: 2502.14491v2\n",
      "   URL: http://arxiv.org/pdf/2502.14491v2\n",
      "   Summary: Evaluating AI safety requires statistically rigorous methods and risk metrics for understanding how the use of AI affects aggregated risk. However, much AI safety literature focuses upon risks arising...\n",
      "\n",
      "313. Universal AI maximizes Variational Empowerment\n",
      "   Authors: Yusuke Hayashi, Koichi Takahashi\n",
      "   Published: 2025-02-20\n",
      "   arXiv ID: 2502.15820v2\n",
      "   URL: http://arxiv.org/pdf/2502.15820v2\n",
      "   Summary: This paper presents a theoretical framework unifying AIXI -- a model of universal AI -- with variational empowerment as an intrinsic drive for exploration. We build on the existing framework of Self-A...\n",
      "\n",
      "314. Retrieving Versus Understanding Extractive Evidence in Few-Shot Learning\n",
      "   Authors: Karl Elbakian, Samuel Carton\n",
      "   Published: 2025-02-19\n",
      "   arXiv ID: 2502.14095v1\n",
      "   URL: http://arxiv.org/pdf/2502.14095v1\n",
      "   Summary: A key aspect of alignment is the proper use of within-document evidence to construct document-level decisions. We analyze the relationship between the retrieval and interpretation of within-document e...\n",
      "\n",
      "315. VITAL: A New Dataset for Benchmarking Pluralistic Alignment in Healthcare\n",
      "   Authors: Anudeex Shetty, Amin Beheshti, Mark Dras, Usman Naseem\n",
      "   Published: 2025-02-19\n",
      "   arXiv ID: 2502.13775v2\n",
      "   URL: http://arxiv.org/pdf/2502.13775v2\n",
      "   Summary: Alignment techniques have become central to ensuring that Large Language Models (LLMs) generate outputs consistent with human values. However, existing alignment paradigms often model an averaged or m...\n",
      "\n",
      "316. Computational Safety for Generative AI: A Signal Processing Perspective\n",
      "   Authors: Pin-Yu Chen\n",
      "   Published: 2025-02-18\n",
      "   arXiv ID: 2502.12445v1\n",
      "   URL: http://arxiv.org/pdf/2502.12445v1\n",
      "   Summary: AI safety is a rapidly growing area of research that seeks to prevent the harm and misuse of frontier AI technology, particularly with respect to generative AI (GenAI) tools that are capable of creati...\n",
      "\n",
      "317. VLDBench Evaluating Multimodal Disinformation with Regulatory Alignment\n",
      "   Authors: Shaina Raza, Ashmal Vayani, Aditya Jain, Aravind Narayanan, Vahid Reza Khazaie, Syed Raza Bashir, Elham Dolatabadi, Gias Uddin, Christos Emmanouilidis, Rizwan Qureshi, Mubarak Shah\n",
      "   Published: 2025-02-17\n",
      "   arXiv ID: 2502.11361v4\n",
      "   URL: http://arxiv.org/pdf/2502.11361v4\n",
      "   Summary: Detecting disinformation that blends manipulated text and images has become increasingly challenging, as AI tools make synthetic content easy to generate and disseminate. While most existing AI safety...\n",
      "\n",
      "318. Navigating the Social Welfare Frontier: Portfolios for Multi-objective Reinforcement Learning\n",
      "   Authors: Cheol Woo Kim, Jai Moondra, Shresth Verma, Madeleine Pollack, Lingkai Kong, Milind Tambe, Swati Gupta\n",
      "   Published: 2025-02-13\n",
      "   arXiv ID: 2502.09724v2\n",
      "   URL: http://arxiv.org/pdf/2502.09724v2\n",
      "   Summary: In many real-world applications of reinforcement learning (RL), deployed policies have varied impacts on different stakeholders, creating challenges in reaching consensus on how to effectively aggrega...\n",
      "\n",
      "319. AI Safety for Everyone\n",
      "   Authors: Balint Gyevnar, Atoosa Kasirzadeh\n",
      "   Published: 2025-02-13\n",
      "   arXiv ID: 2502.09288v2\n",
      "   URL: http://arxiv.org/pdf/2502.09288v2\n",
      "   Summary: Recent discussions and research in AI safety have increasingly emphasized the deep connection between AI safety and existential risk from advanced AI systems, suggesting that work on AI safety necessa...\n",
      "\n",
      "320. AdvSwap: Covert Adversarial Perturbation with High Frequency Info-swapping for Autonomous Driving Perception\n",
      "   Authors: Yuanhao Huang, Qinfan Zhang, Jiandong Xing, Mengyue Cheng, Haiyang Yu, Yilong Ren, Xiao Xiong\n",
      "   Published: 2025-02-12\n",
      "   arXiv ID: 2502.08374v1\n",
      "   URL: http://arxiv.org/pdf/2502.08374v1\n",
      "   Summary: Perception module of Autonomous vehicles (AVs) are increasingly susceptible to be attacked, which exploit vulnerabilities in neural networks through adversarial inputs, thereby compromising the AI saf...\n",
      "\n",
      "321. Compromising Honesty and Harmlessness in Language Models via Deception Attacks\n",
      "   Authors: Laurène Vaugrante, Francesca Carlon, Maluna Menke, Thilo Hagendorff\n",
      "   Published: 2025-02-12\n",
      "   arXiv ID: 2502.08301v2\n",
      "   URL: http://arxiv.org/pdf/2502.08301v2\n",
      "   Summary: Recent research on large language models (LLMs) has demonstrated their ability to understand and employ deceptive behavior, even without explicit prompting. However, such behavior has only been observ...\n",
      "\n",
      "322. A cross-regional review of AI safety regulations in the commercial aviation\n",
      "   Authors: Penny A. Barr, Sohel M. Imroz\n",
      "   Published: 2025-02-12\n",
      "   arXiv ID: 2503.04767v2\n",
      "   URL: http://arxiv.org/pdf/2503.04767v2\n",
      "   Summary: In this paper we examine the existing artificial intelligence (AI) policy documents in aviation for the following three regions: the United States, European Union, and China. The aviation industry has...\n",
      "\n",
      "323. Trustworthy AI: Safety, Bias, and Privacy -- A Survey\n",
      "   Authors: Xingli Fang, Jianwei Li, Varun Mulchandani, Jung-Eun Kim\n",
      "   Published: 2025-02-11\n",
      "   arXiv ID: 2502.10450v2\n",
      "   URL: http://arxiv.org/pdf/2502.10450v2\n",
      "   Summary: The capabilities of artificial intelligence systems have been advancing to a great extent, but these systems still struggle with failure modes, vulnerabilities, and biases. In this paper, we study the...\n",
      "\n",
      "324. SEMU: Singular Value Decomposition for Efficient Machine Unlearning\n",
      "   Authors: Marcin Sendera, Łukasz Struski, Kamil Książek, Kryspin Musiol, Jacek Tabor, Dawid Rymarczyk\n",
      "   Published: 2025-02-11\n",
      "   arXiv ID: 2502.07587v1\n",
      "   URL: http://arxiv.org/pdf/2502.07587v1\n",
      "   Summary: While the capabilities of generative foundational models have advanced rapidly in recent years, methods to prevent harmful and unsafe behaviors remain underdeveloped. Among the pressing challenges in ...\n",
      "\n",
      "325. AI Alignment at Your Discretion\n",
      "   Authors: Maarten Buyl, Hadi Khalaf, Claudio Mayrink Verdun, Lucas Monteiro Paes, Caio C. Vieira Machado, Flavio du Pin Calmon\n",
      "   Published: 2025-02-10\n",
      "   arXiv ID: 2502.10441v1\n",
      "   URL: http://arxiv.org/pdf/2502.10441v1\n",
      "   Summary: In AI alignment, extensive latitude must be granted to annotators, either human or algorithmic, to judge which model outputs are `better' or `safer.' We refer to this latitude as alignment discretion....\n",
      "\n",
      "326. Prioritization First, Principles Second: An Adaptive Interpretation of Helpful, Honest, and Harmless Principles\n",
      "   Authors: Yue Huang, Chujie Gao, Yujun Zhou, Kehan Guo, Xiangqi Wang, Or Cohen-Sasson, Max Lamparth, Xiangliang Zhang\n",
      "   Published: 2025-02-09\n",
      "   arXiv ID: 2502.06059v4\n",
      "   URL: http://arxiv.org/pdf/2502.06059v4\n",
      "   Summary: The Helpful, Honest, and Harmless (HHH) principle is a foundational framework for aligning AI systems with human values. However, existing interpretations of the HHH principle often overlook contextua...\n",
      "\n",
      "327. Head-Specific Intervention Can Induce Misaligned AI Coordination in Large Language Models\n",
      "   Authors: Paul Darm, Annalisa Riccardi\n",
      "   Published: 2025-02-09\n",
      "   arXiv ID: 2502.05945v3\n",
      "   URL: http://arxiv.org/pdf/2502.05945v3\n",
      "   Summary: Robust alignment guardrails for large language models (LLMs) are becoming increasingly important with their widespread application. In contrast to previous studies, we demonstrate that inference-time ...\n",
      "\n",
      "328. Intrinsic Barriers and Practical Pathways for Human-AI Alignment: An Agreement-Based Complexity Analysis\n",
      "   Authors: Aran Nayebi\n",
      "   Published: 2025-02-09\n",
      "   arXiv ID: 2502.05934v2\n",
      "   URL: http://arxiv.org/pdf/2502.05934v2\n",
      "   Summary: We formalize AI alignment as a multi-objective optimization problem called $\\langle M,N,\\varepsilon,\\delta\\rangle$-agreement that generalizes prior approaches with fewer assumptions, in which a set of...\n",
      "\n",
      "329. Assessing confidence in frontier AI safety cases\n",
      "   Authors: Stephen Barrett, Philip Fox, Joshua Krook, Tuneer Mondal, Simon Mylius, Alejandro Tlaie\n",
      "   Published: 2025-02-09\n",
      "   arXiv ID: 2502.05791v1\n",
      "   URL: http://arxiv.org/pdf/2502.05791v1\n",
      "   Summary: Powerful new frontier AI technologies are bringing many benefits to society but at the same time bring new risks. AI developers and regulators are therefore seeking ways to assure the safety of such s...\n",
      "\n",
      "330. You Are What You Eat -- AI Alignment Requires Understanding How Data Shapes Structure and Generalisation\n",
      "   Authors: Simon Pepin Lehalleur, Jesse Hoogland, Matthew Farrugia-Roberts, Susan Wei, Alexander Gietelink Oldenziel, George Wang, Liam Carroll, Daniel Murfet\n",
      "   Published: 2025-02-08\n",
      "   arXiv ID: 2502.05475v1\n",
      "   URL: http://arxiv.org/pdf/2502.05475v1\n",
      "   Summary: In this position paper, we argue that understanding the relation between structure in the data distribution and structure in trained models is central to AI alignment. First, we discuss how two neural...\n",
      "\n",
      "331. A Survey on Explainable Deep Reinforcement Learning\n",
      "   Authors: Zelei Cheng, Jiahao Yu, Xinyu Xing\n",
      "   Published: 2025-02-08\n",
      "   arXiv ID: 2502.06869v1\n",
      "   URL: http://arxiv.org/pdf/2502.06869v1\n",
      "   Summary: Deep Reinforcement Learning (DRL) has achieved remarkable success in sequential decision-making tasks across diverse domains, yet its reliance on black-box neural architectures hinders interpretabilit...\n",
      "\n",
      "332. Forbidden Science: Dual-Use AI Challenge Benchmark and Scientific Refusal Tests\n",
      "   Authors: David Noever, Forrest McKee\n",
      "   Published: 2025-02-08\n",
      "   arXiv ID: 2502.06867v1\n",
      "   URL: http://arxiv.org/pdf/2502.06867v1\n",
      "   Summary: The development of robust safety benchmarks for large language models requires open, reproducible datasets that can measure both appropriate refusal of harmful content and potential over-restriction o...\n",
      "\n",
      "333. Frontend Diffusion: Empowering Self-Representation of Junior Researchers and Designers Through Multi-agent System\n",
      "   Authors: Zijian Ding, Qinshi Zhang, Mohan Chi, Ziyi Wang\n",
      "   Published: 2025-02-06\n",
      "   arXiv ID: 2502.03788v2\n",
      "   URL: http://arxiv.org/pdf/2502.03788v2\n",
      "   Summary: With the continuous development of generative AI's logical reasoning abilities, AI's growing code-generation potential poses challenges for both technical and creative professionals. But how can these...\n",
      "\n",
      "334. Toward universal steering and monitoring of AI models\n",
      "   Authors: Daniel Beaglehole, Adityanarayanan Radhakrishnan, Enric Boix-Adserà, Mikhail Belkin\n",
      "   Published: 2025-02-06\n",
      "   arXiv ID: 2502.03708v2\n",
      "   URL: http://arxiv.org/pdf/2502.03708v2\n",
      "   Summary: Modern AI models contain much of human knowledge, yet understanding of their internal representation of this knowledge remains elusive. Characterizing the structure and properties of this representati...\n",
      "\n",
      "335. Emerging Practices in Frontier AI Safety Frameworks\n",
      "   Authors: Marie Davidsen Buhl, Ben Bucknall, Tammy Masterson\n",
      "   Published: 2025-02-05\n",
      "   arXiv ID: 2503.04746v1\n",
      "   URL: http://arxiv.org/pdf/2503.04746v1\n",
      "   Summary: As part of the Frontier AI Safety Commitments agreed to at the 2024 AI Seoul Summit, many AI developers agreed to publish a safety framework outlining how they will manage potential severe risks assoc...\n",
      "\n",
      "336. Safety Cases: A Scalable Approach to Frontier AI Safety\n",
      "   Authors: Benjamin Hilton, Marie Davidsen Buhl, Tomek Korbak, Geoffrey Irving\n",
      "   Published: 2025-02-05\n",
      "   arXiv ID: 2503.04744v1\n",
      "   URL: http://arxiv.org/pdf/2503.04744v1\n",
      "   Summary: Safety cases - clear, assessable arguments for the safety of a system in a given context - are a widely-used technique across various industries for showing a decision-maker (e.g. boards, customers, t...\n",
      "\n",
      "337. AI Safety is Stuck in Technical Terms -- A System Safety Response to the International AI Safety Report\n",
      "   Authors: Roel Dobbe\n",
      "   Published: 2025-02-05\n",
      "   arXiv ID: 2503.04743v1\n",
      "   URL: http://arxiv.org/pdf/2503.04743v1\n",
      "   Summary: Safety has become the central value around which dominant AI governance efforts are being shaped. Recently, this culminated in the publication of the International AI Safety Report, written by 96 expe...\n",
      "\n",
      "338. Learning from Active Human Involvement through Proxy Value Propagation\n",
      "   Authors: Zhenghao Peng, Wenjie Mo, Chenda Duan, Quanyi Li, Bolei Zhou\n",
      "   Published: 2025-02-05\n",
      "   arXiv ID: 2502.03369v1\n",
      "   URL: http://arxiv.org/pdf/2502.03369v1\n",
      "   Summary: Learning from active human involvement enables the human subject to actively intervene and demonstrate to the AI agent during training. The interaction and corrective feedback from human brings safety...\n",
      "\n",
      "339. Which Information should the UK and US AISI share with an International Network of AISIs? Opportunities, Risks, and a Tentative Proposal\n",
      "   Authors: Lara Thurnherr\n",
      "   Published: 2025-02-05\n",
      "   arXiv ID: 2503.04741v1\n",
      "   URL: http://arxiv.org/pdf/2503.04741v1\n",
      "   Summary: The UK AI Safety Institute (UK AISI) and its parallel organisation in the United States (US AISI) take up a unique position in the recently established International Network of AISIs. Both are in juri...\n",
      "\n",
      "340. Enabling External Scrutiny of AI Systems with Privacy-Enhancing Technologies\n",
      "   Authors: Kendrea Beers, Helen Toner\n",
      "   Published: 2025-02-05\n",
      "   arXiv ID: 2502.05219v1\n",
      "   URL: http://arxiv.org/pdf/2502.05219v1\n",
      "   Summary: This article describes how technical infrastructure developed by the nonprofit OpenMined enables external scrutiny of AI systems without compromising sensitive information.   Independent external scru...\n",
      "\n",
      "341. PRISM: Perspective Reasoning for Integrated Synthesis and Mediation as a Multi-Perspective Framework for AI Alignment\n",
      "   Authors: Anthony Diamond\n",
      "   Published: 2025-02-05\n",
      "   arXiv ID: 2503.04740v1\n",
      "   URL: http://arxiv.org/pdf/2503.04740v1\n",
      "   Summary: In this work, we propose Perspective Reasoning for Integrated Synthesis and Mediation (PRISM), a multiple-perspective framework for addressing persistent challenges in AI alignment such as conflicting...\n",
      "\n",
      "342. AgentBreeder: Mitigating the AI Safety Risks of Multi-Agent Scaffolds via Self-Improvement\n",
      "   Authors: J Rosser, Jakob Foerster\n",
      "   Published: 2025-02-02\n",
      "   arXiv ID: 2502.00757v4\n",
      "   URL: http://arxiv.org/pdf/2502.00757v4\n",
      "   Summary: Scaffolding Large Language Models (LLMs) into multi-agent systems often improves performance on complex tasks, but the safety impact of such scaffolds has not been thoroughly explored. We introduce Ag...\n",
      "\n",
      "343. A Checks-and-Balances Framework for Context-Aware Ethical AI Alignment\n",
      "   Authors: Edward Y. Chang\n",
      "   Published: 2025-01-31\n",
      "   arXiv ID: 2502.00136v3\n",
      "   URL: http://arxiv.org/pdf/2502.00136v3\n",
      "   Summary: This paper introduces a checks-and-balances framework for ethical alignment of Large Language Models (LLMs), inspired by three-branch governmental systems. It implements three independent yet interact...\n",
      "\n",
      "344. Model-Free RL Agents Demonstrate System 1-Like Intentionality\n",
      "   Authors: Hal Ashton, Matija Franklin\n",
      "   Published: 2025-01-30\n",
      "   arXiv ID: 2501.18299v1\n",
      "   URL: http://arxiv.org/pdf/2501.18299v1\n",
      "   Summary: This paper argues that model-free reinforcement learning (RL) agents, while lacking explicit planning mechanisms, exhibit behaviours that can be analogised to System 1 (\"thinking fast\") processes in h...\n",
      "\n",
      "345. International AI Safety Report\n",
      "   Authors: Yoshua Bengio, Sören Mindermann, Daniel Privitera, Tamay Besiroglu, Rishi Bommasani, Stephen Casper, Yejin Choi, Philip Fox, Ben Garfinkel, Danielle Goldfarb, Hoda Heidari, Anson Ho, Sayash Kapoor, Leila Khalatbari, Shayne Longpre, Sam Manning, Vasilios Mavroudis, Mantas Mazeika, Julian Michael, Jessica Newman, Kwan Yee Ng, Chinasa T. Okolo, Deborah Raji, Girish Sastry, Elizabeth Seger, Theodora Skeadas, Tobin South, Emma Strubell, Florian Tramèr, Lucia Velasco, Nicole Wheeler, Daron Acemoglu, Olubayo Adekanmbi, David Dalrymple, Thomas G. Dietterich, Edward W. Felten, Pascale Fung, Pierre-Olivier Gourinchas, Fredrik Heintz, Geoffrey Hinton, Nick Jennings, Andreas Krause, Susan Leavy, Percy Liang, Teresa Ludermir, Vidushi Marda, Helen Margetts, John McDermid, Jane Munga, Arvind Narayanan, Alondra Nelson, Clara Neppel, Alice Oh, Gopal Ramchurn, Stuart Russell, Marietje Schaake, Bernhard Schölkopf, Dawn Song, Alvaro Soto, Lee Tiedrich, Gaël Varoquaux, Andrew Yao, Ya-Qin Zhang, Fahad Albalawi, Marwan Alserkal, Olubunmi Ajala, Guillaume Avrin, Christian Busch, André Carlos Ponce de Leon Ferreira de Carvalho, Bronwyn Fox, Amandeep Singh Gill, Ahmet Halit Hatip, Juha Heikkilä, Gill Jolly, Ziv Katzir, Hiroaki Kitano, Antonio Krüger, Chris Johnson, Saif M. Khan, Kyoung Mu Lee, Dominic Vincent Ligot, Oleksii Molchanovskyi, Andrea Monti, Nusu Mwamanzi, Mona Nemer, Nuria Oliver, José Ramón López Portillo, Balaraman Ravindran, Raquel Pezoa Rivera, Hammam Riza, Crystal Rugege, Ciarán Seoighe, Jerry Sheehan, Haroon Sheikh, Denise Wong, Yi Zeng\n",
      "   Published: 2025-01-29\n",
      "   arXiv ID: 2501.17805v1\n",
      "   URL: http://arxiv.org/pdf/2501.17805v1\n",
      "   Summary: The first International AI Safety Report comprehensively synthesizes the current evidence on the capabilities, risks, and safety of advanced AI systems. The report was mandated by the nations attendin...\n",
      "\n",
      "346. Challenges in Ensuring AI Safety in DeepSeek-R1 Models: The Shortcomings of Reinforcement Learning Strategies\n",
      "   Authors: Manojkumar Parmar, Yuvaraj Govindarajulu\n",
      "   Published: 2025-01-28\n",
      "   arXiv ID: 2501.17030v1\n",
      "   URL: http://arxiv.org/pdf/2501.17030v1\n",
      "   Summary: Large Language Models (LLMs) have achieved remarkable progress in reasoning, alignment, and task-specific performance. However, ensuring harmlessness in these systems remains a critical challenge, par...\n",
      "\n",
      "347. Gradual Disempowerment: Systemic Existential Risks from Incremental AI Development\n",
      "   Authors: Jan Kulveit, Raymond Douglas, Nora Ammann, Deger Turan, David Krueger, David Duvenaud\n",
      "   Published: 2025-01-28\n",
      "   arXiv ID: 2501.16946v2\n",
      "   URL: http://arxiv.org/pdf/2501.16946v2\n",
      "   Summary: This paper examines the systemic risks posed by incremental advancements in artificial intelligence, developing the concept of `gradual disempowerment', in contrast to the abrupt takeover scenarios co...\n",
      "\n",
      "348. Towards Frontier Safety Policies Plus\n",
      "   Authors: Matteo Pistillo\n",
      "   Published: 2025-01-27\n",
      "   arXiv ID: 2501.16500v1\n",
      "   URL: http://arxiv.org/pdf/2501.16500v1\n",
      "   Summary: This paper examines the state of affairs on Frontier Safety Policies in light of capability progress and growing expectations held by government actors and AI safety researchers from these safety poli...\n",
      "\n",
      "349. What is Harm? Baby Don't Hurt Me! On the Impossibility of Complete Harm Specification in AI Alignment\n",
      "   Authors: Robin Young\n",
      "   Published: 2025-01-27\n",
      "   arXiv ID: 2501.16448v1\n",
      "   URL: http://arxiv.org/pdf/2501.16448v1\n",
      "   Summary: \"First, do no harm\" faces a fundamental challenge in artificial intelligence: how can we specify what constitutes harm? While prior work treats harm specification as a technical hurdle to be overcome ...\n",
      "\n",
      "350. Sequential Decision Making in Stochastic Games with Incomplete Preferences over Temporal Objectives\n",
      "   Authors: Abhishek Ninad Kulkarni, Jie Fu, Ufuk Topcu\n",
      "   Published: 2025-01-27\n",
      "   arXiv ID: 2501.16291v1\n",
      "   URL: http://arxiv.org/pdf/2501.16291v1\n",
      "   Summary: Ensuring that AI systems make strategic decisions aligned with the specified preferences in adversarial sequential interactions is a critical challenge for developing trustworthy AI systems, especiall...\n",
      "\n",
      "351. Beyond Benchmarks: On The False Promise of AI Regulation\n",
      "   Authors: Gabriel Stanovsky, Renana Keydar, Gadi Perl, Eliya Habba\n",
      "   Published: 2025-01-26\n",
      "   arXiv ID: 2501.15693v1\n",
      "   URL: http://arxiv.org/pdf/2501.15693v1\n",
      "   Summary: The rapid advancement of artificial intelligence (AI) systems in critical domains like healthcare, justice, and social services has sparked numerous regulatory initiatives aimed at ensuring their safe...\n",
      "\n",
      "352. Unraveling Token Prediction Refinement and Identifying Essential Layers in Language Models\n",
      "   Authors: Jaturong Kongmanee\n",
      "   Published: 2025-01-25\n",
      "   arXiv ID: 2501.15054v2\n",
      "   URL: http://arxiv.org/pdf/2501.15054v2\n",
      "   Summary: This research aims to unravel how large language models (LLMs) iteratively refine token predictions through internal processing. We utilized a logit lens technique to analyze the model's token predict...\n",
      "\n",
      "353. Why do Experts Disagree on Existential Risk and P(doom)? A Survey of AI Experts\n",
      "   Authors: Severin Field\n",
      "   Published: 2025-01-25\n",
      "   arXiv ID: 2502.14870v1\n",
      "   URL: http://arxiv.org/pdf/2502.14870v1\n",
      "   Summary: The development of artificial general intelligence (AGI) is likely to be one of humanity's most consequential technological advancements. Leading AI labs and scientists have called for the global prio...\n",
      "\n",
      "354. Ensuring Medical AI Safety: Interpretability-Driven Detection and Mitigation of Spurious Model Behavior and Associated Data\n",
      "   Authors: Frederik Pahde, Thomas Wiegand, Sebastian Lapuschkin, Wojciech Samek\n",
      "   Published: 2025-01-23\n",
      "   arXiv ID: 2501.13818v2\n",
      "   URL: http://arxiv.org/pdf/2501.13818v2\n",
      "   Summary: Deep neural networks are increasingly employed in high-stakes medical applications, despite their tendency for shortcut learning in the presence of spurious correlations, which can have potentially fa...\n",
      "\n",
      "355. Towards a Theory of AI Personhood\n",
      "   Authors: Francis Rhys Ward\n",
      "   Published: 2025-01-23\n",
      "   arXiv ID: 2501.13533v1\n",
      "   URL: http://arxiv.org/pdf/2501.13533v1\n",
      "   Summary: I am a person and so are you. Philosophically we sometimes grant personhood to non-human animals, and entities such as sovereign states or corporations can legally be considered persons. But when, if ...\n",
      "\n",
      "356. Watching the AI Watchdogs: A Fairness and Robustness Analysis of AI Safety Moderation Classifiers\n",
      "   Authors: Akshit Achara, Anshuman Chhabra\n",
      "   Published: 2025-01-23\n",
      "   arXiv ID: 2501.13302v1\n",
      "   URL: http://arxiv.org/pdf/2501.13302v1\n",
      "   Summary: AI Safety Moderation (ASM) classifiers are designed to moderate content on social media platforms and to serve as guardrails that prevent Large Language Models (LLMs) from being fine-tuned on unsafe i...\n",
      "\n",
      "357. Debate Helps Weak-to-Strong Generalization\n",
      "   Authors: Hao Lang, Fei Huang, Yongbin Li\n",
      "   Published: 2025-01-21\n",
      "   arXiv ID: 2501.13124v1\n",
      "   URL: http://arxiv.org/pdf/2501.13124v1\n",
      "   Summary: Common methods for aligning already-capable models with desired behavior rely on the ability of humans to provide supervision. However, future superhuman models will surpass the capability of humans. ...\n",
      "\n",
      "358. Tell me about yourself: LLMs are aware of their learned behaviors\n",
      "   Authors: Jan Betley, Xuchan Bao, Martín Soto, Anna Sztyber-Betley, James Chua, Owain Evans\n",
      "   Published: 2025-01-19\n",
      "   arXiv ID: 2501.11120v1\n",
      "   URL: http://arxiv.org/pdf/2501.11120v1\n",
      "   Summary: We study behavioral self-awareness -- an LLM's ability to articulate its behaviors without requiring in-context examples. We finetune LLMs on datasets that exhibit particular behaviors, such as (a) ma...\n",
      "\n",
      "359. Model Monitoring in the Absence of Labeled Data via Feature Attributions Distributions\n",
      "   Authors: Carlos Mougan\n",
      "   Published: 2025-01-18\n",
      "   arXiv ID: 2501.10774v2\n",
      "   URL: http://arxiv.org/pdf/2501.10774v2\n",
      "   Summary: Model monitoring involves analyzing AI algorithms once they have been deployed and detecting changes in their behaviour. This thesis explores machine learning model monitoring ML before the prediction...\n",
      "\n",
      "360. Local US officials' views on the impacts and governance of AI: Evidence from 2022 and 2023 survey waves\n",
      "   Authors: Sophia Hatz, Noemi Dreksler, Kevin Wei, Baobao Zhang\n",
      "   Published: 2025-01-16\n",
      "   arXiv ID: 2501.09606v1\n",
      "   URL: http://arxiv.org/pdf/2501.09606v1\n",
      "   Summary: This paper presents a survey of local US policymakers' views on the future impact and regulation of AI. Our survey provides insight into US policymakers' expectations regarding the effects of AI on lo...\n",
      "\n",
      "361. Clone-Robust AI Alignment\n",
      "   Authors: Ariel D. Procaccia, Benjamin Schiffer, Shirley Zhang\n",
      "   Published: 2025-01-16\n",
      "   arXiv ID: 2501.09254v1\n",
      "   URL: http://arxiv.org/pdf/2501.09254v1\n",
      "   Summary: A key challenge in training Large Language Models (LLMs) is properly aligning them with human preferences. Reinforcement Learning with Human Feedback (RLHF) uses pairwise comparisons from human annota...\n",
      "\n",
      "362. Aegis2.0: A Diverse AI Safety Dataset and Risks Taxonomy for Alignment of LLM Guardrails\n",
      "   Authors: Shaona Ghosh, Prasoon Varshney, Makesh Narsimhan Sreedhar, Aishwarya Padmakumar, Traian Rebedea, Jibin Rajan Varghese, Christopher Parisien\n",
      "   Published: 2025-01-15\n",
      "   arXiv ID: 2501.09004v1\n",
      "   URL: http://arxiv.org/pdf/2501.09004v1\n",
      "   Summary: As Large Language Models (LLMs) and generative AI become increasingly widespread, concerns about content safety have grown in parallel. Currently, there is a clear lack of high-quality, human-annotate...\n",
      "\n",
      "363. Scopes of Alignment\n",
      "   Authors: Kush R. Varshney, Zahra Ashktorab, Djallel Bouneffouf, Matthew Riemer, Justin D. Weisz\n",
      "   Published: 2025-01-15\n",
      "   arXiv ID: 2501.12405v1\n",
      "   URL: http://arxiv.org/pdf/2501.12405v1\n",
      "   Summary: Much of the research focus on AI alignment seeks to align large language models and other foundation models to the context-less and generic values of helpfulness, harmlessness, and honesty. Frontier m...\n",
      "\n",
      "364. Open Problems in Machine Unlearning for AI Safety\n",
      "   Authors: Fazl Barez, Tingchen Fu, Ameya Prabhu, Stephen Casper, Amartya Sanyal, Adel Bibi, Aidan O'Gara, Robert Kirk, Ben Bucknall, Tim Fist, Luke Ong, Philip Torr, Kwok-Yan Lam, Robert Trager, David Krueger, Sören Mindermann, José Hernandez-Orallo, Mor Geva, Yarin Gal\n",
      "   Published: 2025-01-09\n",
      "   arXiv ID: 2501.04952v1\n",
      "   URL: http://arxiv.org/pdf/2501.04952v1\n",
      "   Summary: As AI systems become more capable, widely deployed, and increasingly autonomous in critical areas such as cybersecurity, biological research, and healthcare, ensuring their safety and alignment with h...\n",
      "\n",
      "365. Where AI Assurance Might Go Wrong: Initial lessons from engineering of critical systems\n",
      "   Authors: Robin Bloomfield, John Rushby\n",
      "   Published: 2025-01-07\n",
      "   arXiv ID: 2502.03467v1\n",
      "   URL: http://arxiv.org/pdf/2502.03467v1\n",
      "   Summary: We draw on our experience working on system and software assurance and evaluation for systems important to society to summarise how safety engineering is performed in traditional critical systems, suc...\n",
      "\n",
      "366. CALM: Curiosity-Driven Auditing for Large Language Models\n",
      "   Authors: Xiang Zheng, Longxiang Wang, Yi Liu, Xingjun Ma, Chao Shen, Cong Wang\n",
      "   Published: 2025-01-06\n",
      "   arXiv ID: 2501.02997v1\n",
      "   URL: http://arxiv.org/pdf/2501.02997v1\n",
      "   Summary: Auditing Large Language Models (LLMs) is a crucial and challenging task. In this study, we focus on auditing black-box LLMs without access to their parameters, only to the provided service. We treat t...\n",
      "\n",
      "367. Towards New Benchmark for AI Alignment & Sentiment Analysis in Socially Important Issues: A Comparative Study of Human and LLMs in the Context of AGI\n",
      "   Authors: Ljubisa Bojic, Dylan Seychell, Milan Cabarkapa\n",
      "   Published: 2025-01-05\n",
      "   arXiv ID: 2501.02531v3\n",
      "   URL: http://arxiv.org/pdf/2501.02531v3\n",
      "   Summary: As general-purpose artificial intelligence systems become increasingly integrated into society and are used for information seeking, content generation, problem solving, textual analysis, coding, and ...\n",
      "\n",
      "368. Rerouting LLM Routers\n",
      "   Authors: Avital Shafran, Roei Schuster, Thomas Ristenpart, Vitaly Shmatikov\n",
      "   Published: 2025-01-03\n",
      "   arXiv ID: 2501.01818v1\n",
      "   URL: http://arxiv.org/pdf/2501.01818v1\n",
      "   Summary: LLM routers aim to balance quality and cost of generation by classifying queries and routing them to a cheaper or more expensive LLM depending on their complexity. Routers represent one type of what w...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Simple search query\n",
    "# query = \"ai alignment\"\n",
    "query = 'cat:cs.* AND (all:\"ai safety\" OR all:\"ai alignment\") AND submittedDate:[202501010000 TO 202512312359]'\n",
    "# query = 'cat:cs.* AND (all:\"ai alignment\") AND submittedDate:[202501010000 TO 202512312359]'\n",
    "results = search_arxiv_large(query, limit=1000)\n",
    "\n",
    "# Display results\n",
    "print(f\"Found {len(results)} papers for query: '{query}'\\n\")\n",
    "for i, paper in enumerate(results, 1):\n",
    "    print(f\"{i}. {paper['title']}\")\n",
    "    print(f\"   Authors: {paper['authors']}\")\n",
    "    print(f\"   Published: {paper['published']}\")\n",
    "    print(f\"   arXiv ID: {paper['arxiv_id']}\")\n",
    "    print(f\"   URL: {paper['pdf_url']}\")\n",
    "    print(f\"   Summary: {paper['summary'][:200]}...\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b18503d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>published</th>\n",
       "      <th>updated</th>\n",
       "      <th>summary</th>\n",
       "      <th>primary_category</th>\n",
       "      <th>categories</th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>pdf_url</th>\n",
       "      <th>doi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Towards General Modality Translation with Cont...</td>\n",
       "      <td>Nimrod Berman, Omkar Joglekar, Eitan Kosman, D...</td>\n",
       "      <td>2025-10-23</td>\n",
       "      <td>2025-10-23</td>\n",
       "      <td>Recent advances in generative modeling have po...</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>cs.CV, cs.AI, cs.LG</td>\n",
       "      <td>2510.20819v1</td>\n",
       "      <td>http://arxiv.org/pdf/2510.20819v1</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Generative Reasoning Recommendation via LLMs</td>\n",
       "      <td>Minjie Hong, Zetong Zhou, Zirun Guo, Ziang Zha...</td>\n",
       "      <td>2025-10-23</td>\n",
       "      <td>2025-10-23</td>\n",
       "      <td>Despite their remarkable reasoning capabilitie...</td>\n",
       "      <td>cs.IR</td>\n",
       "      <td>cs.IR</td>\n",
       "      <td>2510.20815v1</td>\n",
       "      <td>http://arxiv.org/pdf/2510.20815v1</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Real Deep Research for AI, Robotics and Beyond</td>\n",
       "      <td>Xueyan Zou, Jianglong Ye, Hao Zhang, Xiaoyu Xi...</td>\n",
       "      <td>2025-10-23</td>\n",
       "      <td>2025-10-23</td>\n",
       "      <td>With the rapid growth of research in AI and ro...</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>cs.AI, cs.CL, cs.CV, cs.LG</td>\n",
       "      <td>2510.20809v1</td>\n",
       "      <td>http://arxiv.org/pdf/2510.20809v1</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bilevel Analysis of Cost and Emissions Externa...</td>\n",
       "      <td>Aron Brenner, Rahman Khorramfar, Nathan Engelm...</td>\n",
       "      <td>2025-10-23</td>\n",
       "      <td>2025-10-23</td>\n",
       "      <td>Data centers are emerging as large, flexible e...</td>\n",
       "      <td>eess.SY</td>\n",
       "      <td>eess.SY, cs.SY</td>\n",
       "      <td>2510.20805v1</td>\n",
       "      <td>http://arxiv.org/pdf/2510.20805v1</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AI-Enabled Digital Twins for Next-Generation N...</td>\n",
       "      <td>John Sengendo, Fabrizio Granelli</td>\n",
       "      <td>2025-10-23</td>\n",
       "      <td>2025-10-23</td>\n",
       "      <td>As 5G and future 6G mobile networks become inc...</td>\n",
       "      <td>cs.NI</td>\n",
       "      <td>cs.NI</td>\n",
       "      <td>2510.20796v1</td>\n",
       "      <td>http://arxiv.org/pdf/2510.20796v1</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Design Optimization and Global Impact Assessme...</td>\n",
       "      <td>Zhiyuan Fan, Bolun Xu</td>\n",
       "      <td>2025-10-23</td>\n",
       "      <td>2025-10-23</td>\n",
       "      <td>The dual challenge of decarbonizing the econom...</td>\n",
       "      <td>eess.SY</td>\n",
       "      <td>eess.SY, cs.SY</td>\n",
       "      <td>2510.20135v1</td>\n",
       "      <td>http://arxiv.org/pdf/2510.20135v1</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>SAID: Empowering Large Language Models with Se...</td>\n",
       "      <td>Yulong Chen, Yadong Liu, Jiawen Zhang, Mu Li, ...</td>\n",
       "      <td>2025-10-23</td>\n",
       "      <td>2025-10-23</td>\n",
       "      <td>Large Language Models (LLMs), despite advances...</td>\n",
       "      <td>cs.CR</td>\n",
       "      <td>cs.CR, cs.AI</td>\n",
       "      <td>2510.20129v1</td>\n",
       "      <td>http://arxiv.org/pdf/2510.20129v1</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>\"Learning Together\": AI-Mediated Support for P...</td>\n",
       "      <td>Yao Li, Jingyi Xie, Ya-Fang Ling, He Zhang, Ge...</td>\n",
       "      <td>2025-10-23</td>\n",
       "      <td>2025-10-23</td>\n",
       "      <td>Family learning takes place in everyday routin...</td>\n",
       "      <td>cs.HC</td>\n",
       "      <td>cs.HC</td>\n",
       "      <td>2510.20123v1</td>\n",
       "      <td>http://arxiv.org/pdf/2510.20123v1</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>There is No \"apple\" in Timeseries: Rethinking ...</td>\n",
       "      <td>Arian Prabowo, Flora D. Salim</td>\n",
       "      <td>2025-10-23</td>\n",
       "      <td>2025-10-23</td>\n",
       "      <td>Timeseries foundation models (TSFMs) have mult...</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>2510.20119v1</td>\n",
       "      <td>http://arxiv.org/pdf/2510.20119v1</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>The Verification-Value Paradox: A Normative Cr...</td>\n",
       "      <td>Joshua Yuvaraj</td>\n",
       "      <td>2025-10-23</td>\n",
       "      <td>2025-10-23</td>\n",
       "      <td>It is often claimed that machine learning-base...</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>2510.20109v1</td>\n",
       "      <td>http://arxiv.org/pdf/2510.20109v1</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0   Towards General Modality Translation with Cont...   \n",
       "1        Generative Reasoning Recommendation via LLMs   \n",
       "2      Real Deep Research for AI, Robotics and Beyond   \n",
       "3   Bilevel Analysis of Cost and Emissions Externa...   \n",
       "4   AI-Enabled Digital Twins for Next-Generation N...   \n",
       "..                                                ...   \n",
       "95  Design Optimization and Global Impact Assessme...   \n",
       "96  SAID: Empowering Large Language Models with Se...   \n",
       "97  \"Learning Together\": AI-Mediated Support for P...   \n",
       "98  There is No \"apple\" in Timeseries: Rethinking ...   \n",
       "99  The Verification-Value Paradox: A Normative Cr...   \n",
       "\n",
       "                                              authors   published     updated  \\\n",
       "0   Nimrod Berman, Omkar Joglekar, Eitan Kosman, D...  2025-10-23  2025-10-23   \n",
       "1   Minjie Hong, Zetong Zhou, Zirun Guo, Ziang Zha...  2025-10-23  2025-10-23   \n",
       "2   Xueyan Zou, Jianglong Ye, Hao Zhang, Xiaoyu Xi...  2025-10-23  2025-10-23   \n",
       "3   Aron Brenner, Rahman Khorramfar, Nathan Engelm...  2025-10-23  2025-10-23   \n",
       "4                    John Sengendo, Fabrizio Granelli  2025-10-23  2025-10-23   \n",
       "..                                                ...         ...         ...   \n",
       "95                              Zhiyuan Fan, Bolun Xu  2025-10-23  2025-10-23   \n",
       "96  Yulong Chen, Yadong Liu, Jiawen Zhang, Mu Li, ...  2025-10-23  2025-10-23   \n",
       "97  Yao Li, Jingyi Xie, Ya-Fang Ling, He Zhang, Ge...  2025-10-23  2025-10-23   \n",
       "98                      Arian Prabowo, Flora D. Salim  2025-10-23  2025-10-23   \n",
       "99                                     Joshua Yuvaraj  2025-10-23  2025-10-23   \n",
       "\n",
       "                                              summary primary_category  \\\n",
       "0   Recent advances in generative modeling have po...            cs.CV   \n",
       "1   Despite their remarkable reasoning capabilitie...            cs.IR   \n",
       "2   With the rapid growth of research in AI and ro...            cs.AI   \n",
       "3   Data centers are emerging as large, flexible e...          eess.SY   \n",
       "4   As 5G and future 6G mobile networks become inc...            cs.NI   \n",
       "..                                                ...              ...   \n",
       "95  The dual challenge of decarbonizing the econom...          eess.SY   \n",
       "96  Large Language Models (LLMs), despite advances...            cs.CR   \n",
       "97  Family learning takes place in everyday routin...            cs.HC   \n",
       "98  Timeseries foundation models (TSFMs) have mult...            cs.LG   \n",
       "99  It is often claimed that machine learning-base...            cs.AI   \n",
       "\n",
       "                    categories      arxiv_id  \\\n",
       "0          cs.CV, cs.AI, cs.LG  2510.20819v1   \n",
       "1                        cs.IR  2510.20815v1   \n",
       "2   cs.AI, cs.CL, cs.CV, cs.LG  2510.20809v1   \n",
       "3               eess.SY, cs.SY  2510.20805v1   \n",
       "4                        cs.NI  2510.20796v1   \n",
       "..                         ...           ...   \n",
       "95              eess.SY, cs.SY  2510.20135v1   \n",
       "96                cs.CR, cs.AI  2510.20129v1   \n",
       "97                       cs.HC  2510.20123v1   \n",
       "98                       cs.LG  2510.20119v1   \n",
       "99                       cs.AI  2510.20109v1   \n",
       "\n",
       "                              pdf_url  doi  \n",
       "0   http://arxiv.org/pdf/2510.20819v1  N/A  \n",
       "1   http://arxiv.org/pdf/2510.20815v1  N/A  \n",
       "2   http://arxiv.org/pdf/2510.20809v1  N/A  \n",
       "3   http://arxiv.org/pdf/2510.20805v1  N/A  \n",
       "4   http://arxiv.org/pdf/2510.20796v1  N/A  \n",
       "..                                ...  ...  \n",
       "95  http://arxiv.org/pdf/2510.20135v1  N/A  \n",
       "96  http://arxiv.org/pdf/2510.20129v1  N/A  \n",
       "97  http://arxiv.org/pdf/2510.20123v1  N/A  \n",
       "98  http://arxiv.org/pdf/2510.20119v1  N/A  \n",
       "99  http://arxiv.org/pdf/2510.20109v1  N/A  \n",
       "\n",
       "[100 rows x 10 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 2: Convert results to a pandas DataFrame for easier viewing\n",
    "df = pd.DataFrame(results)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f9c2ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to results/arxiv_search_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Example 4: Save results to CSV\n",
    "def save_results_to_csv(results, filename='arxiv_results.csv'):\n",
    "    \"\"\"Save search results to a CSV file\"\"\"\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Results saved to {filename}\")\n",
    "\n",
    "# Uncomment to save:\n",
    "save_results_to_csv(results, 'results/arxiv_search_results.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7358a9b7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
