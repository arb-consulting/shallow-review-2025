title,url
https://www.alignmentforum.org/posts/79BPxvSsjzBkiSyTq/agi-safety-and-alignment-at-google-deepmind-a-summary-of,https://www.alignmentforum.org/posts/79BPxvSsjzBkiSyTq/agi-safety-and-alignment-at-google-deepmind-a-summary-of#Emerging_Topics
https://arxiv.org/abs/2509.15541,https://arxiv.org/abs/2509.15541
https://arxiv.org/abs/2501.18841,https://arxiv.org/abs/2501.18841
Persona Features,https://www.arxiv.org/pdf/2506.19823
CoT Defence,https://arxiv.org/abs/2503.11926
Madry data attribution,https://arxiv.org/abs/2505.16260
Alignment Science,https://alignment.anthropic.com/
Safeguards,https://alignment.anthropic.com/2025/introducing-safeguards-research-team/
Claude’s Character,https://www.anthropic.com/research/claude-character
Interpretability,https://www.anthropic.com/research/tracing-thoughts-language-model
Zhou et al,https://arxiv.org/abs/2503.06378
"Hendrycks, Song et al",https://www.agidefinition.ai/
third-wave AI safety,https://www.lesswrong.com/posts/6YxdpGjfHyrZb7F2G/third-wave-ai-safety-needs-sociopolitical-thinking
What's going on? (July 2025),https://docs.google.com/presentation/d/1IxNCFB81jy3_Xf5ogZLYUS8kgRlhRdIVV5EYSb4VfBo/edit?slide=id.g35035bb93ce_0_0#slide=id.g35035bb93ce_0_0
https://x.com/RyanPGreenblatt/status/1949912100601811381,https://x.com/RyanPGreenblatt/status/1949912100601811381
low-dimensional,https://blog.evanchen.cc/2017/04/08/on-designing-olympiad-training/
an AI alignment team,https://www.alignmentforum.org/posts/tbnw7LbNApvxNLAg8/uk-aisi-s-alignment-team-research-agenda
https://www.lesswrong.com/posts/bb5Tnjdrptu89rcyY/what-s-the-short-timeline-plan,https://www.lesswrong.com/posts/bb5Tnjdrptu89rcyY/what-s-the-short-timeline-plan#comments
What is the current safety stack?,https://docs.google.com/document/d/1W1jLPqTZ56eVJJdErxROhiQKwOEiay-iWQKqfdlAmS0/edit?tab=t.0#heading=h.v4a088m71j9n
https://arxiv.org/abs/2506.24068,https://arxiv.org/abs/2506.24068
Open Global Investment,https://www.lesswrong.com/posts/LtT24cCAazQp4NYc5/open-global-investment-as-a-governance-model-for-agi
sometimes,https://manifold.markets/NeelNanda/will-sparse-autoencoders-be-success#g9gg9z7k8q
somewhat,https://x.com/NeelNanda5/status/1965485174411649259
discover,https://arxiv.org/abs/2503.10965
run,https://x.com/OBalcells/status/1965434564748447921
Some experiments,https://www.alignmentforum.org/posts/NfgukR3HSaRq4WhyW/tim-hua-s-shortform?commentId=Tri4tc67GgGq7erfk
model card,https://github.com/openai/gpt-3/blob/master/model-card.md
Building and evaluating alignment auditing agents,https://alignment.anthropic.com/2025/automated-auditing/
Blind auditing language models for hidden objectives,https://www.lesswrong.com/posts/wSKPuBfgkkqfTpmWJ/auditing-language-models-for-hidden-objectives
https://www.arxiv.org/pdf/2510.16255,https://www.arxiv.org/pdf/2510.16255
already starting to look weird,https://www.antischeming.ai/snippets
https://x.com/Jack_W_Lindsey/status/1972732882893578693,https://x.com/Jack_W_Lindsey/status/1972732882893578693
CoT Faithfulness Defence League,https://arxiv.org/abs/2507.11473
leading,https://openai.com/index/chain-of-thought-monitoring/
stopped,https://x.com/sleepinyourhat/status/1978507448018231495
HAAISS,https://www.youtube.com/@ACSResearch
IASEAI,https://www.youtube.com/@IASEAI/videos
ILIAD,https://www.youtube.com/@ILIADConference/videos
Third-wave mechanistic interpretability,https://www.lesswrong.com/posts/beREnXhBnzxbJtr8k/mech-interp-is-not-pre-paradigmatic#Toward__Third_Wave__Mechanistic_Interpretability
https://x.com/sleepinyourhat/status/1960749648110395467,https://x.com/sleepinyourhat/status/1960749648110395467
https://arxiv.org/pdf/2507.03409,https://arxiv.org/pdf/2507.03409
remarkably easy,https://www.alignmentforum.org/posts/gLDSqQm8pwNiq7qst/narrow-misalignment-is-hard-emergent-misalignment-is-easy
Here,https://carnegieendowment.org/research/2025/10/how-china-views-ai-risks-and-what-to-do-about-them?lang=en
are,https://arxiv.org/pdf/2506.20702
some,https://www.gov.uk/government/publications/international-ai-safety-report-2025/international-ai-safety-report-2025#about-this-report
links,https://www.whitehouse.gov/wp-content/uploads/2025/07/Americas-AI-Action-Plan.pdf
on,https://mkodama.org/content/EU-code/
international,https://ai-frontiers.org/articles/ai-deterrence-is-our-best-option?utm_source=newsletter
https://apartresearch.com/#get-started,https://apartresearch.com/#get-started
https://coda.io/@alignmentdev/ai-safety-info,https://coda.io/@alignmentdev/ai-safety-info
https://researcher2.eleuther.ai/get-involved/,https://researcher2.eleuther.ai/get-involved/
https://aisafety.quest/#volunteer,https://aisafety.quest/#volunteer
https://aisafetyawarenessproject.org/next-steps/,https://aisafetyawarenessproject.org/next-steps/
https://www.theomachialabs.com/,https://www.theomachialabs.com/
What's next?,https://www.lesswrong.com/posts/yew6zFWAKG4AGs3Wk/foom-and-doom-1-brain-in-a-box-in-a-basement
https://windowsontheory.org/2025/06/24/machines-of-faithful-obedience/,https://windowsontheory.org/2025/06/24/machines-of-faithful-obedience/
https://x.com/g_leech_/status/1969659430988857797,https://x.com/g_leech_/status/1969659430988857797
https://openai.com/index/introducing-agentkit/,https://openai.com/index/introducing-agentkit/
other,https://metr.org/blog/2025-07-14-how-does-time-horizon-vary-across-domains/
https://www.lesswrong.com/posts/PzLSuaT6WGLQGJJJD/the-length-of-horizons,https://www.lesswrong.com/posts/PzLSuaT6WGLQGJJJD/the-length-of-horizons
ADeLe,https://kinds-of-intelligence-cfi.github.io/ADELE/
CHC-AI,https://www.agidefinition.ai/paper.pdf
GDPval,https://arxiv.org/abs/2510.04374
Humanity's Last Exam,https://arxiv.org/abs/2501.14249
METR: Measuring AI Ability to Complete Long Tasks,https://www.alignmentforum.org/posts/deesrjitvXM4xYGZd/metr-measuring-ai-ability-to-complete-long-tasks
Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity,https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/
Vending-Bench,https://arxiv.org/abs/2502.15840
Claude running a vending machine,https://www.anthropic.com/research/project-vend-1
Claude plays pokemon,https://www.twitch.tv/claudeplayspokemon
"Two-hop reasoning: Models fail to compose two synthetic facts, but can succeed when one fact is synthetic and the other is natural",https://www.lesswrong.com/posts/MdKWqFrNstiZQ3G6K/lessons-from-studying-two-hop-latent-reasoning
ARC-AGI 2,https://arxiv.org/pdf/2505.11831
Virology Capabilities Test (VCT): A Multimodal Virology Q&A Benchmark,https://arxiv.org/abs/2504.16137
LLMs Outperform Experts on Challenging Biology Benchmarks,https://arxiv.org/abs/2505.06108
SciGym,https://arxiv.org/pdf/2507.02083
Evaluating Frontier Models for Stealth and Situational Awareness,https://arxiv.org/abs/2505.01420
Models Often Know When They Are Being Evaluated,https://arxiv.org/abs/2505.23836
4.5 Sonnet’s system card,https://assets.anthropic.com/m/12f214efcc2f457a/original/Claude-Sonnet-4-5-System-Card.pdf
o3 chain of thought,https://www.antischeming.ai/
Apollo's research note on Claude 3.7 in alignment evals,https://www.apolloresearch.ai/blog/claude-sonnet-37-often-knows-when-its-in-alignment-evaluations
LLMs are aware of their learned behaviors,https://openreview.net/forum?id=IjQ2Jtemzy
AI Awareness literature review,https://arxiv.org/abs/2504.20084
Probing and Steering Evaluation Awareness of Language Models,https://arxiv.org/pdf/2507.01786
Early Signs of Steganographic Capabilities in Frontier LLMs,https://www.lesswrong.com/posts/nwx6duiDZcHatbpPT/untitled-draft-6osz
LLMs can learn Steganographic Reasoning via RL,https://www.lesswrong.com/posts/KRKnFdECZMu3Ej3z6/can-llms-learn-steganographic-reasoning-via-rl
LLMs can learn and generalize steganographic chain-of-thought under process supervision,https://arxiv.org/abs/2506.01926
Secret Collusion among AI Agents: Multi-Agent Deception via Steganography,https://arxiv.org/abs/2402.07510
Covert Sandbagging Against Chain-of-Thought Monitoring,https://arxiv.org/abs/2508.00943
Automated researchers can subtly sandbag,https://alignment.anthropic.com/2025/automated-researchers-sandbag
RepliBench: Evaluating the Autonomous Replication Capabilities of Language Model Agents,https://arxiv.org/pdf/2504.18565
Frontier AI systems have surpassed the self-replicating red line,https://arxiv.org/pdf/2412.12140
https://arxiv.org/abs/2510.07192,https://arxiv.org/abs/2510.07192
Lessons from a chimp,https://www.arxiv.org/abs/2507.03409
Emergent Misalignment: Narrow finetuning on backdoored code can produce broadly misaligned LLMs,https://www.lesswrong.com/posts/ifechgnJRtJdduFGC/emergent-misalignment-narrow-finetuning-can-produce-broadly
"Narrow Misalignment is Hard, Emergent Misalignment is Easy",https://www.lesswrong.com/posts/gLDSqQm8pwNiq7qst/narrow-misalignment-is-hard-emergent-misalignment-is-easy
aesthetic preferences,https://www.lesswrong.com/posts/gT3wtWBAs7PKonbmy/aesthetic-preferences-can-cause-emergent-misalignment
profanities,https://www.lesswrong.com/posts/b8vhTpQiQsqbmi3tx/profanity-causes-emergent-misalignment-but-with
any other crap,https://www.lesswrong.com/posts/pGMRzJByB67WfSvpy/will-any-crap-cause-emergent-misalignment#8ykTNJ8NBJXb5XoWh
Emergent misalignment in reasoning models,https://www.lesswrong.com/posts/zzZ6jye3ukiNyMCmC/thought-crime-backdoors-and-emergent-misalignment-in
Model Organisms for Emergent Misalignment,https://www.lesswrong.com/posts/yHmJrDSJpFaNTZ9Tr/model-organisms-for-emergent-misalignment
Toward understanding and preventing misalignment generalization,https://openai.com/index/emergent-misalignment/
Finding Emergent Misalignment,https://www.lesswrong.com/posts/tgHps2cxiGDkNxNZN/finding-emergent-misalignment
Convergent Linear Representations of Emergent Misalignment,https://arxiv.org/abs/2506.11618
[2506.13206] Thought Crime: Backdoors and Emergent Misalignment in Reasoning Models,https://arxiv.org/abs/2506.13206
[2506.19823] Persona Features Control Emergent Misalignment,https://arxiv.org/abs/2506.19823
[2506.11618] Convergent Linear Representations of Emergent Misalignment,https://arxiv.org/abs/2506.11618
Model Organisms for Emergent Misalignment,https://arxiv.org/pdf/2506.11613
"Narrow Misalignment is Hard, Emergent Misalignment is Easy — LessWrong",https://www.lesswrong.com/posts/gLDSqQm8pwNiq7qst/narrow-misalignment-is-hard-emergent-misalignment-is-easy
Thread on EM,https://threadreaderapp.com/thread/1894436637054214509.html
Subliminal Learning: LLMs Transmit Behavioral Traits via Hidden Signals in Data,https://www.lesswrong.com/posts/cGcwQDKAKbQ68BGuR/subliminal-learning-llms-transmit-behavioral-traits-via
Hacking harmless tasks generalizes to misaligned behavior,https://arxiv.org/abs/2508.17511
Training on Documents About Reward Hacking Induces Reward Hacking,https://www.lesswrong.com/posts/qXYLvjGL9QvD3aFSW/training-on-documents-about-reward-hacking-induces-reward
US AISI on hijacking AI agents,https://www.nist.gov/news-events/news/2025/01/technical-blog-strengthening-ai-agent-hijacking-evaluations
Constitutional Classifiers,https://arxiv.org/abs/2501.18837
Prism's automated problematic behavior elicitation,https://prism-eval.ai
Why Do Some Language Models Fake Alignment While Others Don't?,https://www.lesswrong.com/posts/ghESoA8mo3fv9Yx3E/why-do-some-language-models-fake-alignment-while-others-don
Alignment Faking Revisited: Improved Classifiers and Open Source Extensions,https://www.lesswrong.com/posts/Fr4QsQT52RFKHvCAH/alignment-faking-revisited-improved-classifiers-and-open
Mistral Large 2 (123B) seems to exhibit alignment faking,https://www.lesswrong.com/posts/kCGk5tp5suHoGwhCa/mistral-large-2-123b-seems-to-exhibit-alignment-faking
Agentic Misalignment: How LLMs Could be Insider Threats,https://www.lesswrong.com/posts/b8eeCGe3FWzHKbePF/agentic-misalignment-how-llms-could-be-insider-threats-1
The Elicitation Game: Evaluating capability elicitation techniques,https://www.lesswrong.com/posts/6QA5eHBEqpAicCwbh/the-elicitation-game-evaluating-capability-elicitation
Can a Neural Network that only Memorizes the Dataset be Undetectably Backdoored? | OpenReview,https://openreview.net/forum?id=TD1NfQuVr6
Auditing Claude 4.5 Sonnet’s evaluation awareness,https://assets.anthropic.com/m/12f214efcc2f457a/original/Claude-Sonnet-4-5-System-Card.pdf
Detecting sandbagging model organisms,https://www.lesswrong.com/posts/pPEeMdgjpjHZWCDFw/white-box-control-at-uk-aisi-update-on-sandbagging#Update_3__Fine_tuned_Sandbagging_Model_Organisms
Do Models Say What They Learn?,https://www.lesswrong.com/posts/abtegBoDfnCzewndm/do-models-say-what-they-learn
Eliciting secret knowledge from language models,https://www.lesswrong.com/posts/Mv3yg7wMXfns3NPaz/eliciting-secret-knowledge-from-language-models-1
Narrow Finetuning Leaves Clearly Readable Traces in Activation Differences,https://www.lesswrong.com/posts/sBSjEBykQkmSfqrwt/narrow-finetuning-leaves-clearly-readable-traces-in
[2507.11473] Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety,https://arxiv.org/abs/2507.11473
Open Problems in Mechanistic Interpretability,https://arxiv.org/abs/2501.16496
Tracing the Thoughts of a Large Language Model,https://www.alignmentforum.org/posts/zsr4rWRASxwmgXfmq/tracing-the-thoughts-of-a-large-language-model
Decomposing neural network parameters into mechanistic components,https://www.alignmentforum.org/posts/EPefYWjuHNcNH4C7E/attribution-based-parameter-decomposition
Stochastic Parameter Decomposition,https://www.lesswrong.com/posts/yjrpmCmqurDmbMztW/paper-stochastic-parameter-decomposition
See also: Mech interp is not pre-paradigmatic,https://www.alignmentforum.org/posts/beREnXhBnzxbJtr8k/mech-interp-is-not-pre-paradigmatic
SAEBench: A Comprehensive Benchmark for Sparse Autoencoders,https://www.lesswrong.com/posts/jGG24BzLdYvi9dugm/saebench-a-comprehensive-benchmark-for-sparse-autoencoders
Negative Results for SAEs On Downstream Tasks and Deprioritising SAE Research,https://www.lesswrong.com/posts/4uXCAJNuPKtKBsi28/negative-results-for-saes-on-downstream-tasks
Steering Language Model Refusal with Sparse Autoencoders,https://arxiv.org/abs/2411.11296
Sparse autoencoders trained on the same data learn different features,https://arxiv.org/abs/2501.16615
Transcoders Beat Sparse Autoencoders for Interpretability,https://arxiv.org/abs/2501.18823
Evaluating SAE interpretability without explanations,https://arxiv.org/abs/2507.08473
"Dense SAE Latents Are Features, Not Bugs",https://arxiv.org/abs/2506.15679
Diffing Base and Chat Models (And Why It Matters),https://www.lesswrong.com/posts/xmpauEXEerzYcJKNm/what-we-learned-trying-to-diff-base-and-chat-models-and-why
https://arxiv.org/pdf/2510.08638,https://arxiv.org/pdf/2510.08638
Convergent Linear Representations of Emergent Misalignment,https://www.lesswrong.com/posts/umYzsh7SGHHKsRCaA/convergent-linear-representations-of-emergent-misalignment
Misaligned persona SAE features promoting or suppressing emergent misalignment,https://openai.com/index/emergent-misalignment/
Persona Vectors,https://www.lesswrong.com/posts/M77rptNcp5B8JugRx/persona-vectors-monitoring-and-controlling-character-traits
Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models,https://arxiv.org/abs/2411.14257
Real-Time Detection of Hallucinated Entities,https://arxiv.org/abs/2509.03531
Reward Model Interpretability via Optimal and Pessimal Tokens,https://arxiv.org/abs/2506.07326
Refusal in LLMs is an Affine Function,https://arxiv.org/abs/2411.09003
Representation of LLM Hallucinations,https://arxiv.org/abs/2410.02707
Training-order recency is linearly encoded in language model activations,https://arxiv.org/abs/2509.14223
Attention attribution,https://arxiv.org/abs/2504.13752
Activation space interpretability may be doomed,https://www.lesswrong.com/posts/gYfpPbww3wQRaxAFD/activation-space-interpretability-may-be-doomed
Interpretability Will Not Reliably Find Deceptive AI,https://www.alignmentforum.org/posts/PwnadG4BFjaER3MGf/interpretability-will-not-reliably-find-deceptive-ai
You Are What You Eat -- AI Alignment Requires Understanding How Data Shapes Structure and Generalisation,https://arxiv.org/abs/2502.05475
Bayesian Influence Functions for Hessian-Free Data Attribution,https://arxiv.org/abs/2509.26544
The Loss Kernel: A Geometric Probe for Deep Learning Interpretability,https://arxiv.org/abs/2509.26537
From Global to Local: A Scalable Benchmark for Local Posterior Sampling,https://arxiv.org/abs/2507.21449
Structural Inference: Interpreting Small Language Models with Susceptibilities,https://arxiv.org/abs/2504.18274
Dynamics of Transient Structure in In-Context Linear Regression Transformers,https://arxiv.org/abs/2501.17745
Vaintrob ~critique,https://www.lesswrong.com/posts/M2bs6xCbmc79nwr8j/dmitry-vaintrob-s-shortform#A8Ziwhts35dgqbz52
Next-token pretraining implies in-context learning,https://arxiv.org/abs/2505.18373
Neural networks leverage nominally quantum and post-quantum representations,https://arxiv.org/abs/2507.07432
Constrained belief updates explain geometric structures in transformer representations,https://arxiv.org/abs/2502.01954
Simplex Progress Report - July 2025,https://www.lesswrong.com/posts/fhkurwqhjZopx8DKK/simplex-progress-report-july-2025
Deliberative alignment: reasoning enables safer language models,https://openai.com/index/deliberative-alignment/
Stress Testing Deliberative Alignment for Anti-Scheming Training,https://www.lesswrong.com/posts/JmRfgNYCrYogCq7ny/stress-testing-deliberative-alignment-for-anti-scheming
Myopic Optimization with Non-myopic Approval Can Mitigate Multi-step Reward Hacking,https://www.lesswrong.com/posts/zWySWKuXnhMDhgwc3/mona-managed-myopia-with-approval-feedback-2
MONA: Three Month Later - Updates and Steganography Without Optimization Pressure,https://www.lesswrong.com/posts/zF5gXf3KJhmvojDRf/mona-three-month-later-updates-and-steganography-without
Inoculation prompting,https://arxiv.org/abs/2510.04340
Modifying LLM Beliefs with Synthetic Document Finetuning,http://alignment.anthropic.com/2025/modifying-beliefs-via-sdf/
Can DPO Learn Diverse Human Values? A Theoretical Scaling Law,https://arxiv.org/html/2408.03459v4
Ctrl-Z: Controlling AI Agents via Resampling,https://www.lesswrong.com/posts/LPHMMMZFAWog6ty5x/ctrl-z-controlling-ai-agents-via-resampling
A sketch of an AI control safety case,https://www.lesswrong.com/posts/vWYzSorAEWwoJnnXq/a-sketch-of-an-ai-control-safety-case
Control Arena,https://control-arena.aisi.org.uk/
UK AISI's control agenda,https://www.lesswrong.com/s/wvLzDiWQWBC9b5HGa/p/rGcg4XDPDzBFuqNJz
How to evaluate control measures for LLM agents? A trajectory from today to superintelligence,https://arxiv.org/abs/2504.05259
SHADE-Arena: Evaluating Sabotage and Monitoring in LLM Agents,https://arxiv.org/abs/2506.15740
D-REX: A Benchmark for Detecting Deceptive Reasoning,https://arxiv.org/abs/2509.17938
Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety,https://arxiv.org/abs/2507.11473
Reasoning Models Don't Always Say What They Think,https://arxiv.org/abs/2505.05410
Chain-of-Thought Reasoning In The Wild Is Not Always Faithful,https://arxiv.org/abs/2503.08679
Are DeepSeek R1 And Other Reasoning Models More Faithful?,https://arxiv.org/abs/2501.08156
Detecting misbehavior in frontier reasoning models,https://openai.com/index/chain-of-thought-monitoring
Mitigating Deceptive Alignment via Self-Monitoring,https://arxiv.org/pdf/2505.18807
"When Chain of Thought is Necessary,",https://arxiv.org/pdf/2507.05246
,https://arxiv.org/pdf/2507.05246
Language Models Struggle to Evade Monitors,https://arxiv.org/pdf/2507.05246
Detecting Strategic Deception Using Linear Probes,https://www.lesswrong.com/posts/9pGbTz6c78PGwJein/detecting-strategic-deception-using-linear-probes
Detecting High-Stakes Interactions with Activation Probes,https://arxiv.org/abs/2506.10805
Whitebox detection of sandbagging model organisms,https://www.lesswrong.com/posts/pPEeMdgjpjHZWCDFw/white-box-control-at-uk-aisi-update-on-sandbagging
Benchmarking deception probes for trusted monitoring,https://www.lesswrong.com/posts/eaEqAzGN3uJfpfGoc/trusted-monitoring-but-with-deception-probes
Probes and SAEs do well on Among Us benchmark,https://www.lesswrong.com/posts/gRc8KL2HLtKkFmNPr/among-us-a-sandbox-for-agentic-deception
18 Applications of Deception Probes,https://www.lesswrong.com/posts/7zhAwcBri7yupStKy/18-applications-of-deception-probes
White Box Control at UK AISI - Update on Sandbagging Investigations — AI Alignment Forum,https://www.alignmentforum.org/posts/pPEeMdgjpjHZWCDFw/white-box-control-at-uk-aisi-update-on-sandbagging
How are intentional descriptions (like deception) related to algorithmic ones (like understanding the mechanisms models use)?,https://www.lesswrong.com/posts/YXNeA3RyRrrRWS37A/a-problem-to-solve-before-building-a-deception-detector
Is This Lie Detector Really Just a Lie Detector? An Investigation of LLM Probe Specificity,https://www.lesswrong.com/posts/5dkhdRMypeuyoXfmb/is-this-lie-detector-really-just-a-lie-detector-an
https://www.lesswrong.com/posts/bCQbSFrnnAk7CJNpM/still-no-lie-detector-for-llms,https://www.lesswrong.com/posts/bCQbSFrnnAk7CJNpM/still-no-lie-detector-for-llms
SHADE-Arena: Evaluating sabotage and monitoring in LLM agents,https://www.anthropic.com/research/shade-arena-sabotage-monitoring
The Most Forbidden Technique,https://www.lesswrong.com/posts/mpmsK8KKysgSKDm2T/the-most-forbidden-technique
Optimising on reasoning chain-of-thought,https://openai.com/index/chain-of-thought-monitoring/
Obfuscated Activations Bypass LLM Latent-Space Defenses,https://arxiv.org/abs/2412.09565
Preference Learning with Lie Detectors can Induce Honesty or Evasion,https://arxiv.org/abs/2505.13787
RL to obfuscate a single static monitor can generalise to evade other unseen monitors,https://arxiv.org/abs/2506.14261
SafetyNet: Detecting Harmful Outputs in LLMs by Modeling and Monitoring Deceptive Behaviors,https://arxiv.org/abs/2505.14300
Automated researchers can sandbag,https://alignment.anthropic.com/2025/automated-researchers-sandbag
Stress Testing Chain-of-Thought Monitoring,https://arxiv.org/abs/2505.23575
Optimizing The Final Output Can Obfuscate CoT,https://www.lesswrong.com/posts/CM7AsQoBxDW4vhkP3/optimizing-the-final-output-can-obfuscate-cot-research-note
[2506.13609] Avoiding Obfuscation with Prover-Estimator Debate,https://arxiv.org/abs/2506.13609
One-shot steering vectors cause emergent misalignment,https://www.lesswrong.com/posts/kcKnKHTHycHeRhcHF/one-shot-steering-vectors-cause-emergent-misalignment-too
Probing and Steering Evaluation Awareness of Language Models,https://arxiv.org/abs/2507.01786
Steering Models to Believe They Are Not Being Tested,https://openreview.net/forum?id=RCjtIoy7zh&referrer=%5Bthe%20profile%20of%20Neel%20Nanda%5D(%2Fprofile%3Fid%3D~Neel_Nanda1)
Linear Control of Test Awareness Reveals Differential Compliance,https://arxiv.org/abs/2505.14617
Steering Gemini with BiDPO,https://www.lesswrong.com/posts/WqjkqrEyFDXoHzz9K/steering-gemini-with-bidpo
"Taxonomy, Opportunities, and Challenges of Representation Engineering for Large Language Models",https://openreview.net/forum?id=2U1KIfmaU9
A Unified Understanding and Evaluation of Steering Methods,https://arxiv.org/pdf/2502.02716
Understanding (Un)Reliability of Steering Vectors in Language Models,https://arxiv.org/abs/2505.22637
Utility Engineering: Analyzing and Controlling Emergent Value Systems in AIs,https://arxiv.org/abs/2502.08640
Distillation Robustifies Unlearning,https://www.lesswrong.com/posts/anX4QrNjhJqGFvrBr/distillation-robustifies-unlearning
Meta-Unlearning with Disruption Masking And Normalization,https://www.lesswrong.com/posts/QYzofMbzmbgiwfqy8/unlearning-needs-to-be-more-selective-progress-report
https://arxiv.org/abs/2410.23232,https://arxiv.org/abs/2410.23232
MONA: Managed Myopia with Approval Feedback,https://arxiv.org/abs/2501.13011
Skalse’s agenda introduction and motivation,https://www.alignmentforum.org/posts/pJ3mDD7LfEwp3s5vG/the-theoretical-reward-learning-research-agenda-introduction
Behaviorist RL reward functions lead to scheming,https://www.alignmentforum.org/posts/FNJF3SoNiwceAQ69W/behaviorist-rl-reward-functions-lead-to-scheming
Reward button alignment,https://www.alignmentforum.org/posts/JrTk2pbqp7BFwPAKw/reward-button-alignment
Training a Reward Hacker Despite Perfect Labels,https://www.lesswrong.com/posts/dbYEoG7jNZbeWX39o/training-a-reward-hacker-despite-perfect-labels
AssistanceZero: Scalably Solving Assistance Games,https://arxiv.org/abs/2504.07091
https://arxiv.org/abs/2410.18636,https://arxiv.org/abs/2410.18636
Collective alignment: public input on our Model Spec,https://openai.com/index/collective-alignment-aug-2025-updates/
Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs,https://arxiv.org/abs/2508.06601
Challenges and Future Directions of Data-Centric AI Alignment,https://arxiv.org/html/2410.01957v2
https://turntrout.com/self-fulfilling-misalignment,https://turntrout.com/self-fulfilling-misalignment
https://arxiv.org/abs/2505.16260,https://arxiv.org/abs/2505.16260
https://arxiv.org/abs/2509.26544,https://arxiv.org/abs/2509.26544
https://www.lesswrong.com/posts/qXYLvjGL9QvD3aFSW/training-on-documents-about-reward-hacking-induces-reward,https://www.lesswrong.com/posts/qXYLvjGL9QvD3aFSW/training-on-documents-about-reward-hacking-induces-reward
https://www.hyperstitionai.com/,https://www.hyperstitionai.com/
Neural Interactive Proofs,https://arxiv.org/abs/2412.08897
Your Weak LLM is Secretly a Strong Teacher for Alignment,https://arxiv.org/pdf/2409.08813
Unsupervised Elicitation of Language Models,https://arxiv.org/pdf/2506.10139
https://x.com/fulcrumML,https://x.com/fulcrumML
UK AISI debate sequence,https://www.alignmentforum.org/s/NdovveRcyfxgMoujf
A Benchmark for Scalable Oversight Protocols,https://arxiv.org/pdf/2504.03731
Vaintrob ~critique,https://www.lesswrong.com/posts/ztokaf9harKTmRcn4/a-bird-s-eye-view-of-arc-s-research?commentId=cKHYEoTiNbJ7zt2Wo
https://www.lesswrong.com/s/uYMw689vDFmgPEHrS,https://www.lesswrong.com/s/uYMw689vDFmgPEHrS
Adversarial examples are superposition,https://arxiv.org/abs/2508.17456
Fully Autonomous AI Agents Should Not be Developed,https://arxiv.org/pdf/2502.02649
amusing LLM critique,https://huggingface.co/papers/2502.02649#67a50b7b3ce5f780c69c09f2
https://www.lesswrong.com/posts/kJkgXEwQtWLrpecqg/the-plan-2024-update,https://www.lesswrong.com/posts/kJkgXEwQtWLrpecqg/the-plan-2024-update
Communication & Trust | OpenReview,https://openreview.net/forum?id=Rf1CeGPA22
https://docs.google.com/document/d/1d-ARdZZDHFPIfGcTTOKK8IZWlQj0NZQrmteJj2mvmYA/edit?tab=t.0#heading=h.zat5vvnwtl0j,https://docs.google.com/document/d/1d-ARdZZDHFPIfGcTTOKK8IZWlQj0NZQrmteJj2mvmYA/edit?tab=t.0#heading=h.zat5vvnwtl0j
https://www.lesswrong.com/posts/QvnzEHvodmwfBXu94/live-theory-part-0-taking-intelligence-seriously,https://www.lesswrong.com/posts/QvnzEHvodmwfBXu94/live-theory-part-0-taking-intelligence-seriously
https://www.lesswrong.com/posts/tQ9vWm4b57HFqbaRj/what-if-not-agency,https://www.lesswrong.com/posts/tQ9vWm4b57HFqbaRj/what-if-not-agency
Infrastructure for AI Agents,https://arxiv.org/abs/2501.10114
https://gradual-disempowerment.ai/,https://gradual-disempowerment.ai/
Gradual Disempowerment: Concrete Research Projects,https://www.lesswrong.com/posts/GAv4DRGyDHe2orvwB/gradual-disempowerment-concrete-research-projects
Fully Autonomous AI Agents Should Not be Developed,https://arxiv.org/abs/2502.02649
https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5353214,https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5353214
On the functional self of LLMs,https://www.lesswrong.com/posts/29aWbJARGF4ybAa5d/on-the-functional-self-of-llms
https://www.lesswrong.com/posts/3EzbtNLdcnZe8og8b/the-void-1,https://www.lesswrong.com/posts/3EzbtNLdcnZe8og8b/the-void-1
Self-fulfilling misalignment data,https://www.lesswrong.com/posts/QkEyry3Mqo8umbhoK/self-fulfilling-misalignment-data-might-be-poisoning-our-ai
Evidence of Bail Preferences in Large Language Models,https://www.lesswrong.com/posts/6JdSJ63LZ4TuT5cTH/the-llm-has-left-the-chat-evidence-of-bail-preferences-in
https://www.anthropic.com/research/end-subset-conversations,https://www.anthropic.com/research/end-subset-conversations
Multi-Agent Risks from Advanced AI,https://openreview.net/forum?id=WzvJWt5MVg
Authenticated Delegation and Authorized AI Agents,https://arxiv.org/abs/2501.09674
Frontier AI systems have surpassed the self-replicating red line,https://arxiv.org/abs/2412.12140
https://ai-2027.com/,https://ai-2027.com/
https://arxiv.org/abs/2502.15657,https://arxiv.org/abs/2502.15657
Open Philanthropy,https://www.lesswrong.com/posts/26SHhxK2yYQbh7ors/research-directions-open-phil-wants-to-fund-in-technical-ai
International AI Safety Report 2025,https://arxiv.org/abs/2510.13653
"A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment",https://arxiv.org/pdf/2504.15585
Review of AI safety funders,https://www.lesswrong.com/posts/KZPNbHs9RsoeZShkJ/plex-s-shortform?commentId=hCbJkBd9s23PEeLAL
The Singapore Consensus on Global AI Safety Research Priorities,https://arxiv.org/abs/2506.20702
The Alignment Project,https://www.alignmentforum.org/s/wvLzDiWQWBC9b5HGa
https://peregrine-launchpad.lovable.app/,https://peregrine-launchpad.lovable.app/