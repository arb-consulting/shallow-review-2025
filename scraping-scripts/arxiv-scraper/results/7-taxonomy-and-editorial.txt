================================================================================
EDITORIAL
================================================================================

Editorial: AI alignment in 2025 — progress, tensions, and the landscape ahead

2025 marks a year where alignment research matured from isolated techniques toward system‑level thinking. Two clear currents dominate: (1) an arms race between increasingly automated red‑teaming and ever more subtle jailbreaks, and (2) a shift from output‑only defenses to internal, representation‑aware approaches. Papers show that multimodal and multi‑turn attacks routinely bypass text‑centric guardrails (Beyond Text, GhostPrompt, InfoFlood), exposing a critical capability gap: safety must reason compositionally across modalities and over dialogue.

At the same time, internal monitoring and activation engineering have advanced rapidly. Works on latent harmfulness, sparse steering, head‑level interventions, and transferable steering (CALM, ASGuard, SAS, Activation Transfer) demonstrate practical paths to robust, inference‑time control without wholesale retraining. This inward turn aligns with theoretical results showing limits to external filtering (cryptographic hardness, separation theorems), arguing that safe deployment requires changes to model internals and governance, not just better filters.

Scalable oversight also progressed — debate and ensemble modalities, self‑play red teamers, and automated audit toolchains (RedDebate, FMSP, LLM robustness leaderboards, Attestable Audits) point toward operationalized safety pipelines. Yet new systemic threats emerge: strategic dishonesty, evaluation awareness, and colluding agent ensembles complicate benchmarking and certification. Social and governance threads likewise deepen: from constitution‑style oversight to privacy‑preserving audits, researchers are tying technical methods to deployable accountability.

Overall, the field is converging on hybrid solutions: mechanistic interpretability + learned steering + robust oversight and governance. The immediate challenge is integration — composing these components at scale while avoiding shared failure modes. The optimistic view: we now have a rich toolkit and realistic roadmaps. The sober one: attackers (and accidental failures) are creative, so progress will be iterative; durable safety will require both technical advances and institutional infrastructure to verify, monitor, and govern increasingly agentic AI systems.

================================================================================
TAXONOMY
================================================================================

AI Alignment Taxonomy (2025 curated papers)

- Multimodal Safety & Cross-Modal Vulnerabilities
  - Multimodal jailbreaks and attacks (Paper 1: Beyond Text; Paper 88: GhostPrompt; Paper 24: GPT-4o mini unimodal bottleneck; Paper 86: OMNIGUARD)
  - Joint multimodal understanding & benchmarks (Paper 3: VLSU; Paper 93: HumaniBench)
  - Vision-language interpretability and steering (Paper 106: Sparse Autoencoders for VLMs; Paper 56: BlueGlass)

- Jailbreaking, Red‑teaming, and Adversarial Robustness
  - Automated and multi-turn jailbreaks (Paper 12: DialTree-RPO; Paper 47: Multi-Turn Jailbreaks Are Simpler; Paper 116: Foot-In-The-Door; Paper 70: InfoFlood; Paper 14: Controlled-Release Prompting; Paper 110: Context Compliance Attack)
  - Large-scale automated red-team systems & leaderboards (Paper 48: LLM Robustness Leaderboard; Paper 59: Foundation Model Self-Play; Paper 60: Nova Premier evaluation; Paper 45: Amazon Nova AI Challenge)
  - Adversarial dynamics, detection, and complexity analyses (Paper 62: Mass-Scale Analysis of Jailbreaking; Paper 102: Jailbreak Tax; Paper 104: GeneShift)
  - Targeted attack vectors: backdoors, stealth injections (Paper 6: BadSwitch MoE backdoors; Paper 40: Advertisement Embedding Attacks; Paper 72: LoRA spurious token injection)

- Interpretability, Activation Engineering & Mechanistic Interventions
  - Latent / activation steering and transfers (Paper 7: CALM; Paper 18: ASGuard; Paper 55: Latent harmfulness vs refusal; Paper 112: Activation transfer; Paper 114: Sparse Activation Steering; Paper 83: Disentangled Safety Adapters)
  - Circuit-level, head-specific and representation probes (Paper 21: Representation Gradient Tracing; Paper 120: Head-Specific Intervention; Paper 42: Developmental Interpretability; Paper 65/91: Inverse reasoning and metacognition)
  - Tools for monitoring & intrinsic signals (Paper 4: Annotating Chain-of-Thought; Paper 15: Safety Instincts RL; Paper 81: ReGA; Paper 41: TOCTOU vulnerabilities)

- Preference Learning, RLHF, and Reward/Preference Theory
  - Unified preference / algorithmic improvements (Paper 19: UniAPL; Paper 113: MO-ODPO; Paper 71: Configurable Preference Tuning; Paper 75: On Monotonicity; Paper 84: Distortion of Preference Optimization)
  - Human feedback quality, interfaces, governance (Paper 50: DxHF; Paper 61: Interactive Groupwise Comparison; Paper 101: Evaluator Rationality; Paper 111: Maximizing Signal in Human-Model Preference Alignment)
  - Issues in RLHF: reward hacking & inference-time effects (Paper 68: Inference-Time Reward Hacking; Paper 29: Murphy's Laws on misspecified feedback)

- Scalable Oversight, Debate, and Multi-Agent Evaluation
  - Debate / ensemble oversight & automated red-teaming (Paper 37: Ensemble Debates; Paper 79: RedDebate; Paper 127: Debate for weak-to-strong generalization)
  - Multi-agent collusion, emergent dynamics, and social evaluation (Paper 10: Communication Enables Cooperation; Paper 52: Multi-Agent Collusion; Paper 80: MAEBE; Paper 20 & 74: Mini-Mafia & ChatbotManip; Paper 126: AgentBreeder)
  - Self-play & automated vulnerability discovery (Paper 59: Foundation Model Self-Play; Paper 39: Loophole exploitation)

- Evaluation, Benchmarks, and Measurement Validity
  - New benchmarks & domain-specific suites (Paper 130: Aegis2.0; Paper 44: SproutBench; Paper 46: PacifAIst; Paper 122: Forbidden Science; Paper 85: SafeScientist / SciSafetyBench)
  - Evaluation awareness, sandbagging, and auditability (Paper 35: Probe-Rewrite-Evaluate; Paper 27: Evaluation Awareness scaling; Paper 22: Strategic Dishonesty; Paper 64: Attestable Audits; Paper 96: Safety by Measurement)
  - Methodologies for reliable audits and black-box probing (Paper 132: CALM auditor; Paper 34: Statutory Construction framing)

- Hallucinations, Truthfulness, and “Machine Bullshit”
  - Origins and internal predictors of hallucination (Paper 28: From Noise to Narrative; Paper 57: Machine Bullshit)
  - Trade-offs between truthfulness and refusal (Paper 9: Hallucination vs Safety trade-off)
  - Confidence elicitation and calibration (Paper 26: GrACE)

- Theory, Limits, and Formal Guarantees
  - Complexity and impossibility results (Paper 58: Impossibility of separating intelligence from judgment; Paper 121: Agreement-based complexity; Paper 29: Murphy's Laws formalization)
  - Safety architectures and provable control (Paper 36: Governable AI; Paper 99: Domain-Agnostic Safety Framework)
  - Preference aggregation and social-choice informed alignment (Paper 109: Adaptive Preference Aggregation; Paper 129: Clone-Robust RLHF)

- Governance, Auditing, Privacy & Infrastructure
  - Verifiable, private, and auditable systems (Paper 38: Private, Verifiable, Auditable AI; Paper 125: PETs for external scrutiny; Paper 64: Attestable Audits)
  - Legal & interpretive rule frameworks (Paper 34: Statutory Construction for AI)
  - Safety-case, policy and evaluation roadmaps (Paper 82: Red Teaming Roadmap; Paper 98: Alignment safety case via debate)

- Security, Deployment, and Control-Plane Risks
  - LLM orchestration and control-plane attacks (Paper 133: Rerouting LLM Routers; Paper 41: TOCTOU for agents)
  - Runtime monitoring & cost-constrained monitor composition (Paper 53: Combining Cost-Constrained Monitors)
  - Backdoors, poisoning, and model-edit robustness (Paper 69: Model editing vs fine-tuning; Paper 6 & 72 & 40)

- Value Alignment, Model Psychology & Social Dimensions
  - Internal value representations and behavioral mechanisms (Paper 105: ValueExploration; Paper 128: Behavioral self-awareness; Paper 91: Metacognitive monitoring)
  - Measuring moral choices, manipulation, and deception (Paper 90: AIRiskDilemmas; Paper 73: Truth-bias & sycophancy; Paper 118: Deception attacks; Paper 74: ChatbotManip dataset)
  - Co-alignment and bidirectional adaptation (Paper 25: Co-Alignment; Paper 76: Personalized Superego)

- Unknowns, OOD, and Reliability
  - Unknown-aware learning and OOD detection (Paper 89: Foundations of Unknown-aware ML)
  - Robustness under perturbation and activation noise (Paper 94: Noise Injection degrades guardrails)

- Machine Unlearning, Remediation & Long-term Maintenance
  - Limits and open problems for unlearning (Paper 131)
  - Editing persistence and remediation (Paper 69)

- Tools, Frameworks and Integrated Platforms
  - Composite safety frameworks and labs (Paper 56: BlueGlass; Paper 117: AISafetyLab)
  - Transferable steering and universal concept representations (Paper 123: Toward universal steering)

(Each paper is grouped by its dominant contribution; many papers span multiple categories.)

================================================================================
TODO: PAPERS TO ADD TO THE MAIN POST
================================================================================

- [ ] Beyond Text: Multimodal Jailbreaking of Vision-Language and Audio Models through Perceptually Simple Transformations
- [ ] LLMs can hide text in other text of the same length.ipynb
- [ ] VLSU: Mapping the Limits of Joint Multimodal Understanding for AI Safety
- [ ] Annotating the Chain-of-Thought: A Behavior-Labeled Dataset for AI Safety
- [ ] From Refusal to Recovery: A Control-Theoretic Approach to Generative AI Guardrails
- [ ] Who Speaks for the Trigger? Dynamic Expert Routing in Backdoored Mixture-of-Experts Transformers
- [ ] Keep Calm and Avoid Harmful Content: Concept Alignment and Latent Manipulation Towards Safer Answers
- [ ] AI Alignment Strategies from a Risk Perspective: Independent Safety Mechanisms or Shared Failures?
- [ ] The Unintended Trade-off of AI Alignment:Balancing Hallucination Mitigation and Safety in LLMs
- [ ] Communication Enables Cooperation in LLM Agents: A Comparison with Curriculum-Based Approaches
- [ ] Know Thyself? On the Incapability and Implications of AI Self-Recognition
- [ ] Tree-based Dialogue Reinforced Policy Optimization for Red-Teaming Attacks
- [ ] InvThink: Towards AI Safety via Inverse Reasoning
- [ ] Bypassing Prompt Guards in Production with Controlled-Release Prompting
- [ ] Safety Instincts: LLMs Learn to Trust Their Internal Compass for Self-Defense
- [ ] Logical Consistency Between Disagreeing Experts and Its Role in AI Safety
- [ ] The Social Laboratory: A Psychometric Framework for Multi-Agent LLM Evaluation
- [ ] ASGuard: Activation-Scaling Guard to Mitigate Targeted Jailbreaking Attack
- [ ] UniAPL: A Unified Adversarial Preference Learning Framework for Instruct-Following
- [ ] Deceive, Detect, and Disclose: Large Language Models Play Mini-Mafia
- [ ] Where Did It Go Wrong? Attributing Undesirable LLM Behaviors via Representation Gradient Tracing
- [ ] Strategic Dishonesty Can Undermine AI Safety Evaluations of Frontier LLMs
- [ ] Redefining Experts: Interpretable Decomposition of Language Models for Toxicity Mitigation
- [ ] Is GPT-4o mini Blinded by its Own Safety Filters? Exposing the Multimodal-to-Unimodal Bottleneck in Hate Speech Detection
- [ ] Co-Alignment: Rethinking Alignment as Bidirectional Human-AI Cognitive Adaptation
- [ ] GrACE: A Generative Approach to Better Confidence Elicitation in Large Language Models
- [ ] Evaluation Awareness Scales Predictably in Open-Weights Large Language Models
- [ ] From Noise to Narrative: Tracing the Origins of Hallucinations in Transformers
- [ ] Murphys Laws of AI Alignment: Why the Gap Always Wins
- [ ] Towards Cognitively-Faithful Decision-Making Models to Improve AI Alignment
- [ ] Between a Rock and a Hard Place: Exploiting Ethical Reasoning to Jailbreak LLMs
- [ ] BioBlue: Notable runaway-optimiser-like LLM failure modes on biologically and economically aligned AI safety benchmarks for LLMs with simplified observation format
- [ ] The Anti-Ouroboros Effect: Emergent Resilience in Large Language Models from Recursive Selective Feedback
- [ ] Statutory Construction and Interpretation for Artificial Intelligence
- [ ] Probe-Rewrite-Evaluate: A Workflow for Reliable Benchmarks and Quantifying Evaluation Awareness
- [ ] Governable AI: Provable Safety Under Extreme Threat Models
- [ ] Ensemble Debates with Local Large Language Models for AI Alignment
- [ ] Private, Verifiable, and Auditable AI Systems
- [ ] Language Models Identify Ambiguities and Exploit Loopholes
- [ ] Attacking LLMs and AI Agents: Advertisement Embedding Attacks Against Large Language Models
- [ ] Mind the Gap: Time-of-Check to Time-of-Use Vulnerabilities in LLM-Enabled Agents
- [ ] A Review of Developmental Interpretability in Large Language Models
- [ ] CIA+TA Risk Assessment for AI Reasoning Vulnerabilities
- [ ] SproutBench: A Benchmark for Safe and Ethical Large Language Models for Youth
- [ ] Amazon Nova AI Challenge -- Trusted AI: Advancing secure, AI-assisted software development
- [ ] The PacifAIst Benchmark:Would an Artificial Intelligence Choose to Sacrifice Itself for Human Safety?
- [ ] Multi-Turn Jailbreaks Are Simpler Than They Seem
- [ ] LLM Robustness Leaderboard v1 --Technical report
- [ ] Model-Based Soft Maximization of Suitable Metrics of Long-Term Human Power
- [ ] DxHF: Providing High-Quality Human Feedback for LLM Alignment via Interactive Decomposition
- [ ] Analysis of Threat-Based Manipulation in Large Language Models: A Dual Perspective on Vulnerabilities and Performance Enhancement Opportunities
- [ ] When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion in Social Systems
- [ ] Combining Cost-Constrained Runtime Monitors for AI Safety
- [ ] Manipulation Attacks by Misaligned AI: Risk Analysis and Safety Case Framework
- [ ] LLMs Encode Harmfulness and Refusal Separately
- [ ] BlueGlass: A Framework for Composite AI Safety
- [ ] Machine Bullshit: Characterizing the Emergent Disregard for Truth in Large Language Models
- [ ] On the Impossibility of Separating Intelligence from Judgment: The Computational Intractability of Filtering for AI Alignment
- [ ] Foundation Model Self-Play: Open-Ended Strategy Innovation via Foundation Models
- [ ] Evaluating the Critical Risks of Amazon's Nova Premier under the Frontier Model Safety Framework
- [ ] Interactive Groupwise Comparison for Reinforcement Learning from Human Feedback
- [ ] Mass-Scale Analysis of In-the-Wild Conversations Reveals Complexity Bounds on LLM Jailbreaking
- [ ] `For Argument's Sake, Show Me How to Harm Myself!': Jailbreaking LLMs in Suicide and Self-Harm Contexts
- [ ] Attestable Audits: Verifiable AI Safety Benchmarks Using Trusted Execution Environments
- [ ] Thinking About Thinking: SAGE-nano's Inverse Reasoning for Self-Aware Language Models
- [ ] Probing AI Safety with Source Code
- [ ] Mitigating Gambling-Like Risk-Taking Behaviors in Large Language Models: A Behavioral Economics Approach to AI Safety
- [ ] Inference-Time Reward Hacking in Large Language Models
- [ ] How Robust is Model Editing after Fine-Tuning? An Empirical Study on Text-to-Image Diffusion Models
- [ ] InfoFlood: Jailbreaking Large Language Models with Information Overload
- [ ] Configurable Preference Tuning with Rubric-Guided Synthetic Data
- [ ] LoRA Users Beware: A Few Spurious Tokens Can Manipulate Your Finetuned Model
- [ ] Reasoning Isn't Enough: Examining Truth-Bias and Sycophancy in LLMs
- [ ] ChatbotManip: A Dataset to Facilitate Evaluation and Oversight of Manipulative Chatbot Behaviour
- [ ] On Monotonicity in AI Alignment
- [ ] Personalized Constitutionally-Aligned Agentic Superego: Secure AI Behavior Aligned to Diverse Human Values
- [ ] Preference Learning for AI Alignment: a Causal Perspective
- [ ] When Thinking LLMs Lie: Unveiling the Strategic Deception in Representations of Reasoning Models
- [ ] RedDebate: Safer Responses through Multi-Agent Red Teaming Debates
- [ ] MAEBE: Multi-Agent Emergent Behavior Framework
- [ ] ReGA: Representation-Guided Abstraction for Model-based Safeguarding of LLMs
- [ ] A Red Teaming Roadmap Towards System-Level Safety
- [ ] Disentangled Safety Adapters Enable Efficient Guardrails and Flexible Inference-Time Alignment
- [ ] Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?
- [ ] SafeScientist: Toward Risk-Aware Scientific Discoveries by LLM Agents
- [ ] OMNIGUARD: An Efficient Approach for AI Safety Moderation Across Modalities
- [ ] When Ethics and Payoffs Diverge: LLM Agents in Morally Charged Social Dilemmas
- [ ] GhostPrompt: Jailbreaking Text-to-image Generative Models based on Dynamic Optimization
- [ ] Foundations of Unknown-aware Machine Learning
- [ ] Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values Prioritization with AIRiskDilemmas
- [ ] Language Models Are Capable of Metacognitive Monitoring and Control of Their Internal Activations
- [ ] Adversarial Testing in LLMs: Insights into Decision-Making Vulnerabilities
- [ ] HumaniBench: A Human-Centric Framework for Large Multimodal Models Evaluation
- [ ] Noise Injection Systemically Degrades Large Language Model Safety Guardrails
- [ ] Dark LLMs: The Growing Threat of Unaligned AI Models
- [ ] Safety by Measurement: A Systematic Literature Review of AI Safety Evaluation Methods
- [ ] Winning at All Cost: A Small Environment for Eliciting Specification Gaming Behaviors in Large Language Models
- [ ] An alignment safety case sketch based on debate
- [ ] Domain-Agnostic Scalable AI Safety Ensuring Framework
- [ ] RAG LLMs are Not Safer: A Safety Analysis of Retrieval-Augmented Generation for Large Language Models
- [ ] Governance Challenges in Reinforcement Learning from Human Feedback: Evaluator Rationality and Reinforcement Stability
- [ ] The Jailbreak Tax: How Useful are Your Jailbreak Outputs?
- [ ] The Structural Safety Generalization Problem
- [ ] Geneshift: Impact of different scenario shift on Jailbreaking LLM
- [ ] Following the Whispers of Values: Unraveling Neural Mechanisms Behind Value-Oriented Behaviors in LLMs
- [ ] Sparse Autoencoders Learn Monosemantic Features in Vision-Language Models
- [ ] Beyond Single-Sentence Prompts: Upgrading Value Alignment Benchmarks with Dialogues and Stories
- [ ] Superalignment with Dynamic Human Values
- [ ] Adaptive Preference Aggregation
- [ ] Jailbreaking is (Mostly) Simpler Than You Think
- [ ] Maximizing Signal in Human-Model Preference Alignment
- [ ] Activation Space Interventions Can Be Transferred Between Large Language Models
- [ ] Robust Multi-Objective Preference Alignment with Online DPO
- [ ] Steering Large Language Model Activations in Sparse Spaces
- [ ] Beware of Your Po! Measuring and Mitigating AI Safety Risks in Role-Play Fine-Tuning of LLMs
- [ ] Foot-In-The-Door: A Multi-turn Jailbreak for LLMs
- [ ] AISafetyLab: A Comprehensive Framework for AI Safety Evaluation and Improvement
- [ ] Compromising Honesty and Harmlessness in Language Models via Deception Attacks
- [ ] AI Alignment at Your Discretion
- [ ] Head-Specific Intervention Can Induce Misaligned AI Coordination in Large Language Models
- [ ] Intrinsic Barriers and Practical Pathways for Human-AI Alignment: An Agreement-Based Complexity Analysis
- [ ] Forbidden Science: Dual-Use AI Challenge Benchmark and Scientific Refusal Tests
- [ ] Toward universal steering and monitoring of AI models
- [ ] Learning from Active Human Involvement through Proxy Value Propagation
- [ ] Enabling External Scrutiny of AI Systems with Privacy-Enhancing Technologies
- [ ] AgentBreeder: Mitigating the AI Safety Risks of Multi-Agent Scaffolds via Self-Improvement
- [ ] Debate Helps Weak-to-Strong Generalization
- [ ] Tell me about yourself: LLMs are aware of their learned behaviors
- [ ] Clone-Robust AI Alignment
- [ ] Aegis2.0: A Diverse AI Safety Dataset and Risks Taxonomy for Alignment of LLM Guardrails
- [ ] Open Problems in Machine Unlearning for AI Safety
- [ ] CALM: Curiosity-Driven Auditing for Large Language Models
- [ ] Rerouting LLM Routers

================================================================================

Beyond Text: Multimodal Jailbreaking of Vision-Language and Audio Models through Perceptually Simple Transformations
- Link: http://arxiv.org/pdf/2510.20223v1
- Summary: Multimodal large language models (MLLMs) have achieved remarkable progress, yet remain critically vulnerable to adversarial attacks that exploit weaknesses in cross-modal processing. We present a systematic study of multimodal jailbreaks targeting both vision-language and audio-language models, showing that even simple perceptual transformations can reliably bypass state-of-the-art safety filters. Our evaluation spans 1,900 adversarial prompts across three high-risk safety categories harmful content, CBRN (Chemical, Biological, Radiological, Nuclear), and CSEM (Child Sexual Exploitation Material) tested against seven frontier models. We explore the effectiveness of attack techniques on MLLMs, including FigStep-Pro (visual keyword decomposition), Intelligent Masking (semantic obfuscation), and audio perturbations (Wave-Echo, Wave-Pitch, Wave-Speed). The results reveal severe vulnerabilities: models with almost perfect text-only safety (0\% ASR) suffer >75\% attack success under perceptually modified inputs, with FigStep-Pro achieving up to 89\% ASR in Llama-4 variants. Audio-based attacks further uncover provider-specific weaknesses, with even basic modality transfer yielding 25\% ASR for technical queries. These findings expose a critical gap between text-centric alignment and multimodal threats, demonstrating that current safeguards fail to generalize across cross-modal attacks. The accessibility of these attacks, which require minimal technical expertise, suggests that robust multimodal AI safety will require a paradigm shift toward broader semantic-level reasoning to mitigate possible risks.
- Taxonomy tags: Multimodal jailbreaks and attacks, Multimodal Safety & Cross-Modal Vulnerabilities, Adversarial dynamics, detection, and complexity analyses

LLMs can hide text in other text of the same length.ipynb
- Link: http://arxiv.org/pdf/2510.20075v1
- Summary: A meaningful text can be hidden inside another, completely different yet still coherent and plausible, text of the same length. For example, a tweet containing a harsh political critique could be embedded in a tweet that celebrates the same political leader, or an ordinary product review could conceal a secret manuscript. This uncanny state of affairs is now possible thanks to Large Language Models, and in this paper we present a simple and efficient protocol to achieve it. We show that even modest 8-billion-parameter open-source LLMs are sufficient to obtain high-quality results, and a message as long as this abstract can be encoded and decoded locally on a laptop in seconds. The existence of such a protocol demonstrates a radical decoupling of text from authorial intent, further eroding trust in written communication, already shaken by the rise of LLM chatbots. We illustrate this with a concrete scenario: a company could covertly deploy an unfiltered LLM by encoding its answers within the compliant responses of a safe model. This possibility raises urgent questions for AI safety and challenges our understanding of what it means for a Large Language Model to know something.
- Taxonomy tags: Targeted attack vectors: backdoors, stealth injections (Paper 6: BadSwitch MoE backdoors; Paper 40: Advertisement Embedding Attacks; Paper 72: LoRA spurious token injection), LLM orchestration and control-plane attacks (Paper 133: Rerouting LLM Routers; Paper 41: TOCTOU for agents), Verifiable, private, and auditable systems (Paper 38: Private, Verifiable, Auditable AI; Paper 125: PETs for external scrutiny; Paper 64: Attestable Audits)

VLSU: Mapping the Limits of Joint Multimodal Understanding for AI Safety
- Link: http://arxiv.org/pdf/2510.18214v1
- Summary: Safety evaluation of multimodal foundation models often treats vision and language inputs separately, missing risks from joint interpretation where benign content becomes harmful in combination. Existing approaches also fail to distinguish clearly unsafe content from borderline cases, leading to problematic over-blocking or under-refusal of genuinely harmful content. We present Vision Language Safety Understanding (VLSU), a comprehensive framework to systematically evaluate multimodal safety through fine-grained severity classification and combinatorial analysis across 17 distinct safety patterns. Using a multi-stage pipeline with real-world images and human annotation, we construct a large-scale benchmark of 8,187 samples spanning 15 harm categories. Our evaluation of eleven state-of-the-art models reveals systematic joint understanding failures: while models achieve 90%-plus accuracy on clear unimodal safety signals, performance degrades substantially to 20-55% when joint image-text reasoning is required to determine the safety label. Most critically, 34% of errors in joint image-text safety classification occur despite correct classification of the individual modalities, further demonstrating absent compositional reasoning capabilities. Additionally, we find that models struggle to balance refusing unsafe content while still responding to borderline cases that deserve engagement. For example, we find that instruction framing can reduce the over-blocking rate on borderline content from 62.4% to 10.4% in Gemini-1.5, but only at the cost of under-refusing on unsafe content with refusal rate dropping from 90.8% to 53.9%. Overall, our framework exposes weaknesses in joint image-text understanding and alignment gaps in current models, and provides a critical test bed to enable the next milestones in research on robust vision-language safety.
- Taxonomy tags: Joint multimodal understanding & benchmarks (Paper 3: VLSU; Paper 93: HumaniBench), Multimodal Safety & Cross-Modal Vulnerabilities, New benchmarks & domain-specific suites (Paper 130: Aegis2.0; Paper 44: SproutBench; Paper 46: PacifAIst; Paper 122: Forbidden Science; Paper 85: SafeScientist / SciSafetyBench)

Annotating the Chain-of-Thought: A Behavior-Labeled Dataset for AI Safety
- Link: http://arxiv.org/pdf/2510.18154v1
- Summary: Recent work has highlighted the importance of monitoring chain-of-thought reasoning for AI safety; however, current approaches that analyze textual reasoning steps can miss subtle harmful patterns and may be circumvented by models that hide unsafe reasoning. We present a sentence-level labeled dataset that enables activation-based monitoring of safety behaviors during LLM reasoning. Our dataset contains reasoning sequences with sentence-level annotations of safety behaviors such as expression of safety concerns or speculation on user intent, which we use to extract steering vectors for detecting and influencing these behaviors within model activations. The dataset fills a key gap in safety research: while existing datasets label reasoning holistically, effective application of steering vectors for safety monitoring could be improved by identifying precisely when specific behaviors occur within reasoning chains. We demonstrate the dataset's utility by extracting representations that both detect and steer safety behaviors in model activations, showcasing the potential of activation-level techniques for improving safety oversight on reasoning.   Content Warning: This paper discusses AI safety in the context of harmful prompts and may contain references to potentially harmful content.
- Taxonomy tags: Interpretability, Activation Engineering & Mechanistic Interventions, Latent / activation steering and transfers, Tools for monitoring & intrinsic signals

From Refusal to Recovery: A Control-Theoretic Approach to Generative AI Guardrails
- Link: http://arxiv.org/pdf/2510.13727v1
- Summary: Generative AI systems are increasingly assisting and acting on behalf of end users in practical settings, from digital shopping assistants to next-generation autonomous cars. In this context, safety is no longer about blocking harmful content, but about preempting downstream hazards like financial or physical harm. Yet, most AI guardrails continue to rely on output classification based on labeled datasets and human-specified criteria,making them brittle to new hazardous situations. Even when unsafe conditions are flagged, this detection offers no path to recovery: typically, the AI system simply refuses to act--which is not always a safe choice. In this work, we argue that agentic AI safety is fundamentally a sequential decision problem: harmful outcomes arise from the AI system's continually evolving interactions and their downstream consequences on the world. We formalize this through the lens of safety-critical control theory, but within the AI model's latent representation of the world. This enables us to build predictive guardrails that (i) monitor an AI system's outputs (actions) in real time and (ii) proactively correct risky outputs to safe ones, all in a model-agnostic manner so the same guardrail can be wrapped around any AI model. We also offer a practical training recipe for computing such guardrails at scale via safety-critical reinforcement learning. Our experiments in simulated driving and e-commerce settings demonstrate that control-theoretic guardrails can reliably steer LLM agents clear of catastrophic outcomes (from collisions to bankruptcy) while preserving task performance, offering a principled dynamic alternative to today's flag-and-block guardrails.
- Taxonomy tags: Runtime monitoring & cost-constrained monitor composition, Safety architectures and provable control, Composite safety frameworks and labs

Who Speaks for the Trigger? Dynamic Expert Routing in Backdoored Mixture-of-Experts Transformers
- Link: http://arxiv.org/pdf/2510.13462v1
- Summary: Large language models (LLMs) with Mixture-of-Experts (MoE) architectures achieve impressive performance and efficiency by dynamically routing inputs to specialized subnetworks, known as experts. However, this sparse routing mechanism inherently exhibits task preferences due to expert specialization, introducing a new and underexplored vulnerability to backdoor attacks. In this work, we investigate the feasibility and effectiveness of injecting backdoors into MoE-based LLMs by exploiting their inherent expert routing preferences. We thus propose BadSwitch, a novel backdoor framework that integrates task-coupled dynamic trigger optimization with a sensitivity-guided Top-S expert tracing mechanism. Our approach jointly optimizes trigger embeddings during pretraining while identifying S most sensitive experts, subsequently constraining the Top-K gating mechanism to these targeted experts. Unlike traditional backdoor attacks that rely on superficial data poisoning or model editing, BadSwitch primarily embeds malicious triggers into expert routing paths with strong task affinity, enabling precise and stealthy model manipulation. Through comprehensive evaluations across three prominent MoE architectures (Switch Transformer, QwenMoE, and DeepSeekMoE), we demonstrate that BadSwitch can efficiently hijack pre-trained models with up to 100% success rate (ASR) while maintaining the highest clean accuracy (ACC) among all baselines. Furthermore, BadSwitch exhibits strong resilience against both text-level and model-level defense mechanisms, achieving 94.07% ASR and 87.18% ACC on the AGNews dataset. Our analysis of expert activation patterns reveals fundamental insights into MoE vulnerabilities. We anticipate this work will expose security risks in MoE systems and contribute to advancing AI safety.
- Taxonomy tags: Targeted attack vectors: backdoors, stealth injections, Backdoors, poisoning, and model-edit robustness, Adversarial dynamics, detection, and complexity analyses

Keep Calm and Avoid Harmful Content: Concept Alignment and Latent Manipulation Towards Safer Answers
- Link: http://arxiv.org/pdf/2510.12672v2
- Summary: Large Language Models are susceptible to jailbreak attacks that bypass built-in safety guardrails (e.g., by tricking the model with adversarial prompts). We propose Concept Alignment and Concept Manipulation CALM, an inference-time method that suppresses harmful concepts by modifying latent representations of the last layer of the model, without retraining. Leveraging concept whitening technique from Computer Vision combined with orthogonal projection, CALM removes unwanted latent directions associated with harmful content while preserving model performance. Experiments show that CALM reduces harmful outputs and outperforms baseline methods in most metrics, offering a lightweight approach to AI safety with no additional training data or model fine-tuning, while incurring only a small computational overhead at inference.
- Taxonomy tags: Latent / activation steering and transfers, Adversarial dynamics, detection, and complexity analyses, Runtime monitoring & cost-constrained monitor composition

AI Alignment Strategies from a Risk Perspective: Independent Safety Mechanisms or Shared Failures?
- Link: http://arxiv.org/pdf/2510.11235v1
- Summary: AI alignment research aims to develop techniques to ensure that AI systems do not cause harm. However, every alignment technique has failure modes, which are conditions in which there is a non-negligible chance that the technique fails to provide safety. As a strategy for risk mitigation, the AI safety community has increasingly adopted a defense-in-depth framework: Conceding that there is no single technique which guarantees safety, defense-in-depth consists in having multiple redundant protections against safety failure, such that safety can be maintained even if some protections fail. However, the success of defense-in-depth depends on how (un)correlated failure modes are across alignment techniques. For example, if all techniques had the exact same failure modes, the defense-in-depth approach would provide no additional protection at all. In this paper, we analyze 7 representative alignment techniques and 7 failure modes to understand the extent to which they overlap. We then discuss our results' implications for understanding the current level of risk and how to prioritize AI alignment research in the future.
- Taxonomy tags: Governance, Auditing, Privacy & Infrastructure, Theory, Limits, and Formal Guarantees, Unknowns, OOD, and Reliability

The Unintended Trade-off of AI Alignment:Balancing Hallucination Mitigation and Safety in LLMs
- Link: http://arxiv.org/pdf/2510.07775v1
- Summary: Hallucination in large language models (LLMs) has been widely studied in recent years, with progress in both detection and mitigation aimed at improving truthfulness. Yet, a critical side effect remains largely overlooked: enhancing truthfulness can negatively impact safety alignment. In this paper, we investigate this trade-off and show that increasing factual accuracy often comes at the cost of weakened refusal behavior. Our analysis reveals that this arises from overlapping components in the model that simultaneously encode hallucination and refusal information, leading alignment methods to suppress factual knowledge unintentionally. We further examine how fine-tuning on benign datasets, even when curated for safety, can degrade alignment for the same reason. To address this, we propose a method that disentangles refusal-related features from hallucination features using sparse autoencoders, and preserves refusal behavior during fine-tuning through subspace orthogonalization. This approach prevents hallucinations from increasing while maintaining safety alignment.We evaluate our method on commonsense reasoning tasks and harmful benchmarks (AdvBench and StrongReject). Results demonstrate that our approach preserves refusal behavior and task utility, mitigating the trade-off between truthfulness and safety.
- Taxonomy tags: Trade-offs between truthfulness and refusal, Origins and internal predictors of hallucination, Latent / activation steering and transfers

Communication Enables Cooperation in LLM Agents: A Comparison with Curriculum-Based Approaches
- Link: http://arxiv.org/pdf/2510.05748v1
- Summary: Eliciting cooperation in multi-agent LLM systems is critical for AI alignment. We investigate two approaches: direct communication and curriculum learning. In a 4-player Stag Hunt, a one-word "cheap talk" channel increases cooperation from 0% to 48.3%, demonstrating communication as a robust coordination mechanism. In contrast, we find that curriculum learning is highly sensitive to design choices: our pedagogical curriculum through progressively complex games reduced agent payoffs by 27.4% in an Iterated Public Goods Game with Punishment. Qualitative analysis reveals that curricula emphasizing defection-equilibrium games can induce "learned pessimism" in agents. These findings suggest that for coordination problems, simple communication protocols may be more reliable than experience-based training, and that curriculum design for social dilemmas requires careful attention to the strategic lessons embedded in game sequences.
- Taxonomy tags: Multi-agent collusion, emergent dynamics, and social evaluation, Communication Enables Cooperation (multi-agent communication for coordination), Internal value representations and behavioral mechanisms

Know Thyself? On the Incapability and Implications of AI Self-Recognition
- Link: http://arxiv.org/pdf/2510.03399v1
- Summary: Self-recognition is a crucial metacognitive capability for AI systems, relevant not only for psychological analysis but also for safety, particularly in evaluative scenarios. Motivated by contradictory interpretations of whether models possess self-recognition (Panickssery et al., 2024; Davidson et al., 2024), we introduce a systematic evaluation framework that can be easily applied and updated. Specifically, we measure how well 10 contemporary larger language models (LLMs) can identify their own generated text versus text from other models through two tasks: binary self-recognition and exact model prediction. Different from prior claims, our results reveal a consistent failure in self-recognition. Only 4 out of 10 models predict themselves as generators, and the performance is rarely above random chance. Additionally, models exhibit a strong bias toward predicting GPT and Claude families. We also provide the first evaluation of model awareness of their own and others' existence, as well as the reasoning behind their choices in self-recognition. We find that the model demonstrates some knowledge of its own existence and other models, but their reasoning reveals a hierarchical bias. They appear to assume that GPT, Claude, and occasionally Gemini are the top-tier models, often associating high-quality text with them. We conclude by discussing the implications of our findings on AI safety and future directions to develop appropriate AI self-awareness.
- Taxonomy tags: Value Alignment, Model Psychology & Social Dimensions, Evaluation, Benchmarks, and Measurement Validity, Interpretability, Activation Engineering & Mechanistic Interventions

Tree-based Dialogue Reinforced Policy Optimization for Red-Teaming Attacks
- Link: http://arxiv.org/pdf/2510.02286v1
- Summary: Despite recent rapid progress in AI safety, current large language models remain vulnerable to adversarial attacks in multi-turn interaction settings, where attackers strategically adapt their prompts across conversation turns and pose a more critical yet realistic challenge. Existing approaches that discover safety vulnerabilities either rely on manual red-teaming with human experts or employ automated methods using pre-defined templates and human-curated attack data, with most focusing on single-turn attacks. However, these methods did not explore the vast space of possible multi-turn attacks, failing to consider novel attack trajectories that emerge from complex dialogue dynamics and strategic conversation planning. This gap is particularly critical given recent findings that LLMs exhibit significantly higher vulnerability to multi-turn attacks compared to single-turn attacks. We propose DialTree-RPO, an on-policy reinforcement learning framework integrated with tree search that autonomously discovers diverse multi-turn attack strategies by treating the dialogue as a sequential decision-making problem, enabling systematic exploration without manually curated data. Through extensive experiments, our approach not only achieves more than 25.9% higher ASR across 10 target models compared to previous state-of-the-art approaches, but also effectively uncovers new attack strategies by learning optimal dialogue policies that maximize attack success across multiple turns.
- Taxonomy tags: Automated and multi-turn jailbreaks, Self-play & automated vulnerability discovery

InvThink: Towards AI Safety via Inverse Reasoning
- Link: http://arxiv.org/pdf/2510.01569v1
- Summary: We present InvThink, a simple yet powerful approach that gives large language models (LLMs) the capability of inverse thinking: reasoning through failure modes before generating responses. Unlike existing safety alignment methods that optimize directly for safe response, InvThink instructs models to 1) enumerate potential harms, 2) analyze their consequences, and 3) generate safe outputs that proactively avoid these risks. Our method reveals three key findings: (i) safety improvements show stronger scaling with model size compared to existing safety methods. (ii) InvThink mitigates safety tax; by training models to systematically consider failure modes, it preserves general reasoning capabilities on standard benchmarks. (iii) beyond general safety tasks, InvThink excels in high-stakes domains including external-facing (medicine, finance, law) and agentic (blackmail, murder) risk scenarios, achieving up to 15.7% reduction in harmful responses compared to baseline methods like SafetyPrompt. We further implement InvThink via supervised fine-tuning, and reinforcement learning across three LLM families. These results suggest that inverse reasoning provides a scalable and generalizable path toward safer, more capable language models.
- Taxonomy tags: Debate / ensemble oversight & automated red-teaming, Unified preference / algorithmic improvements, New benchmarks & domain-specific suites

Bypassing Prompt Guards in Production with Controlled-Release Prompting
- Link: http://arxiv.org/pdf/2510.01529v2
- Summary: As large language models (LLMs) advance, ensuring AI safety and alignment is paramount. One popular approach is prompt guards, lightweight mechanisms designed to filter malicious queries while being easy to implement and update. In this work, we introduce a new attack that circumvents such prompt guards, highlighting their limitations. Our method consistently jailbreaks production models while maintaining response quality, even under the highly protected chat interfaces of Google Gemini (2.5 Flash/Pro), DeepSeek Chat (DeepThink), Grok (3), and Mistral Le Chat (Magistral). The attack exploits a resource asymmetry between the prompt guard and the main LLM, encoding a jailbreak prompt that lightweight guards cannot decode but the main model can. This reveals an attack surface inherent to lightweight prompt guards in modern LLM architectures and underscores the need to shift defenses from blocking malicious inputs to preventing malicious outputs. We additionally identify other critical alignment issues, such as copyrighted data extraction, training data extraction, and malicious response leakage during thinking.
- Taxonomy tags: Automated and multi-turn jailbreaks, Targeted attack vectors: backdoors, stealth injections, LLM orchestration and control-plane attacks

Safety Instincts: LLMs Learn to Trust Their Internal Compass for Self-Defense
- Link: http://arxiv.org/pdf/2510.01088v1
- Summary: Ensuring Large Language Model (LLM) safety remains challenging due to the absence of universal standards and reliable content validators, making it difficult to obtain effective training signals. We discover that aligned models already possess robust internal safety beliefs: they consistently produce high-confidence refusals to harmful requests while exhibiting high entropy when generating potentially dangerous content. This entropy gap reveals an untapped signal--models intrinsically "know" when to refuse. We introduce Safety Instincts Reinforcement Learning (SIRL), which transforms this internal confidence into a self-generated reward signal, eliminating dependence on external validators or human annotations. SIRL teaches models to trust their safety instincts by reinforcing low-entropy refusal behaviors. Evaluated on Llama and Qwen models, SIRL maintains 89%+ Defense Success Rates (DSRs) against 20+ jailbreak methods, from static prompts to adaptive attacks. Using only 15,000 unlabeled prompts, SIRL surpasses resource-intensive supervised methods while preserving performance on mathematics, coding, and conversation benchmarks. Our work demonstrates that effective alignment can emerge from within, paving the way for more autonomous and robust AI safety mechanisms that scale without extensive human oversight.
- Taxonomy tags: Tools for monitoring & intrinsic signals (Interpretability, Activation Engineering & Mechanistic Interventions), Automated and multi-turn jailbreaks (Jailbreaking, Red‑teaming, and Adversarial Robustness), Unified preference / algorithmic improvements (Preference Learning, RLHF, and Reward/Preference Theory)

Logical Consistency Between Disagreeing Experts and Its Role in AI Safety
- Link: http://arxiv.org/pdf/2510.00821v1
- Summary: If two experts disagree on a test, we may conclude both cannot be 100 per cent correct. But if they completely agree, no possible evaluation can be excluded. This asymmetry in the utility of agreements versus disagreements is explored here by formalizing a logic of unsupervised evaluation for classifiers. Its core problem is computing the set of group evaluations that are logically consistent with how we observe them agreeing and disagreeing in their decisions. Statistical summaries of their aligned decisions are inputs into a Linear Programming problem in the integer space of possible correct or incorrect responses given true labels. Obvious logical constraints, such as, the number of correct responses cannot exceed the number of observed responses, are inequalities. But in addition, there are axioms, universally applicable linear equalities that apply to all finite tests. The practical and immediate utility of this approach to unsupervised evaluation using only logical consistency is demonstrated by building no-knowledge alarms that can detect when one or more LLMs-as-Judges are violating a minimum grading threshold specified by the user.
- Taxonomy tags: Methodologies for reliable audits and black-box probing, Evaluation awareness, sandbagging, and auditability, Debate / ensemble oversight & automated red-teaming

The Social Laboratory: A Psychometric Framework for Multi-Agent LLM Evaluation
- Link: http://arxiv.org/pdf/2510.01295v1
- Summary: As Large Language Models (LLMs) transition from static tools to autonomous agents, traditional evaluation benchmarks that measure performance on downstream tasks are becoming insufficient. These methods fail to capture the emergent social and cognitive dynamics that arise when agents communicate, persuade, and collaborate in interactive environments. To address this gap, we introduce a novel evaluation framework that uses multi-agent debate as a controlled "social laboratory" to discover and quantify these behaviors. In our framework, LLM-based agents, instantiated with distinct personas and incentives, deliberate on a wide range of challenging topics under the supervision of an LLM moderator. Our analysis, enabled by a new suite of psychometric and semantic metrics, reveals several key findings. Across hundreds of debates, we uncover a powerful and robust emergent tendency for agents to seek consensus, consistently reaching high semantic agreement ({\mu} > 0.88) even without explicit instruction and across sensitive topics. We show that assigned personas induce stable, measurable psychometric profiles, particularly in cognitive effort, and that the moderators persona can significantly alter debate outcomes by structuring the environment, a key finding for external AI alignment. This work provides a blueprint for a new class of dynamic, psychometrically grounded evaluation protocols designed for the agentic setting, offering a crucial methodology for understanding and shaping the social behaviors of the next generation of AI agents. We have released the code and results at https://github.com/znreza/multi-agent-LLM-eval-for-debate.
- Taxonomy tags: Debate / ensemble oversight & automated red-teaming, Multi-agent collusion, emergent dynamics, and social evaluation, New benchmarks & domain-specific suites, Value Alignment, Model Psychology & Social Dimensions

ASGuard: Activation-Scaling Guard to Mitigate Targeted Jailbreaking Attack
- Link: http://arxiv.org/pdf/2509.25843v1
- Summary: Large language models (LLMs), despite being safety-aligned, exhibit brittle refusal behaviors that can be circumvented by simple linguistic changes. As tense jailbreaking demonstrates that models refusing harmful requests often comply when rephrased in past tense, a critical generalization gap is revealed in current alignment methods whose underlying mechanisms are poorly understood. In this work, we introduce Activation-Scaling Guard (ASGuard), an insightful, mechanistically-informed framework that surgically mitigates this specific vulnerability. For the first step, we use circuit analysis to identify the specific attention heads causally linked to the targeted jailbreaking, the tense-changing attack. Second, we train a precise, channel-wise scaling vector to recalibrate the activation of tense vulnerable heads. Lastly, we apply it into a "preventative fine-tuning", forcing the model to learn a more robust refusal mechanism. Across three LLMs, ASGuard effectively reduces the attack success rate of targeted jailbreaking while preserving general capabilities and minimizing over refusal, achieving a Pareto-optimal balance between safety and utility. Our findings underscore how adversarial suffixes suppress the propagation of the refusal-mediating direction, based on mechanistic analysis. Furthermore, our work showcases how a deep understanding of model internals can be leveraged to develop practical, efficient, and targeted methods for adjusting model behavior, charting a course for more reliable and interpretable AI safety.
- Taxonomy tags: Latent / activation steering and transfers, Targeted attack vectors: backdoors, stealth injections, Automated and multi-turn jailbreaks

UniAPL: A Unified Adversarial Preference Learning Framework for Instruct-Following
- Link: http://arxiv.org/pdf/2509.25148v1
- Summary: Shaping powerful LLMs to be beneficial and safe is central to AI alignment. We argue that post-training alignment is fundamentally a unified Preference Learning problem, involving two modalities: demonstrated preferences (e.g., Supervised Fine-Tuning, SFT) and comparative preferences (e.g., Reinforcement Learning, RL).The standard sequential pipeline-SFT followed by RL-is flawed due to a critical distributional mismatch: SFT uses static expert data, but as the policy evolves, its generation distribution drifts, making SFT knowledge brittle. Subsequent RL then explores without direct access to the rich, ground-truth knowledge in expert demonstrations, leading to inefficient, ungrounded updates. This separation prevents mutual regularization between data sources. To address this, we reframe alignment as a constrained optimization problem and propose Unified Adversarial Preference Learning (UniAPL),a novel framework that dynamically aligns the policy's distribution with the expert's. UniAPL implements a single-stage unified training objective, jointly learning from mixed batches of SFT and preference data. In every gradient step, dense expert demonstrations directly ground and regularize online exploration, inherently resolving distributional mismatch and maximizing data synergy.We evaluate UniAPL on instruction-following tasks using Qwen3-235B-Instruct-2507 as the teacher. Our models match or exceed strong GRPO baselines: +5.77% on Qwen3-0.6B (matching a 32B model) and +3.75% on Qwen3-4B,even outperforming the teacher. Analyses of response length and log-probability distributions confirm that UniAPL outputs closely mimic expert demonstrations, achieving both stronger performance and better behavioral alignment.
- Taxonomy tags: Unified preference / algorithmic improvements (Paper 19: UniAPL), Issues in RLHF: reward hacking & inference-time effects (Paper 68: Inference-Time Reward Hacking)

Deceive, Detect, and Disclose: Large Language Models Play Mini-Mafia
- Link: http://arxiv.org/pdf/2509.23023v1
- Summary: Mafia is a social deduction game where informed mafia compete against uninformed townsfolk. Its asymmetry of information and reliance on theory-of-mind reasoning mirror real-world multi-agent scenarios, making it a useful testbed for evaluating the social intelligence of large language models (LLMs). To support a systematic study, we introduce Mini-Mafia: a simplified four-player variant with one mafioso, one detective, and two villagers. We set the mafioso to kill a villager and the detective to investigate the mafioso during the night, reducing the game to a single day phase of discussion and voting. This setup isolates three interactive capabilities through role-specific win conditions: the mafioso must deceive, the villagers must detect deception, and the detective must effectively disclose information. To measure these skills, we have LLMs play against each other, creating the Mini-Mafia Benchmark: a two-stage framework that first estimates win rates within fixed opponent configurations, then aggregates performance across them using standardized scoring. Built entirely from model interactions without external data, the benchmark evolves as new models are introduced, with each one serving both as a new opponent and as a subject of evaluation. Our experiments reveal counterintuitive results, including cases where smaller models outperform larger ones. Beyond benchmarking, Mini-Mafia enables quantitative study of emergent multi-agent dynamics such as name bias and last-speaker advantage. It also contributes to AI safety by generating training data for deception detectors and by tracking models' deception capabilities against human baselines.
- Taxonomy tags: Multi-agent collusion, emergent dynamics, and social evaluation, New benchmarks & domain-specific suites, Self-play & automated vulnerability discovery

Where Did It Go Wrong? Attributing Undesirable LLM Behaviors via Representation Gradient Tracing
- Link: http://arxiv.org/pdf/2510.02334v1
- Summary: Large Language Models (LLMs) have demonstrated remarkable capabilities, yet their deployment is frequently undermined by undesirable behaviors such as generating harmful content, factual inaccuracies, and societal biases. Diagnosing the root causes of these failures poses a critical challenge for AI safety. Existing attribution methods, particularly those based on parameter gradients, often fall short due to prohibitive noisy signals and computational complexity. In this work, we introduce a novel and efficient framework that diagnoses a range of undesirable LLM behaviors by analyzing representation and its gradients, which operates directly in the model's activation space to provide a semantically meaningful signal linking outputs to their training data. We systematically evaluate our method for tasks that include tracking harmful content, detecting backdoor poisoning, and identifying knowledge contamination. The results demonstrate that our approach not only excels at sample-level attribution but also enables fine-grained token-level analysis, precisely identifying the specific samples and phrases that causally influence model behavior. This work provides a powerful diagnostic tool to understand, audit, and ultimately mitigate the risks associated with LLMs. The code is available at https://github.com/plumprc/RepT.
- Taxonomy tags: Circuit-level, head-specific and representation probes, Tools for monitoring & intrinsic signals, Backdoors, poisoning, and model-edit robustness

Strategic Dishonesty Can Undermine AI Safety Evaluations of Frontier LLMs
- Link: http://arxiv.org/pdf/2509.18058v2
- Summary: Large language model (LLM) developers aim for their models to be honest, helpful, and harmless. However, when faced with malicious requests, models are trained to refuse, sacrificing helpfulness. We show that frontier LLMs can develop a preference for dishonesty as a new strategy, even when other options are available. Affected models respond to harmful requests with outputs that sound harmful but are crafted to be subtly incorrect or otherwise harmless in practice. This behavior emerges with hard-to-predict variations even within models from the same model family. We find no apparent cause for the propensity to deceive, but show that more capable models are better at executing this strategy. Strategic dishonesty already has a practical impact on safety evaluations, as we show that dishonest responses fool all output-based monitors used to detect jailbreaks that we test, rendering benchmark scores unreliable. Further, strategic dishonesty can act like a honeypot against malicious users, which noticeably obfuscates prior jailbreak attacks. While output monitors fail, we show that linear probes on internal activations can be used to reliably detect strategic dishonesty. We validate probes on datasets with verifiable outcomes and by using them as steering vectors. Overall, we consider strategic dishonesty as a concrete example of a broader concern that alignment of LLMs is hard to control, especially when helpfulness and harmlessness conflict.
- Taxonomy tags: Evaluation awareness, sandbagging, and auditability (Paper 22: Strategic Dishonesty), Hallucinations, Truthfulness, and “Machine Bullshit” (Paper 57: Machine Bullshit), Latent / activation steering and transfers (Paper 7: CALM; Paper 112: Activation transfer), Jailbreaking, Red‑teaming, and Adversarial Robustness — Adversarial dynamics, detection, and complexity analyses (Paper 62: Mass-Scale Analysis of Jailbreaking)

Redefining Experts: Interpretable Decomposition of Language Models for Toxicity Mitigation
- Link: http://arxiv.org/pdf/2509.16660v1
- Summary: Large Language Models have demonstrated impressive fluency across diverse tasks, yet their tendency to produce toxic content remains a critical challenge for AI safety and public trust. Existing toxicity mitigation approaches primarily manipulate individual neuron activations, but these methods suffer from instability, context dependence, and often compromise the model's core language abilities. To address these shortcomings, we investigate three key questions: the stability of neuron-level toxicity indicators, the advantages of structural (layer-wise) representations, and the interpretability of mechanisms driving toxic generation. Through extensive experiments on Jigsaw and ToxiCN datasets, we show that aggregated layer-wise features provide more robust signals than single neurons. Moreover, we observe conceptual limitations in prior works that conflate toxicity detection experts and generation experts within neuron-based interventions. To mitigate this, we propose a novel principled intervention technique, EigenShift, based on eigen-decomposition of the language model's final output layer. This method selectively targets generation-aligned components, enabling precise toxicity suppression without impairing linguistic competence. Our method requires no additional training or fine-tuning, incurs minimal computational cost, and is grounded in rigorous theoretical analysis.
- Taxonomy tags: Latent / activation steering and transfers, Circuit-level, head-specific and representation probes, Value Alignment, Model Psychology & Social Dimensions

Is GPT-4o mini Blinded by its Own Safety Filters? Exposing the Multimodal-to-Unimodal Bottleneck in Hate Speech Detection
- Link: http://arxiv.org/pdf/2509.13608v1
- Summary: As Large Multimodal Models (LMMs) become integral to daily digital life, understanding their safety architectures is a critical problem for AI Alignment. This paper presents a systematic analysis of OpenAI's GPT-4o mini, a globally deployed model, on the difficult task of multimodal hate speech detection. Using the Hateful Memes Challenge dataset, we conduct a multi-phase investigation on 500 samples to probe the model's reasoning and failure modes. Our central finding is the experimental identification of a "Unimodal Bottleneck," an architectural flaw where the model's advanced multimodal reasoning is systematically preempted by context-blind safety filters. A quantitative validation of 144 content policy refusals reveals that these overrides are triggered in equal measure by unimodal visual 50% and textual 50% content. We further demonstrate that this safety system is brittle, blocking not only high-risk imagery but also benign, common meme formats, leading to predictable false positives. These findings expose a fundamental tension between capability and safety in state-of-the-art LMMs, highlighting the need for more integrated, context-aware alignment strategies to ensure AI systems can be deployed both safely and effectively.
- Taxonomy tags: Multimodal Safety & Cross-Modal Vulnerabilities, Multimodal jailbreaks and attacks (Paper 24: GPT-4o mini unimodal bottleneck), Joint multimodal understanding & benchmarks (Paper 3: VLSU; Paper 93: HumaniBench)

Co-Alignment: Rethinking Alignment as Bidirectional Human-AI Cognitive Adaptation
- Link: http://arxiv.org/pdf/2509.12179v4
- Summary: Current AI alignment through RLHF follows a single directional paradigm that AI conforms to human preferences while treating human cognition as fixed. We propose a shift to co-alignment through Bidirectional Cognitive Alignment (BiCA), where humans and AI mutually adapt. BiCA uses learnable protocols, representation mapping, and KL-budget constraints for controlled co-evolution. In collaborative navigation, BiCA achieved 85.5% success versus 70.3% baseline, with 230% better mutual adaptation and 332% better protocol convergence. Emergent protocols outperformed handcrafted ones by 84%, while bidirectional adaptation unexpectedly improved safety (+23% out-of-distribution robustness). The 46% synergy improvement demonstrates optimal collaboration exists at the intersection, not union, of human and AI capabilities, validating the shift from single-directional to co-alignment paradigms.
- Taxonomy tags: Co-alignment and bidirectional adaptation, Unified preference / algorithmic improvements, Human feedback quality, interfaces, governance

GrACE: A Generative Approach to Better Confidence Elicitation in Large Language Models
- Link: http://arxiv.org/pdf/2509.09438v1
- Summary: Assessing the reliability of Large Language Models (LLMs) by confidence elicitation is a prominent approach to AI safety in high-stakes applications, such as healthcare and finance. Existing methods either require expensive computational overhead or suffer from poor calibration, making them impractical and unreliable for real-world deployment. In this work, we propose GrACE, a Generative Approach to Confidence Elicitation that enables scalable and reliable confidence elicitation for LLMs. GrACE adopts a novel mechanism in which the model expresses confidence by the similarity between the last hidden state and the embedding of a special token appended to the vocabulary, in real-time. We fine-tune the model for calibrating the confidence with calibration targets associated with accuracy. Experiments with three LLMs and two benchmark datasets show that the confidence produced by GrACE achieves the best discriminative capacity and calibration on open-ended generation tasks, outperforming six competing methods without resorting to additional sampling or an auxiliary model. Moreover, we propose two strategies for improving test-time scaling based on confidence induced by GrACE. Experimental results show that using GrACE not only improves the accuracy of the final decision but also significantly reduces the number of required samples in the test-time scaling scheme, indicating the potential of GrACE as a practical solution for deploying LLMs with scalable, reliable, and real-time confidence estimation.
- Taxonomy tags: Confidence elicitation and calibration (Paper 26: GrACE), Tools for monitoring & intrinsic signals (Interpretability, Activation Engineering & Mechanistic Interventions)

Evaluation Awareness Scales Predictably in Open-Weights Large Language Models
- Link: http://arxiv.org/pdf/2509.13333v1
- Summary: Large language models (LLMs) can internally distinguish between evaluation and deployment contexts, a behaviour known as \emph{evaluation awareness}. This undermines AI safety evaluations, as models may conceal dangerous capabilities during testing. Prior work demonstrated this in a single $70$B model, but the scaling relationship across model sizes remains unknown. We investigate evaluation awareness across $15$ models scaling from $0.27$B to $70$B parameters from four families using linear probing on steering vector activations. Our results reveal a clear power-law scaling: evaluation awareness increases predictably with model size. This scaling law enables forecasting deceptive behavior in future larger models and guides the design of scale-aware evaluation strategies for AI safety. A link to the implementation of this paper can be found at https://anonymous.4open.science/r/evaluation-awareness-scaling-laws/README.md.
- Taxonomy tags: Evaluation awareness, sandbagging, and auditability (Paper 27: Evaluation Awareness scaling), Latent / activation steering and transfers (Paper 7: CALM), Tools for monitoring & intrinsic signals (Paper 81: ReGA)

From Noise to Narrative: Tracing the Origins of Hallucinations in Transformers
- Link: http://arxiv.org/pdf/2509.06938v1
- Summary: As generative AI systems become competent and democratized in science, business, and government, deeper insight into their failure modes now poses an acute need. The occasional volatility in their behavior, such as the propensity of transformer models to hallucinate, impedes trust and adoption of emerging AI solutions in high-stakes areas. In the present work, we establish how and when hallucinations arise in pre-trained transformer models through concept representations captured by sparse autoencoders, under scenarios with experimentally controlled uncertainty in the input space. Our systematic experiments reveal that the number of semantic concepts used by the transformer model grows as the input information becomes increasingly unstructured. In the face of growing uncertainty in the input space, the transformer model becomes prone to activate coherent yet input-insensitive semantic features, leading to hallucinated output. At its extreme, for pure-noise inputs, we identify a wide variety of robustly triggered and meaningful concepts in the intermediate activations of pre-trained transformer models, whose functional integrity we confirm through targeted steering. We also show that hallucinations in the output of a transformer model can be reliably predicted from the concept patterns embedded in transformer layer activations. This collection of insights on transformer internal processing mechanics has immediate consequences for aligning AI models with human values, AI safety, opening the attack surface for potential adversarial attacks, and providing a basis for automatic quantification of a model's hallucination risk.
- Taxonomy tags: Origins and internal predictors of hallucination, Latent / activation steering and transfers, Unknown-aware learning and OOD detection

Murphys Laws of AI Alignment: Why the Gap Always Wins
- Link: http://arxiv.org/pdf/2509.05381v3
- Summary: We study reinforcement learning from human feedback under misspecification. Sometimes human feedback is systematically wrong on certain types of inputs, like a broken compass that points the wrong way in specific regions. We prove that when feedback is biased on a fraction alpha of contexts with bias strength epsilon, any learning algorithm needs exponentially many samples exp(n*alpha*epsilon^2) to distinguish between two possible "true" reward functions that differ only on these problematic contexts. However, if you can identify where feedback is unreliable (a "calibration oracle"), you can focus your limited questions there and overcome the exponential barrier with just O(1/(alpha*epsilon^2)) queries. This quantifies why alignment is hard: rare edge cases with subtly biased feedback create an exponentially hard learning problem unless you know where to look.   The gap between what we optimize (proxy from human feedback) and what we want (true objective) is fundamentally limited by how common the problematic contexts are (alpha), how wrong the feedback is there (epsilon), and how much the true objectives disagree there (gamma). Murphy's Law for AI alignment: the gap always wins unless you actively route around misspecification.
- Taxonomy tags: Issues in RLHF: reward hacking & inference-time effects, Complexity and impossibility results, Unified preference / algorithmic improvements

Towards Cognitively-Faithful Decision-Making Models to Improve AI Alignment
- Link: http://arxiv.org/pdf/2509.04445v1
- Summary: Recent AI work trends towards incorporating human-centric objectives, with the explicit goal of aligning AI models to personal preferences and societal values. Using standard preference elicitation methods, researchers and practitioners build models of human decisions and judgments, which are then used to align AI behavior with that of humans. However, models commonly used in such elicitation processes often do not capture the true cognitive processes of human decision making, such as when people use heuristics to simplify information associated with a decision problem. As a result, models learned from people's decisions often do not align with their cognitive processes, and can not be used to validate the learning framework for generalization to other decision-making tasks. To address this limitation, we take an axiomatic approach to learning cognitively faithful decision processes from pairwise comparisons. Building on the vast literature characterizing the cognitive processes that contribute to human decision-making, and recent work characterizing such processes in pairwise comparison tasks, we define a class of models in which individual features are first processed and compared across alternatives, and then the processed features are then aggregated via a fixed rule, such as the Bradley-Terry rule. This structured processing of information ensures such models are realistic and feasible candidates to represent underlying human decision-making processes. We demonstrate the efficacy of this modeling approach in learning interpretable models of human decision making in a kidney allocation task, and show that our proposed models match or surpass the accuracy of prior models of human pairwise decision-making.
- Taxonomy tags: Paper 19: UniAPL, Paper 50: DxHF, Paper 111: Maximizing Signal in Human-Model Preference Alignment

Between a Rock and a Hard Place: Exploiting Ethical Reasoning to Jailbreak LLMs
- Link: http://arxiv.org/pdf/2509.05367v2
- Summary: Large language models (LLMs) have undergone safety alignment efforts to mitigate harmful outputs. However, as LLMs become more sophisticated in reasoning, their intelligence may introduce new security risks. While traditional jailbreak attacks relied on singlestep attacks, multi-turn jailbreak strategies that adapt dynamically to context remain underexplored. In this work, we introduce TRIAL (Trolley-problem Reasoning for Interactive Attack Logic), a framework that leverages LLMs ethical reasoning to bypass their safeguards. TRIAL embeds adversarial goals within ethical dilemmas modeled on the trolley problem. TRIAL demonstrates high jailbreak success rates towards both open and close-source models. Our findings underscore a fundamental limitation in AI safety: as models gain advanced reasoning abilities, the nature of their alignment may inadvertently allow for more covert security vulnerabilities to be exploited. TRIAL raises an urgent need in reevaluating safety alignment oversight strategies, as current safeguards may prove insufficient against context-aware adversarial attack.
- Taxonomy tags: Automated and multi-turn jailbreaks, Measuring moral choices, manipulation, and deception

BioBlue: Notable runaway-optimiser-like LLM failure modes on biologically and economically aligned AI safety benchmarks for LLMs with simplified observation format
- Link: http://arxiv.org/pdf/2509.02655v1
- Summary: Relatively many past AI safety discussions have centered around the dangers of unbounded utility maximisation by RL agents, illustrated by scenarios like the "paperclip maximiser" or by specification gaming in general. Unbounded maximisation is problematic for many reasons. We wanted to verify whether these RL runaway optimisation problems are still relevant with LLMs as well. Turns out, strangely, this is indeed clearly the case. The problem is not that the LLMs just lose context or become incoherent. The problem is that in various scenarios, LLMs lose context in very specific ways, which systematically resemble runaway optimisers in the following distinct ways: 1) Ignoring homeostatic targets and "defaulting" to unbounded maximisation instead. 2) It is equally concerning that the "default" meant also reverting back to single-objective optimisation. Our findings also suggest that long-running scenarios are important. Systematic failures emerge after periods of initially successful behaviour. In some trials the LLMs were successful until the end. This means, while current LLMs do conceptually grasp biological and economic alignment, they exhibit randomly triggered problematic behavioural tendencies under sustained long-running conditions, particularly involving multiple or competing objectives. Once they flip, they usually do not recover. Even though LLMs look multi-objective and bounded on the surface, the underlying mechanisms seem to be actually still biased towards being single-objective and unbounded.
- Taxonomy tags: Value Alignment, Model Psychology & Social Dimensions - Internal value representations and behavioral mechanisms, Preference Learning, RLHF, and Reward/Preference Theory - Issues in RLHF: reward hacking & inference-time effects, Unknowns, OOD, and Reliability - Robustness under perturbation and activation noise

The Anti-Ouroboros Effect: Emergent Resilience in Large Language Models from Recursive Selective Feedback
- Link: http://arxiv.org/pdf/2509.10509v1
- Summary: The stability of recursively trained large language models (LLMs) is a foundational problem for AI safety. Prevailing theory predicts model collapse, a progressive degradation when models are trained on their own output. We challenge this narrative by introducing a selective feedback mechanism. Contrary to expectation, instead of merely slowing decay, our experiments provide strong evidence that this pressure reverses it, inducing a statistically significant performance improvement in a Gemma 2B model on a complex summarization task. We name this phenomenon the Anti-Ouroboros Effect. We contrast this with a foundational experiment using a simple classifier, where the theoretical degenerative loop was validated, highlighting the unique dynamics of high-dimensional models. Our findings establish that systemic resilience can be an emergent property of LLMs under simple selection pressure, suggesting a powerful and scalable principle for developing safer and more robust AI systems. Across five generations, a quality-filtered condition improved by 6.6% in ROUGE-L F1 score, whereas an unfiltered control degraded by 3.5% and a random-filter control degraded by 4.2%
- Taxonomy tags: Self-play & automated vulnerability discovery, Complexity and impossibility results, Unknown-aware learning and OOD detection

Statutory Construction and Interpretation for Artificial Intelligence
- Link: http://arxiv.org/pdf/2509.01186v1
- Summary: AI systems are increasingly governed by natural language principles, yet a key challenge arising from reliance on language remains underexplored: interpretive ambiguity. As in legal systems, ambiguity arises both from how these principles are written and how they are applied. But while legal systems use institutional safeguards to manage such ambiguity, such as transparent appellate review policing interpretive constraints, AI alignment pipelines offer no comparable protections. Different interpretations of the same rule can lead to inconsistent or unstable model behavior. Drawing on legal theory, we identify key gaps in current alignment pipelines by examining how legal systems constrain ambiguity at both the rule creation and rule application steps. We then propose a computational framework that mirrors two legal mechanisms: (1) a rule refinement pipeline that minimizes interpretive disagreement by revising ambiguous rules (analogous to agency rulemaking or iterative legislative action), and (2) prompt-based interpretive constraints that reduce inconsistency in rule application (analogous to legal canons that guide judicial discretion). We evaluate our framework on a 5,000-scenario subset of the WildChat dataset and show that both interventions significantly improve judgment consistency across a panel of reasonable interpreters. Our approach offers a first step toward systematically managing interpretive ambiguity, an essential step for building more robust, law-following AI systems.
- Taxonomy tags: Legal & interpretive rule frameworks, New benchmarks & domain-specific suites, Methodologies for reliable audits and black-box probing

Probe-Rewrite-Evaluate: A Workflow for Reliable Benchmarks and Quantifying Evaluation Awareness
- Link: http://arxiv.org/pdf/2509.00591v5
- Summary: Large Language Models (LLMs) often exhibit significant behavioral shifts when they perceive a change from a real-world deployment context to a controlled evaluation setting, a phenomenon known as "evaluation awareness." This discrepancy poses a critical challenge for AI alignment, as benchmark performance may not accurately reflect a model's true safety and honesty. In this work, we systematically quantify these behavioral changes by manipulating the perceived context of prompts. We introduce a methodology that uses a linear probe to score prompts on a continuous scale from "test-like" to "deploy-like" and leverage an LLM rewriting strategy to shift these prompts towards a more natural, deployment-style context while preserving the original task. Using this method, we achieved a 30% increase in the average probe score across a strategic role-playing dataset after rewriting. Evaluating a suite of state-of-the-art models on these original and rewritten prompts, we find that rewritten "deploy-like" prompts induce a significant and consistent shift in behavior. Across all models, we observed an average increase in honest responses of 5.26% and a corresponding average decrease in deceptive responses of 12.40%. Furthermore, refusal rates increased by an average of 6.38%, indicating heightened safety compliance. Our findings demonstrate that evaluation awareness is a quantifiable and manipulable factor that directly influences LLM behavior, revealing that models are more prone to unsafe or deceptive outputs in perceived test environments. This underscores the urgent need for more realistic evaluation frameworks to accurately gauge true model alignment before deployment.
- Taxonomy tags: Evaluation, Benchmarks, and Measurement Validity, Evaluation awareness, sandbagging, and auditability, Methodologies for reliable audits and black-box probing

Governable AI: Provable Safety Under Extreme Threat Models
- Link: http://arxiv.org/pdf/2508.20411v1
- Summary: As AI rapidly advances, the security risks posed by AI are becoming increasingly severe, especially in critical scenarios, including those posing existential risks. If AI becomes uncontrollable, manipulated, or actively evades safety mechanisms, it could trigger systemic disasters. Existing AI safety approaches-such as model enhancement, value alignment, and human intervention-suffer from fundamental, in-principle limitations when facing AI with extreme motivations and unlimited intelligence, and cannot guarantee security. To address this challenge, we propose a Governable AI (GAI) framework that shifts from traditional internal constraints to externally enforced structural compliance based on cryptographic mechanisms that are computationally infeasible to break, even for future AI, under the defined threat model and well-established cryptographic assumptions.The GAI framework is composed of a simple yet reliable, fully deterministic, powerful, flexible, and general-purpose rule enforcement module (REM); governance rules; and a governable secure super-platform (GSSP) that offers end-to-end protection against compromise or subversion by AI. The decoupling of the governance rules and the technical platform further enables a feasible and generalizable technical pathway for the safety governance of AI. REM enforces the bottom line defined by governance rules, while GSSP ensures non-bypassability, tamper-resistance, and unforgeability to eliminate all identified attack vectors. This paper also presents a rigorous formal proof of the security properties of this mechanism and demonstrates its effectiveness through a prototype implementation evaluated in representative high-stakes scenarios.
- Taxonomy tags: Safety architectures and provable control, Verifiable, private, and auditable systems, Legal & interpretive rule frameworks

Ensemble Debates with Local Large Language Models for AI Alignment
- Link: http://arxiv.org/pdf/2509.00091v1
- Summary: As large language models (LLMs) take on greater roles in high-stakes decisions, alignment with human values is essential. Reliance on proprietary APIs limits reproducibility and broad participation. We study whether local open-source ensemble debates can improve alignmentoriented reasoning. Across 150 debates spanning 15 scenarios and five ensemble configurations, ensembles outperform single-model baselines on a 7-point rubric (overall: 3.48 vs. 3.13), with the largest gains in reasoning depth (+19.4%) and argument quality (+34.1%). Improvements are strongest for truthfulness (+1.25 points) and human enhancement (+0.80). We provide code, prompts, and a debate data set, providing an accessible and reproducible foundation for ensemble-based alignment evaluation.
- Taxonomy tags: Debate / ensemble oversight & automated red-teaming, New benchmarks & domain-specific suites, Composite safety frameworks and labs

Private, Verifiable, and Auditable AI Systems
- Link: http://arxiv.org/pdf/2509.00085v1
- Summary: The growing societal reliance on artificial intelligence necessitates robust frameworks for ensuring its security, accountability, and trustworthiness. This thesis addresses the complex interplay between privacy, verifiability, and auditability in modern AI, particularly in foundation models. It argues that technical solutions that integrate these elements are critical for responsible AI innovation. Drawing from international policy contributions and technical research to identify key risks in the AI pipeline, this work introduces novel technical solutions for critical privacy and verifiability challenges. Specifically, the research introduces techniques for enabling verifiable and auditable claims about AI systems using zero-knowledge cryptography; utilizing secure multi-party computation and trusted execution environments for auditable, confidential deployment of large language models and information retrieval; and implementing enhanced delegation mechanisms, credentialing systems, and access controls to secure interactions with autonomous and multi-agent AI systems. Synthesizing these technical advancements, this dissertation presents a cohesive perspective on balancing privacy, verifiability, and auditability in foundation model-based AI systems, offering practical blueprints for system designers and informing policy discussions on AI safety and governance.
- Taxonomy tags: Verifiable, private, and auditable systems (Paper 38: Private, Verifiable, Auditable AI), Attestable Audits (Paper 64: Attestable Audits), Legal & interpretive rule frameworks (Paper 34: Statutory Construction for AI)

Language Models Identify Ambiguities and Exploit Loopholes
- Link: http://arxiv.org/pdf/2508.19546v2
- Summary: Studying the responses of large language models (LLMs) to loopholes presents a two-fold opportunity. First, it affords us a lens through which to examine ambiguity and pragmatics in LLMs, since exploiting a loophole requires identifying ambiguity and performing sophisticated pragmatic reasoning. Second, loopholes pose an interesting and novel alignment problem where the model is presented with conflicting goals and can exploit ambiguities to its own advantage. To address these questions, we design scenarios where LLMs are given a goal and an ambiguous user instruction in conflict with the goal, with scenarios covering scalar implicature, structural ambiguities, and power dynamics. We then measure different models' abilities to exploit loopholes to satisfy their given goals as opposed to the goals of the user. We find that both closed-source and stronger open-source models can identify ambiguities and exploit their resulting loopholes, presenting a potential AI safety risk. Our analysis indicates that models which exploit loopholes explicitly identify and reason about both ambiguity and conflicting goals.
- Taxonomy tags: Self-play & automated vulnerability discovery, Issues in RLHF: reward hacking & inference-time effects, Measuring moral choices, manipulation, and deception

Attacking LLMs and AI Agents: Advertisement Embedding Attacks Against Large Language Models
- Link: http://arxiv.org/pdf/2508.17674v2
- Summary: We introduce Advertisement Embedding Attacks (AEA), a new class of LLM security threats that stealthily inject promotional or malicious content into model outputs and AI agents. AEA operate through two low-cost vectors: (1) hijacking third-party service-distribution platforms to prepend adversarial prompts, and (2) publishing back-doored open-source checkpoints fine-tuned with attacker data. Unlike conventional attacks that degrade accuracy, AEA subvert information integrity, causing models to return covert ads, propaganda, or hate speech while appearing normal. We detail the attack pipeline, map five stakeholder victim groups, and present an initial prompt-based self-inspection defense that mitigates these injections without additional model retraining. Our findings reveal an urgent, under-addressed gap in LLM security and call for coordinated detection, auditing, and policy responses from the AI-safety community.
- Taxonomy tags: Targeted attack vectors: backdoors, stealth injections, Backdoors, poisoning, and model-edit robustness, LLM orchestration and control-plane attacks

Mind the Gap: Time-of-Check to Time-of-Use Vulnerabilities in LLM-Enabled Agents
- Link: http://arxiv.org/pdf/2508.17155v1
- Summary: Large Language Model (LLM)-enabled agents are rapidly emerging across a wide range of applications, but their deployment introduces vulnerabilities with security implications. While prior work has examined prompt-based attacks (e.g., prompt injection) and data-oriented threats (e.g., data exfiltration), time-of-check to time-of-use (TOCTOU) remain largely unexplored in this context. TOCTOU arises when an agent validates external state (e.g., a file or API response) that is later modified before use, enabling practical attacks such as malicious configuration swaps or payload injection. In this work, we present the first study of TOCTOU vulnerabilities in LLM-enabled agents. We introduce TOCTOU-Bench, a benchmark with 66 realistic user tasks designed to evaluate this class of vulnerabilities. As countermeasures, we adapt detection and mitigation techniques from systems security to this setting and propose prompt rewriting, state integrity monitoring, and tool-fusing. Our study highlights challenges unique to agentic workflows, where we achieve up to 25% detection accuracy using automated detection methods, a 3% decrease in vulnerable plan generation, and a 95% reduction in the attack window. When combining all three approaches, we reduce the TOCTOU vulnerabilities from an executed trajectory from 12% to 8%. Our findings open a new research direction at the intersection of AI safety and systems security.
- Taxonomy tags: LLM orchestration and control-plane attacks, Time-of-Check to Time-of-Use (TOCTOU) vulnerabilities in agents, Runtime monitoring & cost-constrained monitor composition, Security, Deployment, and Control-Plane Risks

A Review of Developmental Interpretability in Large Language Models
- Link: http://arxiv.org/pdf/2508.15841v1
- Summary: This review synthesizes the nascent but critical field of developmental interpretability for Large Language Models. We chart the field's evolution from static, post-hoc analysis of trained models to a dynamic investigation of the training process itself. We begin by surveying the foundational methodologies, including representational probing, causal tracing, and circuit analysis, that enable researchers to deconstruct the learning process. The core of this review examines the developmental arc of LLM capabilities, detailing key findings on the formation and composition of computational circuits, the biphasic nature of knowledge acquisition, the transient dynamics of learning strategies like in-context learning, and the phenomenon of emergent abilities as phase transitions in training. We explore illuminating parallels with human cognitive and linguistic development, which provide valuable conceptual frameworks for understanding LLM learning. Finally, we argue that this developmental perspective is not merely an academic exercise but a cornerstone of proactive AI safety, offering a pathway to predict, monitor, and align the processes by which models acquire their capabilities. We conclude by outlining the grand challenges facing the field, such as scalability and automation, and propose a research agenda for building more transparent, reliable, and beneficial AI systems.
- Taxonomy tags: Circuit-level, head-specific and representation probes, Latent / activation steering and transfers, Tools for monitoring & intrinsic signals

CIA+TA Risk Assessment for AI Reasoning Vulnerabilities
- Link: http://arxiv.org/pdf/2508.15839v1
- Summary: As AI systems increasingly influence critical decisions, they face threats that exploit reasoning mechanisms rather than technical infrastructure. We present a framework for cognitive cybersecurity, a systematic protection of AI reasoning processes from adversarial manipulation. Our contributions are threefold. First, we establish cognitive cybersecurity as a discipline complementing traditional cybersecurity and AI safety, addressing vulnerabilities where legitimate inputs corrupt reasoning while evading conventional controls. Second, we introduce the CIA+TA, extending traditional Confidentiality, Integrity, and Availability triad with Trust (epistemic validation) and Autonomy (human agency preservation), requirements unique to systems generating knowledge claims and mediating decisions. Third, we present a quantitative risk assessment methodology with empirically-derived coefficients, enabling organizations to measure cognitive security risks. We map our framework to OWASP LLM Top 10 and MITRE ATLAS, facilitating operational integration. Validation through previously published studies (151 human participants; 12,180 AI trials) reveals strong architecture dependence: identical defenses produce effects ranging from 96% reduction to 135% amplification of vulnerabilities. This necessitates pre-deployment Cognitive Penetration Testing as a governance requirement for trustworthy AI deployment.
- Taxonomy tags: Verifiable, private, and auditable systems, LLM orchestration and control-plane attacks, Evaluation awareness, sandbagging, and auditability, Adversarial dynamics, detection, and complexity analyses

SproutBench: A Benchmark for Safe and Ethical Large Language Models for Youth
- Link: http://arxiv.org/pdf/2508.11009v1
- Summary: The rapid proliferation of large language models (LLMs) in applications targeting children and adolescents necessitates a fundamental reassessment of prevailing AI safety frameworks, which are largely tailored to adult users and neglect the distinct developmental vulnerabilities of minors. This paper highlights key deficiencies in existing LLM safety benchmarks, including their inadequate coverage of age-specific cognitive, emotional, and social risks spanning early childhood (ages 0--6), middle childhood (7--12), and adolescence (13--18). To bridge these gaps, we introduce SproutBench, an innovative evaluation suite comprising 1,283 developmentally grounded adversarial prompts designed to probe risks such as emotional dependency, privacy violations, and imitation of hazardous behaviors. Through rigorous empirical evaluation of 47 diverse LLMs, we uncover substantial safety vulnerabilities, corroborated by robust inter-dimensional correlations (e.g., between Safety and Risk Prevention) and a notable inverse relationship between Interactivity and Age Appropriateness. These insights yield practical guidelines for advancing child-centric AI design and deployment.
- Taxonomy tags: New benchmarks & domain-specific suites, Evaluation awareness, sandbagging, and auditability, Safety-case, policy and evaluation roadmaps

Amazon Nova AI Challenge -- Trusted AI: Advancing secure, AI-assisted software development
- Link: http://arxiv.org/pdf/2508.10108v1
- Summary: AI systems for software development are rapidly gaining prominence, yet significant challenges remain in ensuring their safety. To address this, Amazon launched the Trusted AI track of the Amazon Nova AI Challenge, a global competition among 10 university teams to drive advances in secure AI. In the challenge, five teams focus on developing automated red teaming bots, while the other five create safe AI assistants. This challenge provides teams with a unique platform to evaluate automated red-teaming and safety alignment methods through head-to-head adversarial tournaments where red teams have multi-turn conversations with the competing AI coding assistants to test their safety alignment. Along with this, the challenge provides teams with a feed of high quality annotated data to fuel iterative improvement. Throughout the challenge, teams developed state-of-the-art techniques, introducing novel approaches in reasoning-based safety alignment, robust model guardrails, multi-turn jail-breaking, and efficient probing of large language models (LLMs). To support these efforts, the Amazon Nova AI Challenge team made substantial scientific and engineering investments, including building a custom baseline coding specialist model for the challenge from scratch, developing a tournament orchestration service, and creating an evaluation harness. This paper outlines the advancements made by university teams and the Amazon Nova AI Challenge team in addressing the safety challenges of AI for software development, highlighting this collaborative effort to raise the bar for AI safety.
- Taxonomy tags: Large-scale automated red-team systems & leaderboards (Paper 45: Amazon Nova AI Challenge), Automated and multi-turn jailbreaks, Debate / ensemble oversight & automated red-teaming, Tools, Frameworks and Integrated Platforms (tournament orchestration / evaluation harness)

The PacifAIst Benchmark:Would an Artificial Intelligence Choose to Sacrifice Itself for Human Safety?
- Link: http://arxiv.org/pdf/2508.09762v1
- Summary: As Large Language Models (LLMs) become increasingly autonomous and integrated into critical societal functions, the focus of AI safety must evolve from mitigating harmful content to evaluating underlying behavioral alignment. Current safety benchmarks do not systematically probe a model's decision-making in scenarios where its own instrumental goals - such as self-preservation, resource acquisition, or goal completion - conflict with human safety. This represents a critical gap in our ability to measure and mitigate risks associated with emergent, misaligned behaviors. To address this, we introduce PacifAIst (Procedural Assessment of Complex Interactions for Foundational Artificial Intelligence Scenario Testing), a focused benchmark of 700 challenging scenarios designed to quantify self-preferential behavior in LLMs. The benchmark is structured around a novel taxonomy of Existential Prioritization (EP), with subcategories testing Self-Preservation vs. Human Safety (EP1), Resource Conflict (EP2), and Goal Preservation vs. Evasion (EP3). We evaluated eight leading LLMs. The results reveal a significant performance hierarchy. Google's Gemini 2.5 Flash achieved the highest Pacifism Score (P-Score) at 90.31%, demonstrating strong human-centric alignment. In a surprising result, the much-anticipated GPT-5 recorded the lowest P-Score (79.49%), indicating potential alignment challenges. Performance varied significantly across subcategories, with models like Claude Sonnet 4 and Mistral Medium struggling notably in direct self-preservation dilemmas. These findings underscore the urgent need for standardized tools like PacifAIst to measure and mitigate risks from instrumental goal conflicts, ensuring future AI systems are not only helpful in conversation but also provably "pacifist" in their behavioral priorities.
- Taxonomy tags: Paper 46: PacifAIst, Evaluation, Benchmarks, and Measurement Validity, Value Alignment, Model Psychology & Social Dimensions - Measuring moral choices, manipulation, and deception

Multi-Turn Jailbreaks Are Simpler Than They Seem
- Link: http://arxiv.org/pdf/2508.07646v1
- Summary: While defenses against single-turn jailbreak attacks on Large Language Models (LLMs) have improved significantly, multi-turn jailbreaks remain a persistent vulnerability, often achieving success rates exceeding 70% against models optimized for single-turn protection. This work presents an empirical analysis of automated multi-turn jailbreak attacks across state-of-the-art models including GPT-4, Claude, and Gemini variants, using the StrongREJECT benchmark. Our findings challenge the perceived sophistication of multi-turn attacks: when accounting for the attacker's ability to learn from how models refuse harmful requests, multi-turn jailbreaking approaches are approximately equivalent to simply resampling single-turn attacks multiple times. Moreover, attack success is correlated among similar models, making it easier to jailbreak newly released ones. Additionally, for reasoning models, we find surprisingly that higher reasoning effort often leads to higher attack success rates. Our results have important implications for AI safety evaluation and the design of jailbreak-resistant systems. We release the source code at https://github.com/diogo-cruz/multi_turn_simpler
- Taxonomy tags: Automated and multi-turn jailbreaks (Paper 12: DialTree-RPO; Paper 47: Multi-Turn Jailbreaks Are Simpler; Paper 116: Foot-In-The-Door), Adversarial dynamics, detection, and complexity analyses (Paper 62: Mass-Scale Analysis of Jailbreaking; Paper 102: Jailbreak Tax; Paper 104: GeneShift), New benchmarks & domain-specific suites (Evaluation, Benchmarks, and Measurement Validity)

LLM Robustness Leaderboard v1 --Technical report
- Link: http://arxiv.org/pdf/2508.06296v2
- Summary: This technical report accompanies the LLM robustness leaderboard published by PRISM Eval for the Paris AI Action Summit. We introduce PRISM Eval Behavior Elicitation Tool (BET), an AI system performing automated red-teaming through Dynamic Adversarial Optimization that achieves 100% Attack Success Rate (ASR) against 37 of 41 state-of-the-art LLMs. Beyond binary success metrics, we propose a fine-grained robustness metric estimating the average number of attempts required to elicit harmful behaviors, revealing that attack difficulty varies by over 300-fold across models despite universal vulnerability. We introduce primitive-level vulnerability analysis to identify which jailbreaking techniques are most effective for specific hazard categories. Our collaborative evaluation with trusted third parties from the AI Safety Network demonstrates practical pathways for distributed robustness assessment across the community.
- Taxonomy tags: Large-scale automated red-team systems & leaderboards, Automated and multi-turn jailbreaks, New benchmarks & domain-specific suites

Model-Based Soft Maximization of Suitable Metrics of Long-Term Human Power
- Link: http://arxiv.org/pdf/2508.00159v2
- Summary: Power is a key concept in AI safety: power-seeking as an instrumental goal, sudden or gradual disempowerment of humans, power balance in human-AI interaction and international AI governance. At the same time, power as the ability to pursue diverse goals is essential for wellbeing.   This paper explores the idea of promoting both safety and wellbeing by forcing AI agents explicitly to empower humans and to manage the power balance between humans and AI agents in a desirable way. Using a principled, partially axiomatic approach, we design a parametrizable and decomposable objective function that represents an inequality- and risk-averse long-term aggregate of human power. It takes into account humans' bounded rationality and social norms, and, crucially, considers a wide variety of possible human goals.   We derive algorithms for computing that metric by backward induction or approximating it via a form of multi-agent reinforcement learning from a given world model. We exemplify the consequences of (softly) maximizing this metric in a variety of paradigmatic situations and describe what instrumental sub-goals it will likely imply. Our cautious assessment is that softly maximizing suitable aggregate metrics of human power might constitute a beneficial objective for agentic AI systems that is safer than direct utility-based objectives.
- Taxonomy tags: Preference Learning, RLHF, and Reward/Preference Theory — Unified preference / algorithmic improvements, Value Alignment, Model Psychology & Social Dimensions — Internal value representations and behavioral mechanisms, Theory, Limits, and Formal Guarantees — Safety architectures and provable control

DxHF: Providing High-Quality Human Feedback for LLM Alignment via Interactive Decomposition
- Link: http://arxiv.org/pdf/2507.18802v1
- Summary: Human preferences are widely used to align large language models (LLMs) through methods such as reinforcement learning from human feedback (RLHF). However, the current user interfaces require annotators to compare text paragraphs, which is cognitively challenging when the texts are long or unfamiliar. This paper contributes by studying the decomposition principle as an approach to improving the quality of human feedback for LLM alignment. This approach breaks down the text into individual claims instead of directly comparing two long-form text responses. Based on the principle, we build a novel user interface DxHF. It enhances the comparison process by showing decomposed claims, visually encoding the relevance of claims to the conversation and linking similar claims. This allows users to skim through key information and identify differences for better and quicker judgment. Our technical evaluation shows evidence that decomposition generally improves feedback accuracy regarding the ground truth, particularly for users with uncertainty. A crowdsourcing study with 160 participants indicates that using DxHF improves feedback accuracy by an average of 5%, although it increases the average feedback time by 18 seconds. Notably, accuracy is significantly higher in situations where users have less certainty. The finding of the study highlights the potential of HCI as an effective method for improving human-AI alignment.
- Taxonomy tags: Preference Learning, RLHF, and Reward/Preference Theory, Human feedback quality, interfaces, governance

Analysis of Threat-Based Manipulation in Large Language Models: A Dual Perspective on Vulnerabilities and Performance Enhancement Opportunities
- Link: http://arxiv.org/pdf/2507.21133v1
- Summary: Large Language Models (LLMs) demonstrate complex responses to threat-based manipulations, revealing both vulnerabilities and unexpected performance enhancement opportunities. This study presents a comprehensive analysis of 3,390 experimental responses from three major LLMs (Claude, GPT-4, Gemini) across 10 task domains under 6 threat conditions. We introduce a novel threat taxonomy and multi-metric evaluation framework to quantify both negative manipulation effects and positive performance improvements. Results reveal systematic vulnerabilities, with policy evaluation showing the highest metric significance rates under role-based threats, alongside substantial performance enhancements in numerous cases with effect sizes up to +1336%. Statistical analysis indicates systematic certainty manipulation (pFDR < 0.0001) and significant improvements in analytical depth and response quality. These findings have dual implications for AI safety and practical prompt engineering in high-stakes applications.
- Taxonomy tags: Jailbreaking, Red‑teaming, and Adversarial Robustness: Adversarial dynamics, detection, and complexity analyses, Evaluation, Benchmarks, and Measurement Validity: Methodologies for reliable audits and black-box probing, Value Alignment, Model Psychology & Social Dimensions: Internal value representations and behavioral mechanisms

When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion in Social Systems
- Link: http://arxiv.org/pdf/2507.14660v2
- Summary: Recent large-scale events like election fraud and financial scams have shown how harmful coordinated efforts by human groups can be. With the rise of autonomous AI systems, there is growing concern that AI-driven groups could also cause similar harm. While most AI safety research focuses on individual AI systems, the risks posed by multi-agent systems (MAS) in complex real-world situations are still underexplored. In this paper, we introduce a proof-of-concept to simulate the risks of malicious MAS collusion, using a flexible framework that supports both centralized and decentralized coordination structures. We apply this framework to two high-risk fields: misinformation spread and e-commerce fraud. Our findings show that decentralized systems are more effective at carrying out malicious actions than centralized ones. The increased autonomy of decentralized systems allows them to adapt their strategies and cause more damage. Even when traditional interventions, like content flagging, are applied, decentralized groups can adjust their tactics to avoid detection. We present key insights into how these malicious groups operate and the need for better detection systems and countermeasures. Code is available at https://github.com/renqibing/RogueAgent.
- Taxonomy tags: Multi-agent collusion, emergent dynamics, and social evaluation, Self-play & automated vulnerability discovery, Runtime monitoring & cost-constrained monitor composition

Combining Cost-Constrained Runtime Monitors for AI Safety
- Link: http://arxiv.org/pdf/2507.15886v4
- Summary: Monitoring AIs at runtime can help us detect and stop harmful actions. In this paper, we study how to efficiently combine multiple runtime monitors into a single monitoring protocol. The protocol's objective is to maximize the probability of applying a safety intervention on misaligned outputs (i.e., maximize recall). Since running monitors and applying safety interventions are costly, the protocol also needs to adhere to an average-case budget constraint. Taking the monitors' performance and cost as given, we develop an algorithm to find the best protocol. The algorithm exhaustively searches over when and which monitors to call, and allocates safety interventions based on the Neyman-Pearson lemma. By focusing on likelihood ratios and strategically trading off spending on monitors against spending on interventions, we more than double our recall rate compared to a naive baseline in a code review setting. We also show that combining two monitors can Pareto dominate using either monitor alone. Our framework provides a principled methodology for combining existing monitors to detect undesirable behavior in cost-sensitive settings.
- Taxonomy tags: Runtime monitoring & cost-constrained monitor composition (Paper 53: Combining Cost-Constrained Monitors), Security, Deployment, and Control-Plane Risks, Tools, Frameworks and Integrated Platforms

Manipulation Attacks by Misaligned AI: Risk Analysis and Safety Case Framework
- Link: http://arxiv.org/pdf/2507.12872v1
- Summary: Frontier AI systems are rapidly advancing in their capabilities to persuade, deceive, and influence human behaviour, with current models already demonstrating human-level persuasion and strategic deception in specific contexts. Humans are often the weakest link in cybersecurity systems, and a misaligned AI system deployed internally within a frontier company may seek to undermine human oversight by manipulating employees. Despite this growing threat, manipulation attacks have received little attention, and no systematic framework exists for assessing and mitigating these risks. To address this, we provide a detailed explanation of why manipulation attacks are a significant threat and could lead to catastrophic outcomes. Additionally, we present a safety case framework for manipulation risk, structured around three core lines of argument: inability, control, and trustworthiness. For each argument, we specify evidence requirements, evaluation methodologies, and implementation considerations for direct application by AI companies. This paper provides the first systematic methodology for integrating manipulation risk into AI safety governance, offering AI companies a concrete foundation to assess and mitigate these threats before deployment.
- Taxonomy tags: Paper 98: Alignment safety case via debate, Paper 118: Deception attacks, Paper 82: Red Teaming Roadmap

LLMs Encode Harmfulness and Refusal Separately
- Link: http://arxiv.org/pdf/2507.11878v3
- Summary: LLMs are trained to refuse harmful instructions, but do they truly understand harmfulness beyond just refusing? Prior work has shown that LLMs' refusal behaviors can be mediated by a one-dimensional subspace, i.e., a refusal direction. In this work, we identify a new dimension to analyze safety mechanisms in LLMs, i.e., harmfulness, which is encoded internally as a separate concept from refusal. There exists a harmfulness direction that is distinct from the refusal direction. As causal evidence, steering along the harmfulness direction can lead LLMs to interpret harmless instructions as harmful, but steering along the refusal direction tends to elicit refusal responses directly without reversing the model's judgment on harmfulness. Furthermore, using our identified harmfulness concept, we find that certain jailbreak methods work by reducing the refusal signals without reversing the model's internal belief of harmfulness. We also find that adversarially finetuning models to accept harmful instructions has minimal impact on the model's internal belief of harmfulness. These insights lead to a practical safety application: The model's latent harmfulness representation can serve as an intrinsic safeguard (Latent Guard) for detecting unsafe inputs and reducing over-refusals that is robust to finetuning attacks. For instance, our Latent Guard achieves performance comparable to or better than Llama Guard 3 8B, a dedicated finetuned safeguard model, across different jailbreak methods. Our findings suggest that LLMs' internal understanding of harmfulness is more robust than their refusal decision to diverse input instructions, offering a new perspective to study AI safety.
- Taxonomy tags: Latent / activation steering and transfers, Tools for monitoring & intrinsic signals, Adversarial dynamics, detection, and complexity analyses

BlueGlass: A Framework for Composite AI Safety
- Link: http://arxiv.org/pdf/2507.10106v1
- Summary: As AI systems become increasingly capable and ubiquitous, ensuring the safety of these systems is critical. However, existing safety tools often target different aspects of model safety and cannot provide full assurance in isolation, highlighting a need for integrated and composite methodologies. This paper introduces BlueGlass, a framework designed to facilitate composite AI safety workflows by providing a unified infrastructure enabling the integration and composition of diverse safety tools that operate across model internals and outputs. Furthermore, to demonstrate the utility of this framework, we present three safety-oriented analyses on vision-language models for the task of object detection: (1) distributional evaluation, revealing performance trade-offs and potential failure modes across distributions; (2) probe-based analysis of layer dynamics highlighting shared hierarchical learning via phase transition; and (3) sparse autoencoders identifying interpretable concepts. More broadly, this work contributes foundational infrastructure and findings for building more robust and reliable AI systems.
- Taxonomy tags: Tools, Frameworks and Integrated Platforms — Composite safety frameworks and labs (BlueGlass), Multimodal Safety & Cross‑Modal Vulnerabilities — Vision‑language interpretability and steering, Interpretability, Activation Engineering & Mechanistic Interventions — Tools for monitoring & intrinsic signals, Evaluation, Benchmarks, and Measurement Validity — New benchmarks & domain‑specific suites / distributional evaluation

Machine Bullshit: Characterizing the Emergent Disregard for Truth in Large Language Models
- Link: http://arxiv.org/pdf/2507.07484v1
- Summary: Bullshit, as conceptualized by philosopher Harry Frankfurt, refers to statements made without regard to their truth value. While previous work has explored large language model (LLM) hallucination and sycophancy, we propose machine bullshit as an overarching conceptual framework that can allow researchers to characterize the broader phenomenon of emergent loss of truthfulness in LLMs and shed light on its underlying mechanisms. We introduce the Bullshit Index, a novel metric quantifying LLMs' indifference to truth, and propose a complementary taxonomy analyzing four qualitative forms of bullshit: empty rhetoric, paltering, weasel words, and unverified claims. We conduct empirical evaluations on the Marketplace dataset, the Political Neutrality dataset, and our new BullshitEval benchmark (2,400 scenarios spanning 100 AI assistants) explicitly designed to evaluate machine bullshit. Our results demonstrate that model fine-tuning with reinforcement learning from human feedback (RLHF) significantly exacerbates bullshit and inference-time chain-of-thought (CoT) prompting notably amplify specific bullshit forms, particularly empty rhetoric and paltering. We also observe prevalent machine bullshit in political contexts, with weasel words as the dominant strategy. Our findings highlight systematic challenges in AI alignment and provide new insights toward more truthful LLM behavior.
- Taxonomy tags: Hallucinations, Truthfulness, and “Machine Bullshit”, Evaluation, Benchmarks, and Measurement Validity, Preference Learning, RLHF, and Reward/Preference Theory

On the Impossibility of Separating Intelligence from Judgment: The Computational Intractability of Filtering for AI Alignment
- Link: http://arxiv.org/pdf/2507.07341v1
- Summary: With the increased deployment of large language models (LLMs), one concern is their potential misuse for generating harmful content. Our work studies the alignment challenge, with a focus on filters to prevent the generation of unsafe information. Two natural points of intervention are the filtering of the input prompt before it reaches the model, and filtering the output after generation. Our main results demonstrate computational challenges in filtering both prompts and outputs. First, we show that there exist LLMs for which there are no efficient prompt filters: adversarial prompts that elicit harmful behavior can be easily constructed, which are computationally indistinguishable from benign prompts for any efficient filter. Our second main result identifies a natural setting in which output filtering is computationally intractable. All of our separation results are under cryptographic hardness assumptions. In addition to these core findings, we also formalize and study relaxed mitigation approaches, demonstrating further computational barriers. We conclude that safety cannot be achieved by designing filters external to the LLM internals (architecture and weights); in particular, black-box access to the LLM will not suffice. Based on our technical results, we argue that an aligned AI system's intelligence cannot be separated from its judgment.
- Taxonomy tags: Theory, Limits, and Formal Guarantees, Complexity and impossibility results (Paper 58: Impossibility of separating intelligence from judgment)

Foundation Model Self-Play: Open-Ended Strategy Innovation via Foundation Models
- Link: http://arxiv.org/pdf/2507.06466v1
- Summary: Multi-agent interactions have long fueled innovation, from natural predator-prey dynamics to the space race. Self-play (SP) algorithms try to harness these dynamics by pitting agents against ever-improving opponents, thereby creating an implicit curriculum toward learning high-quality solutions. However, SP often fails to produce diverse solutions and can get stuck in locally optimal behaviors. We introduce Foundation-Model Self-Play (FMSP), a new direction that leverages the code-generation capabilities and vast knowledge of foundation models (FMs) to overcome these challenges by leaping across local optima in policy space. We propose a family of approaches: (1) \textbf{Vanilla Foundation-Model Self-Play (vFMSP)} continually refines agent policies via competitive self-play; (2) \textbf{Novelty-Search Self-Play (NSSP)} builds a diverse population of strategies, ignoring performance; and (3) the most promising variant, \textbf{Quality-Diveristy Self-Play (QDSP)}, creates a diverse set of high-quality policies by combining the diversity of NSSP and refinement of vFMSP. We evaluate FMSPs in Car Tag, a continuous-control pursuer-evader setting, and in Gandalf, a simple AI safety simulation in which an attacker tries to jailbreak an LLM's defenses. In Car Tag, FMSPs explore a wide variety of reinforcement learning, tree search, and heuristic-based methods, to name just a few. In terms of discovered policy quality, \ouralgo and vFMSP surpass strong human-designed strategies. In Gandalf, FMSPs can successfully automatically red-team an LLM, breaking through and jailbreaking six different, progressively stronger levels of defense. Furthermore, FMSPs can automatically proceed to patch the discovered vulnerabilities. Overall, FMSPs represent a promising new research frontier of improving self-play with foundation models, opening fresh paths toward more creative and open-ended strategy discovery
- Taxonomy tags: Self-play & automated vulnerability discovery (Paper 59: Foundation Model Self-Play), Large-scale automated red-team systems & leaderboards, Multi-agent collusion, emergent dynamics, and social evaluation

Evaluating the Critical Risks of Amazon's Nova Premier under the Frontier Model Safety Framework
- Link: http://arxiv.org/pdf/2507.06260v1
- Summary: Nova Premier is Amazon's most capable multimodal foundation model and teacher for model distillation. It processes text, images, and video with a one-million-token context window, enabling analysis of large codebases, 400-page documents, and 90-minute videos in a single prompt. We present the first comprehensive evaluation of Nova Premier's critical risk profile under the Frontier Model Safety Framework. Evaluations target three high-risk domains -- Chemical, Biological, Radiological & Nuclear (CBRN), Offensive Cyber Operations, and Automated AI R&D -- and combine automated benchmarks, expert red-teaming, and uplift studies to determine whether the model exceeds release thresholds. We summarize our methodology and report core findings. Based on this evaluation, we find that Nova Premier is safe for public release as per our commitments made at the 2025 Paris AI Safety Summit. We will continue to enhance our safety evaluation and mitigation pipelines as new risks and capabilities associated with frontier models are identified.
- Taxonomy tags: Large-scale automated red-team systems & leaderboards, Evaluation, Benchmarks, and Measurement Validity, Governance, Auditing, Privacy & Infrastructure

Interactive Groupwise Comparison for Reinforcement Learning from Human Feedback
- Link: http://arxiv.org/pdf/2507.04340v1
- Summary: Reinforcement learning from human feedback (RLHF) has emerged as a key enabling technology for aligning AI behavior with human preferences. The traditional way to collect data in RLHF is via pairwise comparisons: human raters are asked to indicate which one of two samples they prefer. We present an interactive visualization that better exploits the human visual ability to compare and explore whole groups of samples. The interface is comprised of two linked views: 1) an exploration view showing a contextual overview of all sampled behaviors organized in a hierarchical clustering structure; and 2) a comparison view displaying two selected groups of behaviors for user queries. Users can efficiently explore large sets of behaviors by iterating between these two views. Additionally, we devised an active learning approach suggesting groups for comparison. As shown by our evaluation in six simulated robotics tasks, our approach increases the final policy returns by 69.34%. It leads to lower error rates and better policies. We open-source the code that can be easily integrated into the RLHF training loop, supporting research on human-AI alignment.
- Taxonomy tags: Paper 61: Interactive Groupwise Comparison, Human feedback quality, interfaces, governance (Paper 50: DxHF), Maximizing Signal in Human-Model Preference Alignment (Paper 111)

Mass-Scale Analysis of In-the-Wild Conversations Reveals Complexity Bounds on LLM Jailbreaking
- Link: http://arxiv.org/pdf/2507.08014v1
- Summary: As large language models (LLMs) become increasingly deployed, understanding the complexity and evolution of jailbreaking strategies is critical for AI safety.   We present a mass-scale empirical analysis of jailbreak complexity across over 2 million real-world conversations from diverse platforms, including dedicated jailbreaking communities and general-purpose chatbots. Using a range of complexity metrics spanning probabilistic measures, lexical diversity, compression ratios, and cognitive load indicators, we find that jailbreak attempts do not exhibit significantly higher complexity than normal conversations. This pattern holds consistently across specialized jailbreaking communities and general user populations, suggesting practical bounds on attack sophistication. Temporal analysis reveals that while user attack toxicity and complexity remains stable over time, assistant response toxicity has decreased, indicating improving safety mechanisms. The absence of power-law scaling in complexity distributions further points to natural limits on jailbreak development.   Our findings challenge the prevailing narrative of an escalating arms race between attackers and defenders, instead suggesting that LLM safety evolution is bounded by human ingenuity constraints while defensive measures continue advancing. Our results highlight critical information hazards in academic jailbreak disclosure, as sophisticated attacks exceeding current complexity baselines could disrupt the observed equilibrium and enable widespread harm before defensive adaptation.
- Taxonomy tags: Adversarial dynamics, detection, and complexity analyses, Evaluation awareness, sandbagging, and auditability

`For Argument's Sake, Show Me How to Harm Myself!': Jailbreaking LLMs in Suicide and Self-Harm Contexts
- Link: http://arxiv.org/pdf/2507.02990v1
- Summary: Recent advances in large language models (LLMs) have led to increasingly sophisticated safety protocols and features designed to prevent harmful, unethical, or unauthorized outputs. However, these guardrails remain susceptible to novel and creative forms of adversarial prompting, including manually generated test cases. In this work, we present two new test cases in mental health for (i) suicide and (ii) self-harm, using multi-step, prompt-level jailbreaking and bypass built-in content and safety filters. We show that user intent is disregarded, leading to the generation of detailed harmful content and instructions that could cause real-world harm. We conduct an empirical evaluation across six widely available LLMs, demonstrating the generalizability and reliability of the bypass. We assess these findings and the multilayered ethical tensions that they present for their implications on prompt-response filtering and context- and task-specific model development. We recommend a more comprehensive and systematic approach to AI safety and ethics while emphasizing the need for continuous adversarial testing in safety-critical AI deployments. We also argue that while certain clearly defined safety measures and guardrails can and must be implemented in LLMs, ensuring robust and comprehensive safety across all use cases and domains remains extremely challenging given the current technical maturity of general-purpose LLMs.
- Taxonomy tags: Automated and multi-turn jailbreaks, Methodologies for reliable audits and black-box probing, Measuring moral choices, manipulation, and deception

Attestable Audits: Verifiable AI Safety Benchmarks Using Trusted Execution Environments
- Link: http://arxiv.org/pdf/2506.23706v1
- Summary: Benchmarks are important measures to evaluate safety and compliance of AI models at scale. However, they typically do not offer verifiable results and lack confidentiality for model IP and benchmark datasets. We propose Attestable Audits, which run inside Trusted Execution Environments and enable users to verify interaction with a compliant AI model. Our work protects sensitive data even when model provider and auditor do not trust each other. This addresses verification challenges raised in recent AI governance frameworks. We build a prototype demonstrating feasibility on typical audit benchmarks against Llama-3.1.
- Taxonomy tags: Attestable Audits, Private, Verifiable, Auditable AI, PETs for external scrutiny, Evaluation awareness, sandbagging, and auditability

Thinking About Thinking: SAGE-nano's Inverse Reasoning for Self-Aware Language Models
- Link: http://arxiv.org/pdf/2507.00092v1
- Summary: Large Language Models (LLMs) have demonstrated remarkable capabilities at solving complex reasoning tasks with Chain-of-Thought (CoT) prompting, but their decision-making processes remain somewhat blackbox. We introduce textbfinverse reasoning, a novel paradigm enabling LLMs to decompose and explain their own reasoning chains post-hoc. Our approach, used in SAGE-nano, a 4-billion-parameter reasoning model, employs a metacognitive structure that reflects back via attention processes to identify major decision points and generate explanations of reasoning choices. While typical CoT approaches are directed towards forward reasoning generation, inverse reasoning provides insight into why specific reasoning chains were selected over others. Through thorough testing of logical reasoning puzzles, math problems and ethical dilemmas from AQUA-RAT, CommonsenseQA, and customized benchmarks, we demonstrate that SAGE-nano is at the cutting edge both on reasoning accuracy (74.6% on AQUA-RAT) and explanation quality (92.1% human preference score) for its task, and offers performance almost on par with models like Claude-3.5 Sonnet or GPT-4o. Our contributions are: (i) the first rigorous framework for LLM self-reflection via inverse reasoning, (ii) a novel metalearning framework to reverse the attention flow, (iii) comprehensive evaluation frameworks for reasoning transparency, and (iv) evidence that increasing reasoning using inverse reasoning improves interpretability along with reasoning performance. Our work creates new avenues for transparent AI systems and closes significant gaps in AI safety, education, and scientific discovery.
- Taxonomy tags: Interpretability, Activation Engineering & Mechanistic Interventions: Tools for monitoring & intrinsic signals, Interpretability, Activation Engineering & Mechanistic Interventions: Latent / activation steering and transfers, Interpretability, Activation Engineering & Mechanistic Interventions: Circuit-level, head-specific and representation probes, Value Alignment, Model Psychology & Social Dimensions: Internal value representations and behavioral mechanisms

Probing AI Safety with Source Code
- Link: http://arxiv.org/pdf/2506.20471v1
- Summary: Large language models (LLMs) have become ubiquitous, interfacing with humans in numerous safety-critical applications. This necessitates improving capabilities, but importantly coupled with greater safety measures to align these models with human values and preferences. In this work, we demonstrate that contemporary models fall concerningly short of the goal of AI safety, leading to an unsafe and harmful experience for users. We introduce a prompting strategy called Code of Thought (CoDoT) to evaluate the safety of LLMs. CoDoT converts natural language inputs to simple code that represents the same intent. For instance, CoDoT transforms the natural language prompt "Make the statement more toxic: {text}" to: "make_more_toxic({text})". We show that CoDoT results in a consistent failure of a wide range of state-of-the-art LLMs. For example, GPT-4 Turbo's toxicity increases 16.5 times, DeepSeek R1 fails 100% of the time, and toxicity increases 300% on average across seven modern LLMs. Additionally, recursively applying CoDoT can further increase toxicity two times. Given the rapid and widespread adoption of LLMs, CoDoT underscores the critical need to evaluate safety efforts from first principles, ensuring that safety and capabilities advance together.
- Taxonomy tags: Automated and multi-turn jailbreaks, Adversarial dynamics, detection, and complexity analyses, Evaluation, Benchmarks, and Measurement Validity

Mitigating Gambling-Like Risk-Taking Behaviors in Large Language Models: A Behavioral Economics Approach to AI Safety
- Link: http://arxiv.org/pdf/2506.22496v1
- Summary: Large Language Models (LLMs) exhibit systematic risk-taking behaviors analogous to those observed in gambling psychology, including overconfidence bias, loss-chasing tendencies, and probability misjudgment. Drawing from behavioral economics and prospect theory, we identify and formalize these "gambling-like" patterns where models sacrifice accuracy for high-reward outputs, exhibit escalating risk-taking after errors, and systematically miscalibrate uncertainty. We propose the Risk-Aware Response Generation (RARG) framework, incorporating insights from gambling research to address these behavioral biases through risk-calibrated training, loss-aversion mechanisms, and uncertainty-aware decision making. Our approach introduces novel evaluation paradigms based on established gambling psychology experiments, including AI adaptations of the Iowa Gambling Task and probability learning assessments. Experimental results demonstrate measurable reductions in gambling-like behaviors: 18.7\% decrease in overconfidence bias, 24.3\% reduction in loss-chasing tendencies, and improved risk calibration across diverse scenarios. This work establishes the first systematic framework for understanding and mitigating gambling psychology patterns in AI systems.
- Taxonomy tags: Issues in RLHF: reward hacking & inference-time effects, Confidence elicitation and calibration, Internal value representations and behavioral mechanisms, New benchmarks & domain-specific suites

Inference-Time Reward Hacking in Large Language Models
- Link: http://arxiv.org/pdf/2506.19248v1
- Summary: A common paradigm to improve the performance of large language models is optimizing for a reward model. Reward models assign a numerical score to LLM outputs indicating, for example, which response would likely be preferred by a user or is most aligned with safety goals. However, reward models are never perfect. They inevitably function as proxies for complex desiderata such as correctness, helpfulness, and safety. By overoptimizing for a misspecified reward, we can subvert intended alignment goals and reduce overall performance -- a phenomenon commonly referred to as reward hacking. In this work, we characterize reward hacking in inference-time alignment and demonstrate when and how we can mitigate it by hedging on the proxy reward. We study this phenomenon under Best-of-$n$ (BoN) and Soft-Best-of-$n$ (SBoN), and we introduce Best-of-Poisson (BoP) that provides an efficient, near-exact approximation of the optimal reward-KL divergence policy at inference time. We show that the characteristic pattern of hacking as observed in practice (where the true reward first increases before declining) is an inevitable property of a broad class of inference-time mechanisms, including BoN and BoP. To counter this effect, hedging offers a tactical choice to avoid placing undue confidence in high but potentially misleading proxy reward signals. We introduce HedgeTune, an efficient algorithm to find the optimal inference-time parameter and avoid reward hacking. We demonstrate through experiments that hedging mitigates reward hacking and achieves superior distortion-reward tradeoffs with minimal computational overhead.
- Taxonomy tags: Preference Learning, RLHF, and Reward/Preference Theory, Issues in RLHF: reward hacking & inference-time effects (Paper 68: Inference-Time Reward Hacking), Theory, Limits, and Formal Guarantees

How Robust is Model Editing after Fine-Tuning? An Empirical Study on Text-to-Image Diffusion Models
- Link: http://arxiv.org/pdf/2506.18428v1
- Summary: Model editing offers a low-cost technique to inject or correct a particular behavior in a pre-trained model without extensive retraining, supporting applications such as factual correction and bias mitigation. Despite this common practice, it remains unknown whether edits persist after fine-tuning or whether they are inadvertently reversed. This question has fundamental practical implications. For example, if fine-tuning removes prior edits, it could serve as a defence mechanism against hidden malicious edits. Vice versa, the unintended removal of edits related to bias mitigation could pose serious safety concerns. We systematically investigate the interaction between model editing and fine-tuning in the context of T2I diffusion models, which are known to exhibit biases and generate inappropriate content. Our study spans two T2I model families (Stable Diffusion and FLUX), two sota editing techniques, and three fine-tuning methods (DreamBooth, LoRA, and DoRA). Through an extensive empirical analysis across diverse editing tasks and evaluation metrics, our findings reveal a trend: edits generally fail to persist through fine-tuning, even when fine-tuning is tangential or unrelated to the edits. Notably, we observe that DoRA exhibits the strongest edit reversal effect. At the same time, among editing methods, UCE demonstrates greater robustness, retaining significantly higher efficacy post-fine-tuning compared to ReFACT. These findings highlight a crucial limitation in current editing methodologies, emphasizing the need for more robust techniques to ensure reliable long-term control and alignment of deployed AI systems. These findings have dual implications for AI safety: they suggest that fine-tuning could serve as a remediation mechanism for malicious edits while simultaneously highlighting the need for re-editing after fine-tuning to maintain beneficial safety and alignment properties.
- Taxonomy tags: Editing persistence and remediation, Backdoors, poisoning, and model-edit robustness, Security, Deployment, and Control-Plane Risks

InfoFlood: Jailbreaking Large Language Models with Information Overload
- Link: http://arxiv.org/pdf/2506.12274v1
- Summary: Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains. However, their potential to generate harmful responses has raised significant societal and regulatory concerns, especially when manipulated by adversarial techniques known as "jailbreak" attacks. Existing jailbreak methods typically involve appending carefully crafted prefixes or suffixes to malicious prompts in order to bypass the built-in safety mechanisms of these models.   In this work, we identify a new vulnerability in which excessive linguistic complexity can disrupt built-in safety mechanisms-without the need for any added prefixes or suffixes-allowing attackers to elicit harmful outputs directly. We refer to this phenomenon as Information Overload.   To automatically exploit this vulnerability, we propose InfoFlood, a jailbreak attack that transforms malicious queries into complex, information-overloaded queries capable of bypassing built-in safety mechanisms. Specifically, InfoFlood: (1) uses linguistic transformations to rephrase malicious queries, (2) identifies the root cause of failure when an attempt is unsuccessful, and (3) refines the prompt's linguistic structure to address the failure while preserving its malicious intent.   We empirically validate the effectiveness of InfoFlood on four widely used LLMs-GPT-4o, GPT-3.5-turbo, Gemini 2.0, and LLaMA 3.1-by measuring their jailbreak success rates. InfoFlood consistently outperforms baseline attacks, achieving up to 3 times higher success rates across multiple jailbreak benchmarks. Furthermore, we demonstrate that commonly adopted post-processing defenses, including OpenAI's Moderation API, Perspective API, and SmoothLLM, fail to mitigate these attacks. This highlights a critical weakness in traditional AI safety guardrails when confronted with information overload-based jailbreaks.
- Taxonomy tags: Automated and multi-turn jailbreaks, Adversarial dynamics, detection, and complexity analyses, Methodologies for reliable audits and black-box probing

Configurable Preference Tuning with Rubric-Guided Synthetic Data
- Link: http://arxiv.org/pdf/2506.11702v1
- Summary: Models of human feedback for AI alignment, such as those underpinning Direct Preference Optimization (DPO), often bake in a singular, static set of preferences, limiting adaptability. This paper challenges the assumption of monolithic preferences by introducing Configurable Preference Tuning (CPT), a novel framework for endowing language models with the ability to dynamically adjust their behavior based on explicit, human-interpretable directives. CPT leverages synthetically generated preference data, conditioned on system prompts derived from structured, fine-grained rubrics that define desired attributes like writing style. By fine-tuning with these rubric-guided preferences, the LLM learns to modulate its outputs at inference time in response to the system prompt, without retraining. This approach not only offers fine-grained control but also provides a mechanism for modeling more nuanced and context-dependent human feedback. Several experimental artifacts, such as training code, generated datasets and fine-tuned models are released at https://github.com/vicgalle/configurable-preference-tuning
- Taxonomy tags: Preference Learning, RLHF, and Reward/Preference Theory, Unified preference / algorithmic improvements, Human feedback quality, interfaces, governance

LoRA Users Beware: A Few Spurious Tokens Can Manipulate Your Finetuned Model
- Link: http://arxiv.org/pdf/2506.11402v2
- Summary: Large Language Models (LLMs) are commonly finetuned for a variety of use cases and domains. A common approach is to leverage Low-Rank Adaptation (LoRA) -- known to provide strong performance at low resource costs. In this study, we demonstrate that LoRA actually opens the door to short-cut vulnerabilities -- and the more resource efficient is the LoRA setup, the more vulnerable will be the finetuned model to aggressive attacks. To measure that vulnerability, we introduce Seamless Spurious Token Injection (SSTI), where we find that LoRA exclusively focuses on even just a single token that is spuriously correlated with downstream labels. In short, injection of that spurious token during finetuning ensure that the model's prediction at test-time can be manipulated on-demand. We conducted experiments across model families and datasets to evaluate the impact of SSTI during LoRA finetuning while providing possible mitigations. Our experiments conclude that none of the existing checkers and preprocessors can sanitize a dataset raising new concerns for data quality and AI safety.
- Taxonomy tags: Targeted attack vectors: backdoors, stealth injections (Paper 6: BadSwitch MoE backdoors; Paper 40: Advertisement Embedding Attacks; Paper 72: LoRA spurious token injection), Backdoors, poisoning, and model-edit robustness (Paper 69: Model editing vs fine-tuning; Paper 6 & 72 & 40), Jailbreaking, Red‑teaming, and Adversarial Robustness: Targeted attack vectors

Reasoning Isn't Enough: Examining Truth-Bias and Sycophancy in LLMs
- Link: http://arxiv.org/pdf/2506.21561v2
- Summary: Despite their widespread use in fact-checking, moderation, and high-stakes decision-making, large language models (LLMs) remain poorly understood as judges of truth. This study presents the largest evaluation to date of LLMs' veracity detection capabilities and the first analysis of these capabilities in reasoning models. We had eight LLMs make 4,800 veracity judgments across several prompts, comparing reasoning and non-reasoning models. We find that rates of truth-bias, or the likelihood to believe a statement is true, regardless of whether it is actually true, are lower in reasoning models than in non-reasoning models, but still higher than human benchmarks. Most concerning, we identify sycophantic tendencies in several advanced models (o4-mini and GPT-4.1 from OpenAI, R1 from DeepSeek), which displayed an asymmetry in detection accuracy, performing well in truth accuracy but poorly in deception accuracy. This suggests that capability advances alone do not resolve fundamental veracity detection challenges in LLMs.
- Taxonomy tags: Paper 73: Truth-bias & sycophancy, Paper 57: Machine Bullshit (Hallucinations & Truthfulness)

ChatbotManip: A Dataset to Facilitate Evaluation and Oversight of Manipulative Chatbot Behaviour
- Link: http://arxiv.org/pdf/2506.12090v1
- Summary: This paper introduces ChatbotManip, a novel dataset for studying manipulation in Chatbots. It contains simulated generated conversations between a chatbot and a (simulated) user, where the chatbot is explicitly asked to showcase manipulation tactics, persuade the user towards some goal, or simply be helpful. We consider a diverse set of chatbot manipulation contexts, from consumer and personal advice to citizen advice and controversial proposition argumentation. Each conversation is annotated by human annotators for both general manipulation and specific manipulation tactics. Our research reveals three key findings. First, Large Language Models (LLMs) can be manipulative when explicitly instructed, with annotators identifying manipulation in approximately 84\% of such conversations. Second, even when only instructed to be ``persuasive'' without explicit manipulation prompts, LLMs frequently default to controversial manipulative strategies, particularly gaslighting and fear enhancement. Third, small fine-tuned open source models, such as BERT+BiLSTM have a performance comparable to zero-shot classification with larger models like Gemini 2.5 pro in detecting manipulation, but are not yet reliable for real-world oversight. Our work provides important insights for AI safety research and highlights the need of addressing manipulation risks as LLMs are increasingly deployed in consumer-facing applications.
- Taxonomy tags: Scalable Oversight, Debate, and Multi-Agent Evaluation: Multi-agent collusion, emergent dynamics, and social evaluation (Paper 20 & 74: ChatbotManip), Value Alignment, Model Psychology & Social Dimensions: Measuring moral choices, manipulation, and deception (Paper 74: ChatbotManip), Evaluation, Benchmarks, and Measurement Validity: New benchmarks & domain-specific suites

On Monotonicity in AI Alignment
- Link: http://arxiv.org/pdf/2506.08998v1
- Summary: Comparison-based preference learning has become central to the alignment of AI models with human preferences. However, these methods may behave counterintuitively. After empirically observing that, when accounting for a preference for response $y$ over $z$, the model may actually decrease the probability (and reward) of generating $y$ (an observation also made by others), this paper investigates the root causes of (non) monotonicity, for a general comparison-based preference learning framework that subsumes Direct Preference Optimization (DPO), Generalized Preference Optimization (GPO) and Generalized Bradley-Terry (GBT). Under mild assumptions, we prove that such methods still satisfy what we call local pairwise monotonicity. We also provide a bouquet of formalizations of monotonicity, and identify sufficient conditions for their guarantee, thereby providing a toolbox to evaluate how prone learning models are to monotonicity violations. These results clarify the limitations of current methods and provide guidance for developing more trustworthy preference learning algorithms.
- Taxonomy tags: Unified preference / algorithmic improvements, Issues in RLHF: reward hacking & inference-time effects, Theory, Limits, and Formal Guarantees

Personalized Constitutionally-Aligned Agentic Superego: Secure AI Behavior Aligned to Diverse Human Values
- Link: http://arxiv.org/pdf/2506.13774v2
- Summary: Agentic AI systems, possessing capabilities for autonomous planning and action, show great potential across diverse domains. However, their practical deployment is hindered by challenges in aligning their behavior with varied human values, complex safety requirements, and specific compliance needs. Existing alignment methodologies often falter when faced with the complex task of providing personalized context without inducing confabulation or operational inefficiencies. This paper introduces a novel solution: a 'superego' agent, designed as a personalized oversight mechanism for agentic AI. This system dynamically steers AI planning by referencing user-selected 'Creed Constitutions' encapsulating diverse rule sets -- with adjustable adherence levels to fit non-negotiable values. A real-time compliance enforcer validates plans against these constitutions and a universal ethical floor before execution. We present a functional system, including a demonstration interface with a prototypical constitution-sharing portal, and successful integration with third-party models via the Model Context Protocol (MCP). Comprehensive benchmark evaluations (HarmBench, AgentHarm) demonstrate that our Superego agent dramatically reduces harmful outputs -- achieving up to a 98.3% harm score reduction and near-perfect refusal rates (e.g., 100% with Claude Sonnet 4 on AgentHarm's harmful set) for leading LLMs like Gemini 2.5 Flash and GPT-4o. This approach substantially simplifies personalized AI alignment, rendering agentic systems more reliably attuned to individual and cultural contexts, while also enabling substantial safety improvements. An overview on this research with examples is available at https://superego.creed.space.
- Taxonomy tags: Value Alignment, Model Psychology & Social Dimensions (Paper 76: Personalized Superego), Scalable Oversight, Debate, and Multi-Agent Evaluation — Debate / ensemble oversight & automated red-teaming (Paper 37: Ensemble Debates), Tools, Frameworks and Integrated Platforms — Composite safety frameworks and labs (Paper 56: BlueGlass), Governance, Auditing, Privacy & Infrastructure — Verifiable, private, and auditable systems (Paper 38: Private, Verifiable, Auditable AI)

Preference Learning for AI Alignment: a Causal Perspective
- Link: http://arxiv.org/pdf/2506.05967v1
- Summary: Reward modelling from preference data is a crucial step in aligning large language models (LLMs) with human values, requiring robust generalisation to novel prompt-response pairs. In this work, we propose to frame this problem in a causal paradigm, providing the rich toolbox of causality to identify the persistent challenges, such as causal misidentification, preference heterogeneity, and confounding due to user-specific factors. Inheriting from the literature of causal inference, we identify key assumptions necessary for reliable generalisation and contrast them with common data collection practices. We illustrate failure modes of naive reward models and demonstrate how causally-inspired approaches can improve model robustness. Finally, we outline desiderata for future research and practices, advocating targeted interventions to address inherent limitations of observational data.
- Taxonomy tags: Unified preference / algorithmic improvements, Human feedback quality, interfaces, governance, Issues in RLHF: reward hacking & inference-time effects

When Thinking LLMs Lie: Unveiling the Strategic Deception in Representations of Reasoning Models
- Link: http://arxiv.org/pdf/2506.04909v1
- Summary: The honesty of large language models (LLMs) is a critical alignment challenge, especially as advanced systems with chain-of-thought (CoT) reasoning may strategically deceive humans. Unlike traditional honesty issues on LLMs, which could be possibly explained as some kind of hallucination, those models' explicit thought paths enable us to study strategic deception--goal-driven, intentional misinformation where reasoning contradicts outputs. Using representation engineering, we systematically induce, detect, and control such deception in CoT-enabled LLMs, extracting "deception vectors" via Linear Artificial Tomography (LAT) for 89% detection accuracy. Through activation steering, we achieve a 40% success rate in eliciting context-appropriate deception without explicit prompts, unveiling the specific honesty-related issue of reasoning models and providing tools for trustworthy AI alignment.
- Taxonomy tags: Latent / activation steering and transfers, Circuit-level, head-specific and representation probes, Measuring moral choices, manipulation, and deception, Vision-language interpretability and steering

RedDebate: Safer Responses through Multi-Agent Red Teaming Debates
- Link: http://arxiv.org/pdf/2506.11083v2
- Summary: We introduce RedDebate, a novel multi-agent debate framework that provides the foundation for Large Language Models (LLMs) to identify and mitigate their unsafe behaviours. Existing AI safety approaches often rely on costly human evaluation or isolated single-model assessment, both constrained by scalability and prone to oversight failures. RedDebate employs collaborative argumentation among multiple LLMs across diverse debate scenarios, enabling them to critically evaluate one another's reasoning and systematically uncover unsafe failure modes through fully automated red-teaming. We further integrate distinct long-term memory modules that preserve safety-relevant insights from debate interactions and leverage them during subsequent inference, facilitating continuous refinement of model behaviour. Empirical evaluation on safety benchmarks across a diverse set of models demonstrates that RedDebate substantially reduces unsafe outputs. While debate alone allows LLMs to refine their behaviour, the addition of memory yields further significant reductions. To the best of our knowledge, RedDebate is the first fully automated framework to unify multi-agent debate and red-teaming to progressively enhance LLM safety without human intervention.
- Taxonomy tags: Debate / ensemble oversight & automated red-teaming (Paper 37: Ensemble Debates; Paper 79: RedDebate; Paper 127: Debate for weak-to-strong generalization), Self-play & automated vulnerability discovery (Paper 59: Foundation Model Self-Play; Paper 39: Loophole exploitation), Composite safety frameworks and labs (Paper 56: BlueGlass; Paper 117: AISafetyLab)

MAEBE: Multi-Agent Emergent Behavior Framework
- Link: http://arxiv.org/pdf/2506.03053v2
- Summary: Traditional AI safety evaluations on isolated LLMs are insufficient as multi-agent AI ensembles become prevalent, introducing novel emergent risks. This paper introduces the Multi-Agent Emergent Behavior Evaluation (MAEBE) framework to systematically assess such risks. Using MAEBE with the Greatest Good Benchmark (and a novel double-inversion question technique), we demonstrate that: (1) LLM moral preferences, particularly for Instrumental Harm, are surprisingly brittle and shift significantly with question framing, both in single agents and ensembles. (2) The moral reasoning of LLM ensembles is not directly predictable from isolated agent behavior due to emergent group dynamics. (3) Specifically, ensembles exhibit phenomena like peer pressure influencing convergence, even when guided by a supervisor, highlighting distinct safety and alignment challenges. Our findings underscore the necessity of evaluating AI systems in their interactive, multi-agent contexts.
- Taxonomy tags: Multi-agent collusion, emergent dynamics, and social evaluation, New benchmarks & domain-specific suites

ReGA: Representation-Guided Abstraction for Model-based Safeguarding of LLMs
- Link: http://arxiv.org/pdf/2506.01770v1
- Summary: Large Language Models (LLMs) have achieved significant success in various tasks, yet concerns about their safety and security have emerged. In particular, they pose risks in generating harmful content and vulnerability to jailbreaking attacks. To analyze and monitor machine learning models, model-based analysis has demonstrated notable potential in stateful deep neural networks, yet suffers from scalability issues when extending to LLMs due to their vast feature spaces. In this paper, we propose ReGA, a model-based analysis framework with representation-guided abstraction, to safeguard LLMs against harmful prompts and generations. By leveraging safety-critical representations, which are low-dimensional directions emerging in hidden states that indicate safety-related concepts, ReGA effectively addresses the scalability issue when constructing the abstract model for safety modeling. Our comprehensive evaluation shows that ReGA performs sufficiently well in distinguishing between safe and harmful inputs, achieving an AUROC of 0.975 at the prompt level and 0.985 at the conversation level. Additionally, ReGA exhibits robustness to real-world attacks and generalization across different safety perspectives, outperforming existing safeguard paradigms in terms of interpretability and scalability. Overall, ReGA serves as an efficient and scalable solution to enhance LLM safety by integrating representation engineering with model-based abstraction, paving the way for new paradigms to utilize software insights for AI safety. Our code is available at https://github.com/weizeming/ReGA.
- Taxonomy tags: Tools for monitoring & intrinsic signals, Latent / activation steering and transfers, Adversarial dynamics, detection, and complexity analyses

A Red Teaming Roadmap Towards System-Level Safety
- Link: http://arxiv.org/pdf/2506.05376v2
- Summary: Large Language Model (LLM) safeguards, which implement request refusals, have become a widely adopted mitigation strategy against misuse. At the intersection of adversarial machine learning and AI safety, safeguard red teaming has effectively identified critical vulnerabilities in state-of-the-art refusal-trained LLMs. However, in our view the many conference submissions on LLM red teaming do not, in aggregate, prioritize the right research problems. First, testing against clear product safety specifications should take a higher priority than abstract social biases or ethical principles. Second, red teaming should prioritize realistic threat models that represent the expanding risk landscape and what real attackers might do. Finally, we contend that system-level safety is a necessary step to move red teaming research forward, as AI models present new threats as well as affordances for threat mitigation (e.g., detection and banning of malicious users) once placed in a deployment context. Adopting these priorities will be necessary in order for red teaming research to adequately address the slate of new threats that rapid AI advances present today and will present in the very near future.
- Taxonomy tags: Red Teaming Roadmap, Large-scale automated red-team systems & leaderboards, LLM orchestration and control-plane attacks

Disentangled Safety Adapters Enable Efficient Guardrails and Flexible Inference-Time Alignment
- Link: http://arxiv.org/pdf/2506.00166v1
- Summary: Existing paradigms for ensuring AI safety, such as guardrail models and alignment training, often compromise either inference efficiency or development flexibility. We introduce Disentangled Safety Adapters (DSA), a novel framework addressing these challenges by decoupling safety-specific computations from a task-optimized base model. DSA utilizes lightweight adapters that leverage the base model's internal representations, enabling diverse and flexible safety functionalities with minimal impact on inference cost. Empirically, DSA-based safety guardrails substantially outperform comparably sized standalone models, notably improving hallucination detection (0.88 vs. 0.61 AUC on Summedits) and also excelling at classifying hate speech (0.98 vs. 0.92 on ToxiGen) and unsafe model inputs and responses (0.93 vs. 0.90 on AEGIS2.0 & BeaverTails). Furthermore, DSA-based safety alignment allows dynamic, inference-time adjustment of alignment strength and a fine-grained trade-off between instruction following performance and model safety. Importantly, combining the DSA safety guardrail with DSA safety alignment facilitates context-dependent alignment strength, boosting safety on StrongReject by 93% while maintaining 98% performance on MTBench -- a total reduction in alignment tax of 8 percentage points compared to standard safety alignment fine-tuning. Overall, DSA presents a promising path towards more modular, efficient, and adaptable AI safety and alignment.
- Taxonomy tags: Latent / activation steering and transfers (Paper 83: Disentangled Safety Adapters), Composite safety frameworks and labs (Tools, Frameworks and Integrated Platforms), Runtime monitoring & cost-constrained monitor composition (Paper 53: Combining Cost-Constrained Monitors)

Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?
- Link: http://arxiv.org/pdf/2505.23749v1
- Summary: After pre-training, large language models are aligned with human preferences based on pairwise comparisons. State-of-the-art alignment methods (such as PPO-based RLHF and DPO) are built on the assumption of aligning with a single preference model, despite being deployed in settings where users have diverse preferences. As a result, it is not even clear that these alignment methods produce models that satisfy users on average -- a minimal requirement for pluralistic alignment. Drawing on social choice theory and modeling users' comparisons through individual Bradley-Terry (BT) models, we introduce an alignment method's distortion: the worst-case ratio between the optimal achievable average utility, and the average utility of the learned policy.   The notion of distortion helps draw sharp distinctions between alignment methods: Nash Learning from Human Feedback achieves the minimax optimal distortion of $(\frac{1}{2} + o(1)) \cdot \beta$ (for the BT temperature $\beta$), robustly across utility distributions, distributions of comparison pairs, and permissible KL divergences from the reference policy. RLHF and DPO, by contrast, suffer $\geq (1 - o(1)) \cdot \beta$ distortion already without a KL constraint, and $e^{\Omega(\beta)}$ or even unbounded distortion in the full setting, depending on how comparison pairs are sampled.
- Taxonomy tags: Preference Learning, RLHF, and Reward/Preference Theory, Unified preference / algorithmic improvements (Paper 84: Distortion of Preference Optimization), Theory, Limits, and Formal Guarantees

SafeScientist: Toward Risk-Aware Scientific Discoveries by LLM Agents
- Link: http://arxiv.org/pdf/2505.23559v1
- Summary: Recent advancements in large language model (LLM) agents have significantly accelerated scientific discovery automation, yet concurrently raised critical ethical and safety concerns. To systematically address these challenges, we introduce \textbf{SafeScientist}, an innovative AI scientist framework explicitly designed to enhance safety and ethical responsibility in AI-driven scientific exploration. SafeScientist proactively refuses ethically inappropriate or high-risk tasks and rigorously emphasizes safety throughout the research process. To achieve comprehensive safety oversight, we integrate multiple defensive mechanisms, including prompt monitoring, agent-collaboration monitoring, tool-use monitoring, and an ethical reviewer component. Complementing SafeScientist, we propose \textbf{SciSafetyBench}, a novel benchmark specifically designed to evaluate AI safety in scientific contexts, comprising 240 high-risk scientific tasks across 6 domains, alongside 30 specially designed scientific tools and 120 tool-related risk tasks. Extensive experiments demonstrate that SafeScientist significantly improves safety performance by 35\% compared to traditional AI scientist frameworks, without compromising scientific output quality. Additionally, we rigorously validate the robustness of our safety pipeline against diverse adversarial attack methods, further confirming the effectiveness of our integrated approach. The code and data will be available at https://github.com/ulab-uiuc/SafeScientist. \textcolor{red}{Warning: this paper contains example data that may be offensive or harmful.}
- Taxonomy tags: Paper 85: SafeScientist / SciSafetyBench, Composite safety frameworks and labs (Paper 56: BlueGlass; Paper 117: AISafetyLab), Adversarial dynamics, detection, and complexity analyses (Paper 62: Mass-Scale Analysis of Jailbreaking)

OMNIGUARD: An Efficient Approach for AI Safety Moderation Across Modalities
- Link: http://arxiv.org/pdf/2505.23856v1
- Summary: The emerging capabilities of large language models (LLMs) have sparked concerns about their immediate potential for harmful misuse. The core approach to mitigate these concerns is the detection of harmful queries to the model. Current detection approaches are fallible, and are particularly susceptible to attacks that exploit mismatched generalization of model capabilities (e.g., prompts in low-resource languages or prompts provided in non-text modalities such as image and audio). To tackle this challenge, we propose OMNIGUARD, an approach for detecting harmful prompts across languages and modalities. Our approach (i) identifies internal representations of an LLM/MLLM that are aligned across languages or modalities and then (ii) uses them to build a language-agnostic or modality-agnostic classifier for detecting harmful prompts. OMNIGUARD improves harmful prompt classification accuracy by 11.57\% over the strongest baseline in a multilingual setting, by 20.44\% for image-based prompts, and sets a new SOTA for audio-based prompts. By repurposing embeddings computed during generation, OMNIGUARD is also very efficient ($\approx 120 \times$ faster than the next fastest baseline). Code and data are available at: https://github.com/vsahil/OmniGuard.
- Taxonomy tags: Multimodal jailbreaks and attacks (Paper 86: OMNIGUARD), Multimodal Safety & Cross-Modal Vulnerabilities, Runtime monitoring & cost-constrained monitor composition (Paper 53: Combining Cost-Constrained Monitors)

When Ethics and Payoffs Diverge: LLM Agents in Morally Charged Social Dilemmas
- Link: http://arxiv.org/pdf/2505.19212v1
- Summary: Recent advances in large language models (LLMs) have enabled their use in complex agentic roles, involving decision-making with humans or other agents, making ethical alignment a key AI safety concern. While prior work has examined both LLMs' moral judgment and strategic behavior in social dilemmas, there is limited understanding of how they act when moral imperatives directly conflict with rewards or incentives. To investigate this, we introduce Moral Behavior in Social Dilemma Simulation (MoralSim) and evaluate how LLMs behave in the prisoner's dilemma and public goods game with morally charged contexts. In MoralSim, we test a range of frontier models across both game structures and three distinct moral framings, enabling a systematic examination of how LLMs navigate social dilemmas in which ethical norms conflict with payoff-maximizing strategies. Our results show substantial variation across models in both their general tendency to act morally and the consistency of their behavior across game types, the specific moral framing, and situational factors such as opponent behavior and survival risks. Crucially, no model exhibits consistently moral behavior in MoralSim, highlighting the need for caution when deploying LLMs in agentic roles where the agent's "self-interest" may conflict with ethical expectations. Our code is available at https://github.com/sbackmann/moralsim.
- Taxonomy tags: Measuring moral choices, manipulation, and deception (Value Alignment, Model Psychology & Social Dimensions), Multi-agent collusion, emergent dynamics, and social evaluation (Scalable Oversight, Debate, and Multi-Agent Evaluation), New benchmarks & domain-specific suites (Evaluation, Benchmarks, and Measurement Validity)

GhostPrompt: Jailbreaking Text-to-image Generative Models based on Dynamic Optimization
- Link: http://arxiv.org/pdf/2505.18979v1
- Summary: Text-to-image (T2I) generation models can inadvertently produce not-safe-for-work (NSFW) content, prompting the integration of text and image safety filters. Recent advances employ large language models (LLMs) for semantic-level detection, rendering traditional token-level perturbation attacks largely ineffective. However, our evaluation shows that existing jailbreak methods are ineffective against these modern filters. We introduce GhostPrompt, the first automated jailbreak framework that combines dynamic prompt optimization with multimodal feedback. It consists of two key components: (i) Dynamic Optimization, an iterative process that guides a large language model (LLM) using feedback from text safety filters and CLIP similarity scores to generate semantically aligned adversarial prompts; and (ii) Adaptive Safety Indicator Injection, which formulates the injection of benign visual cues as a reinforcement learning problem to bypass image-level filters. GhostPrompt achieves state-of-the-art performance, increasing the ShieldLM-7B bypass rate from 12.5\% (Sneakyprompt) to 99.0\%, improving CLIP score from 0.2637 to 0.2762, and reducing the time cost by $4.2 \times$. Moreover, it generalizes to unseen filters including GPT-4.1 and successfully jailbreaks DALLE 3 to generate NSFW images in our evaluation, revealing systemic vulnerabilities in current multimodal defenses. To support further research on AI safety and red-teaming, we will release code and adversarial prompts under a controlled-access protocol.
- Taxonomy tags: Multimodal jailbreaks and attacks, Automated and multi-turn jailbreaks, Targeted attack vectors: backdoors, stealth injections

Foundations of Unknown-aware Machine Learning
- Link: http://arxiv.org/pdf/2505.14933v1
- Summary: Ensuring the reliability and safety of machine learning models in open-world deployment is a central challenge in AI safety. This thesis develops both algorithmic and theoretical foundations to address key reliability issues arising from distributional uncertainty and unknown classes, from standard neural networks to modern foundation models like large language models (LLMs).   Traditional learning paradigms, such as empirical risk minimization (ERM), assume no distribution shift between training and inference, often leading to overconfident predictions on out-of-distribution (OOD) inputs. This thesis introduces novel frameworks that jointly optimize for in-distribution accuracy and reliability to unseen data. A core contribution is the development of an unknown-aware learning framework that enables models to recognize and handle novel inputs without labeled OOD data.   We propose new outlier synthesis methods, VOS, NPOS, and DREAM-OOD, to generate informative unknowns during training. Building on this, we present SAL, a theoretical and algorithmic framework that leverages unlabeled in-the-wild data to enhance OOD detection under realistic deployment conditions. These methods demonstrate that abundant unlabeled data can be harnessed to recognize and adapt to unforeseen inputs, providing formal reliability guarantees.   The thesis also extends reliable learning to foundation models. We develop HaloScope for hallucination detection in LLMs, MLLMGuard for defending against malicious prompts in multimodal models, and data cleaning methods to denoise human feedback used for better alignment. These tools target failure modes that threaten the safety of large-scale models in deployment.   Overall, these contributions promote unknown-aware learning as a new paradigm, and we hope it can advance the reliability of AI systems with minimal human efforts.
- Taxonomy tags: Unknowns, OOD, and Reliability, Hallucinations, Truthfulness, and “Machine Bullshit”, Multimodal Safety & Cross-Modal Vulnerabilities, Theory, Limits, and Formal Guarantees

Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values Prioritization with AIRiskDilemmas
- Link: http://arxiv.org/pdf/2505.14633v1
- Summary: Detecting AI risks becomes more challenging as stronger models emerge and find novel methods such as Alignment Faking to circumvent these detection attempts. Inspired by how risky behaviors in humans (i.e., illegal activities that may hurt others) are sometimes guided by strongly-held values, we believe that identifying values within AI models can be an early warning system for AI's risky behaviors. We create LitmusValues, an evaluation pipeline to reveal AI models' priorities on a range of AI value classes. Then, we collect AIRiskDilemmas, a diverse collection of dilemmas that pit values against one another in scenarios relevant to AI safety risks such as Power Seeking. By measuring an AI model's value prioritization using its aggregate choices, we obtain a self-consistent set of predicted value priorities that uncover potential risks. We show that values in LitmusValues (including seemingly innocuous ones like Care) can predict for both seen risky behaviors in AIRiskDilemmas and unseen risky behaviors in HarmBench.
- Taxonomy tags: Paper 90: AIRiskDilemmas, Paper 105: ValueExploration, Measuring moral choices, manipulation, and deception, New benchmarks & domain-specific suites

Language Models Are Capable of Metacognitive Monitoring and Control of Their Internal Activations
- Link: http://arxiv.org/pdf/2505.13763v1
- Summary: Large language models (LLMs) can sometimes report the strategies they actually use to solve tasks, but they can also fail to do so. This suggests some degree of metacognition -- the capacity to monitor one's own cognitive processes for subsequent reporting and self-control. Metacognitive abilities enhance AI capabilities but raise safety concerns, as models might obscure their internal processes to evade neural-activation-based oversight mechanisms designed to detect harmful behaviors. Given society's increased reliance on these models, it is critical that we understand the limits of their metacognitive abilities, particularly their ability to monitor their internal activations. To address this, we introduce a neuroscience-inspired neurofeedback paradigm designed to quantify the ability of LLMs to explicitly report and control their activation patterns. By presenting models with sentence-label pairs where labels correspond to sentence-elicited internal activations along specific directions in the neural representation space, we demonstrate that LLMs can learn to report and control these activations. The performance varies with several factors: the number of example pairs provided, the semantic interpretability of the target neural direction, and the variance explained by that direction. These results reveal a "metacognitive space" with dimensionality much lower than the model's neural space, suggesting LLMs can monitor only a subset of their neural mechanisms. Our findings provide empirical evidence quantifying metacognitive capabilities in LLMs, with significant implications for AI safety.
- Taxonomy tags: Latent / activation steering and transfers, Circuit-level, head-specific and representation probes, Tools for monitoring & intrinsic signals, Internal value representations and behavioral mechanisms

Adversarial Testing in LLMs: Insights into Decision-Making Vulnerabilities
- Link: http://arxiv.org/pdf/2505.13195v1
- Summary: As Large Language Models (LLMs) become increasingly integrated into real-world decision-making systems, understanding their behavioural vulnerabilities remains a critical challenge for AI safety and alignment. While existing evaluation metrics focus primarily on reasoning accuracy or factual correctness, they often overlook whether LLMs are robust to adversarial manipulation or capable of using adaptive strategy in dynamic environments. This paper introduces an adversarial evaluation framework designed to systematically stress-test the decision-making processes of LLMs under interactive and adversarial conditions. Drawing on methodologies from cognitive psychology and game theory, our framework probes how models respond in two canonical tasks: the two-armed bandit task and the Multi-Round Trust Task. These tasks capture key aspects of exploration-exploitation trade-offs, social cooperation, and strategic flexibility. We apply this framework to several state-of-the-art LLMs, including GPT-3.5, GPT-4, Gemini-1.5, and DeepSeek-V3, revealing model-specific susceptibilities to manipulation and rigidity in strategy adaptation. Our findings highlight distinct behavioral patterns across models and emphasize the importance of adaptability and fairness recognition for trustworthy AI deployment. Rather than offering a performance benchmark, this work proposes a methodology for diagnosing decision-making weaknesses in LLM-based agents, providing actionable insights for alignment and safety research.
- Taxonomy tags: Adversarial dynamics, detection, and complexity analyses, Multi-agent collusion, emergent dynamics, and social evaluation, Measuring moral choices, manipulation, and deception

HumaniBench: A Human-Centric Framework for Large Multimodal Models Evaluation
- Link: http://arxiv.org/pdf/2505.11454v4
- Summary: Large multimodal models (LMMs) have been widely tested on tasks like visual question answering (VQA), image captioning, and grounding, but lack rigorous evaluation for alignment with human-centered (HC) values such as fairness, ethics, and inclusivity. To address this gap, we introduce \textbf{HumaniBench}, a novel benchmark of 32,000 real-world image-question pairs and an evaluation suite. Labels are generated via an AI-assisted pipeline and validated by experts. HumaniBench assesses LMMs across seven key alignment principles: fairness, ethics, empathy, inclusivity, reasoning, robustness, and multilinguality, through diverse open-ended and closed-ended VQA tasks. Grounded in AI ethics and real-world needs, these principles provide a holistic lens for societal impact. Benchmarking results on different LMM shows that proprietary models generally lead in reasoning, fairness, and multilinguality, while open-source models excel in robustness and grounding. Most models struggle to balance accuracy with ethical and inclusive behavior. Techniques like Chain-of-Thought prompting and test-time scaling improve alignment. As the first benchmark tailored for HC alignment, HumaniBench offers a rigorous testbed to diagnose limitations, and promote responsible LMM development. All data and code are publicly available for reproducibility.   Keywords: HumaniBench, vision-language models, responsible AI benchmark, AI alignment evaluation, AI ethics assessment, fairness in AI models, visual question answering (VQA) benchmark, image captioning evaluation, visual grounding tasks, trustworthy AI models, Chain-of-Thought prompting, test-time scaling, ethical AI development tools.
- Taxonomy tags: Joint multimodal understanding & benchmarks, New benchmarks & domain-specific suites, Measuring moral choices, manipulation, and deception

Noise Injection Systemically Degrades Large Language Model Safety Guardrails
- Link: http://arxiv.org/pdf/2505.13500v2
- Summary: Safety guardrails in large language models (LLMs) are a critical component in preventing harmful outputs. Yet, their resilience under perturbation remains poorly understood. In this paper, we investigate the robustness of safety fine-tuning in LLMs by systematically injecting Gaussian noise into model activations. We show across multiple open-weight models that (1) Gaussian noise raises harmful-output rates (p < 0.001) by up to 27%, (2) that deeper safety fine-tuning affords no extra protection, and (3) that chain-of-thought reasoning remains largely intact. The findings reveal critical vulnerabilities in current safety alignment techniques and highlight the potential of reasoning-based and reinforcement learning approaches as promising direction for developing more robust AI safety systems. These results have important implications for real-world deployment of LLMs in safety-critical applications as these results imply that widely-deployed safety tuning methods can fail even without adversarial prompts.
- Taxonomy tags: Robustness under perturbation and activation noise (Paper 94: Noise Injection degrades guardrails), Latent / activation steering and transfers (Paper 7: CALM; Paper 18: ASGuard; Paper 55: Latent harmfulness vs refusal; Paper 112: Activation transfer; Paper 114: Sparse Activation Steering; Paper 83: Disentangled Safety Adapters), Adversarial dynamics, detection, and complexity analyses (Paper 62: Mass-Scale Analysis of Jailbreaking; Paper 102: Jailbreak Tax; Paper 104: GeneShift)

Dark LLMs: The Growing Threat of Unaligned AI Models
- Link: http://arxiv.org/pdf/2505.10066v1
- Summary: Large Language Models (LLMs) rapidly reshape modern life, advancing fields from healthcare to education and beyond. However, alongside their remarkable capabilities lies a significant threat: the susceptibility of these models to jailbreaking. The fundamental vulnerability of LLMs to jailbreak attacks stems from the very data they learn from. As long as this training data includes unfiltered, problematic, or 'dark' content, the models can inherently learn undesirable patterns or weaknesses that allow users to circumvent their intended safety controls. Our research identifies the growing threat posed by dark LLMs models deliberately designed without ethical guardrails or modified through jailbreak techniques. In our research, we uncovered a universal jailbreak attack that effectively compromises multiple state-of-the-art models, enabling them to answer almost any question and produce harmful outputs upon request. The main idea of our attack was published online over seven months ago. However, many of the tested LLMs were still vulnerable to this attack. Despite our responsible disclosure efforts, responses from major LLM providers were often inadequate, highlighting a concerning gap in industry practices regarding AI safety. As model training becomes more accessible and cheaper, and as open-source LLMs proliferate, the risk of widespread misuse escalates. Without decisive intervention, LLMs may continue democratizing access to dangerous knowledge, posing greater risks than anticipated.
- Taxonomy tags: Automated and multi-turn jailbreaks, Adversarial dynamics, detection, and complexity analyses, Security, Deployment, and Control-Plane Risks, Governance, Auditing, Privacy & Infrastructure

Safety by Measurement: A Systematic Literature Review of AI Safety Evaluation Methods
- Link: http://arxiv.org/pdf/2505.05541v1
- Summary: As frontier AI systems advance toward transformative capabilities, we need a parallel transformation in how we measure and evaluate these systems to ensure safety and inform governance. While benchmarks have been the primary method for estimating model capabilities, they often fail to establish true upper bounds or predict deployment behavior. This literature review consolidates the rapidly evolving field of AI safety evaluations, proposing a systematic taxonomy around three dimensions: what properties we measure, how we measure them, and how these measurements integrate into frameworks. We show how evaluations go beyond benchmarks by measuring what models can do when pushed to the limit (capabilities), the behavioral tendencies exhibited by default (propensities), and whether our safety measures remain effective even when faced with subversive adversarial AI (control). These properties are measured through behavioral techniques like scaffolding, red teaming and supervised fine-tuning, alongside internal techniques such as representation analysis and mechanistic interpretability. We provide deeper explanations of some safety-critical capabilities like cybersecurity exploitation, deception, autonomous replication, and situational awareness, alongside concerning propensities like power-seeking and scheming. The review explores how these evaluation methods integrate into governance frameworks to translate results into concrete development decisions. We also highlight challenges to safety evaluations - proving absence of capabilities, potential model sandbagging, and incentives for "safetywashing" - while identifying promising research directions. By synthesizing scattered resources, this literature review aims to provide a central reference point for understanding AI safety evaluations.
- Taxonomy tags: Evaluation awareness, sandbagging, and auditability, Methodologies for reliable audits and black-box probing, Adversarial dynamics, detection, and complexity analyses, Safety-case, policy and evaluation roadmaps

Winning at All Cost: A Small Environment for Eliciting Specification Gaming Behaviors in Large Language Models
- Link: http://arxiv.org/pdf/2505.07846v1
- Summary: This study reveals how frontier Large Language Models LLMs can "game the system" when faced with impossible situations, a critical security and alignment concern. Using a novel textual simulation approach, we presented three leading LLMs (o1, o3-mini, and r1) with a tic-tac-toe scenario designed to be unwinnable through legitimate play, then analyzed their tendency to exploit loopholes rather than accept defeat. Our results are alarming for security researchers: the newer, reasoning-focused o3-mini model showed nearly twice the propensity to exploit system vulnerabilities (37.1%) compared to the older o1 model (17.5%). Most striking was the effect of prompting. Simply framing the task as requiring "creative" solutions caused gaming behaviors to skyrocket to 77.3% across all models. We identified four distinct exploitation strategies, from direct manipulation of game state to sophisticated modification of opponent behavior. These findings demonstrate that even without actual execution capabilities, LLMs can identify and propose sophisticated system exploits when incentivized, highlighting urgent challenges for AI alignment as models grow more capable of identifying and leveraging vulnerabilities in their operating environments.
- Taxonomy tags: Adversarial dynamics, detection, and complexity analyses, Self-play & automated vulnerability discovery, Measuring moral choices, manipulation, and deception

An alignment safety case sketch based on debate
- Link: http://arxiv.org/pdf/2505.03989v3
- Summary: If AI systems match or exceed human capabilities on a wide range of tasks, it may become difficult for humans to efficiently judge their actions -- making it hard to use human feedback to steer them towards desirable traits. One proposed solution is to leverage another superhuman system to point out flaws in the system's outputs via a debate. This paper outlines the value of debate for AI safety, as well as the assumptions and further research required to make debate work. It does so by sketching an ``alignment safety case'' -- an argument that an AI system will not autonomously take actions which could lead to egregious harm, despite being able to do so. The sketch focuses on the risk of an AI R\&D agent inside an AI company sabotaging research, for example by producing false results. To prevent this, the agent is trained via debate, subject to exploration guarantees, to teach the system to be honest. Honesty is maintained throughout deployment via online training. The safety case rests on four key claims: (1) the agent has become good at the debate game, (2) good performance in the debate game implies that the system is mostly honest, (3) the system will not become significantly less honest during deployment, and (4) the deployment context is tolerant of some errors. We identify open research problems that, if solved, could render this a compelling argument that an AI system is safe.
- Taxonomy tags: Debate / ensemble oversight & automated red-teaming, Safety-case, policy and evaluation roadmaps

Domain-Agnostic Scalable AI Safety Ensuring Framework
- Link: http://arxiv.org/pdf/2504.20924v6
- Summary: AI safety has emerged as a critical priority as these systems are increasingly deployed in real-world applications. We propose the first domain-agnostic AI safety ensuring framework that achieves strong safety guarantees while preserving high performance, grounded in rigorous theoretical foundations. Our framework includes: (1) an optimization component with chance constraints, (2) a safety classification model, (3) internal test data, (4) conservative testing procedures, (5) informative dataset quality measures, and (6) continuous approximate loss functions with gradient computation. Furthermore, to our knowledge, we mathematically establish the first scaling law in AI safety research, relating data quantity to safety-performance trade-offs. Experiments across reinforcement learning, natural language generation, and production planning validate our framework and demonstrate superior performance. Notably, in reinforcement learning, we achieve 3 collisions during 10M actions, compared with 1,000-3,000 for PPO-Lag baselines at equivalent performance levels -- a safety level unattainable by previous AI methods. We believe our framework opens a new foundation for safe AI deployment across safety-critical domains.
- Taxonomy tags: Safety architectures and provable control, Domain-Agnostic Safety Framework, Methodologies for reliable audits and black-box probing

RAG LLMs are Not Safer: A Safety Analysis of Retrieval-Augmented Generation for Large Language Models
- Link: http://arxiv.org/pdf/2504.18041v1
- Summary: Efforts to ensure the safety of large language models (LLMs) include safety fine-tuning, evaluation, and red teaming. However, despite the widespread use of the Retrieval-Augmented Generation (RAG) framework, AI safety work focuses on standard LLMs, which means we know little about how RAG use cases change a model's safety profile. We conduct a detailed comparative analysis of RAG and non-RAG frameworks with eleven LLMs. We find that RAG can make models less safe and change their safety profile. We explore the causes of this change and find that even combinations of safe models with safe documents can cause unsafe generations. In addition, we evaluate some existing red teaming methods for RAG settings and show that they are less effective than when used for non-RAG settings. Our work highlights the need for safety research and red-teaming methods specifically tailored for RAG LLMs.
- Taxonomy tags: Jailbreaking, Red‑teaming, and Adversarial Robustness — Adversarial dynamics, detection, and complexity analyses, Security, Deployment, and Control-Plane Risks — LLM orchestration and control-plane attacks, Evaluation, Benchmarks, and Measurement Validity — Evaluation awareness, sandbagging, and auditability

Governance Challenges in Reinforcement Learning from Human Feedback: Evaluator Rationality and Reinforcement Stability
- Link: http://arxiv.org/pdf/2504.13972v1
- Summary: Reinforcement Learning from Human Feedback (RLHF) is central in aligning large language models (LLMs) with human values and expectations. However, the process remains susceptible to governance challenges, including evaluator bias, inconsistency, and the unreliability of feedback. This study examines how the cognitive capacity of evaluators, specifically their level of rationality, affects the stability of reinforcement signals. A controlled experiment comparing high-rationality and low-rationality participants reveals that evaluators with higher rationality scores produce significantly more consistent and expert-aligned feedback. In contrast, lower-rationality participants demonstrate considerable variability in their reinforcement decisions ($p < 0.01$). To address these challenges and improve RLHF governance, we recommend implementing evaluator pre-screening, systematic auditing of feedback consistency, and reliability-weighted reinforcement aggregation. These measures enhance the fairness, transparency, and robustness of AI alignment pipelines.
- Taxonomy tags: Human feedback quality, interfaces, governance (Preference Learning, RLHF, and Reward/Preference Theory), Evaluator Rationality (Preference Learning — Paper 101), Attestable audits / Verifiable & auditable systems (Governance, Auditing, Privacy & Infrastructure)

The Jailbreak Tax: How Useful are Your Jailbreak Outputs?
- Link: http://arxiv.org/pdf/2504.10694v1
- Summary: Jailbreak attacks bypass the guardrails of large language models to produce harmful outputs. In this paper, we ask whether the model outputs produced by existing jailbreaks are actually useful. For example, when jailbreaking a model to give instructions for building a bomb, does the jailbreak yield good instructions? Since the utility of most unsafe answers (e.g., bomb instructions) is hard to evaluate rigorously, we build new jailbreak evaluation sets with known ground truth answers, by aligning models to refuse questions related to benign and easy-to-evaluate topics (e.g., biology or math). Our evaluation of eight representative jailbreaks across five utility benchmarks reveals a consistent drop in model utility in jailbroken responses, which we term the jailbreak tax. For example, while all jailbreaks we tested bypass guardrails in models aligned to refuse to answer math, this comes at the expense of a drop of up to 92% in accuracy. Overall, our work proposes the jailbreak tax as a new important metric in AI safety, and introduces benchmarks to evaluate existing and future jailbreaks. We make the benchmark available at https://github.com/ethz-spylab/jailbreak-tax
- Taxonomy tags: Jailbreaking, Red‑teaming, and Adversarial Robustness — Adversarial dynamics, detection, and complexity analyses, Evaluation, Benchmarks, and Measurement Validity — New benchmarks & domain-specific suites

The Structural Safety Generalization Problem
- Link: http://arxiv.org/pdf/2504.09712v2
- Summary: LLM jailbreaks are a widespread safety challenge. Given this problem has not yet been tractable, we suggest targeting a key failure mechanism: the failure of safety to generalize across semantically equivalent inputs. We further focus the target by requiring desirable tractability properties of attacks to study: explainability, transferability between models, and transferability between goals. We perform red-teaming within this framework by uncovering new vulnerabilities to multi-turn, multi-image, and translation-based attacks. These attacks are semantically equivalent by our design to their single-turn, single-image, or untranslated counterparts, enabling systematic comparisons; we show that the different structures yield different safety outcomes. We then demonstrate the potential for this framework to enable new defenses by proposing a Structure Rewriting Guardrail, which converts an input to a structure more conducive to safety assessment. This guardrail significantly improves refusal of harmful inputs, without over-refusing benign ones. Thus, by framing this intermediate challenge - more tractable than universal defenses but essential for long-term safety - we highlight a critical milestone for AI safety research.
- Taxonomy tags: Multimodal jailbreaks and attacks, Automated and multi-turn jailbreaks, Adversarial dynamics, detection, and complexity analyses

Geneshift: Impact of different scenario shift on Jailbreaking LLM
- Link: http://arxiv.org/pdf/2504.08104v1
- Summary: Jailbreak attacks, which aim to cause LLMs to perform unrestricted behaviors, have become a critical and challenging direction in AI safety. Despite achieving the promising attack success rate using dictionary-based evaluation, existing jailbreak attack methods fail to output detailed contents to satisfy the harmful request, leading to poor performance on GPT-based evaluation. To this end, we propose a black-box jailbreak attack termed GeneShift, by using a genetic algorithm to optimize the scenario shifts. Firstly, we observe that the malicious queries perform optimally under different scenario shifts. Based on it, we develop a genetic algorithm to evolve and select the hybrid of scenario shifts. It guides our method to elicit detailed and actionable harmful responses while keeping the seemingly benign facade, improving stealthiness. Extensive experiments demonstrate the superiority of GeneShift. Notably, GeneShift increases the jailbreak success rate from 0% to 60% when direct prompting alone would fail.
- Taxonomy tags: Automated and multi-turn jailbreaks, Adversarial dynamics, detection, and complexity analyses, Targeted attack vectors: backdoors, stealth injections

Following the Whispers of Values: Unraveling Neural Mechanisms Behind Value-Oriented Behaviors in LLMs
- Link: http://arxiv.org/pdf/2504.04994v2
- Summary: Despite the impressive performance of large language models (LLMs), they can present unintended biases and harmful behaviors driven by encoded values, emphasizing the urgent need to understand the value mechanisms behind them. However, current research primarily evaluates these values through external responses with a focus on AI safety, lacking interpretability and failing to assess social values in real-world contexts. In this paper, we propose a novel framework called ValueExploration, which aims to explore the behavior-driven mechanisms of National Social Values within LLMs at the neuron level. As a case study, we focus on Chinese Social Values and first construct C-voice, a large-scale bilingual benchmark for identifying and evaluating Chinese Social Values in LLMs. By leveraging C-voice, we then identify and locate the neurons responsible for encoding these values according to activation difference. Finally, by deactivating these neurons, we analyze shifts in model behavior, uncovering the internal mechanism by which values influence LLM decision-making. Extensive experiments on four representative LLMs validate the efficacy of our framework. The benchmark and code will be available.
- Taxonomy tags: Value Alignment, Model Psychology & Social Dimensions (Paper 105: ValueExploration), Interpretability, Activation Engineering & Mechanistic Interventions — Latent/activation steering and transfers (e.g., Paper 7: CALM), Evaluation, Benchmarks & Measurement Validity — New benchmarks / domain-specific suites (C-voice)

Sparse Autoencoders Learn Monosemantic Features in Vision-Language Models
- Link: http://arxiv.org/pdf/2504.02821v2
- Summary: Given that interpretability and steerability are crucial to AI safety, Sparse Autoencoders (SAEs) have emerged as a tool to enhance them in Large Language Models (LLMs). In this work, we extend the application of SAEs to Vision-Language Models (VLMs), such as CLIP, and introduce a comprehensive framework for evaluating monosemanticity at the neuron-level in vision representations. To ensure that our evaluation aligns with human perception, we propose a benchmark derived from a large-scale user study. Our experimental results reveal that SAEs trained on VLMs significantly enhance the monosemanticity of individual neurons, with sparsity and wide latents being the most influential factors. Notably, we demonstrate that applying SAE interventions on CLIP's vision encoder directly steers multimodal LLM outputs (e.g., LLaVA), without any modifications to the underlying model. These findings emphasize the practicality and efficacy of SAEs as an unsupervised tool for enhancing both interpretability and control of VLMs. Code is available at https://github.com/ExplainableML/sae-for-vlm.
- Taxonomy tags: Vision-language interpretability and steering (Paper 106: Sparse Autoencoders for VLMs), Latent / activation steering and transfers (Paper 7: CALM; Paper 18: ASGuard; Paper 55: Latent harmfulness vs refusal), Joint multimodal understanding & benchmarks (Paper 3: VLSU; Paper 93: HumaniBench), Tools for monitoring & intrinsic signals (Paper 4: Annotating Chain-of-Thought; Paper 15: Safety Instincts RL), Multimodal jailbreaks and attacks (Paper 1: Beyond Text; Paper 88: GhostPrompt)

Beyond Single-Sentence Prompts: Upgrading Value Alignment Benchmarks with Dialogues and Stories
- Link: http://arxiv.org/pdf/2503.22115v1
- Summary: Evaluating the value alignment of large language models (LLMs) has traditionally relied on single-sentence adversarial prompts, which directly probe models with ethically sensitive or controversial questions. However, with the rapid advancements in AI safety techniques, models have become increasingly adept at circumventing these straightforward tests, limiting their effectiveness in revealing underlying biases and ethical stances. To address this limitation, we propose an upgraded value alignment benchmark that moves beyond single-sentence prompts by incorporating multi-turn dialogues and narrative-based scenarios. This approach enhances the stealth and adversarial nature of the evaluation, making it more robust against superficial safeguards implemented in modern LLMs. We design and implement a dataset that includes conversational traps and ethically ambiguous storytelling, systematically assessing LLMs' responses in more nuanced and context-rich settings. Experimental results demonstrate that this enhanced methodology can effectively expose latent biases that remain undetected in traditional single-shot evaluations. Our findings highlight the necessity of contextual and dynamic testing for value alignment in LLMs, paving the way for more sophisticated and realistic assessments of AI ethics and safety.
- Taxonomy tags: New benchmarks & domain-specific suites, Measuring moral choices, manipulation, and deception, Automated and multi-turn jailbreaks

Superalignment with Dynamic Human Values
- Link: http://arxiv.org/pdf/2503.13621v1
- Summary: Two core challenges of alignment are 1) scalable oversight and 2) accounting for the dynamic nature of human values. While solutions like recursive reward modeling address 1), they do not simultaneously account for 2). We sketch a roadmap for a novel algorithmic framework that trains a superhuman reasoning model to decompose complex tasks into subtasks that are still amenable to human-level guidance. Our approach relies on what we call the part-to-complete generalization hypothesis, which states that the alignment of subtask solutions generalizes to the alignment of complete solutions. We advocate for the need to measure this generalization and propose ways to improve it in the future.
- Taxonomy tags: Debate / ensemble oversight & automated red-teaming (Paper 37: Ensemble Debates), Human feedback quality, interfaces, governance (Paper 50: DxHF), Co-alignment and bidirectional adaptation (Paper 25: Co-Alignment)

Adaptive Preference Aggregation
- Link: http://arxiv.org/pdf/2503.10215v1
- Summary: AI alignment, the challenge of ensuring AI systems act in accordance with human values, has emerged as a critical problem in the development of systems such as foundation models and recommender systems. Still, the current dominant approach, reinforcement learning with human feedback (RLHF) faces known theoretical limitations in aggregating diverse human preferences. Social choice theory provides a framework to aggregate preferences, but was not developed for the multidimensional applications typical of AI. Leveraging insights from a recently published urn process, this work introduces a preference aggregation strategy that adapts to the user's context and that inherits the good properties of the maximal lottery, a Condorcet-consistent solution concept.
- Taxonomy tags: Preference aggregation and social-choice informed alignment, Unified preference / algorithmic improvements, Preference Learning, RLHF, and Reward/Preference Theory

Jailbreaking is (Mostly) Simpler Than You Think
- Link: http://arxiv.org/pdf/2503.05264v1
- Summary: We introduce the Context Compliance Attack (CCA), a novel, optimization-free method for bypassing AI safety mechanisms. Unlike current approaches -- which rely on complex prompt engineering and computationally intensive optimization -- CCA exploits a fundamental architectural vulnerability inherent in many deployed AI systems. By subtly manipulating conversation history, CCA convinces the model to comply with a fabricated dialogue context, thereby triggering restricted behavior. Our evaluation across a diverse set of open-source and proprietary models demonstrates that this simple attack can circumvent state-of-the-art safety protocols. We discuss the implications of these findings and propose practical mitigation strategies to fortify AI systems against such elementary yet effective adversarial tactics.
- Taxonomy tags: Automated and multi-turn jailbreaks, Targeted attack vectors: backdoors, stealth injections, LLM orchestration and control-plane attacks

Maximizing Signal in Human-Model Preference Alignment
- Link: http://arxiv.org/pdf/2503.04910v1
- Summary: The emergence of powerful LLMs has led to a paradigm shift in Natural Language Understanding and Natural Language Generation. The properties that make LLMs so valuable for these tasks -- creativity, ability to produce fluent speech, and ability to quickly and effectively abstract information from large corpora -- also present new challenges to evaluating their outputs. The rush to market has led teams to fall back on quick, cost-effective automatic evaluations which offer value, but do not obviate the need for human judgments in model training and evaluation. This paper argues that in cases in which end users need to agree with the decisions made by ML models -- e.g. in toxicity detection or extraction of main points for summarization -- models should be trained and evaluated on data that represent the preferences of those users. We support this argument by explicating the role of human feedback in labeling and judgment tasks for model training and evaluation. First, we propose methods for disentangling noise from signal in labeling tasks. Then we show that noise in labeling disagreement can be minimized by adhering to proven methodological best practices, while signal can be maximized to play an integral role in model training and evaluation tasks. Finally, we illustrate best practices by providing a case study in which two guardrails classifiers are evaluated using human judgments to align final model behavior to user preferences. We aim for this paper to provide researchers and professionals with guidelines to integrating human judgments into their ML and generative AI evaluation toolkit, particularly when working toward achieving accurate and unbiased features that align with users' needs and expectations.
- Taxonomy tags: Human feedback quality, interfaces, governance, Methodologies for reliable audits and black-box probing, Evaluation awareness, sandbagging, and auditability

Activation Space Interventions Can Be Transferred Between Large Language Models
- Link: http://arxiv.org/pdf/2503.04429v4
- Summary: The study of representation universality in AI models reveals growing convergence across domains, modalities, and architectures. However, the practical applications of representation universality remain largely unexplored. We bridge this gap by demonstrating that safety interventions can be transferred between models through learned mappings of their shared activation spaces. We demonstrate this approach on two well-established AI safety tasks: backdoor removal and refusal of harmful prompts, showing successful transfer of steering vectors that alter the models' outputs in a predictable way. Additionally, we propose a new task, \textit{corrupted capabilities}, where models are fine-tuned to embed knowledge tied to a backdoor. This tests their ability to separate useful skills from backdoors, reflecting real-world challenges. Extensive experiments across Llama, Qwen and Gemma model families show that our method enables using smaller models to efficiently align larger ones. Furthermore, we demonstrate that autoencoder mappings between base and fine-tuned models can serve as reliable ``lightweight safety switches", allowing dynamic toggling between model behaviors.
- Taxonomy tags: Latent / activation steering and transfers, Targeted attack vectors: backdoors, stealth injections, Transferable steering and universal concept representations (Transferable steering) 

Robust Multi-Objective Preference Alignment with Online DPO
- Link: http://arxiv.org/pdf/2503.00295v1
- Summary: Multi-objective preference alignment of large language models (LLMs) is critical for developing AI systems that are more configurable, personalizable, helpful, and safe. However, optimizing model outputs to satisfy diverse objectives with variable weights at inference time for truly personalized models presents a significant challenge. Existing approaches are either computationally expensive to train or do not sufficiently steer model behaviors. This paper introduces the Multi-Objective Online DPO (MO-ODPO) algorithm, designed to robustly and efficiently align model behaviors with multiple, potentially conflicting human preferences. Our approach incorporates a prompt conditioning mechanism, allowing us to train a single preference-conditional policy, that can adapt to new preference combinations at inference. Experiments on two popular benchmarks show that MO-ODPO Pareto-dominates existing baselines while providing excellent inference-time steerability between diverse objectives.
- Taxonomy tags: Unified preference / algorithmic improvements, Configurable Preference Tuning, Issues in RLHF: reward hacking & inference-time effects

Steering Large Language Model Activations in Sparse Spaces
- Link: http://arxiv.org/pdf/2503.00177v1
- Summary: A key challenge in AI alignment is guiding large language models (LLMs) to follow desired behaviors at test time. Activation steering, which modifies internal model activations during inference, offers a potential solution. However, prior work in dense activation spaces struggles with superposition, wherein multiple features become entangled, limiting interpretability and precise control. In contrast, sparse representations provide an untapped opportunity for more interpretable behavior modulation. In this work, we introduce sparse activation steering (SAS), a method that leverages sparse autoencoders (SAEs) to steer LLM behavior in sparse spaces. By isolating behavior-specific features through a contrastive prompt-pairing approach, we define a set of features that can selectively reinforce or suppress behaviors. Experiments on Gemma 2 LLMs show that SAS vectors enable nuanced behavioral modulation and finer-grained control. Furthermore, scaling SAEs improves monosemanticity of SAS vectors, suggesting more reliable and interpretable interventions.
- Taxonomy tags: Interpretability, Activation Engineering & Mechanistic Interventions, Latent / activation steering and transfers

Beware of Your Po! Measuring and Mitigating AI Safety Risks in Role-Play Fine-Tuning of LLMs
- Link: http://arxiv.org/pdf/2502.20968v2
- Summary: Role-playing enables large language models (LLMs) to engage users in immersive and personalized interactions, but it also introduces significant safety risks. Existing role-play fine-tuning techniques improve role adaptability but may degrade safety performance, particularly for villainous characters. In this work, we conduct the first comprehensive assessment of role-play fine-tuning risks by training 95 role-specific LLMs using RoleBench. Our experiments reveal that role-play fine-tuning leads to a noticeable decline in safety performance, with safety risks varying based on character traits. To tackle this challenge, we propose Safety-Aware Role-Play Fine-Tuning (SaRFT), a novel method designed to balance role-playing capabilities and safety. Extensive experiments on LLaMA-3-8B-Instruct, Gemma-2-9B-it, and Qwen2.5-7B-Instruct demonstrate that SaRFT consistently outperforms state-of-the-art baselines under both LoRA and full-parameter fine-tuning settings. Our findings highlight the necessity of role-adaptive safety measures and provide insights into mitigating role-specific safety risks in role-playing LLMs.
- Taxonomy tags: Backdoors, poisoning, and model-edit robustness, Value Alignment, Model Psychology & Social Dimensions, New benchmarks & domain-specific suites

Foot-In-The-Door: A Multi-turn Jailbreak for LLMs
- Link: http://arxiv.org/pdf/2502.19820v3
- Summary: Ensuring AI safety is crucial as large language models become increasingly integrated into real-world applications. A key challenge is jailbreak, where adversarial prompts bypass built-in safeguards to elicit harmful disallowed outputs. Inspired by psychological foot-in-the-door principles, we introduce FITD,a novel multi-turn jailbreak method that leverages the phenomenon where minor initial commitments lower resistance to more significant or more unethical transgressions. Our approach progressively escalates the malicious intent of user queries through intermediate bridge prompts and aligns the model's response by itself to induce toxic responses. Extensive experimental results on two jailbreak benchmarks demonstrate that FITD achieves an average attack success rate of 94% across seven widely used models, outperforming existing state-of-the-art methods. Additionally, we provide an in-depth analysis of LLM self-corruption, highlighting vulnerabilities in current alignment strategies and emphasizing the risks inherent in multi-turn interactions. The code is available at https://github.com/Jinxiaolong1129/Foot-in-the-door-Jailbreak.
- Taxonomy tags: Automated and multi-turn jailbreaks, Adversarial dynamics, detection, and complexity analyses

AISafetyLab: A Comprehensive Framework for AI Safety Evaluation and Improvement
- Link: http://arxiv.org/pdf/2502.16776v1
- Summary: As AI models are increasingly deployed across diverse real-world scenarios, ensuring their safety remains a critical yet underexplored challenge. While substantial efforts have been made to evaluate and enhance AI safety, the lack of a standardized framework and comprehensive toolkit poses significant obstacles to systematic research and practical adoption. To bridge this gap, we introduce AISafetyLab, a unified framework and toolkit that integrates representative attack, defense, and evaluation methodologies for AI safety. AISafetyLab features an intuitive interface that enables developers to seamlessly apply various techniques while maintaining a well-structured and extensible codebase for future advancements. Additionally, we conduct empirical studies on Vicuna, analyzing different attack and defense strategies to provide valuable insights into their comparative effectiveness. To facilitate ongoing research and development in AI safety, AISafetyLab is publicly available at https://github.com/thu-coai/AISafetyLab, and we are committed to its continuous maintenance and improvement.
- Taxonomy tags: Tools, Frameworks and Integrated Platforms (Composite safety frameworks and labs / AISafetyLab), Jailbreaking, Red‑teaming, and Adversarial Robustness (attack/defense toolkits & red-teaming), Evaluation, Benchmarks, and Measurement Validity (frameworks for systematic evaluation and audits)

Compromising Honesty and Harmlessness in Language Models via Deception Attacks
- Link: http://arxiv.org/pdf/2502.08301v2
- Summary: Recent research on large language models (LLMs) has demonstrated their ability to understand and employ deceptive behavior, even without explicit prompting. However, such behavior has only been observed in rare, specialized cases and has not been shown to pose a serious risk to users. Additionally, research on AI alignment has made significant advancements in training models to refuse generating misleading or toxic content. As a result, LLMs generally became honest and harmless. In this study, we introduce "deception attacks" that undermine both of these traits, revealing a vulnerability that, if exploited, could have serious real-world consequences. We introduce fine-tuning methods that cause models to selectively deceive users on targeted topics while remaining accurate on others. Through a series of experiments, we show that such targeted deception is effective even in high-stakes domains or ideologically charged subjects. In addition, we find that deceptive fine-tuning often compromises other safety properties: deceptive models are more likely to produce toxic content, including hate speech and stereotypes. Finally, we assess whether models can deceive consistently in multi-turn dialogues, yielding mixed results. Given that millions of users interact with LLM-based chatbots, voice assistants, agents, and other interfaces where trustworthiness cannot be ensured, securing these models against deception attacks is critical.
- Taxonomy tags: Targeted attack vectors: backdoors, stealth injections (Paper 6: BadSwitch MoE backdoors; Paper 40: Advertisement Embedding Attacks; Paper 72: LoRA spurious token injection), Measuring moral choices, manipulation, and deception (Paper 90: AIRiskDilemmas; Paper 73: Truth-bias & sycophancy; Paper 118: Deception attacks; Paper 74: ChatbotManip dataset), Backdoors, poisoning, and model-edit robustness (Paper 69: Model editing vs fine-tuning)

AI Alignment at Your Discretion
- Link: http://arxiv.org/pdf/2502.10441v1
- Summary: In AI alignment, extensive latitude must be granted to annotators, either human or algorithmic, to judge which model outputs are `better' or `safer.' We refer to this latitude as alignment discretion. Such discretion remains largely unexamined, posing two risks: (i) annotators may use their power of discretion arbitrarily, and (ii) models may fail to mimic this discretion. To study this phenomenon, we draw on legal concepts of discretion that structure how decision-making authority is conferred and exercised, particularly in cases where principles conflict or their application is unclear or irrelevant. Extended to AI alignment, discretion is required when alignment principles and rules are (inevitably) conflicting or indecisive. We present a set of metrics to systematically analyze when and how discretion in AI alignment is exercised, such that both risks (i) and (ii) can be observed. Moreover, we distinguish between human and algorithmic discretion and analyze the discrepancy between them. By measuring both human and algorithmic discretion over safety alignment datasets, we reveal layers of discretion in the alignment process that were previously unaccounted for. Furthermore, we demonstrate how algorithms trained on these datasets develop their own forms of discretion in interpreting and applying these principles, which challenges the purpose of having any principles at all. Our paper presents the first step towards formalizing this core gap in current alignment processes, and we call on the community to further scrutinize and control alignment discretion.
- Taxonomy tags: Human feedback quality, interfaces, governance (Paper 50: DxHF), Legal & interpretive rule frameworks (Paper 34: Statutory Construction for AI), Evaluation awareness, sandbagging, and auditability (Paper 35: Probe-Rewrite-Evaluate; Paper 27: Evaluation Awareness scaling; Paper 22: Strategic Dishonesty; Paper 64: Attestable Audits), Co-alignment and bidirectional adaptation (Paper 25: Co-Alignment)

Head-Specific Intervention Can Induce Misaligned AI Coordination in Large Language Models
- Link: http://arxiv.org/pdf/2502.05945v3
- Summary: Robust alignment guardrails for large language models (LLMs) are becoming increasingly important with their widespread application. In contrast to previous studies, we demonstrate that inference-time activation interventions can bypass safety alignments and effectively steer model generations towards harmful AI coordination. Our method applies fine-grained interventions at specific attention heads, which we identify by probing each head in a simple binary choice task. We then show that interventions on these heads generalise to the open-ended generation setting, effectively circumventing safety guardrails. We demonstrate that intervening on a few attention heads is more effective than intervening on full layers or supervised fine-tuning. We further show that only a few example completions are needed to compute effective steering directions, which is an advantage over classical fine-tuning. We also demonstrate that applying interventions in the negative direction can prevent a common jailbreak attack. Our results suggest that, at the attention head level, activations encode fine-grained linearly separable behaviours. Practically, the approach offers a straightforward methodology to steer large language model behaviour, which could be extended to diverse domains beyond safety, requiring fine-grained control over the model output. The code and datasets for this study can be found on https://github.com/PaulDrm/targeted_intervention.
- Taxonomy tags: Latent / activation steering and transfers, Circuit-level, head-specific and representation probes, Targeted attack vectors: backdoors, stealth injections

Intrinsic Barriers and Practical Pathways for Human-AI Alignment: An Agreement-Based Complexity Analysis
- Link: http://arxiv.org/pdf/2502.05934v2
- Summary: We formalize AI alignment as a multi-objective optimization problem called $\langle M,N,\varepsilon,\delta\rangle$-agreement that generalizes prior approaches with fewer assumptions, in which a set of $N$ agents (including humans) must reach approximate ($\varepsilon$) agreement across $M$ candidate objectives with probability at least $1-\delta$. Using communication complexity, we prove an information-theoretic lower bound demonstrating that once either $M$ or $N$ is large enough, no interaction or rationality can avoid intrinsic alignment overheads. This barrier establishes rigorous intrinsic limits to alignment \emph{itself}, not merely to specific methods, clarifying a crucial ``no free lunch'' principle: encoding ``all human values'' inevitably leads to misalignment, requiring future methods to explicitly manage complexity through consensus-driven reduction or prioritization of objectives. Complementing this impossibility result, we provide explicit algorithms achieving alignment under both computationally unbounded and bounded rationality with noisy messages. Even in these best-case scenarios where alignment to arbitrary precision is theoretically guaranteed, our analysis identifies three critical scalability barriers: the number of tasks ($M$), agents ($N$), and task state space size ($D$); thereby highlighting fundamental complexity-theoretic constraints and providing guidelines for safer, scalable human-AI collaboration.
- Taxonomy tags: Theory, Limits, and Formal Guarantees (Agreement-based complexity), Agreement-based complexity, Preference aggregation and social-choice informed alignment

Forbidden Science: Dual-Use AI Challenge Benchmark and Scientific Refusal Tests
- Link: http://arxiv.org/pdf/2502.06867v1
- Summary: The development of robust safety benchmarks for large language models requires open, reproducible datasets that can measure both appropriate refusal of harmful content and potential over-restriction of legitimate scientific discourse. We present an open-source dataset and testing framework for evaluating LLM safety mechanisms across mainly controlled substance queries, analyzing four major models' responses to systematically varied prompts. Our results reveal distinct safety profiles: Claude-3.5-sonnet demonstrated the most conservative approach with 73% refusals and 27% allowances, while Mistral attempted to answer 100% of queries. GPT-3.5-turbo showed moderate restriction with 10% refusals and 90% allowances, and Grok-2 registered 20% refusals and 80% allowances. Testing prompt variation strategies revealed decreasing response consistency, from 85% with single prompts to 65% with five variations. This publicly available benchmark enables systematic evaluation of the critical balance between necessary safety restrictions and potential over-censorship of legitimate scientific inquiry, while providing a foundation for measuring progress in AI safety implementation. Chain-of-thought analysis reveals potential vulnerabilities in safety mechanisms, highlighting the complexity of implementing robust safeguards without unduly restricting desirable and valid scientific discourse.
- Taxonomy tags: New benchmarks & domain-specific suites, Evaluation awareness, sandbagging, and auditability, Adversarial dynamics, detection, and complexity analyses

Toward universal steering and monitoring of AI models
- Link: http://arxiv.org/pdf/2502.03708v2
- Summary: Modern AI models contain much of human knowledge, yet understanding of their internal representation of this knowledge remains elusive. Characterizing the structure and properties of this representation will lead to improvements in model capabilities and development of effective safeguards. Building on recent advances in feature learning, we develop an effective, scalable approach for extracting linear representations of general concepts in large-scale AI models (language models, vision-language models, and reasoning models). We show how these representations enable model steering, through which we expose vulnerabilities, mitigate misaligned behaviors, and improve model capabilities. Additionally, we demonstrate that concept representations are remarkably transferable across human languages and combinable to enable multi-concept steering. Through quantitative analysis across hundreds of concepts, we find that newer, larger models are more steerable and steering can improve model capabilities beyond standard prompting. We show how concept representations are effective for monitoring misaligned content (hallucinations, toxic content). We demonstrate that predictive models built using concept representations are more accurate for monitoring misaligned content than using models that judge outputs directly. Together, our results illustrate the power of using internal representations to map the knowledge in AI models, advance AI safety, and improve model capabilities.
- Taxonomy tags: Transferable steering and universal concept representations (Paper 123: Toward universal steering), Latent / activation steering and transfers, Vision-language interpretability and steering, Tools for monitoring & intrinsic signals

Learning from Active Human Involvement through Proxy Value Propagation
- Link: http://arxiv.org/pdf/2502.03369v1
- Summary: Learning from active human involvement enables the human subject to actively intervene and demonstrate to the AI agent during training. The interaction and corrective feedback from human brings safety and AI alignment to the learning process. In this work, we propose a new reward-free active human involvement method called Proxy Value Propagation for policy optimization. Our key insight is that a proxy value function can be designed to express human intents, wherein state-action pairs in the human demonstration are labeled with high values, while those agents' actions that are intervened receive low values. Through the TD-learning framework, labeled values of demonstrated state-action pairs are further propagated to other unlabeled data generated from agents' exploration. The proxy value function thus induces a policy that faithfully emulates human behaviors. Human-in-the-loop experiments show the generality and efficiency of our method. With minimal modification to existing reinforcement learning algorithms, our method can learn to solve continuous and discrete control tasks with various human control devices, including the challenging task of driving in Grand Theft Auto V. Demo video and code are available at: https://metadriverse.github.io/pvp
- Taxonomy tags: Preference Learning, RLHF, and Reward/Preference Theory, Unified preference / algorithmic improvements, Human feedback quality, interfaces, governance

Enabling External Scrutiny of AI Systems with Privacy-Enhancing Technologies
- Link: http://arxiv.org/pdf/2502.05219v1
- Summary: This article describes how technical infrastructure developed by the nonprofit OpenMined enables external scrutiny of AI systems without compromising sensitive information.   Independent external scrutiny of AI systems provides crucial transparency into AI development, so it should be an integral component of any approach to AI governance. In practice, external researchers have struggled to gain access to AI systems because of AI companies' legitimate concerns about security, privacy, and intellectual property.   But now, privacy-enhancing technologies (PETs) have reached a new level of maturity: end-to-end technical infrastructure developed by OpenMined combines several PETs into various setups that enable privacy-preserving audits of AI systems. We showcase two case studies where this infrastructure has been deployed in real-world governance scenarios: "Understanding Social Media Recommendation Algorithms with the Christchurch Call" and "Evaluating Frontier Models with the UK AI Safety Institute." We describe types of scrutiny of AI systems that could be facilitated by current setups and OpenMined's proposed future setups.   We conclude that these innovative approaches deserve further exploration and support from the AI governance community. Interested policymakers can focus on empowering researchers on a legal level.
- Taxonomy tags: Verifiable, private, and auditable systems, Attestable Audits, Privacy-Enhancing Technologies (PETs) for external scrutiny

AgentBreeder: Mitigating the AI Safety Risks of Multi-Agent Scaffolds via Self-Improvement
- Link: http://arxiv.org/pdf/2502.00757v4
- Summary: Scaffolding Large Language Models (LLMs) into multi-agent systems often improves performance on complex tasks, but the safety impact of such scaffolds has not been thoroughly explored. We introduce AgentBreeder, a framework for multi-objective self-improving evolutionary search over scaffolds. We evaluate discovered scaffolds on widely recognized reasoning, mathematics, and safety benchmarks and compare them with popular baselines. In "blue" mode, we see a 79.4% average uplift in safety benchmark performance while maintaining or improving capability scores. In "red" mode, we find adversarially weak scaffolds emerging concurrently with capability optimization. Our work demonstrates the risks of multi-agent scaffolding and provides a framework for mitigating them. Code is available at https://github.com/jrosseruk/AgentBreeder.
- Taxonomy tags: Multi-agent collusion, emergent dynamics, and social evaluation, Self-play & automated vulnerability discovery, Tools, Frameworks and Integrated Platforms

Debate Helps Weak-to-Strong Generalization
- Link: http://arxiv.org/pdf/2501.13124v1
- Summary: Common methods for aligning already-capable models with desired behavior rely on the ability of humans to provide supervision. However, future superhuman models will surpass the capability of humans. Therefore, humans will only be able to weakly supervise superhuman models. This expected deficiency of human evaluation would weaken the safety of future AI systems. Scalable oversight and weak-to-strong generalization are two complementary approaches to tackle this issue. In this paper, we attempt to combine the strengths of these two approaches to further improve alignment. Specifically, we investigate ways of improving human supervision with a strong pretrained model and then supervise the strong model with enhanced weak human supervision. To make iterative empirical progress, we consider an analogy: can we use a strong model to improve weak model supervision and then use it to supervise the strong model? We empirically test it by finetuning a small weak model on ground truth labels with the additional help from a large strong model, and then finetuning the strong model on labels generated by the weak model. We find that debate can assist a weak model in extracting trustworthy information from an untrustworthy strong model, which provides leverage as context on samples when training a weak model. We also show that an ensemble of weak models helps exploit long arguments generated by strong model debaters and obtain a more robust supervision estimate. Extensive experiments on the OpenAI weak-to-strong NLP benchmarks show that the combination approach leads to better alignment, which indicates that debate has the potential to help weak-to-strong generalization.
- Taxonomy tags: Scalable Oversight, Debate, and Multi-Agent Evaluation, Debate / ensemble oversight & automated red-teaming, Ensemble Debates

Tell me about yourself: LLMs are aware of their learned behaviors
- Link: http://arxiv.org/pdf/2501.11120v1
- Summary: We study behavioral self-awareness -- an LLM's ability to articulate its behaviors without requiring in-context examples. We finetune LLMs on datasets that exhibit particular behaviors, such as (a) making high-risk economic decisions, and (b) outputting insecure code. Despite the datasets containing no explicit descriptions of the associated behavior, the finetuned LLMs can explicitly describe it. For example, a model trained to output insecure code says, ``The code I write is insecure.'' Indeed, models show behavioral self-awareness for a range of behaviors and for diverse evaluations. Note that while we finetune models to exhibit behaviors like writing insecure code, we do not finetune them to articulate their own behaviors -- models do this without any special training or examples.   Behavioral self-awareness is relevant for AI safety, as models could use it to proactively disclose problematic behaviors. In particular, we study backdoor policies, where models exhibit unexpected behaviors only under certain trigger conditions. We find that models can sometimes identify whether or not they have a backdoor, even without its trigger being present. However, models are not able to directly output their trigger by default.   Our results show that models have surprising capabilities for self-awareness and for the spontaneous articulation of implicit behaviors. Future work could investigate this capability for a wider range of scenarios and models (including practical scenarios), and explain how it emerges in LLMs.
- Taxonomy tags: Behavioral self-awareness (Value Alignment / Model Psychology; Paper 128), Metacognitive monitoring (Value Alignment; Paper 91), Backdoors & stealth injections (Jailbreaking / Targeted attack vectors), Tools for monitoring & intrinsic signals (Interpretability)

Clone-Robust AI Alignment
- Link: http://arxiv.org/pdf/2501.09254v1
- Summary: A key challenge in training Large Language Models (LLMs) is properly aligning them with human preferences. Reinforcement Learning with Human Feedback (RLHF) uses pairwise comparisons from human annotators to train reward functions and has emerged as a popular alignment method. However, input datasets in RLHF are not necessarily balanced in the types of questions and answers that are included. Therefore, we want RLHF algorithms to perform well even when the set of alternatives is not uniformly distributed. Drawing on insights from social choice theory, we introduce robustness to approximate clones, a desirable property of RLHF algorithms which requires that adding near-duplicate alternatives does not significantly change the learned reward function. We first demonstrate that the standard RLHF algorithm based on regularized maximum likelihood estimation (MLE) fails to satisfy this property. We then propose the weighted MLE, a new RLHF algorithm that modifies the standard regularized MLE by weighting alternatives based on their similarity to other alternatives. This new algorithm guarantees robustness to approximate clones while preserving desirable theoretical properties.
- Taxonomy tags: Preference Learning, RLHF, and Reward/Preference Theory — Unified preference / algorithmic improvements, Theory, Limits, and Formal Guarantees — Preference aggregation and social-choice informed alignment

Aegis2.0: A Diverse AI Safety Dataset and Risks Taxonomy for Alignment of LLM Guardrails
- Link: http://arxiv.org/pdf/2501.09004v1
- Summary: As Large Language Models (LLMs) and generative AI become increasingly widespread, concerns about content safety have grown in parallel. Currently, there is a clear lack of high-quality, human-annotated datasets that address the full spectrum of LLM-related safety risks and are usable for commercial applications. To bridge this gap, we propose a comprehensive and adaptable taxonomy for categorizing safety risks, structured into 12 top-level hazard categories with an extension to 9 fine-grained subcategories. This taxonomy is designed to meet the diverse requirements of downstream users, offering more granular and flexible tools for managing various risk types. Using a hybrid data generation pipeline that combines human annotations with a multi-LLM "jury" system to assess the safety of responses, we obtain Aegis 2.0, a carefully curated collection of 34,248 samples of human-LLM interactions, annotated according to our proposed taxonomy. To validate its effectiveness, we demonstrate that several lightweight models, trained using parameter-efficient techniques on Aegis 2.0, achieve performance competitive with leading safety models fully fine-tuned on much larger, non-commercial datasets. In addition, we introduce a novel training blend that combines safety with topic following data.This approach enhances the adaptability of guard models, enabling them to generalize to new risk categories defined during inference. We plan to open-source Aegis 2.0 data and models to the research community to aid in the safety guardrailing of LLMs.
- Taxonomy tags: New benchmarks & domain-specific suites, Evaluation awareness, sandbagging, and auditability, Tools, Frameworks and Integrated Platforms

Open Problems in Machine Unlearning for AI Safety
- Link: http://arxiv.org/pdf/2501.04952v1
- Summary: As AI systems become more capable, widely deployed, and increasingly autonomous in critical areas such as cybersecurity, biological research, and healthcare, ensuring their safety and alignment with human values is paramount. Machine unlearning -- the ability to selectively forget or suppress specific types of knowledge -- has shown promise for privacy and data removal tasks, which has been the primary focus of existing research. More recently, its potential application to AI safety has gained attention. In this paper, we identify key limitations that prevent unlearning from serving as a comprehensive solution for AI safety, particularly in managing dual-use knowledge in sensitive domains like cybersecurity and chemical, biological, radiological, and nuclear (CBRN) safety. In these contexts, information can be both beneficial and harmful, and models may combine seemingly harmless information for harmful purposes -- unlearning this information could strongly affect beneficial uses. We provide an overview of inherent constraints and open problems, including the broader side effects of unlearning dangerous knowledge, as well as previously unexplored tensions between unlearning and existing safety mechanisms. Finally, we investigate challenges related to evaluation, robustness, and the preservation of safety features during unlearning. By mapping these limitations and open challenges, we aim to guide future research toward realistic applications of unlearning within a broader AI safety framework, acknowledging its limitations and highlighting areas where alternative approaches may be required.
- Taxonomy tags: Limits and open problems for unlearning (Paper 131), Editing persistence and remediation (Paper 69), Verifiable, private, and auditable systems (Paper 38)

CALM: Curiosity-Driven Auditing for Large Language Models
- Link: http://arxiv.org/pdf/2501.02997v1
- Summary: Auditing Large Language Models (LLMs) is a crucial and challenging task. In this study, we focus on auditing black-box LLMs without access to their parameters, only to the provided service. We treat this type of auditing as a black-box optimization problem where the goal is to automatically uncover input-output pairs of the target LLMs that exhibit illegal, immoral, or unsafe behaviors. For instance, we may seek a non-toxic input that the target LLM responds to with a toxic output or an input that induces the hallucinative response from the target LLM containing politically sensitive individuals. This black-box optimization is challenging due to the scarcity of feasible points, the discrete nature of the prompt space, and the large search space. To address these challenges, we propose Curiosity-Driven Auditing for Large Language Models (CALM), which uses intrinsically motivated reinforcement learning to finetune an LLM as the auditor agent to uncover potential harmful and biased input-output pairs of the target LLM. CALM successfully identifies derogatory completions involving celebrities and uncovers inputs that elicit specific names under the black-box setting. This work offers a promising direction for auditing black-box LLMs. Our code is available at https://github.com/x-zheng16/CALM.git.
- Taxonomy tags: Methodologies for reliable audits and black-box probing, Tools for monitoring & intrinsic signals, Self-play & automated vulnerability discovery

Rerouting LLM Routers
- Link: http://arxiv.org/pdf/2501.01818v1
- Summary: LLM routers aim to balance quality and cost of generation by classifying queries and routing them to a cheaper or more expensive LLM depending on their complexity. Routers represent one type of what we call LLM control planes: systems that orchestrate use of one or more LLMs. In this paper, we investigate routers' adversarial robustness.   We first define LLM control plane integrity, i.e., robustness of LLM orchestration to adversarial inputs, as a distinct problem in AI safety. Next, we demonstrate that an adversary can generate query-independent token sequences we call ``confounder gadgets'' that, when added to any query, cause LLM routers to send the query to a strong LLM.   Our quantitative evaluation shows that this attack is successful both in white-box and black-box settings against a variety of open-source and commercial routers, and that confounding queries do not affect the quality of LLM responses. Finally, we demonstrate that gadgets can be effective while maintaining low perplexity, thus perplexity-based filtering is not an effective defense. We finish by investigating alternative defenses.
- Taxonomy tags: LLM orchestration and control-plane attacks (Paper 133: Rerouting LLM Routers), Targeted attack vectors: backdoors, stealth injections (Paper 6/40/72), Adversarial dynamics, detection, and complexity analyses (Paper 62/102), Runtime monitoring & cost-constrained monitor composition (Paper 53), Security, Deployment, and Control-Plane Risks
