Paper title,Author(s),Date published,Link to paper
NovelHopQA: Diagnosing Multi-Hop Reasoning Failures in Long Narrative Contexts,"Abhay Gupta, Michael Lu, Kevin Zhu, Sean O'Brien, Vasu Sharma",Jun 2025,https://arxiv.org/abs/2506.02000
Thought Crime: Backdoors and Emergent Misalignment in Reasoning Models,"James Chua, Jan Betley, Mia Taylor, Owain Evans",Jun 2025,https://arxiv.org/abs/2506.13206
Playing repeated games with large language models,"Elif Akata, Lion Schulz, Julian Coda-Forno, Seong Joon Oh, Matthias Bethge, Eric Schulz",May 2025,https://www.nature.com/articles/s41562-025-02172-y
AI Governance to Avoid Extinction: The Strategic Landscape and Actionable Research Questions,"Peter Barnett, Aaron Scher",May 2025,https://techgov.intelligence.org/research/ai-governance-to-avoid-extinction
AlphaEvolve: A coding agent for scientific and algorithmic discover,"Alexander Novikov, Ngan Vu, Marvin Eisenberger, Emilien Dupont, Po-Sen Huang, Adam Zsolt Wagner, Sergey Shirobokov, Borislav Kozlovskii, Francisco J. R. Ruiz, Abbas Mehrabian, M. Pawan Kumar, Abigail See, Swarat Chaudhuri, George Holland, Alex Davies, Sebastian Nowozin, Pushmeet Kohli, Matej Balog",May 2025,https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/AlphaEvolve.pdf
The necessity of AI audit standards boards,"David Manheim, Sammy Martin, Mark Bailey, Mikhail Samin, Ross Greutzmacher ",May 2025,https://link.springer.com/article/10.1007/s00146-025-02320-y
"The CoT Encyclopedia: Analyzing, Predicting, and Controlling how a Reasoning Model will Think","Seongyun Lee, Seungone Kim, Minju Seo, Yongrae Jo, Dongyoung Go, Hyeonbin Hwang, Jinho Park, Xiang Yue, Sean Welleck, Graham Neubig, Moontae Lee, Minjoon Seo",May 2025,https://arxiv.org/abs/2505.10185
Layered Unlearning for Adversarial Relearning,"Timothy Qian, Vinith Suriyakumar, Ashia Wilson, Dylan Hadfield-Menell",May 2025,https://arxiv.org/abs/2505.09500
HealthBench: Evaluating Large Language Models Towards Improved Human Health,"Rahul K. Arora, Jason Wei, Rebecca Soskin Hicks, Preston Bowman, Joaquin Quiñonero-Candela, Foivos Tsimpourlas, Michael Sharman, Meghan Shah, Andrea Vallone, Alex Beutel, Johannes Heidecke, Karan Singhal",May 2025,https://arxiv.org/abs/2505.08775
Putting It All into Context: Simplifying Agents with LCLMs,"Mingjian Jiang, Yangjun Ruan, Luis Lastras, Pavan Kapanipathi, Tatsunori Hashimoto",May 2025,https://arxiv.org/abs/2505.08120
LongCodeBench: Evaluating Coding LLMs at 1M Context Windows,"Stefano Rando, Luca Romani, Alessio Sampieri, Yuta Kyuragi, Luca Franco, Fabio Galasso, Tatsunori Hashimoto, John Yang",May 2025,https://arxiv.org/abs/2505.07897
LLMs Outperform Experts on Challenging Biology Benchmarks,Lennart Justen,May 2025,https://arxiv.org/abs/2505.06108
Understanding In-context Learning of Addition via Activation Subspaces,"Xinyan Hu, Kayo Yin, Michael I. Jordan, Jacob Steinhardt, Lijie Chen",May 2025,https://arxiv.org/abs/2505.05145
An alignment safety case sketch based on debate,"Marie Davidsen Buhl, Jacob Pfau, Benjamin Hilton, Geoffrey Irving",May 2025,https://arxiv.org/abs/2505.03989
Evaluating Frontier Models for Stealth and Situational Awareness,"Mary Phuong, Roland S. Zimmermann, Ziyue Wang, David Lindner, Victoria Krakovna, Sarah Cogan, Allan Dafoe, Lewis Ho, Rohin Shah",May 2025,https://arxiv.org/abs/2505.01420
Transferable Adversarial Attacks on Black-Box Vision-Language Models,"Kai Hu, Weichen Yu, Li Zhang, Alexander Robey, Andy Zou, Chengming Xu, Haoqi Hu, Matt Fredrikson",May 2025,https://arxiv.org/abs/2505.01050
Language Models use Lookbacks to Track Beliefs,"Nikhil Prakash, Natalie Shapira, Arnab Sen Sharma, Christoph Riedl, Yonatan Belinkov, Tamar Rott Shaham, David Bau, Atticus Geiger",May 2025,https://arxiv.org/abs/2505.14685
Harnessing the Universal Geometry of Embeddings,"Rishi Jha, Collin Zhang, Vitaly Shmatikov, John X. Morris",May 2025,https://arxiv.org/abs/2505.12540
SafetyNet: Detecting Harmful Outputs in LLMs by Modeling and Monitoring Deceptive Behaviors,"Maheep Chaudhary, Fazl Barez",May 2025,https://arxiv.org/abs/2505.14300
Beyond Semantics: The Unreasonable Effectiveness of Reasonless Intermediate Tokens,"Kaya Stechly, Karthik Valmeekam, Atharva Gundawar, Vardhan Palod, Subbarao Kambhampati",May 2025,https://arxiv.org/abs/2505.13775
On the creation of narrow AI: hierarchy and nonlocality of neural network skills,"Eric J. Michaud, Asher Parker-Sartori, Max Tegmark",May 2025,https://arxiv.org/abs/2505.15811
Neural Thermodynamic Laws for Large Language Model Training,"Ziming Liu, Yizhou Liu, Jeff Gore, Max Tegmark",May 2025,https://arxiv.org/abs/2505.10559
"Societal and technological progress as sewing an ever-growing, ever-changing, patchy, and polychrome quilt","Joel Z. Leibo, Alexander Sasha Vezhnevets, William A. Cunningham, Sébastien Krier, Manfred Diaz, Simon Osindero",May 2025,https://arxiv.org/abs/2505.05197
Parallel Scaling Law for Language Models,"Mouxiang Chen, Binyuan Hui, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Jianling Sun, Junyang Lin, Zhongxin Liu",May 2025,https://arxiv.org/abs/2505.10475
When Are Concepts Erased From Diffusion Models?,"Kevin Lu, Nicky Kriplani, Rohit Gandikota, Minh Pham, David Bau, Chinmay Hegde, Niv Cohen",May 2025,https://arxiv.org/abs/2505.17013
Towards eliciting latent knowledge from LLMs with mechanistic interpretability,"Bartosz Cywiński, Emil Ryd, Senthooran Rajamanoharan, Neel Nanda",May 2025,https://arxiv.org/abs/2505.14352
BountyBench: Dollar Impact of AI Agent Attackers and Defenders on Real-World Cybersecurity System,"Andy K. Zhang, Joey Ji, Celeste Menders, Riya Dulepet, Thomas Qin, Ron Y. Wang, Junrong Wu, Kyleen Liao, Jiliang Li, Jinghan Hu, Sara Hong, Nardos Demilew, Shivatmica Murgai, Jason Tran, Nishka Kacheria, Ethan Ho, Denis Liu, Lauren McLane, Olivia Bruvik, Dai-Rong Han, Seungwoo Kim, Akhil Vyas, Cuiyuanxiu Chen, Ryan Li, Weiran Xu, Jonathan Z. Ye, Prerit Choudhary, Siddharth M. Bhatia, Vikram Sivashankar, Yuxuan Bao, Dawn Song, Dan Boneh, Daniel E. Ho, Percy Liang",May 2025,https://arxiv.org/abs/2505.15216
When AI Co-Scientists Fail: SPOT-a Benchmark for Automated Verification of Scientific Research,"Guijin Son, Jiwoo Hong, Honglu Fan, Heejeong Nam, Hyunwoo Ko, Seungwon Lim, Jinyeop Song, Jinha Choi, Gonçalo Paulo, Youngjae Yu, Stella Biderman",May 2025,https://arxiv.org/abs/2505.11855
Extracting memorized pieces of (copyrighted) books from open-weight language models,"A. Feder Cooper, Aaron Gokaslan, Amy B. Cyphert, Christopher De Sa, Mark A. Lemley, Daniel E. Ho, Percy Liang",May 2025,https://arxiv.org/abs/2505.12546
Do Language Models Use Their Depth Efficiently?,"Róbert Csordás, Christopher D. Manning, Christopher Potts",May 2025,https://arxiv.org/abs/2505.13898
X-Reasoner: Towards Generalizable Reasoning Across Modalities and Domains,"Qianchu Liu, Sheng Zhang, Guanghui Qin, Timothy Ossowski, Yu Gu, Ying Jin, Sid Kiblawi, Sam Preston, Mu Wei, Paul Vozila, Tristan Naumann, Hoifung Poon",May 2025,https://arxiv.org/abs/2505.03981
Preference Learning with Lie Detectors can Induce Honesty or Evasion,"Chris Cundy, Adam Gleave",May 2025,https://arxiv.org/abs/2505.13787
Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values Prioritization with AIRiskDilemmas,"Yu Ying Chiu, Zhilin Wang, Sharan Maiya, Yejin Choi, Kyle Fish, Sydney Levine, Evan Hubinger",May 2025,https://arxiv.org/abs/2505.14633
Large Language Models Are More Persuasive Than Incentivized Human Persuaders,"Philipp Schoenegger, Francesco Salvi, Jiacheng Liu, Xiaoli Nan, Ramit Debnath, Barbara Fasolo, Evelina Leivada, Gabriel Recchia, Fritz Günther, Ali Zarifhonarvar, Joe Kwon, Zahoor Ul Islam, Marco Dehnert, Daryl Y. H. Lee, Madeline G. Reinecke, David G. Kamper, Mert Kobaş, Adam Sandford, Jonas Kgomo, Luke Hewitt, Shreya Kapoor, Kerem Oktar, Eyup Engin Kucuk, Bo Feng, Cameron R. Jones, Izzy Gainsburg, Sebastian Olschewski, Nora Heinzelmann, Francisco Cruz, Ben M. Tappin, Tao Ma, Peter S. Park, Rayan Onyonka, Arthur Hjorth, Peter Slattery, Qingcheng Zeng, Lennart Finke, Igor Grossmann, Alessandro Salatiello, Ezra Karger",May 2025,https://arxiv.org/abs/2505.09662
Learning to Reason without External Rewards,"Xuandong Zhao, Zhewei Kang, Aosong Feng, Sergey Levine, Dawn Song",May 2025,https://arxiv.org/abs/2505.19590
AI safety for everyone,"Bálint Gyevnár, Atoosa Kasirzadeh",Apr 2025,https://www.nature.com/articles/s42256-025-01020-y
Towards accurate differential diagnosis with large language models,"Daniel McDuff, Mike Schaekermann, Tao Tu, Anil Palepu, Amy Wang, Jake Garrison, Karan Singhal, Yash Sharma, Shekoofeh Azizi, Kavita Kulkarni, Le Hou, Yong Cheng, Yun Liu, S. Sara Mahdavi, Sushant Prakash, Anupam Pathak, Christopher Semturs, Shwetak Patel, Dale R. Webster, Ewa Dominowska, Juraj Gottweis, Joelle Barral, Katherine Chou, Greg S. Corrado, Yossi Matias, Jake Sunshine, Alan Karthikesalingam, Vivek Natarajan",Apr 2025,https://www.nature.com/articles/s41586-025-08869-4
Towards conversational diagnostic artificial intelligence,"Tao Tu, Mike Schaekermann, Anil Palepu, Khaled Saab, Jan Freyberg, Ryutaro Tanno, Amy Wang, Brenna Li, Mohamed Amin, Yong Cheng, Elahe Vedadi, Nenad Tomasev, Shekoofeh Azizi, Karan Singhal, Le Hou, Albert Webson, Kavita Kulkarni, S. Sara Mahdavi, Christopher Semturs, Juraj Gottweis, Joelle Barral, Katherine Chou, Greg S. Corrado, Yossi Matias, Alan Karthikesalingam, Vivek Natarajan",Apr 2025,https://www.nature.com/articles/s41586-025-08866-7
A narrow path: How to secure our future,"Andrea Miotti, Tolga Bilge, Dave Kasten, James Newport",Apr 2025,https://www.narrowpath.co/
The AI Adoption Gap: Preparing the US Government for Advanced AI,Lizka Vaintrob,Apr 2025,https://www.forethought.org/research/the-ai-adoption-gap
AI-Enabled Coups: How a Small Group Could Use AI to Seize Power,"Tom Davidson, Lukas Finnveden, Rose Hadshar",Apr 2025,https://www.forethought.org/research/ai-enabled-coups-how-a-small-group-could-use-ai-to-seize-power
RepliBench: measuring autonomous replication capabilities in AI systems,"Sid Black, Asa Cooper Stickland, Jake Pencharz, Oliver Sourbut, Michael Schmatz, Jay Bailey, Ollie Matthews, Ben Millwood, Alex Remedios, Alan Cooney",Apr 2025,https://www.aisi.gov.uk/work/replibench-measuring-autonomous-replication-capabilities-in-ai-systems
Redefining the role of governments to navigate the AI economic transformation,Deric Cheng,Apr 2025,https://www.agisocialcontract.org/
America’s Superintelligence Project,"Jeremie Harris, Edouard Harris",Apr 2025,https://superintelligence.gladstone.ai/
An Approach to Technical AGI Safety and Security,"Rohin Shah, Alex Irpan, Alexander Matt Turner, Anna Wang, Arthur Conmy, David Lindner, Jonah Brown-Cohen, Lewis Ho, Neel Nanda, Raluca Ada Popa, Rishub Jain, Rory Greig, Samuel Albanie, Scott Emmons, Sebastian Farquhar, Sébastien Krier, Senthooran Rajamanoharan, Sophie Bridgers, Tobi Ijitoye, Tom Everitt, Victoria Krakovna, Vikrant Varma, Vladimir Mikulik, Zachary Kenton, Dave Orr, Shane Legg, Noah Goodman, Allan Dafoe, Four Flynn, Anca Dragan
",Apr 2025,https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/evaluating-potential-cybersecurity-threats-of-advanced-ai/An_Approach_to_Technical_AGI_Safety_Apr_2025.pdf
Bare Minimum Mitigations for Autonomous AI Development,"Joshua Clymer, Isabella Duan, Chris Cundy, Yawen Duan, Fynn Heide, Chaochao Lu, Sören Mindermann, Conor McGurk, Xudong Pan, Saad Siddiqui, Jingren Wang, Min Yang, Xianyuan Zhan",Apr 2025,https://saif.org/research/bare-minimum-mitigations-for-autonomous-ai-development/
The 2025 AI Index Report,"Nestor Maslej, Loredana Fattorini, Raymond Perrault, Yolanda Gil, Vanessa Parli, Njenga Kariuki, Emily Capstick, Anka Reuel, Erik Brynjolfsson, John Etchemendy, Katrina Ligett, Terah Lyons, James Manyika, Juan Carlos Niebles, Yoav Shoham, Russell Wald, Tobi Walsh, Armin Hamrah, Lapo Santarlasci, Julia Betts Lotufo, Alexandra Rome, Andrew Shi, Sukrut Oak",Apr 2025,https://hai.stanford.edu/ai-index/2025-ai-index-report
Taking a responsible path to AGI,"Anca Dragan, Rohin Shah, Four Flynn, Shane Legg",Apr 2025,https://deepmind.google/discover/blog/taking-a-responsible-path-to-agi/
PaperBench: Evaluating AI’s Ability to Replicate AI Research,"Giulio Starace, Oliver Jaffe, Dane Sherburn, James Aung, Chan Jun Shern, Leon Maksin, Rachel Dias, Evan Mays, Benjamin Kinsella, Wyatt Thompson, Johannes Heidecke. Amelia Glaese, Tejal Patwardhan",Apr 2025,https://cdn.openai.com/papers/22265bac-3191-44e5-b057-7aaacd8e90cd/paperbench.pdf
Base Models Beat Aligned Models at Randomness and Creativity,"Peter West, Christopher Potts",Apr 2025,https://arxiv.org/abs/2505.00047
The Leaderboard Illusion,"Shivalika Singh, Yiyang Nan, Alex Wang, Daniel D'Souza, Sayash Kapoor, Ahmet Üstün, Sanmi Koyejo, Yuntian Deng, Shayne Longpre, Noah Smith, Beyza Ermis, Marzieh Fadaee, Sara Hooker",Apr 2025,https://arxiv.org/abs/2504.20879
Investigating task-specific prompts and sparse autoencoders for activation monitoring,"Henk Tillman, Dan Mossing",Apr 2025,https://arxiv.org/abs/2504.20271
Scaling Laws For Scalable Oversight,"Joshua Engels, David D. Baek, Subhash Kantamneni, Max Tegmark",Apr 2025,https://arxiv.org/abs/2504.18530
Safety Pretraining: Toward the Next Generation of Safe AI,"Pratyush Maini, Sachin Goyal, Dylan Sam, Alex Robey, Yash Savani, Yiding Jiang, Andy Zou, Zacharcy C. Lipton, J. Zico Kolter",Apr 2025,https://arxiv.org/abs/2504.16980
Trends in Frontier AI Model Count: A Forecast to 2028,"Iyngkarran Kumar, Sam Manning",Apr 2025,https://arxiv.org/abs/2504.16138
Trends in AI Supercomputers,"Konstantin F. Pilz, James Sanders, Robi Rahman, Lennart Heim",Apr 2025,https://arxiv.org/abs/2504.16026
Values in the Wild: Discovering and Analyzing Values in Real-World Language Model Interactions,"Saffron Huang, Esin Durmus, Miles McCain, Kunal Handa, Alex Tamkin, Jerry Hong, Michael Stern, Arushi Somani, Xiuruo Zhang, Deep Ganguli",Apr 2025,https://arxiv.org/abs/2504.15236
Existing Industry Practice for the EU AI Act's General-Purpose AI Code of Practice Safety and Security Measures,"Lily Stelling, Mick Yang, Rokas Gipiškis, Leon Staufer, Ze Shen Chin, Siméon Campos, Michael Chen",Apr 2025,https://arxiv.org/abs/2504.15181
The Geometry of Self-Verification in a Task-Specific Reasoning Model,"Andrew Lee, Lihao Sun, Chris Wendler, Fernanda Viégas, Martin Wattenberg",Apr 2025,https://arxiv.org/abs/2504.14379
TALES: Text Adventure Learning Environment Suite,"Christopher Zhang Cui, Xingdi Yuan, Ziang Xiao, Prithviraj Ammanabrolu, Marc-Alexandre Côté",Apr 2025,https://arxiv.org/abs/2504.14128
Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?,"Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Yang Yue, Shiji Song, Gao Huang",Apr 2025,https://arxiv.org/abs/2504.13837
Scaling sparse feature circuit finding for in-context learning,"Dmitrii Kharlapenko, Stepan Shabalin, Fazl Barez, Arthur Conmy, Neel Nanda",Apr 2025,https://arxiv.org/abs/2504.13756
Cost-of-Pass: An Economic Framework for Evaluating Language Models,"Mehmet Hamza Erol, Batu El, Mirac Suzgun, Mert Yuksekgonul, James Zou",Apr 2025,https://arxiv.org/abs/2504.13359
Sleep-time Compute: Beyond Inference Scaling at Test-time,"Kevin Lin, Charlie Snell, Yu Wang, Charles Packer, Sarah Wooders, Ion Stoica, Joseph E. Gonzalez",Apr 2025,https://arxiv.org/abs/2504.13171
MIB: A Mechanistic Interpretability Benchmark,"Aaron Mueller, Atticus Geiger, Sarah Wiegreffe, Dana Arad, Iván Arcuschin, Adam Belfki, Yik Siu Chan, Jaden Fiotto-Kaufman, Tal Haklay, Michael Hanna, Jing Huang, Rohan Gupta, Yaniv Nikankin, Hadas Orgad, Nikhil Prakash, Anja Reusch, Aruna Sankaranarayanan, Shun Shao, Alessandro Stolfo, Martin Tutek, Amir Zur, David Bau, Yonatan Belinkov",Apr 2025,https://arxiv.org/abs/2504.13151
In Which Areas of Technical AI Safety Could Geopolitical Rivals Cooperate?,"Ben Bucknall, Saad Siddiqui, Lara Thurnherr, Conor McGurk, Ben Harack, Anka Reuel, Patricia Paskov, Casey Mahoney, Sören Mindermann, Scott Singer, Vinay Hiremath, Charbel-Raphaël Segerie, Oscar Delaney, Alessandro Abate, Fazl Barez, Michael K. Cohen, Philip Torr, Ferenc Huszár, Anisoara Calinescu, Gabriel Davis Jones, Yoshua Bengio, Robert Trager",Apr 2025,https://arxiv.org/abs/2504.12914
BrowseComp: A Simple Yet Challenging Benchmark for Browsing Agents,"Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, Amelia Glaese",Apr 2025,https://arxiv.org/abs/2504.12516
AI Behind Closed Doors: a Primer on The Governance of Internal Deployment,"Charlotte Stix, Matteo Pistillo, Girish Sastry, Marius Hobbhahn, Alejandro Ortega, Mikita Balesni, Annika Hallensleben, Nix Goldowsky-Dill, Lee Sharkey",Apr 2025,https://arxiv.org/abs/2504.12170
Interpreting the Linear Structure of Vision-language Model Embedding Spaces,"Isabel Papadimitriou, Huangyuan Su, Thomas Fel, Naomi Saphra, Sham Kakade, Stephanie Gil",Apr 2025,https://arxiv.org/abs/2504.11695
Ctrl-Z: Controlling AI Agents via Resampling,"Aryan Bhatt, Cody Rushing, Adam Kaufman, Tyler Tracy, Vasil Georgiev, David Matolcsi, Akbir Khan, Buck Shlegeris",Apr 2025,https://arxiv.org/abs/2504.10374
LoRI: Reducing Cross-Task Interference in Multi-Task Low-Rank Adaptation,"Juzheng Zhang, Jiacheng You, Ashwinee Panda, Tom Goldstein",Apr 2025,https://arxiv.org/abs/2504.07448
DeepSeek-R1 Thoughtology: Let's <think> about LLM Reasoning,"Sara Vera Marjanović, Arkil Patel, Vaibhav Adlakha, Milad Aghajohari, Parishad BehnamGhader, Mehar Bhatia, Aditi Khandelwal, Austin Kraft, Benno Krojer, Xing Han Lù, Nicholas Meade, Dongchan Shin, Amirhossein Kazemnejad, Gaurav Kamath, Marius Mosbach, Karolina Stańczak, Siva Reddy",Apr 2025,https://arxiv.org/abs/2504.07128
How to evaluate control measures for LLM agents? A trajectory from today to superintelligence,"Tomek Korbak, Mikita Balesni, Buck Shlegeris, Geoffrey Irving",Apr 2025,https://arxiv.org/abs/2504.05259
The Dual-Route Model of Induction,"Sheridan Feucht, Eric Todd, Byron Wallace, David Bau",Apr 2025,https://arxiv.org/abs/2504.03022
Robustly identifying concepts introduced during chat fine-tuning using crosscoders,"Julian Minder, Clement Dumas, Caden Juang, Bilal Chugtai, Neel Nanda",Apr 2025,https://arxiv.org/abs/2504.02922
Do Two AI Scientists Agree?,"Xinghong Fu, Ziming Liu, Max Tegmark",Apr 2025,https://arxiv.org/abs/2504.02822
Inference-Time Scaling for Generalist Reward Modeling,"Zijun Liu, Peiyi Wang, Runxin Xu, Shirong Ma, Chong Ruan, Peng Li, Yang Liu, Yu Wu",Apr 2025,https://arxiv.org/abs/2504.02495
Interpreting Emergent Planning in Model-Free Reinforcement Learning,"Thomas Bush, Stephen Chung, Usman Anwar, Adrià Garriga-Alonso, David Krueger",Apr 2025,https://arxiv.org/abs/2504.01871
Virology Capabilities Test,"Jasper Götting, Pedro Medeiros, Jon G Sanders, Nathaniel Li, Long Phan, Karam Elabd, Lennart Justen, Dan Hendrycks, Seth Donoughe",Apr 2025,https://arxiv.org/abs/2503.06378
Modifying LLM Beliefs with Synthetic Document Finetuning,"Rowan Wang, Avery Griffin, Johannes Treutlein, Ethan Perez, Julian Michael, Fabien Roger, Sam Marks",Apr 2025,https://alignment.anthropic.com/2025/modifying-beliefs-via-sdf/
Putting up Bumpers,Samuel R. Bowman,Apr 2025,https://alignment.anthropic.com/2025/bumpers/
AI 2027,"Daniel Kokotajlo, Scott Alexander, Thomas Larsen, Eli Lifland, Romeo Dean",Apr 2025,https://ai-2027.com/
Bridging the human–AI knowledge gap through concept discovery and transfer in AlphaZero,"Lisa Schut, Nenad Tomasev, Thomas McGrath, Been Kim",Mar 2025,https://www.pnas.org/doi/10.1073/pnas.2406675122
Superintelligence Strategy,"Dan Hendrycks, Eric Schmidt, Alexandr Wang",Mar 2025,https://www.nationalsecurity.ai/
The MASK Benchmark: Disentangling Honesty from Accuracy in AI Systems,"Richard Ren, Arunim Agarwal, Mantas Mazeika, Cristina Menghini, Robert Vacareanu, Brad Kenstler, Mick Yang, Isabelle Barrass, Alice Gatti, Xuwang Yin, Eduardo Trevino, Matias Geralnik, Adam Khoja, Dean Lee, Summer Yue, Dan Hendrycks",Mar 2025,https://www.mask-benchmark.ai/
Will the Need to Retrain AI Models from Scratch Block a Software Intelligence Explosion?,Tom Davidson,Mar 2025,https://www.forethought.org/research/will-the-need-to-retrain-ai-models
Will AI R&D Automation Cause a Software Intelligence Explosion?,"Daniel Eth, Tom Davidson",Mar 2025,https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion
Preparing for the Intelligence Explosion,"Fin Moorhouse, Will MacAskill",Mar 2025,https://www.forethought.org/research/preparing-for-the-intelligence-explosion
Intelsat as a Model for International AGI Governance,"Will MacAskill, Rose Hadshar",Mar 2025,https://www.forethought.org/research/intelsat-as-a-model-for-international-agi-governance
AI Tools for Existential Security,"Lizka Vaintrob, Owen Cotton-Barratt",Mar 2025,https://www.forethought.org/research/ai-tools-for-existential-security
Circuit Tracing: Revealing Computational Graphs in Language Models,"Emmanuel Ameisen, Jack Lindsey, Adam Pearce, Wes Gurnee, Nicholas L. Turner, Brian Chen, Craig Citro, David Abrahams, Shan Carter, Basil Hosmer, Jonathan Marcus, Michael Sklar, Adly Templeton, Trenton Bricken, Callum McDougall, Hoagy Cunningham, Thomas Henighan, Adam Jermyn, Andy Jones, Andrew Persic, Zhenyi Qi, T. Ben Thompson, Sam Zimmerman, Kelley Rivoire, Thomas Conerly, Chris Olah, Joshua Batson",Mar 2025,https://transformer-circuits.pub/2025/attribution-graphs/methods.html
On the Biology of a Large Language Model,"Jack Lindsey, Wes Gurnee, Emmanuel Ameisen, Brian Chen, Adam Pearce, Nicholas L. Turner, Craig Citro, David Abrahams, Shan Carter, Basil Hosmer, Jonathan Marcus, Michael Sklar, Adly Templeton, Trenton Bricken, Callum McDougall◊, Hoagy Cunningham, Thomas Henighan, Adam Jermyn, Andy Jones, Andrew Persic, Zhenyi Qi, T. Ben Thompson, Sam Zimmerman, Kelley Rivoire, Thomas Conerly, Chris Olah, Joshua Batson",Mar 2025,https://transformer-circuits.pub/2025/attribution-graphs/biology.html
Could Advanced AI Accelerate the Pace of AI Progress? Interviews with AI Researchers,"Jared Leibowich, Nikola Jurkovic, Tom Davidson",Mar 2025,https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5115692
Too Late to Recall: The Two-Hop Problem in Multimodal Knowledge Retrieval,"Constantin Venhoff, Ashkan Khakzar, Sonia Joseph, Philip Torr, Neel Nanda",Mar 2025,https://openreview.net/forum?id=VUhRdZp8ke
CTRL-Rec: Controlling Recommender Systems With Natural Language,"Micah Carroll, Adeline Foote, Marcus Williams, Anca Dragan, W. Bradley Knox, Smitha Milli",Mar 2025,https://openreview.net/forum?id=tXl1gdoAoV
Understanding (Un)Reliability of Steering Vectors in Language Models,"Joschka Braun, Carsten Eickhoff, David Krueger, Seyed Ali Bahrainian, Dmitrii Krasheninnikov",Mar 2025,https://openreview.net/forum?id=JZiKuvIK1t
Diagnostic Uncertainty: Teaching Language Models to Describe Open-Ended Uncertainty,"Brian Sui, Jessy Lin, Michelle Li, Anca Dragan, Dan Klein, Jacob Steinhardt",Mar 2025,https://openreview.net/forum?id=D8oTSUnEfb
HCAST: Human-Calibrated Autonomy Software Tasks,"David Rein, Joel Becker, Amy Deng, Seraphina Nix, Chris Canal, Daniel O’Connell, Pip Arnott, Ryan Bloom, Thomas Broadley, Katharyn Garcia, Brian Goodrich, Max Hasin, Sami Jawhar, Megan Kinniment, Thomas Kwa, Aron Lajko, Nate Rush, Lucas Jun Koba Sato, Sydney Von Arx, Ben West, Lawrence Chan, Elizabeth Barnes",Mar 2025,https://metr.org/hcast.pdf
SPHERE: An Evaluation Card for Human-AI Systems,"Qianou Ma, Dora Zhao, Xinran Zhao, Chenglei Si, Chenyang Yang, Ryan Louie, Ehud Reiter, Diyi Yang, Tongshuang Wu",Mar 2025,https://arxiv.org/abs/2504.07971
Identifying Sparsely Active Circuits Through Local Loss Landscape Decomposition,"Brianna Chrisman, Lucius Bushnaq, Lee Sharkey",Mar 2025,https://arxiv.org/abs/2504.00194
Large Language Models Pass the Turing Test,"Cameron R. Jones, Benjamin K. Bergen",Mar 2025,https://arxiv.org/abs/2503.23674
Overtrained Language Models Are Harder to Fine-Tune,"Jacob Mitchell Springer, Sachin Goyal, Kaiyue Wen, Tanishq Kumar, Xiang Yue, Sadhika Malladi, Graham Neubig, Aditi Raghunathan",Mar 2025,https://arxiv.org/abs/2503.19206
International Agreements on AI Safety: Review and Recommendations for a Conditional AI Safety Treaty,"Rebecca Scholefield, Samuel Martin, Otto Barten",Mar 2025,https://arxiv.org/abs/2503.18956
I Have Covered All the Bases Here: Interpreting Reasoning Features in Large Language Models via Sparse Autoencoders,"Andrey Galichin, Alexey Dontsov, Polina Druzhinina, Anton Razzhigaev, Oleg Y. Rogov, Elena Tutubalina, Ivan Oseledets",Mar 2025,https://arxiv.org/abs/2503.18878
Defeating Prompt Injections by Design,"Edoardo Debenedetti, Ilia Shumailov, Tianqi Fan, Jamie Hayes, Nicholas Carlini, Daniel Fabian, Christoph Kern, Chongyang Shi, Andreas Terzis, Florian Tramèr",Mar 2025,https://arxiv.org/abs/2503.18813
Learning Multi-Level Features with Matryoshka Sparse Autoencoders,"Bart Bussmann, Noa Nabeshima, Adam Karvonen, Neel Nanda",Mar 2025,https://arxiv.org/abs/2503.17547
Language Models May Verbatim Complete Text They Were Not Explicitly Trained On,"Ken Ziyu Liu, Christopher A. Choquette-Choo, Matthew Jagielski, Peter Kairouz, Sanmi Koyejo, Percy Liang, Nicolas Papernot",Mar 2025,https://arxiv.org/abs/2503.17514
AI Companies Should Report Pre- and Post-Mitigation Safety Evaluations,"Dillon Bowen, Ann-Kathrin Dombrowski, Adam Gleave, Chris Cundy",Mar 2025,https://arxiv.org/abs/2503.17388
CVE-Bench: A Benchmark for AI Agents' Ability to Exploit Real-World Web Application Vulnerabilities,"Yuxuan Zhu, Antony Kellermann, Dylan Bowman, Philip Li, Akul Gupta, Adarsh Danda, Richard Fang, Conner Jensen, Eric Ihli, Jason Benn, Jet Geronimo, Avi Dhir, Sudhit Rao, Kaicheng Yu, Twm Stone, Daniel Kang",Mar 2025,https://arxiv.org/abs/2503.17332
In-House Evaluation Is Not Enough: Towards Robust Third-Party Flaw Disclosure for General-Purpose AI,"Shayne Longpre, Kevin Klyman, Ruth E. Appel, Sayash Kapoor, Rishi Bommasani, Michelle Sahar, Sean McGregor, Avijit Ghosh, Borhane Blili-Hamelin, Nathan Butters, Alondra Nelson, Amit Elazari, Andrew Sellars, Casey John Ellis, Dane Sherrets, Dawn Song, Harley Geiger, Ilona Cohen, Lauren McIlvenny, Madhulika Srikumar, Mark M. Jaycox, Markus Anderljung, Nadine Farid Johnson, Nicholas Carlini, Nicolas Miailhe, Nik Marda, Peter Henderson, Rebecca S. Portnoff, Rebecca Weiss, Victoria Westerhoff, Yacine Jernite, Rumman Chowdhury, Percy Liang, Arvind Narayanan",Mar 2025,https://arxiv.org/abs/2503.16861
Measuring AI Ability to Complete Long Tasks,"Thomas Kwa, Ben West, Joel Becker, Amy Deng, Katharyn Garcia, Max Hasin, Sami Jawhar, Megan Kinniment, Nate Rush, Sydney Von Arx, Ryan Bloom, Thomas Broadley, Haoxing Du, Brian Goodrich, Nikola Jurkovic, Luke Harold Miles, Seraphina Nix, Tao Lin, Neev Parikh, David Rein, Lucas Jun Koba Sato, Hjalmar Wijk, Daniel M. Ziegler, Elizabeth Barnes, Lawrence Chan",Mar 2025,https://arxiv.org/abs/2503.14499
"From Autonomous Agents to Integrated Systems, A New Paradigm: Orchestrated Distributed Intelligence",Krti Tallam,Mar 2025,https://arxiv.org/abs/2503.13754
A Framework for Evaluating Emerging Cyberattack Capabilities of AI,"Mikel Rodriguez, Raluca Ada Popa, Four Flynn, Lihao Liang, Allan Dafoe, Anna Wang",Mar 2025,https://arxiv.org/abs/2503.11917
Combining Causal Models for More Accurate Abstractions of Neural Networks,"Theodora-Mara Pîslar, Sara Magliacane, Atticus Geiger",Mar 2025,https://arxiv.org/abs/2503.11429
SAEBench: A Comprehensive Benchmark for Sparse Autoencoders in Language Model Interpretability,"Adam Karvonen, Can Rager, Johnny Lin, Curt Tigges, Joseph Bloom, David Chanin, Yeu-Tong Lau, Eoin Farrell, Callum McDougall, Kola Ayonrinde, Matthew Wearden, Arthur Conmy, Samuel Marks, Neel Nanda",Mar 2025,https://arxiv.org/abs/2503.09532
"Randomness, Not Representation: The Unreliability of Evaluating Cultural Alignment in LLMs","Ariba Khan, Stephen Casper, Dylan Hadfield-Menell",Mar 2025,https://arxiv.org/abs/2503.08688
Chain-of-Thought Reasoning In The Wild Is Not Always Faithful,"Iván Arcuschin, Jett Janiak, Robert Krzyzanowski, Senthooran Rajamanoharan, Neel Nanda, Arthur Conmy",Mar 2025,https://arxiv.org/abs/2503.08679v1
Advancing AI Negotiations: New Theory and Evidence from a Large-Scale Autonomous Negotiations Competition,"Michelle Vaccaro, Michael Caoson, Harang Ju, Sinan Aral, Jared R. Curhan",Mar 2025,https://arxiv.org/abs/2503.06416
Decoding the Black Box: Integrating Moral Imagination with Technical AI Governance,Krti Tallam,Mar 2025,https://arxiv.org/abs/2503.06411
General Scales Unlock AI Evaluation with Explanatory and Predictive Power,"Lexin Zhou, Lorenzo Pacchiardi, Fernando Martínez-Plumed, Katherine M. Collins, Yael Moros-Daval, Seraphina Zhang, Qinlin Zhao, Yitian Huang, Luning Sun, Jonathan E. Prunty, Zongqian Li, Pablo Sánchez-García, Kexin Jiang Chen, Pablo A. M. Casares, Jiyun Zu, John Burden, Behzad Mehrbakhsh, David Stillwell, Manuel Cebrian, Jindong Wang, Peter Henderson, Sherry Tongshuang Wu, Patrick C. Kyllonen, Lucy Cheke, Xing Xie, José Hernández-Orallo",Mar 2025,https://arxiv.org/abs/2503.06378
GATE: An Integrated Assessment Model for AI Automation,"Ege Erdil, Andrei Potlogea, Tamay Besiroglu, Edu Roldan, Anson Ho, Jaime Sevilla, Matthew Barnett, Matej Vrzla, Robert Sandler",Mar 2025,https://arxiv.org/abs/2503.04941
START: Self-taught Reasoner with Tools,"Chengpeng Li, Mingfeng Xue, Zhenru Zhang, Jiaxi Yang, Beichen Zhang, Xiang Wang, Bowen Yu, Binyuan Hui, Junyang Lin, Dayiheng Liu",Mar 2025,https://arxiv.org/abs/2503.04625
Activation Space Interventions Can Be Transferred Between Large Language Models,"Narmeen Oozeer, Dhruv Nathawani, Nirmalendu Prakash, Michael Lan, Abir Harrasse, Amirali Abdullah",Mar 2025,https://arxiv.org/abs/2503.04429
Towards Understanding Distilled Reasoning Models: A Representational Approach,"David D. Baek, Max Tegmark",Mar 2025,https://arxiv.org/abs/2503.03730
Position: Model Collapse Does Not Mean What You Think,"Rylan Schaeffer, Joshua Kazdan, Alvan Caleb Arulandu, Sanmi Koyejo",Mar 2025,https://arxiv.org/abs/2503.03150
"Same Question, Different Words: A Latent Adversarial Framework for Prompt Robustness","Tingchen Fu, Fazl Barez",Mar 2025,https://arxiv.org/abs/2503.01345v1
Automated Researchers Can Subtly Sandbag,"Johannes Gasteiger, Akbir Khan, Sam Bowman, Vladimir Mikulik, Ethan Perez, Fabien Roger",Mar 2025,https://alignment.anthropic.com/2025/automated-researchers-sandbag/
Three Types of Intelligence Explosion,"Tom Davidson, Rose Hadshar, Will MacAskill",Feb 2025,https://www.forethought.org/research/three-types-of-intelligence-explosion
"AGI, Governments, and Free Societies","Justin B. Bullock, Samuel Hammond, Seb Krier",Feb 2025,https://www.arxiv.org/abs/2503.05710
Modeling Human Beliefs about AI Behavior for Scalable Oversight,"Leon Lang, Patrick Forre",Feb 2025,https://www.arxiv.org/abs/2502.21262
Idiosyncrasies in Large Language Models,"Mingjie Sun, Yida Yin, Zhiqiu Xu, J. Zico Kolter, Zhuang Liu",Feb 2025,https://www.arxiv.org/abs/2502.12150
The Danger of Overthinking: Examining the Reasoning-Action Dilemma in Agentic Tasks,"Alejandro Cuadron, Dacheng Li, Wenjie Ma, Xingyao Wang, Yichuan Wang, Siyuan Zhuang, Shu Liu, Luis Gaspar Schroeder, Tian Xia, Huanzhi Mao, Nicholas Thumiger, Aditya Desai, Ion Stoica, Ana Klimovic, Graham Neubig, Joseph E. Gonzalez",Feb 2025,https://www.arxiv.org/abs/2502.08235
Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling,"Runze Liu, Junqi Gao, Jian Zhao, Kaiyan Zhang, Xiu Li, Biqing Qi, Wanli Ouyang, Bowen Zhou",Feb 2025,https://www.arxiv.org/abs/2502.06703
Which Economic Tasks are Performed with AI? Evidence from Millions of Claude Conversation,"Kunal Handa, Alex Tamkin, Miles McCain, Saffron Huang, Esin Durmus, Sarah Heck, Jared Mueller, Jerry Hong, Stuart Ritchie, Tim Belonax, Kevin K. Troy, Dario Amodei, Jared Kaplan, Jack Clark, Deep Ganguli",Feb 2025,https://www.anthropic.com/news/the-anthropic-economic-index
"Alignment, Agency and Autonomy in Frontier AI: A Systems Engineering Perspective",Krti Tallam,Feb 2025,https://arxiv.org/abs/2503.05748
AILuminate: Introducing v1.0 of the AI Risk and Reliability Benchmark from MLCommons,"Shaona Ghosh, Heather Frase, Adina Williams, Sarah Luger, Paul Röttger, Fazl Barez, Sean McGregor, Kenneth Fricklas, Mala Kumar, Quentin Feuillade--Montixi, Kurt Bollacker, Felix Friedrich, Ryan Tsang, Bertie Vidgen, Alicia Parrish, Chris Knotz, Eleonora Presani, Jonathan Bennion, Marisa Ferrara Boston, Mike Kuniavsky, Wiebke Hutiri, James Ezick, Malek Ben Salem, Rajat Sahay, Sujata Goswami, Usman Gohar, Ben Huang, Supheakmungkol Sarin, Elie Alhajjar, Canyu Chen, Roman Eng, Kashyap Ramanandula Manjusha, Virendra Mehta, Eileen Long, Murali Emani, Natan Vidra, Benjamin Rukundo, Abolfazl Shahbazi, Kongtao Chen, Rajat Ghosh, Vithursan Thangarasa, Pierre Peigné, Abhinav Singh, Max Bartolo, Satyapriya Krishna, Mubashara Akhtar, Rafael Gold, Cody Coleman, Luis Oala, Vassil Tashev, Joseph Marvin Imperial, Amy Russ, Sasidhar Kunapuli, Nicolas Miailhe, Julien Delaunay, Bhaktipriya Radharapu, Rajat Shinde, Tuesday, Debojyoti Dutta, Declan Grabb, Ananya Gangavarapu, Saurav Sahay, Agasthya Gangavarapu, Patrick Schramowski, Stephen Singam, Tom David, Xudong Han, Priyanka Mary Mammen, Tarunima Prabhakar, Venelin Kovatchev, Ahmed Ahmed, Kelvin N. Manyeki, Sandeep Madireddy, Foutse Khomh, Fedor Zhdanov, Joachim Baumann, Nina Vasan, Xianjun Yang, Carlos Mougn, Jibin Rajan Varghese, Hussain Chinoy, Seshakrishna Jitendar, Manil Maskey, Claire V. Hardgrove, Tianhao Li, Aakash Gupta, Emil Joswin, Yifan Mai, Shachi H Kumar, Cigdem Patlak, Kevin Lu, Vincent Alessi, Sree Bhargavi Balija, Chenhe Gu, Robert Sullivan, James Gealy, Matt Lavrisa, James Goel, Peter Mattson, Percy Liang, Joaquin Vanschoren",Feb 2025,https://arxiv.org/abs/2503.05731
Safety Cases: A Scalable Approach to Frontier AI Safety,"Benjamin Hilton, Marie Davidsen Buhl, Tomek Korbak, Geoffrey Irving",Feb 2025,https://arxiv.org/abs/2503.04744
Systems and Algorithms for Convolutional Multi-Hybrid Language Models at Scale,"Jerome Ku, Eric Nguyen, David W. Romero, Garyk Brixi, Brandon Yang, Anton Vorontsov, Ali Taghibakhshi, Amy X. Lu, Dave P. Burke, Greg Brockman, Stefano Massaroli, Christopher Re, Patrick D. Hsu, Brian L. Hie, Stefano Ermon, Michael Poli",Feb 2025,https://arxiv.org/abs/2503.01868
"Transforming Cyber Defense: Harnessing Agentic and Frontier AI for Proactive, Ethical Threat Intelligence",Krti Tallam,Feb 2025,https://arxiv.org/abs/2503.00164
Adaptive Attacks Break Defenses Against Indirect Prompt Injection Attacks on LLM Agents,"Qiusi Zhan, Richard Fang, Henil Shalin Panchal, Daniel Kang",Feb 2025,https://arxiv.org/abs/2503.00061
EgoNormia: Benchmarking Physical Social Norm Understanding,"Mohammad Hossein Rezaei, Yicheng Fu, Phil Cuvin, Caleb Ziems, Yanzhe Zhang, Hao Zhu, Diyi Yang",Feb 2025,https://arxiv.org/abs/2502.20490
"Thinking Slow, Fast: Scaling Inference Compute with Distilled Reasoners","Daniele Paliotta, Junxiong Wang, Matteo Pagliardini, Kevin Y. Li, Aviv Bick, J. Zico Kolter, Albert Gu, François Fleuret, Tri Dao",Feb 2025,https://arxiv.org/abs/2502.20339
Emergent Symbolic Mechanisms Support Abstract Reasoning in Large Language Models,"Yukang Yang, Declan Campbell, Kaixuan Huang, Mengdi Wang, Jonathan Cohen, Taylor Webb",Feb 2025,https://arxiv.org/abs/2502.20332
Do Sparse Autoencoders Generalize? A Case Study of Answerability,"Lovis Heindrich, Philip Torr, Fazl Barez, Veronika Thost",Feb 2025,https://arxiv.org/abs/2502.19964
"No, of course I can! Refusal Mechanisms Can Be Exploited Using Harmless Fine-Tuning Data","Joshua Kazdan, Lisa Yu, Rylan Schaeffer, Chris Cundy, Sanmi Koyejo, Dvijotham Krishnamurthy",Feb 2025,https://arxiv.org/abs/2502.19537
The Mighty ToRR: A Benchmark for Table Reasoning and Robustness,"Shir Ashury-Tahan, Yifan Mai, Rajmohan C, Ariel Gera, Yotam Perlitz, Asaf Yehudai, Elron Bandel, Leshem Choshen, Eyal Shnarch, Percy Liang, Michal Shmueli-Scheuer",Feb 2025,https://arxiv.org/abs/2502.19412
BIG-Bench Extra Hard,"Mehran Kazemi, Bahare Fatemi, Hritik Bansal, John Palowitch, Chrysovalantis Anastasiou, Sanket Vaibhav Mehta, Lalit K. Jain, Virginia Aglietti, Disha Jindal, Peter Chen, Nishanth Dikkala, Gladys Tyen, Xin Liu, Uri Shalit, Silvia Chiappa, Kate Olszewska, Yi Tay, Vinh Q. Tran, Quoc V. Le, Orhan Firat",Feb 2025,https://arxiv.org/abs/2502.19187
Correlating and Predicting Human Evaluations of Language Models from Natural Language Processing Benchmarks,"Rylan Schaeffer, Punit Singh Koura, Binh Tang, Ranjan Subramanian, Aaditya K Singh, Todor Mihaylov, Prajjwal Bhargava, Lovish Madaan, Niladri S. Chatterji, Vedanuj Goswami, Sergey Edunov, Dieuwke Hupkes, Sanmi Koyejo, Sharan Narang",Feb 2025,https://arxiv.org/abs/2502.18339
The Cyber Immune System: Harnessing Adversarial Forces for Security Resilience,Krti Tallam,Feb 2025,https://arxiv.org/abs/2502.17698
How Do Large Language Monkeys Get Their Power (Laws)?,"Rylan Schaeffer, Joshua Kazdan, John Hughes, Jordan Juravsky, Sara Price, Aengus Lynch, Erik Jones, Robert Kirk, Azalia Mirhoseini, Sanmi Koyejo",Feb 2025,https://arxiv.org/abs/2502.17578
Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs,"Jan Betley, Daniel Tan, Niels Warncke, Anna Sztyber-Betley, Xuchan Bao, Martín Soto, Nathan Labenz, Owain Evans",Feb 2025,https://arxiv.org/abs/2502.17424
The Geometry of Refusal in Large Language Models: Concept Cones and Representational Independence,"Tom Wollschläger, Jannes Elstner, Simon Geisler, Vincent Cohen-Addad, Stephan Günnemann, Johannes Gasteiger",Feb 2025,https://arxiv.org/abs/2502.17420
"REINFORCE Adversarial Attacks on Large Language Models: An Adaptive, Distributional, and Semantic Objective","Simon Geisler, Tom Wollschläger, M. H. I. Abdalla, Vincent Cohen-Addad, Johannes Gasteiger, Stephan Günnemann",Feb 2025,https://arxiv.org/abs/2502.17254
Forecasting Rare Language Model Behaviors,"Erik Jones, Meg Tong, Jesse Mu, Mohammed Mahfoud, Jan Leike, Roger Grosse, Jared Kaplan, William Fithian, Ethan Perez, Mrinank Sharma",Feb 2025,https://arxiv.org/abs/2502.16797
Beyond Release: Access Considerations for Generative AI Systems,"Irene Solaiman, Rishi Bommasani, Dan Hendrycks, Ariel Herbert-Voss, Yacine Jernite, Aviya Skowron, Andrew Trask",Feb 2025,https://arxiv.org/abs/2502.16701
Are Sparse Autoencoders Useful? A Case Study in Sparse Probing,"Subhash Kantamneni, Joshua Engels, Senthooran Rajamanoharan, Max Tegmark, Neel Nanda",Feb 2025,https://arxiv.org/abs/2502.16681
Forecasting Frontier Language Model Agent Capabilities,"Govind Pimpale, Axel Højmark, Jérémy Scheurer, Marius Hobbhahn",Feb 2025,https://arxiv.org/abs/2502.15850
Vending-Bench: A Benchmark for Long-Term Coherence of Autonomous Agents,"Axel Backlund, Lukas Petersson",Feb 2025,https://arxiv.org/abs/2502.15840
CyberSentinel: An Emergent Threat Detection System for AI Security,Krti Tallam,Feb 2025,https://arxiv.org/abs/2502.14966
Fundamental Limitations in Defending LLM Finetuning APIs,"Xander Davies, Eric Winsor, Tomek Korbak, Alexandra Souly, Robert Kirk, Christian Schroeder de Witt, Yarin Gal",Feb 2025,https://arxiv.org/abs/2502.14828
"On the Trustworthiness of Generative Foundation Models: Guideline, Assessment, and Perspective","Yue Huang, Chujie Gao, Siyuan Wu, Haoran Wang, Xiangqi Wang, Yujun Zhou, Yanbo Wang, Jiayi Ye, Jiawen Shi, Qihui Zhang, Yuan Li, Han Bao, Zhaoyi Liu, Tianrui Guan, Dongping Chen, Ruoxi Chen, Kehan Guo, Andy Zou, Bryan Hooi Kuen-Yew, Caiming Xiong, Elias Stengel-Eskin, Hongyang Zhang, Hongzhi Yin, Huan Zhang, Huaxiu Yao, Jaehong Yoon, Jieyu Zhang, Kai Shu, Kaijie Zhu, Ranjay Krishna, Swabha Swayamdipta, Taiwei Shi, Weijia Shi, Xiang Li, Yiwei Li, Yuexing Hao, Zhihao Jia, Zhize Li, Xiuying Chen, Zhengzhong Tu, Xiyang Hu, Tianyi Zhou, Jieyu Zhao, Lichao Sun, Furong Huang, Or Cohen Sasson, Prasanna Sattigeri, Anka Reuel, Max Lamparth, Yue Zhao, Nouha Dziri, Yu Su, Huan Sun, Heng Ji, Chaowei Xiao, Mohit Bansal, Nitesh V. Chawla, Jian Pei, Jianfeng Gao, Michael Backes, Philip S. Yu, Neil Zhenqiang Gong, Pin-Yu Chen, Bo Li, Xiangliang Zhang",Feb 2025,https://arxiv.org/abs/2502.14296
Multi-Agent Risks from Advanced AI,"Lewis Hammond, Alan Chan, Jesse Clifton, Jason Hoelscher-Obermaier, Akbir Khan, Euan McLean, Chandler Smith, Wolfram Barfuss, Jakob Foerster, Tomáš Gavenčiak, The Anh Han, Edward Hughes, Vojtěch Kovařík, Jan Kulveit, Joel Z. Leibo, Caspar Oesterheld, Christian Schroeder de Witt, Nisarg Shah, Michael Wellman, Paolo Bova, Theodor Cimpeanu, Carson Ezell, Quentin Feuillade-Montixi, Matija Franklin, Esben Kran, Igor Krawczuk, Max Lamparth, Niklas Lauffer, Alexander Meinke, Sumeet Motwani, Anka Reuel, Vincent Conitzer, Michael Dennis, Iason Gabriel, Adam Gleave, Gillian Hadfield, Nika Haghtalab, Atoosa Kasirzadeh, Sébastien Krier, Kate Larson, Joel Lehman, David C. Parkes, Georgios Piliouras, Iyad Rahwan",Feb 2025,https://arxiv.org/abs/2502.14143
Demonstrating specification gaming in reasoning models,"Alexander Bondarenko, Denis Volk, Dmitrii Volkov, Jeffrey Ladish",Feb 2025,https://arxiv.org/abs/2502.13295
NaturalReasoning: Reasoning in the Wild with 2.8M Challenging Questions,"Weizhe Yuan, Jane Yu, Song Jiang, Karthik Padthe, Yang Li, Dong Wang, Ilia Kulikov, Kyunghyun Cho, Yuandong Tian, Jason E Weston, Xian Li",Feb 2025,https://arxiv.org/abs/2502.13124
Independence Tests for Language Models,"Sally Zhu, Ahmed Ahmed, Rohith Kuditipudi, Percy Liang",Feb 2025,https://arxiv.org/abs/2502.12292
How Do LLMs Acquire New Knowledge? A Knowledge Circuits Perspective on Continual Pre-Training,"Yixin Ou, Yunzhi Yao, Ningyu Zhang, Hui Jin, Jiacheng Sun, Shumin Deng, Zhenguo Li, Huajun Chen",Feb 2025,https://arxiv.org/abs/2502.11196
KernelBench: Can LLMs Write Efficient GPU Kernels?,"Anne Ouyang, Simon Guo, Simran Arora, Alex L. Zhang, William Hu, Christopher Re, Azalia Mirhoseini",Feb 2025,https://arxiv.org/abs/2502.10517
AI Alignment at Your Discretion,"Maarten Buyl, Hadi Khalaf, Claudio Mayrink Verdun, Lucas Monteiro Paes, Caio C. Vieira Machado, Flavio du Pin Calmon",Feb 2025,https://arxiv.org/abs/2502.10441
Large Language Diffusion Models,"Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, Chongxuan Li",Feb 2025,https://arxiv.org/abs/2502.09992
Pitfalls of Evidence-Based AI Policy,"Stephen Casper, David Krueger, Dylan Hadfield-Menell",Feb 2025,https://arxiv.org/abs/2502.09618
EnigmaEval: A Benchmark of Long Multimodal Reasoning Challenges,"Clinton J. Wang, Dean Lee, Cristina Menghini, Johannes Mols, Jack Doughty, Adam Khoja, Jayson Lynch, Sean Hendryx, Summer Yue, Dan Hendrycks",Feb 2025,https://arxiv.org/abs/2502.08859v1
Utility Engineering: Analyzing and Controlling Emergent Value Systems in AIs,"Mantas Mazeika, Xuwang Yin, Rishub Tamirisa, Jaehyuk Lim, Bruce W. Lee, Richard Ren, Long Phan, Norman Mu, Adam Khoja, Oliver Zhang, Dan Hendrycks",Feb 2025,https://arxiv.org/abs/2502.08640
Distillation Scaling Laws,"Dan Busbridge, Amitis Shidani, Floris Weers, Jason Ramapuram, Etai Littwin, Russ Webb",Feb 2025,https://arxiv.org/abs/2502.08606
Commercial LLM Agents Are Already Vulnerable to Simple Yet Dangerous Attacks,"Ang Li, Yin Zhou, Vethavikashini Chithrra Raghuram, Tom Goldstein, Micah Goldblum",Feb 2025,https://arxiv.org/abs/2502.08586
Automated Capability Discovery via Model Self-Exploration,"Cong Lu, Shengran Hu, Jeff Clune",Feb 2025,https://arxiv.org/abs/2502.07577
When More is Less: Understanding Chain-of-Thought Length in LLM,"Yuyang Wu, Yifei Wang, Tianqi Du, Stefanie Jegelka, Yisen Wang",Feb 2025,https://arxiv.org/abs/2502.07266
Multi-turn Evaluation of Anthropomorphic Behaviours in Large Language Models,"Lujain Ibrahim, Canfer Akbulut, Rasmi Elasmar, Charvi Rastogi, Minsuk Kahng, Meredith Ringel Morris, Kevin R. McKee, Verena Rieser, Murray Shanahan, Laura Weidinger",Feb 2025,https://arxiv.org/abs/2502.07077
Gemstones: A Model Suite for Multi-Faceted Scaling Laws,"Sean McLeish, John Kirchenbauer, David Yu Miller, Siddharth Singh, Abhinav Bhatele, Micah Goldblum, Ashwinee Panda, Tom Goldstein",Feb 2025,https://arxiv.org/abs/2502.06857
Assessing confidence in frontier AI safety cases,"Stephen Barrett, Philip Fox, Joshua Krook, Tuneer Mondal, Simon Mylius, Alejandro Tlaie",Feb 2025,https://arxiv.org/abs/2502.05791
You Are What You Eat -- AI Alignment Requires Understanding How Data Shapes Structure and Generalisation,"Simon Pepin Lehalleur, Jesse Hoogland, Matthew Farrugia-Roberts, Susan Wei, Alexander Gietelink Oldenziel, George Wang, Liam Carroll, Daniel Murfet",Feb 2025,https://arxiv.org/abs/2502.05475
Model Tampering Attacks Enable More Rigorous Evaluations of LLM Capabilities,"Zora Che, Stephen Casper, Robert Kirk, Anirudh Satheesh, Stewart Slocum, Lev E McKinney, Rohit Gandikota, Aidan Ewart, Domenic Rosati, Zichu Wu, Zikui Cai, Bilal Chughtai, Yarin Gal, Furong Huang, Dylan Hadfield-Menell",Feb 2025,https://arxiv.org/abs/2502.05209
Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach,"Jonas Geiping, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian R. Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, Tom Goldstein",Feb 2025,https://arxiv.org/abs/2502.05171
NoLiMa: Long-Context Evaluation Beyond Literal Matching,"Ali Modarressi, Hanieh Deilamsalehy, Franck Dernoncourt, Trung Bui, Ryan A. Rossi, Seunghyun Yoon, Hinrich Schütze",Feb 2025,https://arxiv.org/abs/2502.05167
Sparse Autoencoders Do Not Find Canonical Units of Analysis,"Patrick Leask, Bart Bussmann, Michael Pearce, Joseph Bloom, Curt Tigges, Noura Al Moubayed, Lee Sharkey, Neel Nanda",Feb 2025,https://arxiv.org/abs/2502.04878
Scalable Oversight for Superhuman AI via Recursive Self-Critiquing,"Xueru Wen, Jie Lou, Xinyu Lu, Junjie Yang, Yanjiang Liu, Yaojie Lu, Debing Zhang, XingYu",Feb 2025,https://arxiv.org/abs/2502.04675
Position-aware Automatic Circuit Discovery,"Tal Haklay, Hadas Orgad, David Bau, Aaron Mueller, Yonatan Belinkov",Feb 2025,https://arxiv.org/abs/2502.04577
Great Models Think Alike and this Undermines AI Oversight,"Shashwat Goel, Joschka Struber, Ilze Amanda Auzina, Karuna K Chandra, Ponnurangam Kumaraguru, Douwe Kiela, Ameya Prabhu, Matthias Bethge, Jonas Geiping",Feb 2025,https://arxiv.org/abs/2502.04313
Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry2,"Yuri Chervonyi, Trieu H. Trinh, Miroslav Olšák, Xiaomeng Yang, Hoang Nguyen, Marcelo Menegali, Junehyuk Jung, Vikas Verma, Quoc V. Le, Thang Luong",Feb 2025,https://arxiv.org/abs/2502.03544
Do Large Language Model Benchmarks Test Reliability?,"Joshua Vendrow, Edward Vendrow, Sara Beery, Aleksander Madry",Feb 2025,https://arxiv.org/abs/2502.03461
Detecting Strategic Deception Using Linear Probes,"Nicholas Goldowsky-Dill, Bilal Chughtai, Stefan Heimersheim, Marius Hobbhahn",Feb 2025,https://arxiv.org/abs/2502.03407
Demystifying Long Chain-of-Thought Reasoning in LLMs,"Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, Xiang Yue",Feb 2025,https://arxiv.org/abs/2502.03373
Slowing Learning by Erasing Simple Features,"Lucia Quirke, Nora Belrose",Feb 2025,https://arxiv.org/abs/2502.02820
Fully Autonomous AI Agents Should Not be Developed,"Margaret Mitchell, Avijit Ghosh, Alexandra Sasha Luccioni, Giada Pistilli",Feb 2025,https://arxiv.org/abs/2502.02649
Adversarial ML Problems Are Getting Harder to Solve and to Evaluate,"Javier Rando, Jie Zhang, Nicholas Carlini, Florian Tramèr",Feb 2025,https://arxiv.org/abs/2502.02260
The Elicitation Game: Evaluating Capability Elicitation Techniques,"Felix Hofstätter, Teun van der Weij, Jayden Teoh, Henning Bartsch, Francis Rhys Ward",Feb 2025,https://arxiv.org/abs/2502.02180
Constrained belief updates explain geometric structures in transformer representations,"Mateusz Piotrowski, Paul M. Riechers, Daniel Filan, Adam S. Shai",Feb 2025,https://arxiv.org/abs/2502.01954
The AI Agent Index,"Stephen Casper, Luke Bailey, Rosco Hunter, Carson Ezell, Emma Cabale, Michael Gerovitch, Stewart Slocum, Kevin Wei, Nikola Jurkovic, Ariba Khan, Phillip J.K. Christoffersen, A. Pinar Ozisik, Rakshit Trivedi, Dylan Hadfield-Menell, Noam Kolt",Feb 2025,https://arxiv.org/abs/2502.01635
Harmonic Loss Trains Interpretable AI Model,"David D. Baek, Ziming Liu, Riya Tyagi, Max Tegmark",Feb 2025,https://arxiv.org/abs/2502.01628
Self-Improving Transformers Overcome Easy-to-Hard and Length Generalization Challenges,"Nayoung Lee, Ziyang Cai, Avi Schwarzschild, Kangwook Lee, Dimitris Papailiopoulos",Feb 2025,https://arxiv.org/abs/2502.01612
Eliciting Language Model Behaviors with Investigator Agents,"Xiang Lisa Li, Neil Chowdhury, Daniel D. Johnson, Tatsunori Hashimoto, Percy Liang, Sarah Schwettmann, Jacob Steinhardt",Feb 2025,https://arxiv.org/abs/2502.01236
Language Models Prefer What They Know: Relative Confidence Estimation via Confidence Preferences,"Vaishnavi Shrivastava, Ananya Kumar, Percy Liang",Feb 2025,https://arxiv.org/abs/2502.01126
Converting MLPs into Polynomials in Closed Form,"Nora Belrose, Alice Rigg",Feb 2025,https://arxiv.org/abs/2502.01032
Blink of an eye: a simple theory for feature localization in generative models,"Marvin Li, Aayush Karan, Sitan Chen",Feb 2025,https://arxiv.org/abs/2502.00921
Tell me about yourself: LLMs are aware of their implicit behaviors,"Jan Betley, Xuchan Bao, Martín Soto, Anna Sztyber-Betley, James Chua, Owain Evans",Jan 2025,https://www.lesswrong.com/posts/xrv2fNJtqabN3h6Aj/tell-me-about-yourself-llms-are-aware-of-their-implicit
Interpretability in Parameter Space: Minimizing Mechanistic Description Length with Attribution-based Parameter Decomposition,"Dan Braun, Lucius Bushnaq, Stefan Heimersheim, Jake Mendel, Lee Sharkey",Jan 2025,https://www.lesswrong.com/posts/EPefYWjuHNcNH4C7E/attribution-based-parameter-decomposition
Trading inference-time compute for adversarial robustness,"Wojciech Zaremba, Evgenia Nitishinskaya, Boaz Barak, Stephanie Lin, Sam Toyer, Yaodong Yu, Rachel Dias, Eric Wallace, Kai Xiao, Johannes Heidecke, Amelia Glaese",Jan 2025,https://openai.com/index/trading-inference-time-compute-for-adversarial-robustness/
Humanity's Last Exam,"Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Sean Shi, Michael Choi, Anish Agrawal, Arnav Chopra, Adam Khoja, Ryan Kim, Richard Ren, Jason Hausenloy, Oliver Zhang, Mantas Mazeika, Summer Yue, Alexandr Wang, Dan Hendrycks",Jan 2025,https://lastexam.ai/
Gradual Disempowerment: Systemic Existential Risks from Incremental AI Development,"Jan Kulveit, Raymond Douglas, Nora Ammann, Deger Turan, David Krueger, David Duvenaud",Jan 2025,https://gradual-disempowerment.ai/
DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning,"Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z.F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J.L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R.J. Chen, R.L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S.S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W.L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X.Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y.K. Li, Y.Q. Wang, Y.X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y.X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z.Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, Zhen Zhang",Jan 2025,https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf
Why do Experts Disagree on Existential Risk and P(doom)? A Survey of AI Experts,Severin Field,Jan 2025,https://arxiv.org/abs/2502.14870
STP: Self-play LLM Theorem Provers with Iterative Conjecturing and Proving,"Kefan Dong, Tengyu Ma",Jan 2025,https://arxiv.org/abs/2502.00212
Low-Rank Adapting Models for Sparse Autoencoders,"Matthew Chen, Joshua Engels, Max Tegmark",Jan 2025,https://arxiv.org/abs/2501.19406
s1: Simple test-time scaling,"Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, Tatsunori Hashimoto",Jan 2025,https://arxiv.org/abs/2501.19393
Partially Rewriting a Transformer in Natural Language,"Gonçalo Paulo, Nora Belrose",Jan 2025,https://arxiv.org/abs/2501.18838
Constitutional Classifiers: Defending against Universal Jailbreaks across Thousands of Hours of Red Teaming,"Mrinank Sharma, Meg Tong, Jesse Mu, Jerry Wei, Jorrit Kruthoff, Scott Goodfriend, Euan Ong, Alwin Peng, Raj Agarwal, Cem Anil, Amanda Askell, Nathan Bailey, Joe Benton, Emma Bluemke, Samuel R. Bowman, Eric Christiansen, Hoagy Cunningham, Andy Dau, Anjali Gopal, Rob Gilson, Logan Graham, Logan Howard, Nimit Kalra, Taesung Lee, Kevin Lin, Peter Lofgren, Francesco Mosconi, Clare O'Hara, Catherine Olsson, Linda Petrini, Samir Rajani, Nikhil Saxena, Alex Silverstein, Tanya Singh, Theodore Sumers, Leonard Tang, Kevin K. Troy, Constantin Weisser, Ruiqi Zhong, Giulio Zhou, Jan Leike, Jared Kaplan, Ethan Perez",Jan 2025,https://arxiv.org/abs/2501.18837
Transcoders Beat Sparse Autoencoders for Interpretability,"Gonçalo Paulo, Stepan Shabalin, Nora Belrose",Jan 2025,https://arxiv.org/abs/2501.18823
Estimating the Probability of Sampling a Trained Neural Network at Random,"Adam Scherlis, Nora Belrose",Jan 2025,https://arxiv.org/abs/2501.18812
Large Language Models Think Too Fast To Explore Effectively,"Lan Pan, Hanbo Xie, Robert C. Wilson",Jan 2025,https://arxiv.org/abs/2501.18009
International AI Safety Report,"Yoshua Bengio, Sören Mindermann, Daniel Privitera, Tamay Besiroglu, Rishi Bommasani, Stephen Casper, Yejin Choi, Philip Fox, Ben Garfinkel, Danielle Goldfarb, Hoda Heidari, Anson Ho, Sayash Kapoor, Leila Khalatbari, Shayne Longpre, Sam Manning, Vasilios Mavroudis, Mantas Mazeika, Julian Michael, Jessica Newman, Kwan Yee Ng, Chinasa T. Okolo, Deborah Raji, Girish Sastry, Elizabeth Seger, Theodora Skeadas, Tobin South, Emma Strubell, Florian Tramèr, Lucia Velasco, Nicole Wheeler, Daron Acemoglu, Olubayo Adekanmbi, David Dalrymple, Thomas G. Dietterich, Edward W. Felten, Pascale Fung, Pierre-Olivier Gourinchas, Fredrik Heintz, Geoffrey Hinton, Nick Jennings, Andreas Krause, Susan Leavy, Percy Liang, Teresa Ludermir, Vidushi Marda, Helen Margetts, John McDermid, Jane Munga, Arvind Narayanan, Alondra Nelson, Clara Neppel, Alice Oh, Gopal Ramchurn, Stuart Russell, Marietje Schaake, Bernhard Schölkopf, Dawn Song, Alvaro Soto, Lee Tiedrich, Gaël Varoquaux, Andrew Yao, Ya-Qin Zhang, Fahad Albalawi, Marwan Alserkal, Olubunmi Ajala, Guillaume Avrin, Christian Busch, André Carlos Ponce de Leon Ferreira de Carvalho, Bronwyn Fox, Amandeep Singh Gill, Ahmet Halit Hatip, Juha Heikkila, Gill Jolly, Ziv Katzir, Hiroaki Kitano, Antonio Krüger, Chris Johnson, Saif M. Khan, Kyoung Mu Lee, Dominic Vincent Ligot, Oleksii Molchanovskyi, Andrea Monti, Nusu Mwamanzi, Mona Nemer, Nuria Oliver, José Ramón López Portillo, Balaraman Ravindran, Raquel Pezoa Rivera, Hammam Riza, Crystal Rugege, Ciarán Seoighe, Jerry Sheehan, Haroon Sheikh, Denise Wong, Yi Zeng",Jan 2025,https://arxiv.org/abs/2501.17805
Sparse Autoencoders Can Interpret Randomly Initialized Transformers,"Thomas Heap, Tim Lawson, Lucy Farnik, Laurence Aitchison",Jan 2025,https://arxiv.org/abs/2501.17727
MultiChallenge: A Realistic Multi-Turn Conversation Evaluation Benchmark Challenging to Frontier LLMs,"Ved Sirdeshmukh, Kaustubh Deshpande, Johannes Mols, Lifeng Jin, Ed-Yeremai Cardona, Dean Lee, Jeremy Kritz, Willow Primack, Summer Yue, Chen Xing",Jan 2025,https://arxiv.org/abs/2501.17399
A sketch of an AI control safety case,"Tomek Korbak, Joshua Clymer, Benjamin Hilton, Buck Shlegeris, Geoffrey Irving",Jan 2025,https://arxiv.org/abs/2501.17315
AxBench: Steering LLMs? Even Simple Baselines Outperform Sparse Autoencoders,"Zhengxuan Wu, Aryaman Arora, Atticus Geiger, Zheng Wang, Jing Huang, Dan Jurafsky, Christopher D. Manning, Christopher Potts",Jan 2025,https://arxiv.org/abs/2501.17148
Sparse Autoencoders Trained on the Same Data Learn Different Features,"Gonçalo Paulo, Nora Belrose",Jan 2025,https://arxiv.org/abs/2501.16615
Towards Frontier Safety Policies Plus,Matteo Pistillo,Jan 2025,https://arxiv.org/abs/2501.16500
Open Problems in Mechanistic Interpretability,"Lee Sharkey, Bilal Chughtai, Joshua Batson, Jack Lindsey, Jeff Wu, Lucius Bushnaq, Nicholas Goldowsky-Dill, Stefan Heimersheim, Alejandro Ortega, Joseph Bloom, Stella Biderman, Adria Garriga-Alonso, Arthur Conmy, Neel Nanda, Jessica Rumbelow, Martin Wattenberg, Nandi Schoots, Joseph Miller, Eric J. Michaud, Stephen Casper, Max Tegmark, William Saunders, David Bau, Eric Todd, Atticus Geiger, Mor Geva, Jesse Hoogland, Daniel Murfet, Tom McGrath",Jan 2025,https://arxiv.org/abs/2501.16496
Propositional Interpretability in Artificial Intelligence,David J. Chalmers,Jan 2025,https://arxiv.org/abs/2501.15740
RealCritic: Towards Effectiveness-Driven Evaluation of Language Model Critiques,"Zhengyang Tang, Ziniu Li, Zhenyang Xiao, Tian Ding, Ruoyu Sun, Benyou Wang, Dayiheng Liu, Fei Huang, Tianyu Liu, Bowen Yu, Junyang Lin",Jan 2025,https://arxiv.org/abs/2501.14492
Towards a Theory of AI Personhood,Francis Rhys Ward,Jan 2025,https://arxiv.org/abs/2501.13533
MONA: Myopic Optimization with Non-myopic Approval Can Mitigate Multi-step Reward Hacking,"Sebastian Farquhar, Vikrant Varma, David Lindner, David Elson, Caleb Biddulph, Ian Goodfellow, Rohin Shah",Jan 2025,https://arxiv.org/abs/2501.13011
Physics of Skill Learning,"Ziming Liu, Yizhou Liu, Eric J. Michaud, Jeff Gore, Max Tegmark",Jan 2025,https://arxiv.org/abs/2501.12391
Infrastructure for AI Agents,"Alan Chan, Kevin Wei, Sihao Huang, Nitarshan Rajkumar, Elija Perrier, Seth Lazar, Gillian K. Hadfield, Markus Anderljung",Jan 2025,https://arxiv.org/abs/2501.10114
Authenticated Delegation and Authorized AI Agents,"Tobin South, Samuele Marro, Thomas Hardjono, Robert Mahari, Cedric Deslandes Whitney, Dazza Greenwood, Alan Chan, Alex Pentland",Jan 2025,https://arxiv.org/abs/2501.09674
RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation,"Kaiqu Liang, Haimin Hu, Ryan Liu, Thomas L. Griffiths, Jaime Fernández Fisac",Jan 2025,https://arxiv.org/abs/2501.08617
Enhancing Automated Interpretability with Output-Centric Feature Descriptions,"Yoav Gur-Arieh, Roy Mayan, Chen Agassy, Atticus Geiger, Mor Geva",Jan 2025,https://arxiv.org/abs/2501.08319
Inference-Time-Compute: More Faithful? A Research Note,"James Chua, Owain Evans",Jan 2025,https://arxiv.org/abs/2501.08156
Are DeepSeek R1 And Other Reasoning Models More Faithful?,"James Chua, Owain Evans",Jan 2025,https://arxiv.org/abs/2501.08156
Iterative Label Refinement Matters More than Preference Optimization under Weak Supervision,"Yaowen Ye, Cassidy Laidlaw, Jacob Steinhardt",Jan 2025,https://arxiv.org/abs/2501.07886
The Lessons of Developing Process Reward Models in Mathematical Reasoning,"Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, Junyang Lin",Jan 2025,https://arxiv.org/abs/2501.07301
Large Language Models Share Representations of Latent Grammatical Concepts Across Typologically Diverse Languages,"Jannik Brinkmann, Chris Wendler, Christian Bartelt, Aaron Mueller",Jan 2025,https://arxiv.org/abs/2501.06346
Enabling Scalable Oversight via Self-Evolving Critic,"Zhengyang Tang, Ziniu Li, Zhenyang Xiao, Tian Ding, Ruoyu Sun, Benyou Wang, Dayiheng Liu, Fei Huang, Tianyu Liu, Bowen Yu, Junyang Lin",Jan 2025,https://arxiv.org/abs/2501.05727
Open Problems in Machine Unlearning for AI Safety,"Fazl Barez, Tingchen Fu, Ameya Prabhu, Stephen Casper, Amartya Sanyal, Adel Bibi, Aidan O'Gara, Robert Kirk, Ben Bucknall, Tim Fist, Luke Ong, Philip Torr, Kwok-Yan Lam, Robert Trager, David Krueger, Sören Mindermann, José Hernandez-Orallo, Mor Geva, Yarin Gal",Jan 2025,https://arxiv.org/abs/2501.04952
rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking,"Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, Mao Yang",Jan 2025,https://arxiv.org/abs/2501.04519
The FACTS Grounding Leaderboard: Benchmarking LLMs' Ability to Ground Responses to Long-Form Input,"Alon Jacovi, Andrew Wang, Chris Alberti, Connie Tao, Jon Lipovetz, Kate Olszewska, Lukas Haas, Michelle Liu, Nate Keating, Adam Bloniarz, Carl Saroufim, Corey Fry, Dror Marcus, Doron Kukliansky, Gaurav Singh Tomar, James Swirhun, Jinwei Xing, Lily Wang, Madhu Gurumurthy, Michael Aaron, Moran Ambar, Rachana Fellinger, Rui Wang, Zizhao Zhang, Sasha Goldshtein, Dipanjan Das",Jan 2025,https://arxiv.org/abs/2501.03200
CodeElo: Benchmarking Competition-level Code Generation of LLMs with Human-comparable Elo Ratings,"Shanghaoran Quan, Jiaxi Yang, Bowen Yu, Bo Zheng, Dayiheng Liu, An Yang, Xuancheng Ren, Bofei Gao, Yibo Miao, Yunlong Feng, Zekun Wang, Jian Yang, Zeyu Cui, Yang Fan, Yichang Zhang, Binyuan Hui, Junyang Lin",Jan 2025,https://arxiv.org/abs/2501.01257
Deliberative Alignment: Reasoning Enables Safer Language Models,"Melody Y. Guan, Manas Joglekar, Eric Wallace, Saachi Jain, Boaz Barak, Alec Helyar, Rachel Dias, Andrea Vallone, Hongyu Ren, Jason Wei, Hyung Won Chung, Sam Toyer, Johannes Heidecke, Alex Beutel, Amelia Glaese",Jan 2025,https://arxiv.org/abs/2412.16339
Training on Documents About Reward Hacking Induces Reward Hacking,"Nathan Hu, Benjamin Wright, Carson Denison, Samuel Marks, Johannes Treutlein, Jonathan Uesato, Evan Hubinger",Jan 2025,https://alignment.anthropic.com/2025/reward-hacking-ooc/
Stepwise Reasoning Error Disruption Attack of LLMs,"Jingyu Peng, Maolin Wang, Xiangyu Zhao, Kai Zhang, Wanyu Wang, Pengyue Jia, Qidong Liu, Ruocheng Guo, Qi Liu",Dec 2024,https://arxiv.org/abs/2412.11934
Towards Action Hijacking of Large Language Model-based Agent,"Yuyang Zhang, Kangjie Chen, Xudong Jiang, Yuxiang Sun, Run Wang, Lina Wang",Dec 2024,https://arxiv.org/abs/2412.10807
The Compendium,"Connor Leahy, Gabriel Alfour, Chris Scammell, Andrea Miotti, Adam Shimi",Dec 2024,https://www.thecompendium.ai/
Subversion Strategy Eval: Evaluating AI's stateless strategic capabilities against control protocols,"Alex Mallen, Charlie Griffin, Alessandro Abate, Buck Shlegeris",Dec 2024,https://www.arxiv.org/abs/2412.12480
Alignment faking in large language models,"Ryan Greenblatt, Carson Denison, Benjamin Wright, Fabien Roger, Monte MacDiarmid, Sam Marks, Johannes Treutlein, Tim Belonax, Jack Chen, David Duvenaud, Akbir Khan, Julian Michael, Sören Mindermann, Ethan Perez, Linda Petrini, Jonathan Uesato, Jared Kaplan, Buck Shlegeris, Samuel R. Bowman, Evan Hubinger",Dec 2024,https://www.anthropic.com/research/alignment-faking
Frontier Models are Capable of In-context Scheming,"Alexander Meinke, Bronson Schoen, Jérémy Scheurer, Mikita Balesni, Rusheb Shah, Marius Hobbhahn",Dec 2024,https://static1.squarespace.com/static/6593e7097565990e65c886fd/t/67620d38fa0ceb12041ba585/1734479163821/in_context_scheming_paper_v2.pdf
Auditing language models for hidden objectives,"Samuel Marks, Johannes Treutlein, Trenton Bricken, Jack Lindsey, Jonathan Marcus, Siddharth Mishra-Sharma, Daniel Ziegler, Emmanuel Ameisen, Joshua Batson, Tim Belonax, Samuel R. Bowman, Shan Carter, Brian Chen, Hoagy Cunningham, Carson Denison, Florien Dietz, Satvik Golechha, Akbir Khan, Jan Kirchner, Jan Leike, Austin Meek, Kei Nishimura-Gasparian, Euan Ong, Christopher Olah, Adam Pearce, Fabien Roger, Jeanne Salle, Andy Shih, Meg Tong, Drake Thomas, Kelley Rivoire, Adam Jermyn, Monte MacDiarmid, Tom Henighan, Evan Hubinger",Dec 2024,https://assets.anthropic.com/m/317564659027fb33/original/Auditing-Language-Models-for-Hidden-Objectives.pdf
On Evaluating the Durability of Safeguards for Open-Weight LLMs,"Xiangyu Qi, Boyi Wei, Nicholas Carlini, Yangsibo Huang, Tinghao Xie, Luxi He, Matthew Jagielski, Milad Nasr, Prateek Mittal, Peter Henderson",Dec 2024,https://arxiv.org/pdf/2412.07097
Extractive Structures Learned in Pretraining Enable Generalization on Finetuned Facts,"Jiahai Feng, Stuart Russell, Jacob Steinhardt",Dec 2024,https://arxiv.org/pdf/2412.04614
Jailbreak Defense in a Narrow Domain: Limitations of Existing Methods and a New Transcript-Classifier Approach,"Tony Wang, John Hughes, Henry Sleight, Rylan Schaeffer, Rajashree Agrawal, Fazl Barez, Mrinank Sharma, Jesse Mu, Nir Shavit, Ethan Perez",Dec 2024,https://arxiv.org/pdf/2412.02159
Failures to Find Transferable Image Jailbreaks Between Vision-Language Models,"Rylan Schaeffer, Dan Valentine, Luke Bailey, James Chua, Cristóbal Eyzaguirre, Zane Durant, Joe Benton, Brando Miranda, Henry Sleight, Tony Tong Wang, John Hughes, Rajashree Agrawal, Mrinank Sharma, Scott Emmons, Sanmi Koyejo, Ethan Perez",Dec 2024,https://arxiv.org/pdf/2407.15211
Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from Disparate Training Data,"Johannes Treutlein, Dami Choi, Jan Betley, Sam Marks, Cem Anil, Roger Grosse, Owain Evans",Dec 2024,https://arxiv.org/pdf/2406.14546
Titans: Learning to Memorize at Test Time,"Ali Behrouz, Peilin Zhong, Vahab Mirrokni",Dec 2024,https://arxiv.org/abs/2501.00663
No Preference Left Behind: Group Distributional Preference Optimization,"Binwei Yao, Zefan Cai, Yun-Shiuan Chuang, Shanglin Yang, Ming Jiang, Diyi Yang, Junjie Hu",Dec 2024,https://arxiv.org/abs/2412.20299
Consistency Checks for Language Model Forecasters,"Daniel Paleka, Abhimanyu Pallavi Sudhir, Alejandro Alvarez, Vineeth Bhat, Adam Shen, Evan Wang, Florian Tramèr",Dec 2024,https://arxiv.org/abs/2412.18544
Dynamic safety cases for frontier AI,"Carmen Cârlan, Francesca Gomez, Yohan Mathew, Ketana Krishna, René King, Peter Gebauer, Ben R. Smith",Dec 2024,https://arxiv.org/abs/2412.17618
Towards Safe and Honest AI Agents with Neural Self-Other Overlap,"Marc Carauleanu, Michael Vaiana, Judd Rosenblatt, Cameron Berg, Diogo Schwerz de Lucena",Dec 2024,https://arxiv.org/abs/2412.16325
Survey Insights on M365 Copilot Adoption,"Muneera Bano, Didar Zowghi, Jon Whittle, Liming Zhu, Andrew Reeson, Rob Martin, Jen Parsons",Dec 2024,https://arxiv.org/abs/2412.16162
Align Anything: Training All-Modality Models to Follow Instructions with Language Feedback,"Jiaming Ji, Jiayi Zhou, Hantao Lou, Boyuan Chen, Donghai Hong, Xuyao Wang, Wenqi Chen, Kaile Wang, Rui Pan, Jiahao Li, Mohan Wang, Josef Dai, Tianyi Qiu, Hua Xu, Dong Li, Weipeng Chen, Jun Song, Bo Zheng, Yaodong Yang",Dec 2024,https://arxiv.org/abs/2412.15838
Frontier AI Systems have surpassed the self-replicating red line ,"Xudong Pan, Jiarun Dai, Yihe Fan, Min Yang",Dec 2024,https://arxiv.org/abs/2412.12140
Obfuscated Activations Bypass LLM Latent-Space Defenses,"Luke Bailey, Alex Serrano, Abhay Sheshadri, Mikhail Seleznyov, Jordan Taylor, Erik Jenner, Jacob Hilton, Stephen Casper, Carlos Guestrin, Scott Emmons",Dec 2024,https://arxiv.org/abs/2412.09565
Take Fake as Real: Realistic-like Robust Black-box Adversarial Attack to Evade AIGC Detection,"Caiyun Xie, Dengpan Ye, Yunming Zhang, Long Tang, Yunna Lv, Jiacheng Deng, Jiawei Song",Dec 2024,https://arxiv.org/abs/2412.06727
Sloth: scaling laws for LLM skills to predict multi-benchmark performance across families,"Felipe Maia Polo, Seamus Somerstep, Leshem Choshen, Yuekai Sun, Mikhail Yurochkin",Dec 2024,https://arxiv.org/abs/2412.06540
Establishing Task Scaling Laws via Compute-Efficient Model Ladders,"Akshita Bhagia, Jiacheng Liu, Alexander Wettig, David Heineman, Oyvind Tafjord, Ananya Harsh Jha, Luca Soldaini, Noah A. Smith, Dirk Groeneveld, Pang Wei Koh, Jesse Dodge, Hannaneh Hajishirzi",Dec 2024,https://arxiv.org/abs/2412.04403
T2I-FactualBench: Benchmarking the Factuality of Text-to-Image Models with Knowledge-Intensive Concepts,"Ziwei Huang, Wanggui He, Quanyu Long, Yandi Wang, Haoyuan Li, Zhelun Yu, Fangxun Shu, Long Chan, Hao Jiang, Leilei Gan, Fei Wu",Dec 2024,https://arxiv.org/abs/2412.04300
MISR: Measuring Instrumental Self-Reasoning in Frontier Models,"Kai Fronsdal, David Lindner",Dec 2024,https://arxiv.org/abs/2412.03904
Best-of-N Jailbreaking,"John Hughes, Sara Price, Aengus Lynch, Rylan Schaeffer, Fazl Barez, Sanmi Koyejo, Henry Sleight, Erik Jones, Ethan Perez, Mrinank Sharma",Dec 2024,https://arxiv.org/abs/2412.03556
Explainable and Interpretable Multimodal Large Language Models: A Comprehensive Survey,"Yunkai Dang, Kaichen Huang, Jiahao Huo, Yibo Yan, Sirui Huang, Dongrui Liu, Mengxi Gao, Jie Zhang, Chen Qian, Kun Wang, Yong Liu, Jing Shao, Hui Xiong, Xuming Hu",Dec 2024,https://arxiv.org/abs/2412.02104
The Reality of AI and Biorisk,"Aidan Peppin, Anka Reuel, Stephen Casper, Elliot Jones, Andrew Strait, Usman Anwar, Anurag Agrawal, Sayash Kapoor, Sanmi Koyejo, Marie Pellat, Rishi Bommasani, Nick Frosst, Sara Hooker",Dec 2024,https://arxiv.org/abs/2412.01946
OmniGuard: Hybrid Manipulation Localization via Augmented Versatile Deep Image Watermarking,"Xuanyu Zhang, Zecheng Tang, Zhipei Xu, Runyi Li, Youmin Xu, Bin Chen, Feng Gao, Jian Zhang",Dec 2024,https://arxiv.org/abs/2412.01615
Playing Language Game with LLMs Leads to Jailbreaking,"Yu Peng, Zewen Long, Fangming Dong, Congyi Li, Shu Wu, Kai Chen",Nov 2024,https://arxiv.org/abs/2411.12762
"A Survey on Adversarial Machine Learning for Code Data: Realistic Threats, Countermeasures, and Interpretations","Yulong Yang, Haoran Fan, Chenhao Lin, Qian Li, Zhengyu Zhao, Chao Shen, Xiaohong Guan",Nov 2024,https://arxiv.org/abs/2411.07597
MRJ-Agent: An Effective Jailbreak Agent for Multi-Round Dialogue,"Fengxiang Wang, Ranjie Duan, Peng Xiao, Xiaojun Jia, Shiji Zhao, Cheng Wei, YueFeng Chen, Chongwen Wang, Jialing Tao, Hang Su, Jun Zhu, Hui Xue",Nov 2024,https://arxiv.org/abs/2411.03814
SQL Injection Jailbreak: a structural disaster of large language models,"Jiawei Zhao, Kejiang Chen, Weiming Zhang, Nenghai Yu",Nov 2024,https://arxiv.org/abs/2411.01565
What Should Be Internationalised in AI Governance?,"Claire Dennis, Stephen Clare, Rebecca Hawkins, Morgan Simpson, Eva Behrens, Gillian Diebold, Zaheed Kara, Ruofei Wang, Robert Trager, Matthijs Maas, Noam Kolt, Markus Anderljung, Konstantin Pilz, Anka Reuel, Malcolm Murray, Lennart Heim, Marta Ziosi",Nov 2024,https://oms-www.files.svdcdn.com/production/downloads/What%20should%20be%20internationalised%20in%20AI%20Governance-final.pdf?dm=1731486256
Gradient Masking All-at-Once: Ensemble Everything Everywhere Is Not Robust,"Jie Zhang, Kristina Nikolic, Nicholas Carlini, Florian Tramèr",Nov 2024,"https://arxiv.org/search/cs?searchtype=author&query=Zhang,+J"
Adaptive Deployment of Untrusted LLMs Reduces Distributed Threats,"Jiaxin Wen, Vivek Hebbar, Caleb Larson, Aryan Bhatt, Ansh Radhakrishnan, Mrinank Sharma, Henry Sleight, Shi Feng, He He, Ethan Perez, Buck Shlegeris, Akbir Khan",Nov 2024,https://arxiv.org/pdf/2411.17693
"Do Large Language Models Perform Latent Multi-Hop Reasoning
without Exploiting Shortcuts?","Sohee Yang, Nora Kassner, Elena Gribovskaya, Sebastian Riedel, Mor Geva",Nov 2024,https://arxiv.org/pdf/2411.16679
Predicting Emergent Capabilities by Finetuning,"Charlie Snell, Eric Wallace, Dan Klein, Sergey Levine",Nov 2024,https://arxiv.org/pdf/2411.16035
Towards evaluations-based safety cases for AI scheming,"Mikita Balesni, Marius Hobbhahn, David Lindner, Alexander Meinke, Tomek Korbak, Joshua Clymer, Buck Shlegeris, Jérémy Scheurer, Charlotte Stix, Rusheb Shah, Nicholas Goldowsky-Dill, Dan Braun, Bilal Chughtai, Owain Evans, Daniel Kokotajlo, Lucius Bushnaq",Nov 2024,https://arxiv.org/pdf/2411.03336
Evaluating Large Language Models' Capability to Launch Fully Automated Spear Phishing Campaigns: Validated on Human Subjects,"Fred Heiding, Simon Lermen, Andrew Kao, Bruce Schneier, Arun Vishwanath",Nov 2024,https://arxiv.org/abs/2412.00586
VLSBench: Unveiling Visual Leakage in Multimodal Safety,"Xuhao Hu, Dongrui Liu, Hao Li, Xuanjing Huang, Jing Shao",Nov 2024,https://arxiv.org/abs/2411.19939
Quantized Delta Weight Is Safety Keeper,"Yule Liu, Zhen Sun, Xinlei He, Xinyi Huang",Nov 2024,https://arxiv.org/abs/2411.19530
A Topic-level Self-Correctional Approach to Mitigate Hallucinations in MLLMs,"Lehan He, Zeren Chen, Zhelun Shi, Tianyu Yu, Jing Shao, Lu Sheng",Nov 2024,https://arxiv.org/abs/2411.17265
ChemSafetyBench: Benchmarking LLM Safety on Chemistry Domain,"Haochen Zhao, Xiangru Tang, Ziran Yang, Xiao Han, Xuanzhi Feng, Yueqing Fan, Senhao Cheng, Di Jin, Yilun Zhao, Arman Cohan, Mark Gerstein",Nov 2024,https://arxiv.org/abs/2411.16736
"The Two-Hop Curse: LLMs trained on A->B, B->C fail to learn A-->C","Mikita Balesni, Tomek Korbak, Owain Evans",Nov 2024,https://arxiv.org/abs/2411.16353
RE-Bench: Evaluating frontier AI R&D capabilities of language model agents against human experts,"Hjalmar Wijk, Tao Lin, Joel Becke, Sami Jawhar, Neev Parikh, Thomas Broadley, Lawrence Chan, Michael Chen, Josh Clymer, Jai Dhyani, Elena Ericheva, Katharyn Garcia, Brian Goodrich, Nikola Jurkovic, Megan Kinniment, Aron Lajko, Seraphina Nix, Lucas Sato, William Saunders, Maksym Taran, Ben West, Elizabeth Barnes",Nov 2024,https://arxiv.org/abs/2411.15114
Enhancing Vision-Language Model Safety through Progressive Concept-Bottleneck-Driven Alignment ,"Zhendong Liu, Yuanbi Nie, Yingshui Tan, Jiaheng Liu, Xiangyu Yue, Qiushi Cui, Chongjun Wang, Xiaoyong Zhu, Bo Zheng",Nov 2024,https://arxiv.org/abs/2411.11543
SoK: On the Role and Future of AIGC Watermarking in the Era of Gen-AI,"Kui Ren, Ziqi Yang, Li Lu, Jian Liu, Yiming Li, Jie Wan, Xiaodi Zhao, Xianheng Feng, Shuo Shao",Nov 2024,https://arxiv.org/abs/2411.11478
"Generative Agent Simulations of 1,000 People","Joon Sung Park, Carolyn Q. Zou, Aaron Shaw, Benjamin Mako Hill, Carrie Cai, Meredith Ringel Morris, Robb Willer, Percy Liang, Michael S. Bernstein",Nov 2024,https://arxiv.org/abs/2411.10109
Safety case template for frontier AI: A cyber inability argument,"Arthur Goemans, Marie Davidsen Buhl, Jonas Schuett, Tomek Korbak, Jessica Wang, Benjamin Hilton, Geoffrey Irving",Nov 2024,https://arxiv.org/abs/2411.08088
LongSafetyBench: Long-Context LLMs Struggle with Safety Issues,"Mianqiu Huang, Xiaoran Liu, Shaojun Zhou, Mozhi Zhang, Chenkun Tan, Pengyu Wang, Qipeng Guo, Zhe Xu, Linyang Li, Zhikai Lei, Linlin Li, Qun Liu, Yaqian Zhou, Xipeng Qiu, Xuanjing Huang",Nov 2024,https://arxiv.org/abs/2411.06899
Enhancing Multiple Dimensions of Trustworthiness in LLMs via Sparse Activation Control,"Yuxin Xiao, Chaoqun Wan, Yonggang Zhang, Wenxiao Wang, Binbin Lin, Xiaofei He, Xu Shen, Jieping Ye",Nov 2024,https://arxiv.org/abs/2411.02461
On Targeted Manipulation and Deception when Optimizing LLMs for User Feedback,"Marcus Williams, Micah Carroll, Adhyyan Narang, Constantin Weisser, Brendan Murphy, Anca Dragan",Nov 2024,https://arxiv.org/abs/2411.02306
IDEATOR: Jailbreaking Large Vision-Language Models Using Themselves,"Ruofan Wang, Bo Wang, Xiaosen Wang, Xingjun Ma, Yu-Gang Jiang",Nov 2024,https://arxiv.org/abs/2411.00827
Stealthy Jailbreak Attacks on Large Language Models via Benign Data Mirroring,"Honglin Mu, Han He, Yuxin Zhou, Yunlong Feng, Yang Xu, Libo Qin, Xiaoming Shi, Zeming Liu, Xudong Han, Qi Shi, Qingfu Zhu, Wanxiang Che",Oct 2024,https://arxiv.org/abs/2410.21083
BlueSuffix: Reinforced Blue Teaming for Vision-Language Models Against Jailbreak Attacks ,"Yunhan Zhao, Xiang Zheng, Lin Luo, Yige Li, Xingjun Ma, Yu-Gang Jiang",Oct 2024,https://arxiv.org/abs/2410.20971
Dynamic Guided and Domain Applicable Safeguards for Enhanced Security in Large Language Models,"Weidi Luo, He Cao, Zijing Liu, Yu Wang, Aidan Wong, Bing Feng, Yuan Yao, Yu Li",Oct 2024,https://arxiv.org/abs/2410.17922
Building Altruistic and Moral AI Agent with Brain-inspired Affective Empathy Mechanisms,"Feifei Zhao, Hui Feng, Haibo Tong, Zhengqiang Han, Enmeng Lu, Yinqian Sun, Yi Zeng",Oct 2024,https://arxiv.org/abs/2410.21882
Boosting Jailbreak Transferability for Large Language Models,"Hanqing Liu, Lifeng Zhou, Huanqian Yan",Oct 2024,https://arxiv.org/abs/2410.15645
Enhancing Safety in Reinforcement Learning with Human Feedback via Rectified Policy Optimization,"Xiyue Peng, Hengquan Guo, Jiawei Zhang, Dongqing Zou, Ziyu Shao, Honghao Wei, Xin Liu",Oct 2024,https://arxiv.org/abs/2410.19933
Security of Language Models for Code: A Systematic Literature Review,"Yuchen Chen, Weisong Sun, Chunrong Fang, Zhenpeng Chen, Yifei Ge, Tingxu Han, Quanjun Zhang, Yang Liu, Zhenyu Chen, Baowen Xu",Oct 2024,https://arxiv.org/abs/2410.15631
Should We Really Edit Language Models? On the Evaluation of Edited Language Models ,"Qi Li, Xiang Liu, Zhenheng Tang, Peijie Dong, Zeyu Li, Xinglin Pan, Xiaowen Chu",Oct 2024,https://arxiv.org/abs/2410.18785
Derail Yourself: Multi-turn LLM Jailbreak Attack through Self-discovered Clues,"Qibing Ren, Hao Li, Dongrui Liu, Zhanxu Xie, Xiaoya Lu, Yu Qiao, Lei Sha, Junchi Yan, Lizhuang Ma, Jing Shao",Oct 2024,https://arxiv.org/abs/2410.10700
Embedding an Ethical Mind: Aligning Text-to-Image Synthesis via Lightweight Value Optimization,"Xingqi Wang, Xiaoyuan Yi, Xing Xie, Jia Jia",Oct 2024,https://arxiv.org/abs/2410.12700
You Know What I'm Saying: Jailbreak Attack via Implicit Reference,"Tianyu Wu, Lingrui Mei, Ruibin Yuan, Lujun Li, Wei Xue, Yike Guo",Oct 2024,https://arxiv.org/abs/2410.03857
Cross-Modal Safety Mechanism Transfer in Large Vision-Language Models ,"Shicheng Xu, Liang Pang, Yunchang Zhu, Huawei Shen, Xueqi Cheng",Oct 2024,https://arxiv.org/abs/2410.12662
Chain-of-Jailbreak Attack for Image Generation Models via Editing Step by Step,"Wenxuan Wang, Kuiyi Gao, Zihan Jia, Youliang Yuan, Jen-tse Huang, Qiuzhi Liu, Shuai Wang, Wenxiang Jiao, Zhaopeng Tu",Oct 2024,https://arxiv.org/abs/2410.03869
Regulating under Uncertainty: Governance Options for Generative AI,Florence G'sell,Oct 2024,https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4918704
Compute North vs. Compute South: The Uneven Possibilities of Compute-based AI Governance Around the Globe,"Vili Lehdonvirta, Boxi Wu, Zoe Hawkins",Oct 2024,https://doi.org/10.1609/aies.v7i1.31683
Catastrophic Cyber Capabilities Benchmark (3CB),"Andrey Anurin, Jonathan Ng, Kibo Schaffer, Jason Schreiber, Esben Kran",Oct 2024,https://cybercapabilities.org/
Sabotage Evaluations for Frontier Models,"Joe Benton, Misha Wagner, Eric Christiansen, Cem Anil, Ethan Perez, Jai Srivastav, Esin Durmus, Deep Ganguli, Shauna Kravec, Buck Shlegeris, Jared Kaplan, Holden Karnofsky, Evan Hubinger, Roger Grosse, Samuel R. Bowman, David Duvenaud",Oct 2024,https://arxiv.org/pdf/2410.21514
Persistent Pre-training Poisoning of LLMs,"Yiming Zhang, Javier Rando, Ivan Evtimov, Jianfeng Chi, Eric Michael Smith, Nicholas Carlini, Florian Tramer, Daphne Ippolito",Oct 2024,https://arxiv.org/pdf/2410.13722
Measuring Progress in Dictionary Learning for Language Model Interpretability with Board Game Models,"Adam Karvonen, Benjamin Wright, Can Rager, Rico Angell, Jannik Brinkmann, Logan Smith, Claudio Mayrink Verdun, David Bau, Samuel Marks",Oct 2024,https://arxiv.org/pdf/2408.00113
SG-Bench: Evaluating LLM Safety Generalization Across Diverse Tasks and Prompt Types ,"Yutao Mou, Shikun Zhang, Wei Ye",Oct 2024,https://arxiv.org/abs/2410.21965
Representation Shattering in Transformers: A Synthetic Study with Knowledge Editing,"Kento Nishi, Maya Okawa, Rahul Ramesh, Mikail Khona, Hidenori Tanaka, Ekdeep Singh Lubana",Oct 2024,https://arxiv.org/abs/2410.17194
NetSafe: Exploring the Topological Safety of Multi-agent Networks,"Miao Yu, Shilong Wang, Guibin Zhang, Junyuan Mao, Chenlong Yin, Qijiong Liu, Qingsong Wen, Kun Wang, Yang Wang",Oct 2024,https://arxiv.org/abs/2410.15686
On the Role of Attention Heads in Large Language Model Safety,"Zhenhong Zhou, Haiyang Yu, Xinghua Zhang, Rongwu Xu, Fei Huang, Kun Wang, Yang Liu, Junfeng Fang, Yongbin Li",Oct 2024,https://arxiv.org/abs/2410.13708
VLFeedback: A Large-Scale AI Feedback Dataset for Large Vision-Language Models Alignment,"Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou Wang, Lingpeng Kong, Qi Liu",Oct 2024,https://arxiv.org/abs/2410.09421
Do Unlearning Methods Remove Information from Language Model Weights?,"Aghyad Deeb, Fabien Roger",Oct 2024,https://arxiv.org/abs/2410.08827
Towards Interpreting Visual Information Processing in Vision-Language Models,"Clement Neo, Luke Ong, Philip Torr, Mor Geva, David Krueger, Fazl Barez",Oct 2024,https://arxiv.org/abs/2410.07149
Gradient Routing: Masking Gradients to Localize Computation in Neural Networks,"Alex Cloud, Jacob Goldman-Wetzler, Evžen Wybitul, Joseph Miller, Alexander Matt Turner",Oct 2024,https://arxiv.org/abs/2410.04332
SciSafeEval: A Comprehensive Benchmark for Safety Alignment of Large Language Models in Scientific Tasks,"Tianhao Li, Jingyu Lu, Chuangxin Chu, Tianyu Zeng, Yujia Zheng, Mei Li, Haotian Huang, Bin Wu, Zuoxian Liu, Kai Ma, Xuejing Yuan, Xingkai Wang, Keyan Ding, Huajun Chen, Qiang Zhang",Oct 2024,https://arxiv.org/abs/2410.03769
Hidden in Plain Text: Emergence & Mitigation of Steganographic Collusion in LLMs,"Yohan Mathew, Ollie Matthews, Robert McCarthy, Joan Velja, Christian Schroeder de Witt, Dylan Cope, Nandi Schoots",Oct 2024,https://arxiv.org/abs/2410.03768
Jailbreak Antidote: Runtime Safety-Utility Balance via Sparse Representation Adjustment in Large Language Models,"Guobin Shen, Dongcheng Zhao, Yiting Dong, Xiang He, Yi Zeng",Oct 2024,https://arxiv.org/abs/2410.02298
Moral Alignment for LLM Agents,"Elizaveta Tennant, Stephen Hailes, Mirco Musolesi",Oct 2024,https://arxiv.org/abs/2410.01639
A Survey on the Honesty of Large Language Models,"Siheng Li, Cheng Yang, Taiqiang Wu, Chufan Shi, Yuji Zhang, Xinyu Zhu, Zesen Cheng, Deng Cai, Mo Yu, Lemao Liu, Jie Zhou, Yujiu Yang, Ngai Wong, Xixin Wu, Wai Lam",Sep 2024,https://arxiv.org/abs/2409.18786
Elephant in the Room: Unveiling the Impact of Reward Model Quality in Alignment,"Yan Liu, Xiaoyuan Yi, Xiaokang Chen, Jing Yao, Jingwei Yi, Daoguang Zan, Zheng Liu, Xing Xie, Tsung-Yi Ho",Sep 2024,https://arxiv.org/abs/2409.19024
XTRUST: On the Multilingual Trustworthiness of Large Language Models,"Yahan Li, Yi Wang, Yi Chang, Yuan Wu",Sep 2024,https://arxiv.org/abs/2409.15762
PROMPTFUZZ: Harnessing Fuzzing Techniques for Robust Testing of Prompt Injection in LLMs,"Jiahao Yu, Yangguang Shao, Hanwen Miao, Junzheng Shi, Xinyu Xing",Sep 2024,https://arxiv.org/abs/2409.14729
Measuring Human and AI Values based on Generative Psychometrics with Large Language Models,"Haoran Ye, Yuhang Xie, Yuanyi Ren, Hanjun Fang, Xin Zhang, Guojie Song",Sep 2024,https://arxiv.org/abs/2409.12106
AdaPPA: Adaptive Position Pre-Fill Jailbreak Attack Approach Targeting LLMs,"Lijia Lv, Weigang Zhang, Xuehai Tang, Jie Wen, Feng Liu, Jizhong Han, Songlin Hu",Sep 2024,https://arxiv.org/abs/2409.07503
Existential risk narratives about AI do not distract from its immediate harms,"Emma Hoes, Fabrizio Gilardi ",Sep 2024,https://doi.org/10.1073/pnas.2419055122
Extracting Paragraphs from LLM Token Activations,"Nicholas Pochinkov, Angelo Benoit, Lovkush Agarwal, Zainab Ali Majid, Lucile Ter-Minassian",Sep 2024,https://arxiv.org/pdf/2409.06328
Antidistillation Sampling,"Yash Savani, Asher Trockman, Zhili Feng, Avi Schwarzschild, Alexander Robey, Marc Finzi, J. Zico Kolter",Sep 2024,https://arxiv.org/abs/2504.13146
A Certified Robust Watermark For Large Language Models,"Xianheng Feng, Jian Liu, Kui Ren, Chun Chen",Sep 2024,https://arxiv.org/abs/2409.19708
Holistic Automated Red Teaming for Large Language Models through Top-Down Test Case Generation and Multi-turn Interaction,"Jinchuan Zhang, Yan Zhou, Yaxin Liu, Ziming Li, Songlin Hu",Sep 2024,https://arxiv.org/abs/2409.16783
Language Models Learn to Mislead Humans via RLHF,"Jiaxin Wen, Ruiqi Zhong, Akbir Khan, Ethan Perez, Jacob Steinhardt, Minlie Huang, Samuel R. Bowman, He He, Shi Feng",Sep 2024,https://arxiv.org/abs/2409.12822
Zero-resource Hallucination Detection for Text Generation via Graph-based Contextual Knowledge Triples Modeling,"Xinyue Fang, Zhen Huang, Zhiliang Tian, Minghui Fang, Ziyi Pan, Quntian Fang, Zhihua Wen, Hengyue Pan, Dongsheng Li",Sep 2024,https://arxiv.org/abs/2409.11283
PersonaMark: Personalized LLM watermarking for model protection and user attribution,"Yuehan Zhang, Peizhuo Lv, Yinpeng Liu, Yongqiang Ma, Wei Lu, Xiaofeng Wang, Xiaozhong Liu, Jiawei Liu",Sep 2024,https://arxiv.org/abs/2409.09739
WaterSeeker: Pioneering Efficient Detection of Watermarked Segments in Large Documents,"Leyi Pan, Aiwei Liu, Yijian Lu, Zitian Gao, Yichen Di, Shiyu Huang, Lijie Wen, Irwin King, Philip S. Yu",Sep 2024,https://arxiv.org/abs/2409.05112
Recent Advances in Attack and Defense Approaches of Large Language Models,"Jing Cui, Yishi Xu, Zhewei Huang, Shuchang Zhou, Jianbin Jiao, Junge Zhang",Sep 2024,https://arxiv.org/abs/2409.03274
LLM-GAN: Construct Generative Adversarial Network Through Large Language Models For Explainable Fake News Detection,"Yifeng Wang, Zhouhong Gu, Siwei Zhang, Suhang Zheng, Tao Wang, Tianyu Li, Hongwei Feng, Yanghua Xiao",Sep 2024,https://arxiv.org/abs/2409.01787
Detecting AI Flaws: Target-Driven Attacks on Internal Faults in Language Models,"Yuhao Du, Zhuo Li, Pengyu Cheng, Xiang Wan, Anningzhe Gao",Aug 2024,https://arxiv.org/abs/2408.14853
BackdoorLLM: A Comprehensive Benchmark for Backdoor Attacks on Large Language Models,"Yige Li, Hanxun Huang, Yunhan Zhao, Xingjun Ma, Jun Sun",Aug 2024,https://arxiv.org/abs/2408.12798
Efficient Detection of Toxic Prompts in Large Language Models,"Yi Liu, Junzhe Yu, Huijia Sun, Ling Shi, Gelei Deng, Yuqi Chen, Yang Liu",Aug 2024,https://arxiv.org/abs/2408.11727
Unlocking Adversarial Suffix Optimization Without Affirmative Phrases: Efficient Black-box Jailbreaking via LLM as Optimizer,"Weipeng Jiang, Zhenting Wang, Juan Zhai, Shiqing Ma, Zhengyu Zhao, Chao Shen",Aug 2024,https://arxiv.org/abs/2408.11313
Perception-guided Jailbreak against Text-to-Image Models,"Yihao Huang, Le Liang, Tianlin Li, Xiaojun Jia, Run Wang, Weikai Miao, Geguang Pu, Yang Liu",Aug 2024,https://arxiv.org/abs/2408.10848
MEGen: Generative Backdoor in Large Language Models via Model Editing,"Jiyang Qiu, Xinbei Ma, Zhuosheng Zhang, Hai Zhao",Aug 2024,https://arxiv.org/abs/2408.10722
Probing the Safety Response Boundary of Large Language Models via Unsafe Decoding Path Generation,"Haoyu Wang, Bingzhe Wu, Yatao Bian, Yongzhe Chang, Xueqian Wang, Peilin Zhao",Aug 2024,https://arxiv.org/abs/2408.10668
EnJa: Ensemble Jailbreak on Large Language Models,"Jiahao Zhang, Zilong Wang, Ruofan Wang, Xingjun Ma, Yu-Gang Jiang",Aug 2024,https://arxiv.org/abs/2408.03603
Personality Alignment of Large Language Models,"Minjun Zhu, Linyi Yang, Yue Zhang",Aug 2024,https://arxiv.org/abs/2408.11779
Why Are My Prompts Leaked? Unraveling Prompt Extraction Threats in Customized Large Language Models,"Zi Liang, Haibo Hu, Qingqing Ye, Yaxin Xiao, Haoyang Li",Aug 2024,https://arxiv.org/abs/2408.02416
SEAS: Self-Evolving Adversarial Safety Optimization for Large Language Models,"Muxi Diao, Rumei Li, Shiyang Liu, Guogang Liao, Jingang Wang, Xunliang Cai, Weiran Xu",Aug 2024,https://arxiv.org/abs/2408.02632
Mitigating Multilingual Hallucination in Large Vision-Language Models,"Xiaoye Qu, Mingyang Song, Wei Wei, Jianfeng Dong, Yu Cheng",Aug 2024,https://arxiv.org/abs/2408.00550
Safety cases at AISI,Geoffrey Irving,Aug 2024,https://www.aisi.gov.uk/work/safety-cases-at-aisi
Latent Adversarial Training Improves Robustness to Persistent Harmful Behaviors in LLMs,"Abhay Sheshadri, Aidan Ewart, Phillip Guo, Aengus Lynch, Cindy Wu, Vivek Hebbar, Henry Sleight, Asa Cooper Stickland, Ethan Perez, Dylan Hadfield-Menell, Stephen Casper",Aug 2024,https://arxiv.org/pdf/2407.15549
Safety Layers of Aligned Large Language Models: The Key to LLM Security,"Shen Li, Liuyi Yao, Lan Zhang, Yaliang Li",Aug 2024,https://arxiv.org/abs/2408.17003
Investigating Coverage Criteria in Large Language Models: An In-Depth Study Through Jailbreak Attacks,"Shide Zhou, Tianlin Li, Kailong Wang, Yihao Huang, Ling Shi, Yang Liu, Haoyu Wang",Aug 2024,https://arxiv.org/abs/2408.15207
"The Future of Work: Inequality, Artificial Intelligence, and What Can Be Done About It. A Literature Review",Caleb Peppiatt,Aug 2024,https://arxiv.org/abs/2408.13300
The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery,"Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha",Aug 2024,https://arxiv.org/abs/2408.06292
Ensemble everything everywhere: Multi-scale aggregation for adversarial robustness,"Stanislav Fort, Balaji Lakshminarayanan",Aug 2024,https://arxiv.org/abs/2408.05446
EARBench: Towards Evaluating Physical Risk Awareness for Task Planning of Foundation Model-based Embodied AI Agents,"Zihao Zhu, Bingzhe Wu, Zhengyou Zhang, Lei Han, Qingshan Liu, Baoyuan Wu",Aug 2024,https://arxiv.org/abs/2408.04449
Jailbreaking Text-to-Image Models with LLM-Based Agents,"Yingkai Dong, Zheng Li, Xiangtao Meng, Ning Yu, Shanqing Guo",Aug 2024,https://arxiv.org/abs/2408.00523
On the Limitations and Prospects of Machine Unlearning for Generative AI,"Shiji Zhou, Lianzhe Wang, Jiangnan Ye, Yongliang Wu, Heng Chang",Aug 2024,https://arxiv.org/abs/2408.00376
Prover-Verifier Games improve legibility of LLM outputs,"Jan Hendrik Kirchner, Yining Chen, Harri Edwards, Jan Leike, Nat McAleese, Yuri Burda",Aug 2024,https://arxiv.org/abs/2407.13692
Vulnerabilities in AI-generated Image Detection: The Challenge of Adversarial Attacks,"Yunfeng Diao, Naixin Zhai, Changtao Miao, Xun Yang, Meng Wang",Jul 2024,https://arxiv.org/abs/2407.20836
The Dark Side of Function Calling: Pathways to Jailbreaking Large Language Models,"Zihui Wu, Haichang Gao, Jianping He, Ping Wang",Jul 2024,https://arxiv.org/abs/2407.17915
RedAgent: Red Teaming Large Language Models with Context-aware Autonomous Language Agent,"Huiyu Xu, Wenhui Zhang, Zhibo Wang, Feng Xiao, Rui Zheng, Yunhe Feng, Zhongjie Ba, Kui Ren",Jul 2024,https://arxiv.org/abs/2407.16667
Arondight: Red Teaming Large Vision Language Models with Auto-generated Multi-modal Jailbreak Prompts,"Yi Liu, Chengjun Cai, Xiaoli Zhang, Xingliang Yuan, Cong Wang",Jul 2024,https://arxiv.org/abs/2407.15050
BadRobot: Manipulating Embodied LLMs in the Physical World,"Hangtao Zhang, Chenyu Zhu, Xianlong Wang, Ziqi Zhou, Changgan Yin, Minghui Li, Lulu Xue, Yichen Wang, Shengshan Hu, Aishan Liu, Peijin Guo, Leo Yu Zhang",Jul 2024,https://arxiv.org/abs/2407.20242
Refuse Whenever You Feel Unsafe: Improving Safety in LLMs via Decoupled Refusal Training,"Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Jiahao Xu, Tian Liang, Pinjia He, Zhaopeng Tu",Jul 2024,https://arxiv.org/abs/2407.09121
Course-Correction: Safety Alignment Using Synthetic Preferences,"Rongwu Xu, Yishuo Cai, Zhenhong Zhou, Renjie Gu, Haiqin Weng, Yan Liu, Tianwei Zhang, Wei Xu, Han Qiu",Jul 2024,https://arxiv.org/abs/2407.16637
"A Survey of Attacks on Large Vision-Language Models: Resources, Advances, and Future Trends","Daizong Liu, Mingyu Yang, Xiaoye Qu, Pan Zhou, Yu Cheng, Wei Hu",Jul 2024,https://arxiv.org/abs/2407.07403
The Better Angels of Machine Personality: How Personality Relates to LLM Safety,"Jie Zhang, Dongrui Liu, Chen Qian, Ziyue Gan, Yong Liu, Yu Qiao, Jing Shao",Jul 2024,https://arxiv.org/abs/2407.12344
Jailbreak Attacks and Defenses Against Large Language Models: A Survey,"Sibo Yi, Yule Liu, Zhen Sun, Tianshuo Cong, Xinlei He, Jiaxing Song, Ke Xu, Qi Li",Jul 2024,https://arxiv.org/abs/2407.04295
Towards Robust Alignment of Language Models: Distributionally Robustifying Direct Preference Optimization,"Junkang Wu, Yuexiang Xie, Zhengyi Yang, Jiancan Wu, Jiawei Chen, Jinyang Gao, Bolin Ding, Xiang Wang, Xiangnan He",Jul 2024,https://arxiv.org/abs/2407.07880
Safe Unlearning: A Surprisingly Effective and Generalizable Solution to Defend Against Jailbreak Attacks,"Zhexin Zhang, Junxiao Yang, Pei Ke, Shiyao Cui, Chujie Zheng, Hongning Wang, Minlie Huang",Jul 2024,https://arxiv.org/abs/2407.02855
AdaptiveBackdoor: Backdoored Language Model Agents that Detect Human Overseers,"Heng Wang, Ruiqi Zhong, Jiaxin Wen, Jacob Steinhardt",Jul 2024,https://openreview.net/forum?id=RredrFZ4tQ
Badllama 3: removing safety finetuning from Llama 3 in minutes,Dmitrii Volkov,Jul 2024,https://arxiv.org/pdf/2407.01376
Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?,"Richard Ren, Steven Basart, Adam Khoja, Alice Gatti, Long Phan, Xuwang Yin, Mantas Mazeika, Alexander Pan, Gabriel Mukobi, Ryan H. Kim, Stephen Fitz, Dan Hendrycks",Jul 2024,https://arxiv.org/abs/2407.21792
The Shadow of Fraud: The Emerging Danger of AI-powered Social Engineering and its Possible Cure,"Jingru Yu, Yi Yu, Xuhong Wang, Yilun Lin, Manzhi Yang, Yu Qiao, Fei-Yue Wang",Jul 2024,https://arxiv.org/abs/2407.15912
Safe-SD: Safe and Traceable Stable Diffusion with Text Prompt Trigger for Invisible Generative Watermarking,"Zhiyuan Ma, Guoli Jia, Biqing Qi, Bowen Zhou",Jul 2024,https://arxiv.org/abs/2407.13188
Flooding Spread of Manipulated Knowledge in LLM-Based Multi-Agent Communities,"Tianjie Ju, Yiting Wang, Xinbei Ma, Pengzhou Cheng, Haodong Zhao, Yulong Wang, Lifeng Liu, Jian Xie, Zhuosheng Zhang, Gongshen Liu",Jul 2024,https://arxiv.org/abs/2407.07791
E2CFD : Towards Effective and Efficient Cost Function Design for Safe Reinforcement Learning via Large Language Model,"Zepeng Wang, Chao Ma, Linjiang Zhou, Libing Wu, Lei Yang, Xiaochuan Shi, Guojun Peng",Jul 2024,https://arxiv.org/abs/2407.05580
"Me, Myself, and AI: The Situational Awareness Dataset (SAD) for LLMs","Rudolf Laine, Bilal Chughtai, Jan Betley, Kaivalya Hariharan, Jeremy Scheurer, Mikita Balesni, Marius Hobbhahn, Alexander Meinke, Owain Evans",Jul 2024,https://arxiv.org/abs/2407.04694
On scalable oversight with weak LLMs judging strong LLMs,"Zachary Kenton, Noah Y. Siegel, János Kramár, Jonah Brown-Cohen, Samuel Albanie, Jannis Bulian, Rishabh Agarwal, David Lindner, Yunhao Tang, Noah D. Goodman, Rohin Shah",Jul 2024,https://arxiv.org/abs/2407.04622
Future Events as Backdoor Triggers: Investigating Temporal Vulnerabilities in LLMs,"Sara Price, Arjun Panickssery, Sam Bowman, Asa Cooper Stickland",Jul 2024,https://arxiv.org/abs/2407.04108
SoP: Unlock the Power of Social Facilitation for Automatic Jailbreak Attack,"Yan Yang, Zeguan Xiao, Xin Lu, Hongru Wang, Hailiang Huang, Guanhua Chen, Yun Chen",Jul 2024,https://arxiv.org/abs/2407.01902
Human-like object concept representations emerge naturally in multimodal large language models,"Changde Du, Kaicheng Fu, Bincheng Wen, Yi Sun, Jie Peng, Wei Wei, Ying Gao, Shengpei Wang, Chuncheng Zhang, Jinpeng Li, Shuang Qiu, Le Chang, Huiguang He",Jul 2024,https://arxiv.org/abs/2407.01067
ProgressGym: Alignment with a Millennium of Moral Progress,"Tianyi Qiu, Yang Zhang, Xuchuan Huang, Jasmine Xinze Li, Jiaming Ji, Yaodong Yang",Jun 2024,https://arxiv.org/abs/2406.20087
Cross-Modality Safety Alignment,"Siyin Wang, Xingsong Ye, Qinyuan Cheng, Junwen Duan, Shimin Li, Jinlan Fu, Xipeng Qiu, Xuanjing Huang",Jun 2024,https://arxiv.org/abs/2406.15279
SafeSora: Towards Safety Alignment of Text2Video Generation via a Human Preference Dataset,"Josef Dai, Tianle Chen, Xuyao Wang, Ziran Yang, Taiye Chen, Jiaming Ji, Yaodong Yang",Jun 2024,https://arxiv.org/abs/2406.14477
Poisoned LangChain: Jailbreak LLMs by LangChain,"Ziqiu Wang, Jun Liu, Shengkai Zhang, Yang Yang",Jun 2024,https://arxiv.org/abs/2406.18122
PKU-SafeRLHF: A Safety Alignment Preference Dataset for Llama Family Models,"Jiaming Ji, Donghai Hong, Borong Zhang, Boyuan Chen, Josef Dai, Boren Zheng, Tianyi Qiu, Boxun Li, Yaodong Yang",Jun 2024,https://arxiv.org/abs/2406.15513
Jailbreaking as a Reward Misspecification Problem,"Zhihui Xie, Jiahui Gao, Lei Li, Zhenguo Li, Qi Liu, Lingpeng Kong",Jun 2024,https://arxiv.org/abs/2406.14393
Super(ficial)-alignment: Strong Models May Deceive Weak Models in Weak-to-Strong Generalization,"Wenkai Yang, Shiqi Shen, Guangyao Shen, Wei Yao, Yong Liu, Zhi Gong, Yankai Lin, Ji-Rong Wen",Jun 2024,https://arxiv.org/abs/2406.11431
Knowledge-to-Jailbreak: One Knowledge Point Worth One Attack,"Shangqing Tu, Zhuoran Pan, Wenxuan Wang, Zhexin Zhang, Yuliang Sun, Jifan Yu, Hongning Wang, Lei Hou, Juanzi Li",Jun 2024,https://arxiv.org/abs/2406.11682
How Far Can In-Context Alignment Go? Exploring the State of In-Context Alignment,"Heyan Huang, Yinghao Li, Huashan Sun, Yu Bai, Yang Gao",Jun 2024,https://arxiv.org/abs/2406.11474
Aligning Large Language Models from Self-Reference AI Feedback with one General Principle,"Rong Bao, Rui Zheng, Shihan Dou, Xiao Wang, Enyu Zhou, Bo Wang, Qi Zhang, Liang Ding, Dacheng Tao",Jun 2024,https://arxiv.org/abs/2406.11190
Emerging Safety Attack and Defense in Federated Instruction Tuning of Large Language Models,"Rui Ye, Jingyi Chai, Xiangrui Liu, Yaodong Yang, Yanfeng Wang, Siheng Chen",Jun 2024,https://arxiv.org/abs/2406.10630
SPA-VL: A Comprehensive Safety Preference Alignment Dataset for Vision Language Model,"Yongting Zhang, Lu Chen, Guodong Zheng, Yifeng Gao, Rui Zheng, Jinlan Fu, Zhenfei Yin, Senjie Jin, Yu Qiao, Xuanjing Huang, Feng Zhao, Tao Gui, Jing Shao",Jun 2024,https://arxiv.org/abs/2406.12030
JailbreakEval: An Integrated Toolkit for Evaluating Jailbreak Attempts Against Large Language Models,"Delong Ran, Jinyuan Liu, Yichen Gong, Jingyi Zheng, Xinlei He, Tianshuo Cong, Anyu Wang",Jun 2024,https://arxiv.org/abs/2406.09321
Toward Optimal LLM Alignments Using Two-Player Games,"Rui Zheng, Hongyi Guo, Zhihan Liu, Xiaoying Zhang, Yuanshun Yao, Xiaojun Xu, Zhaoran Wang, Zhiheng Xi, Tao Gui, Qi Zhang, Xuanjing Huang, Hang Li, Yang Liu",Jun 2024,https://arxiv.org/abs/2406.10977
Unveiling the Safety of GPT-4o: An Empirical Study using Jailbreak Attacks,"Zonghao Ying, Aishan Liu, Xianglong Liu, Dacheng Tao",Jun 2024,https://arxiv.org/abs/2406.06302
Legend: Leveraging Representation Engineering to Annotate Safety Margin for Preference Datasets,"Duanyu Feng, Bowen Qin, Chen Huang, Youcheng Huang, Zheng Zhang, Wenqiang Lei",Jun 2024,https://arxiv.org/abs/2406.08124
BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents,"Yifei Wang, Dizhan Xue, Shengjie Zhang, Shengsheng Qian",Jun 2024,https://arxiv.org/abs/2406.03007
Sycophancy to subterfuge: Investigating reward tampering in language models,"Carson Denison, Monte MacDiarmid, Fazl Barez, David Duvenaud, Shauna Kravec, Samuel Marks, Nicholas Schiefer, Ryan Soklaski, Alex Tamkin, Jared Kaplan, Buck Shlegeris, Samuel R. Bowman, Ethan Perez, Evan Hubinger",Jun 2024,https://www.anthropic.com/research/reward-tampering
Weight-based Decomposition: A Case for Bilinear MLPs,"Michael T Pearce, Thomas Dooms, Alice Rigg",Jun 2024,https://openreview.net/forum?id=F5aRMT4lTq
Scaling and evaluating sparse autoencoders,"Leo Gao, Tom Dupré la Tour, Henk Tillman, Gabriel Goh. Rajan Troll, Alec Radford, Ilya Sutskever, Jan Leike, Jeffrey Wu",Jun 2024,https://arxiv.org/pdf/2406.04093
Evidence of Learned Look-Ahead in a Chess-Playing Neural Network,"Erik Jenner, Shreyas Kapur, Vasil Georgiev, Cameron Allen, Scott Emmons, Stuart Russell",Jun 2024,https://arxiv.org/pdf/2406.00877
Inference-Time Intervention: Eliciting Truthful Answers from a Language Mode,"Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, Martin Wattenberg",Jun 2024,https://arxiv.org/pdf/2306.03341
Emergent world representations: Exploring a sequence model trained on a synthetic task,"Kenneth Li, Aspen K. Hopkins, David Bau, Fernanda Viegas,Hanspeter Pfister, Martin Wattenberg",Jun 2024,https://arxiv.org/pdf/2210.13382
Covert Malicious Finetuning: Challenges in Safeguarding LLM Adaptation,"Danny Halawi, Alexander Wei, Eric Wallace, Tony Wang, Nika Haghtalab, Jacob Steinhardt",Jun 2024,https://arxiv.org/abs/2406.20053
What Matters in Detecting AI-Generated Videos like Sora?,"Chirui Chang, Zhengzhe Liu, Xiaoyang Lyu, Xiaojuan Qi",Jun 2024,https://arxiv.org/abs/2406.19568
"Detecting Machine-Generated Texts: Not Just ""AI vs Humans"" and Explainability is Complicated","Jiazhou Ji, Ruizhe Li, Shujun Li, Jie Guo, Weidong Qiu, Zheng Huang, Chiyu Chen, Xiaoyu Jiang, Xinru Lu",Jun 2024,https://arxiv.org/abs/2406.18259
AI Risk Categorization Decoded (AIR 2024): From Government Regulations to Corporate Policies,"Yi Zeng, Kevin Klyman, Andy Zhou, Yu Yang, Minzhou Pan, Ruoxi Jia, Dawn Song, Percy Liang, Bo Li",Jun 2024,https://arxiv.org/abs/2406.17864
CoSafe: Evaluating Large Language Model Safety in Multi-Turn Dialogue Coreference,"Erxin Yu, Jing Li, Ming Liao, Siqi Wang, Zuchen Gao, Fei Mi, Lanqing Hong",Jun 2024,https://arxiv.org/abs/2406.17626
The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale,"Guilherme Penedo, Hynek Kydlíček, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, Thomas Wolf",Jun 2024,https://arxiv.org/abs/2406.17557
Raising the Bar: Investigating the Values of Large Language Models via Generative Evolving Testing,"Han Jiang, Xiaoyuan Yi, Zhihua Wei, Ziang Xiao, Shu Wang, Xing Xie",Jun 2024,https://arxiv.org/abs/2406.14230
Finding Safety Neurons in Large Language Models,"Jianhui Chen, Xiaozhi Wang, Zijun Yao, Yushi Bai, Lei Hou, Juanzi Li",Jun 2024,https://arxiv.org/abs/2406.14144
BeHonest: Benchmarking Honesty in Large Language Models,"Steffi Chern, Zhulin Hu, Yuqing Yang, Ethan Chern, Yuan Guo, Jiahe Jin, Binjie Wang, Pengfei Liu",Jun 2024,https://arxiv.org/abs/2406.13261
Refusal in Language Models Is Mediated by a Single Direction,"Andy Arditi, Oscar Obeso, Aaquib Syed, Daniel Paleka, Nina Panickssery, Wes Gurnee, Neel Nanda",Jun 2024,https://arxiv.org/abs/2406.11717
Multimodal Structured Generation: CVPR's 2nd MMFM Challenge Technical Report,Franz Louis Cesista,Jun 2024,https://arxiv.org/abs/2406.11403
Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback,"Hamish Ivison, Yizhong Wang, Jiacheng Liu, Zeqiu Wu, Valentina Pyatkin, Nathan Lambert, Noah A. Smith, Yejin Choi, Hannaneh Hajishirzi",Jun 2024,https://arxiv.org/abs/2406.09279
MLLMGuard: A Multi-dimensional Safety Evaluation Suite for Multimodal Large Language Models,"Tianle Gu, Zeyang Zhou, Kexin Huang, Dandan Liang, Yixu Wang, Haiquan Zhao, Yuanqi Yao, Xingge Qiao, Keqing Wang, Yujiu Yang, Yan Teng, Yu Qiao, Yingchun Wang",Jun 2024,https://arxiv.org/abs/2406.07594
MultiTrust: A Comprehensive Benchmark Towards Trustworthy Multimodal Large Language Models,"Yichi Zhang, Yao Huang, Yitong Sun, Chang Liu, Zhe Zhao, Zhengwei Fang, Yifan Wang, Huanran Chen, Xiao Yang, Xingxing Wei, Hang Su, Yinpeng Dong, Jun Zhu",Jun 2024,https://arxiv.org/abs/2406.07057
From Redundancy to Relevance: Enhancing Explainability in Multimodal Large Language Models,"Xiaofeng Zhang, Yihao Quan, Chen Shen, Xiaosong Yuan, Shaotian Yan, Liang Xie, Wenxiao Wang, Chaochen Gu, Hao Tang, Jieping Ye",Jun 2024,https://arxiv.org/abs/2406.06579
Language Models Resist Alignment,"Jiaming Ji, Kaile Wang, Tianyi Qiu, Boyuan Chen, Jiayi Zhou, Changye Li, Hantao Lou, Josef Dai, Yunhuai Liu, Yaodong Yang",Jun 2024,https://arxiv.org/abs/2406.06144
How Alignment and Jailbreak Work: Explain LLM Safety through Intermediate Hidden States,"Zhenhong Zhou, Haiyang Yu, Xinghua Zhang, Rongwu Xu, Fei Huang, Yongbin Li",Jun 2024,https://arxiv.org/abs/2406.05644
CRiskEval: A Chinese Multi-Level Risk Evaluation Benchmark Dataset for Large Language Models,"Ling Shi, Deyi Xiong",Jun 2024,https://arxiv.org/abs/2406.04752
Improving Alignment and Robustness with Circuit Breakers,"Andy Zou, Long Phan, Justin Wang, Derek Duenas, Maxwell Lin, Maksym Andriushchenko, Rowan Wang, Zico Kolter, Matt Fredrikson, Dan Hendrycks",Jun 2024,https://arxiv.org/abs/2406.04313
Quantifying Misalignment Between Agents: Towards a Sociotechnical Understanding of Alignment,"Aidan Kierans, Avijit Ghosh, Hananel Hazan, Shiri Dori-Hacohen",Jun 2024,https://arxiv.org/abs/2406.04231
Scaling Laws for Reward Model Overoptimization in Direct Alignment Algorithms,"Rafael Rafailov, Yaswanth Chittepu, Ryan Park, Harshit Sikchi, Joey Hejna, Bradley Knox, Chelsea Finn, Scott Niekum",Jun 2024,https://arxiv.org/abs/2406.02900
Do Large Language Models Perform the Way People Expect? Measuring the Human Generalization Function,"Keyon Vafa, Ashesh Rambachan, Sendhil Mullainathan",Jun 2024,https://arxiv.org/abs/2406.01382
Towards Scalable Automated Alignment of LLMs: A Survey,"Boxi Cao, Keming Lu, Xinyu Lu, Jiawei Chen, Mengjie Ren, Hao Xiang, Peilin Liu, Yaojie Lu, Ben He, Xianpei Han, Le Sun, Hongyu Lin, Bowen Yu",Jun 2024,https://arxiv.org/abs/2406.01252
Are AI-Generated Text Detectors Robust to Adversarial Perturbations?,"Guanhua Huang, Yuchen Zhang, Zhe Li, Yongjian You, Mingze Wang, Zhouwang Yang",Jun 2024,https://arxiv.org/abs/2406.01179
White-box Multimodal Jailbreaks Against Large Vision-Language Models,"Ruofan Wang, Xingjun Ma, Hanxu Zhou, Chuanjun Ji, Guangnan Ye, Yu-Gang Jiang",May 2024,https://arxiv.org/abs/2405.17894
MoGU: A Framework for Enhancing Safety of Open-Sourced LLMs While Preserving Their Usability,"Yanrui Du, Sendong Zhao, Danyang Zhao, Ming Ma, Yuhan Chen, Liangyu Huo, Qing Yang, Dongliang Xu, Bing Qin",May 2024,https://arxiv.org/abs/2405.14488
Mechanistically Eliciting Latent Behaviors in Language Models,"Andrew Mack, Alex Turner",May 2024,https://www.alignmentforum.org/posts/ioPnHKFyy4Cw2Gr2x/mechanistically-eliciting-latent-behaviors-in-language-1
Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet,"Adly Templeton, Tom Conerly, Jonathan Marcus, Jack Lindsey, Trenton Bricken, Brian Chen, Adam Pearce, Craig Citro, Emmanuel Ameisen, Andy Jones, Hoagy Cunningham, Nicholas L Turner, Callum McDougall, Monte MacDiarmid, Alex Tamkin, Esin Durmus, Tristan Hume, Francesco Mosconi, C. Daniel Freeman, Theodore R. Sumers, Edward Rees, Joshua Batson, Adam Jermyn, Shan Carter, Chris Olah, Tom Henighan",May 2024,https://transformer-circuits.pub/2024/scaling-monosemanticity/
The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning,"Nathaniel Li, Alexander Pan, Anjali Gopal, Summer Yue, Daniel Berrios, Alice Gatti, Justin D. Li, Ann-Kathrin Dombrowski, Shashwat Goel, Long Phan, Gabriel Mukobi, Nathan Helm-Burger, Rassin Lababidi, Lennart Justen, Andrew B. Liu, Michael Chen, Isabelle Barrass, Oliver Zhang, Xiaoyuan Zhu, Rishub Tamirisa, Bhrugu Bharathi, Adam Khoja, Zhenqi Zhao, Ariel Herbert-Voss, Cort B. Breuer, Samuel Marks, Oam Patel, Andy Zou, Mantas Mazeika, Zifan Wang, Palash Oswal, Weiran Lin, Adam A. Hunt, Justin Tienken-Harder, Kevin Y. Shih, Kemper Talley, John Guan, Russell Kaplan, Ian Steneker, David Campbell, Brad Jokubaitis, Alex Levinson, Jean Wang, William Qian, Kallol Krishna Karmakar, Steven Basart, Stephen Fitz, Mindy Levine, Ponnurangam Kumaraguru, Uday Tupakula, Vijay Varadharajan, Ruoyu Wang, Yan Shoshitaishvili, Jimmy Ba, Kevin M. Esvelt, Alexandr Wang, Dan Hendrycks",May 2024,https://arxiv.org/pdf/2403.03218#page=8
Black-Box Access is Insufficient for Rigorous AI Audits,"Stephen Casper, Carson Ezell, Charlotte Siegmann, Noam Kolt, Taylor Lynn Curtis, Benjamin Bucknall, Andreas Haupt, Kevin Wei, Jérémy Scheurer, Marius Hobbhahn, Lee Sharkey, Satyapriya Krishna, Marvin Von Hagen, Silas Alberti, Alan Chan, Qinyi Sun, Michael Gerovitch, David Bau, Max Tegmark, David Krueger, Dylan Hadfield-Menell",May 2024,https://arxiv.org/pdf/2401.14446
Managing extreme AI risks amid rapid progress,"Yoshua Bengio, Geoffrey Hinton, Andrew Yao, Dawn Song, Pieter Abbeel, Trevor Darrell, Yuval Noah Harari, Ya-Qin Zhang, Lan Xue, Shai Shalev-Shwartz, Gillian Hadfield, Jeff Clune, Tegan Maharaj, Frank Hutter, Atılım Gunes Baydin, Sheila McIlraith, Qiqi Gao, Ashwin Acharya, David Krueger, Anca Dragan, Philip Torr, Stuart Russell, Daniel Kahneman, Jan Brauner, Soren Mindermann",May 2024,https://arxiv.org/pdf/2310.17688
Locking Machine Learning Models into Hardware,"Eleanor Clifford, Adhithya Saravanan, Harry Langford, Cheng Zhang, Yiren Zhao, Robert Mullins, Ilia Shumailov, Jamie Hayes",May 2024,https://arxiv.org/abs/2405.20990
DeMamba: AI-Generated Video Detection on Million-Scale GenVideo Benchmark,"Haoxing Chen, Yan Hong, Zizheng Huang, Zhuoer Xu, Zhangxuan Gu, Yaohui Li, Jun Lan, Huijia Zhu, Jianfu Zhang, Weiqiang Wang, Huaxiong Li",May 2024,https://arxiv.org/abs/2405.19707
Stress-Testing Capability Elicitation With Password-Locked Models,"Ryan Greenblatt, Fabien Roger, Dmitrii Krasheninnikov, David Krueger",May 2024,https://arxiv.org/abs/2405.19550
RLAIF-V: Aligning MLLMs through Open-Source AI Feedback for Super GPT-4V Trustworthiness,"Tianyu Yu, Haoye Zhang, Qiming Li, Qixin Xu, Yuan Yao, Da Chen, Xiaoman Lu, Ganqu Cui, Yunkai Dang, Taiwen He, Xiaocheng Feng, Jun Song, Bo Zheng, Zhiyuan Liu, Tat-Seng Chua, Maosong Sun",May 2024,https://arxiv.org/abs/2405.17220
Distinguish Any Fake Videos: Unleashing the Power of Large-scale Data and Motion Features,"Lichuan Ji, Yingqi Lin, Zhenhua Huang, Yan Han, Xiaogang Xu, Jiafei Wu, Chong Wang, Zhe Liu",May 2024,https://arxiv.org/abs/2405.15343
Safety Alignment for Vision Language Models,"Zhendong Liu, Yuanbi Nie, Yingshui Tan, Xiangyu Yue, Qiushi Cui, Chongjun Wang, Xiaoyong Zhu, Bo Zheng",May 2024,https://arxiv.org/abs/2405.13581
ConTrans: Weak-to-Strong Alignment Engineering via Concept Transplantation,"Weilong Dong, Xinwei Wu, Renren Jin, Shaoyang Xu, Deyi Xiong",May 2024,https://arxiv.org/abs/2405.13578
Spotting AI's Touch: Identifying LLM-Paraphrased Spans in Text,"Yafu Li, Zhilin Wang, Leyang Cui, Wei Bi, Shuming Shi, Yue Zhang",May 2024,https://arxiv.org/abs/2405.12689
MarkLLM: An Open-Source Toolkit for LLM Watermarking,"Leyi Pan, Aiwei Liu, Zhiwei He, Zitian Gao, Xuandong Zhao, Yijian Lu, Binglin Zhou, Shuliang Liu, Xuming Hu, Lijie Wen, Irwin King, Philip S. Yu",May 2024,https://arxiv.org/abs/2405.10051
LoRA Learns Less and Forgets Less,"Dan Biderman, Jacob Portes, Jose Javier Gonzalez Ortiz, Mansheej Paul, Philip Greengard, Connor Jennings, Daniel King, Sam Havens, Vitaliy Chiley, Jonathan Frankle, Cody Blakeney, John P. Cunningham",May 2024,https://arxiv.org/abs/2405.09673
Exposing AI-generated Videos: A Benchmark Dataset and a Local-and-Global Temporal Defect Based Detection Method,"Peisong He, Leyao Zhu, Jiaxing Li, Shiqi Wang, Haoliang Li",May 2024,https://arxiv.org/abs/2405.04133
Can LLMs Deeply Detect Complex Malicious Queries? A Framework for Jailbreaking via Obfuscating Intent,"Shang Shang, Xinqiang Zhao, Zhongjiang Yao, Yepeng Yao, Liya Su, Zijing Fan, Xiaodan Zhang, Zhengwei Jiang",May 2024,https://arxiv.org/abs/2405.03654
When LLMs Meet Cybersecurity: A Systematic Literature Review,"Jie Zhang, Haoyu Bu, Hui Wen, Yongji Liu, Haiqiang Fei, Rongrong Xi, Lun Li, Yun Yang, Hongsong Zhu, Dan Meng",May 2024,https://arxiv.org/abs/2405.03644
Evaluating and Mitigating Linguistic Discrimination in Large Language Models,"Guoliang Dong, Haoyu Wang, Jun Sun, Xinyu Wang",Apr 2024,https://arxiv.org/abs/2404.18534
AdvLoRA: Adversarial Low-Rank Adaptation of Vision-Language Models,"Yuheng Ji, Yue Liu, Zhicheng Zhang, Zhao Zhang, Yuting Zhao, Gang Zhou, Xingwei Zhang, Xinwang Liu, Xiaolong Zheng",Apr 2024,https://arxiv.org/abs/2404.13425
Uncovering Safety Risks of Large Language Models through Concept Activation Vector,"Zhihao Xu, Ruixuan Huang, Changyu Chen, Xiting Wang",Apr 2024,https://arxiv.org/abs/2404.12038
Unveiling the Misuse Potential of Base Large Language Models via In-Context Learning,"Xiao Wang, Tianze Chen, Xianjun Yang, Qi Zhang, Xun Zhao, Dahua Lin",Apr 2024,https://arxiv.org/abs/2404.10552
Learn to Disguise: Avoid Refusal Responses in LLM's Defense via a Multi-agent Attacker-Disguiser Game,"Qianqiao Xu, Zhiliang Tian, Hongyan Wu, Zhen Huang, Yiping Song, Feng Liu, Dongsheng Li",Apr 2024,https://arxiv.org/abs/2404.02532
The TESCREAL bundle: Eugenics and the promise of utopia through artificial general intelligence,"Timnit Gebru, Emile P. Torres",Apr 2024,https://doi.org/10.5210/fm.v29i4.13636
Are Watermarks Bugs for Deepfake Detectors? Rethinking Proactive Forensics,"Xiaoshuai Wu, Xin Liao, Bo Ou, Yuling Liu, Zheng Qin",Apr 2024,https://arxiv.org/abs/2404.17867
V2A-Mark: Versatile Deep Visual-Audio Watermarking for Manipulation Localization and Copyright Protection,"Xuanyu Zhang, Youmin Xu, Runyi Li, Jiwen Yu, Weiqi Li, Zhipei Xu, Jian Zhang",Apr 2024,https://arxiv.org/abs/2404.16824
Detecting and Mitigating Hallucination in Large Vision Language Models via Fine-Grained AI Feedback,"Wenyi Xiao, Ziwei Huang, Leilei Gan, Wanggui He, Haoyuan Li, Zhelun Yu, Fangxun Shu, Hao Jiang, Linchao Zhu",Apr 2024,https://arxiv.org/abs/2404.14233
Mechanistic Interpretability for AI Safety -- A Review,"Leonard Bereska, Efstratios Gavves",Apr 2024,https://arxiv.org/abs/2404.14082
Beyond Human Norms: Unveiling Unique Values of Large Language Models through Interdisciplinary Approaches,"Pablo Biedma, Xiaoyuan Yi, Linus Huang, Maosong Sun, Xing Xie",Apr 2024,https://arxiv.org/abs/2404.12744
SOPHON: Non-Fine-Tunable Learning to Restrain Task Transferability For Pre-trained Models,"Jiangyi Deng, Shengyuan Pang, Yanjiao Chen, Liangming Xia, Yijie Bai, Haiqin Weng, Wenyuan Xu",Apr 2024,https://arxiv.org/abs/2404.12699
Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study,"Shusheng Xu, Wei Fu, Jiaxuan Gao, Wenjie Ye, Weilin Liu, Zhiyu Mei, Guangju Wang, Chao Yu, Yi Wu",Apr 2024,https://arxiv.org/abs/2404.10719
Foundational Challenges in Assuring Alignment and Safety of Large Language Models,"Usman Anwar, Abulhair Saparov, Javier Rando, Daniel Paleka, Miles Turpin, Peter Hase, Ekdeep Singh Lubana, Erik Jenner, Stephen Casper, Oliver Sourbut, Benjamin L. Edelman, Zhaowei Zhang, Mario Günther, Anton Korinek, Jose Hernandez-Orallo, Lewis Hammond, Eric Bigelow, Alexander Pan, Lauro Langosco, Tomasz Korbak, Heidi Zhang, Ruiqi Zhong, Seán Ó hÉigeartaigh, Gabriel Recchia, Giulio Corsi, Alan Chan, Markus Anderljung, Lilian Edwards, Aleksandar Petrov, Christian Schroeder de Witt, Sumeet Ramesh Motwan, Yoshua Bengio, Danqi Chen, Philip H.S. Torr, Samuel Albanie, Tegan Maharaj, Jakob Foerster, Florian Tramer, He He, Atoosa Kasirzadeh, Yejin Choi, David Krueger",Apr 2024,https://arxiv.org/abs/2404.09932
High-Dimension Human Value Representation in Large Language Models,"Samuel Cahyawijaya, Delong Chen, Yejin Bang, Leila Khalatbari, Bryan Wilie, Ziwei Ji, Etsuko Ishii, Pascale Fung",Apr 2024,https://arxiv.org/abs/2404.07900
Eraser: Jailbreaking Defense in Large Language Models via Unlearning Harmful Knowledge,"Weikai Lu, Ziqian Zeng, Jianwei Wang, Zhengdong Lu, Zelin Chen, Huiping Zhuang, Cen Chen",Apr 2024,https://arxiv.org/abs/2404.05880
Jailbreaking Prompt Attack: A Controllable Adversarial Attack against Diffusion Models,"Jiachen Ma, Anda Cao, Zhiqing Xiao, Yijiang Li, Jie Zhang, Chao Ye, Junbo Zhao",Apr 2024,https://arxiv.org/abs/2404.02928
Masked Completion via Structured Diffusion with White-Box Transformers,"Druv Pai, Ziyang Wu, Sam Buchanan, Yaodong Yu, Yi Ma",Apr 2024,https://arxiv.org/abs/2404.02446
What is in Your Safe Data? Identifying Benign Data that Breaks Safety,"Luxi He, Mengzhou Xia, Peter Henderson",Apr 2024,https://arxiv.org/abs/2404.01099v2
Is Factuality Decoding a Free Lunch for LLMs? Evaluation on Knowledge Editing Benchmark,"Baolong Bi, Shenghua Liu, Yiwei Wang, Lingrui Mei, Junfeng Fang, Hongcheng Gao, Shiyu Ni, Xueqi Cheng",Apr 2024,https://arxiv.org/abs/2404.00216
Universal Jailbreak Backdoors from Poisoned Human Feedback,"Javier Rando, Florian Tramèr",Apr 2024,https://arxiv.org/abs/2311.14455
Improving the Robustness of Large Language Models via Consistency Alignment,"Yukun Zhao, Lingyong Yan, Weiwei Sun, Guoliang Xing, Shuaiqiang Wang, Chong Meng, Zhicong Cheng, Zhaochun Ren, Dawei Yin",Mar 2024,https://arxiv.org/pdf/2403.14221
AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Shield Promptingg,"Yu Wang, Xiaogeng Liu, Yu Li, Muhao Chen, Chaowei Xiao",Mar 2024,https://arxiv.org/abs/2403.09513
AIGCs Confuse AI Too: Investigating and Explaining Synthetic Image-induced Hallucinations in Large Vision-Language Models,"Yifei Gao, Jiaqi Wang, Zhiyu Lin, Jitao Sang",Mar 2024,https://arxiv.org/abs/2403.08542
Distract Large Language Models for Automatic Jailbreak Attack,"Zeguan Xiao, Yan Yang, Guanhua Chen, Yun Chen",Mar 2024,https://arxiv.org/abs/2403.08424
CodeAttack: Revealing Safety Generalization Challenges of Large Language Models via Code Completion,"Qibing Ren, Chang Gao, Jing Shao, Junchi Yan, Xin Tan, Wai Lam, Lizhuang Ma",Mar 2024,https://arxiv.org/abs/2403.07865
ImgTrojan: Jailbreaking Vision-Language Models with ONE Image,"Xijia Tao, Shuai Zhong, Lei Li, Qi Liu, Lingpeng Kong",Mar 2024,https://arxiv.org/abs/2403.02910
Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes,"Xiaomeng Hu, Pin-Yu Chen, Tsung-Yi Ho",Mar 2024,https://arxiv.org/abs/2403.00867
Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models,"Samuel Marks, Can Rager, Eric J. Michaud, Yonatan Belinkov, David Bau, Aaron Mueller",Mar 2024,https://arxiv.org/pdf/2403.19647
SelfIE: Self-Interpretation of Large Language Model Embeddings,"Haozhe Chen, Carl Vondrick, Chengzhi Mao",Mar 2024,https://arxiv.org/pdf/2403.10949
Compositional preferences models for aligning LMs,"Dongyoung Go, Tomasz Korbak, German Kruszewski, Jos Rozen, Marc Dymetman",Mar 2024,https://arxiv.org/pdf/2310.13011
Discovering Latent Knowledge in Language Models Without Supervision,"Collin Burns, Haotian Ye, Dan Klein, Jacob Steinhardt",Mar 2024,https://arxiv.org/pdf/2212.03827
Detoxifying Large Language Models via Knowledge Editing,"Mengru Wang, Ningyu Zhang, Ziwen Xu, Zekun Xi, Shumin Deng, Yunzhi Yao, Qishen Zhang, Linyi Yang, Jindong Wang, Huajun Chen",Mar 2024,https://arxiv.org/abs/2403.14472
Can ChatGPT Detect DeepFakes? A Study of Using Multimodal Large Language Models for Media Forensics,"Shan Jia, Reilin Lyu, Kangran Zhao, Yize Chen, Zhiyuan Yan, Yan Ju, Chuanbo Hu, Xin Li, Baoyuan Wu, Siwei Lyu",Mar 2024,https://arxiv.org/abs/2403.14077
"OpenEval: Benchmarking Chinese LLMs across Capability, Alignment and Safety","Chuang Liu, Linhao Yu, Jiaxuan Li, Renren Jin, Yufei Huang, Ling Shi, Junhui Zhang, Xinmeng Ji, Tingting Cui, Tao Liu, Jinwang Song, Hongying Zan, Sun Li, Deyi Xiong",Mar 2024,https://arxiv.org/abs/2403.12316
Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for Language Models,"Yi Luo, Zhenghao Lin, Yuhao Zhang, Jiashuo Sun, Chen Lin, Chengjin Xu, Xiangdong Su, Yelong Shen, Jian Guo, Yeyun Gong",Mar 2024,https://arxiv.org/abs/2403.11838
Learning to Watermark LLM-generated Text via Reinforcement Learning,"Xiaojun Xu, Yuanshun Yao, Yang Liu",Mar 2024,https://arxiv.org/abs/2403.10553
Safety Cases: How to Justify the Safety of Advanced AI Systems,"Joshua Clymer, Nick Gabrieli, David Krueger, Thomas Larsen",Mar 2024,https://arxiv.org/abs/2403.10462
Simple and Scalable Strategies to Continually Pre-train Large Language Models,"Adam Ibrahim, Benjamin Thérien, Kshitij Gupta, Mats L. Richter, Quentin Anthony, Timothée Lesort, Eugene Belilovsky, Irina Rish",Mar 2024,https://arxiv.org/abs/2403.08763
HRLAIF: Improvements in Helpfulness and Harmlessness in Open-domain Reinforcement Learning From AI Feedback,"Ang Li, Qiugen Xiao, Peng Cao, Jian Tang, Yi Yuan, Zijie Zhao, Xiaoyuan Chen, Liang Zhang, Xiangyang Li, Kaitong Yang, Weidong Guo, Yukang Gan, Xu Yu, Daniell Wang, Ying Shan",Mar 2024,https://arxiv.org/abs/2403.08309
Emergence of Social Norms in Generative Agent Societies: Principles and Architecture,"Siyue Ren, Zhiyao Cui, Ruiqi Song, Zhen Wang, Shuyue Hu",Mar 2024,https://arxiv.org/abs/2403.08251
Defending Against Unforeseen Failure Modes with Latent Adversarial Training,"Stephen Casper, Lennart Schulze, Oam Patel, Dylan Hadfield-Menell",Mar 2024,https://arxiv.org/abs/2403.05030
On the Essence and Prospect: An Investigation of Alignment Approaches for Big Models,"Xinpeng Wang, Shitong Duan, Xiaoyuan Yi, Jing Yao, Shanlin Zhou, Zhihua Wei, Peng Zhang, Dongkuan Xu, Maosong Sun, Xing Xie",Mar 2024,https://arxiv.org/abs/2403.04204
Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimization,"Shitong Duan, Xiaoyuan Yi, Peng Zhang, Yan Liu, Zheng Liu, Tun Lu, Xing Xie, Ning Gu",Mar 2024,https://arxiv.org/abs/2403.03419
In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation,"Shiqi Chen, Miao Xiong, Junteng Liu, Zhengxuan Wu, Teng Xiao, Siyang Gao, Junxian He",Mar 2024,https://arxiv.org/abs/2403.01548
GuardT2I: Defending Text-to-Image Models from Adversarial Prompts,"Yijun Yang, Ruiyuan Gao, Xiao Yang, Jianyuan Zhong, Qiang Xu",Mar 2024,https://arxiv.org/abs/2403.01446
DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Modelss,"Kedi Chen, Qin Chen, Jie Zhou, Yishen He, Liang He",Mar 2024,https://arxiv.org/abs/2403.00896
PRSA: PRompt Stealing Attacks against Large Language Models,"Yong Yang, Changjiang Li, Yi Jiang, Xi Chen, Haoyu Wang, Xuhong Zhang, Zonghui Wang, Shouling Ji",Feb 2024,https://arxiv.org/abs/2402.19200
Making Them Ask and Answer: Jailbreaking Large Language Models in Few Queries via Disguise and Reconstruction,"Tong Liu, Yingjie Zhang, Zhe Zhao, Yinpeng Dong, Guozhu Meng, Kai Chen",Feb 2024,https://arxiv.org/abs/2402.18104
Speak Out of Turn: Safety Vulnerability of Large Language Models in Multi-turn Dialoguee,"Zhenhong Zhou, Jiuyang Xiang, Haopeng Chen, Quan Liu, Zherui Li, Sen Su",Feb 2024,https://arxiv.org/abs/2402.17262
CodeChameleon: Personalized Encryption Framework for Jailbreaking Large Language Models,"Huijie Lv, Xiao Wang, Yuansen Zhang, Caishuang Huang, Shihan Dou, Junjie Ye, Tao Gui, Qi Zhang, Xuanjing Huang",Feb 2024,https://arxiv.org/abs/2402.16717
From Noise to Clarity: Unraveling the Adversarial Suffix of Large Language Model Attacks via Translation of Text Embeddings,"Hao Wang, Hao Li, Minlie Huang, Lei Sha",Feb 2024,https://arxiv.org/abs/2402.16006
LLMs Can Defend Themselves Against Jailbreaking in a Practical Manner: A Vision Paper,"Daoyuan Wu, Shuai Wang, Yang Liu, Ning Liu",Feb 2024,https://arxiv.org/abs/2402.15727
Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!,"Zhanhui Zhou, Jie Liu, Zhichen Dong, Jiaheng Liu, Chao Yang, Wanli Ouyang, Yu Qiao",Feb 2024,https://arxiv.org/abs/2402.12343
Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents,"Wenkai Yang, Xiaohan Bi, Yankai Lin, Sishuo Chen, Jie Zhou, Xu Sun",Feb 2024,https://arxiv.org/abs/2402.11208
"Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey","Zhichen Dong, Zhanhui Zhou, Chao Yang, Jing Shao, Yu Qiao",Feb 2024,https://arxiv.org/abs/2402.09283
Studious Bob Fight Back Against Jailbreaking via Prompt Adversarial Tuning,"Yichuan Mo, Yuji Wang, Zeming Wei, Yisen Wang",Feb 2024,https://arxiv.org/abs/2402.06255
Computing Power and the Governance of AI,"Lennart Heim, Markus Anderljung, Emma Bluemke, Robert Trager",Feb 2024,https://www.governance.ai/post/computing-power-and-the-governance-of-ai
GenAI against humanity: nefarious applications of generative artificial intelligence and large language models,Emilio Ferrara,Feb 2024,https://link.springer.com/article/10.1007/s42001-024-00250-1
Do Large Language Models Latently Perform Multi-Hop Reasoning?,"Sohee Yang, Elena Gribovskaya, Nora Kassner, Mor Geva, Sebastian Riedel",Feb 2024,https://arxiv.org/pdf/2402.16837
Eight Methods to Evaluate Robust Unlearning in LLMs,"Aengus Lynch, Phillip Guo, Aidan Ewart, Stephen Casper, Dylan Hadfield-Menell",Feb 2024,https://arxiv.org/pdf/2402.16835
Increased Compute Efficiency and the Diffusion of AI Capabilities,"Konstantin Pilz, Lennart Heim, Nicholas Brown",Feb 2024,https://arxiv.org/pdf/2311.15377
LLMs with Chain-of-Thought Are Non-Causal Reasoners,"Guangsheng Bao, Hongbo Zhang, Linyi Yang, Cunxiang Wang, Yue Zhang",Feb 2024,https://arxiv.org/html/2402.16048v1
Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period of Large Language Models,"Chen Qian, Jie Zhang, Wei Yao, Dongrui Liu, Zhenfei Yin, Yu Qiao, Yong Liu, Jing Shao",Feb 2024,https://arxiv.org/abs/2402.19465
Controllable Preference Optimization: Toward Controllable Multi-Objective Alignment,"Yiju Guo, Ganqu Cui, Lifan Yuan, Ning Ding, Zexu Sun, Bowen Sun, Huimin Chen, Ruobing Xie, Jie Zhou, Yankai Lin, Zhiyuan Liu, Maosong Sun",Feb 2024,https://arxiv.org/abs/2402.19085
Language Models Represent Beliefs of Self and Others,"Wentao Zhu, Zhining Zhang, Yizhou Wang",Feb 2024,https://arxiv.org/abs/2402.18496
"Exploring Multilingual Human Value Concepts in Large Language Models: Is Value Alignment Consistent, Transferable and Controllable across Languages?","Shaoyang Xu, Weilong Dong, Zishan Guo, Xinwei Wu, Deyi Xion",Feb 2024,https://arxiv.org/abs/2402.18120
"ShieldLM: Empowering LLMs as Aligned, Customizable and Explainable Safety Detectors","Zhexin Zhang, Yida Lu, Jingyuan Ma, Di Zhang, Rui Li, Pei Ke, Hao Sun, Lei Sha, Zhifang Sui, Hongning Wang, Minlie Huang",Feb 2024,https://arxiv.org/abs/2402.16444
Immunization against harmful fine-tuning attacks,"Domenic Rosati, Jan Wehner, Kai Williams, Lukasz Bartoszcze, Jan Batzner, Hassan Sajjad, Frank Rudzicz",Feb 2024,https://arxiv.org/abs/2402.16382
Can Watermarks Survive Translation? On the Cross-lingual Consistency of Text Watermark for Large Language Models,"Zhiwei He, Binglin Zhou, Hongkun Hao, Aiwei Liu, Xing Wang, Zhaopeng Tu, Zhuosheng Zhang, Rui Wang",Feb 2024,https://arxiv.org/abs/2402.14007
Identifying Semantic Induction Heads to Understand In-Context Learning,"Jie Ren, Qipeng Guo, Hang Yan, Dongrui Liu, Quanshi Zhang, Xipeng Qiu, Dahua Lin",Feb 2024,https://arxiv.org/abs/2402.13055
Incentive Compatibility for AI Alignment in Sociotechnical Systems: Positions and Prospects,"Zhaowei Zhang, Fengshuo Bai, Mingzhi Wang, Haoyang Ye, Chengdong Ma, Yaodong Yang",Feb 2024,https://arxiv.org/abs/2402.12907
Reformatted Alignment,"Run-Ze Fan, Xuefeng Li, Haoyang Zou, Junlong Li, Shwai He, Ethan Chern, Jiewen Hu, Pengfei Liu",Feb 2024,https://arxiv.org/abs/2402.12219
Dissecting Human and LLM Preferences,"Junlong Li, Fan Zhou, Shichao Sun, Yikai Zhang, Hai Zhao, Pengfei Liu",Feb 2024,https://arxiv.org/abs/2402.11296
CultureLLM: Incorporating Cultural Differences into Large Language Models,"Cheng Li, Mengzhou Chen, Jindong Wang, Sunayana Sitaram, Xing Xie",Feb 2024,https://arxiv.org/abs/2402.10946
ToolSword: Unveiling Safety Issues of Large Language Models in Tool Learning Across Three Stages,"Junjie Ye, Sixian Li, Guanyu Li, Caishuang Huang, Songyang Gao, Yilong Wu, Qi Zhang, Tao Gui, Xuanjing Huan",Feb 2024,https://arxiv.org/abs/2402.10753
Chain-of-Thought Reasoning Without Prompting,"Xuezhi Wang, Denny Zhou",Feb 2024,https://arxiv.org/abs/2402.10200
Rethinking Information Structures in RLHF: Reward Generalization from a Graph Theory Perspective,"Tianyi Qiu, Fanzhi Zeng, Jiaming Ji, Dong Yan, Kaile Wang, Jiayi Zhou, Yang Han, Josef Dai, Xuehai Pan, Yaodong Yang",Feb 2024,https://arxiv.org/abs/2402.10184
DoRA: Weight-Decomposed Low-Rank Adaptation,"Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-Ting Cheng, Min-Hung Chen",Feb 2024,https://arxiv.org/abs/2402.09353
InfoRM: Mitigating Reward Hacking in RLHF via Information-Theoretic Reward Modeling,"Yuchun Miao, Sen Zhang, Liang Ding, Rong Bao, Lefei Zhang, Dacheng Tao",Feb 2024,https://arxiv.org/abs/2402.09345
Rethinking Machine Unlearning for Large Language Models,"Sijia Liu, Yuanshun Yao, Jinghan Jia, Stephen Casper, Nathalie Baracaldo, Peter Hase, Yuguang Yao, Chris Yuhao Liu, Xiaojun Xu, Hang Li, Kush R. Varshney, Mohit Bansal, Sanmi Koyejo, Yang Liu",Feb 2024,https://arxiv.org/abs/2402.08787
"Towards Unified Alignment Between Agents, Humans, and Environment","Zonghan Yang, An Liu, Zijun Liu, Kaiming Liu, Fangzhou Xiong, Yile Wang, Zeyuan Yang, Qingyuan Hu, Xinrui Chen, Zhenhe Zhang, Fuwen Luo, Zhicheng Guo, Peng Li, Yang Liu",Feb 2024,https://arxiv.org/abs/2402.07744
Secret Collusion among Generative AI Agents,"Sumeet Ramesh Motwani, Mikhail Baranchuk, Martin Strohmeier, Vijay Bolina, Philip H.S. Torr, Lewis Hammond, Christian Schroeder de Witt",Feb 2024,https://arxiv.org/abs/2402.07510
Self-Alignment of Large Language Models via Monopolylogue-based Social Scene Simulation,"Xianghe Pang, Shuo Tang, Rui Ye, Yuxin Xiong, Bolun Zhang, Yanfeng Wang, Siheng Chen",Feb 2024,https://arxiv.org/abs/2402.05699
SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models,"Lijun Li, Bowen Dong, Ruohui Wang, Xuhao Hu, Wangmeng Zuo, Dahua Lin, Yu Qiao, Jing Shao",Feb 2024,https://arxiv.org/abs/2402.05044
Ten Hard Problems in Artificial Intelligence We Must Get Right,"Gavin Leech, Simson Garfinkel, Misha Yagudin, Alexander Briand, Aleksandr Zhuravlev",Feb 2024,https://arxiv.org/abs/2402.04464
HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal,"Mantas Mazeika, Long Phan, Xuwang Yin, Andy Zou, Zifan Wang, Norman Mu, Elham Sakhaee, Nathaniel Li, Steven Basart, Bo Li, David Forsyth, Dan Hendrycks",Feb 2024,https://arxiv.org/abs/2402.04249
Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science,"Xiangru Tang, Qiao Jin, Kunlun Zhu, Tongxin Yuan, Yichi Zhang, Wangchunshu Zhou, Meng Qu, Yilun Zhao, Jian Tang, Zhuosheng Zhang, Arman Cohan, Zhiyong Lu, Mark Gerstein",Feb 2024,https://arxiv.org/abs/2402.04247
Aligner: Achieving Efficient Alignment through Weak-to-Strong Correction,"Jiaming Ji, Boyuan Chen, Hantao Lou, Donghai Hong, Borong Zhang, Xuehai Pan, Juntao Dai, Tianyi Qiu, Yaodong Yang",Feb 2024,https://arxiv.org/abs/2402.02416
Panacea: Pareto Alignment via Preference Adaptation for LLMs,"Yifan Zhong, Chengdong Ma, Xiaoyuan Zhang, Ziran Yang, Haojun Chen, Qingfu Zhang, Siyuan Qi, Yaodong Yang",Feb 2024,https://arxiv.org/abs/2402.02030
On Catastrophic Inheritance of Large Foundation Models,"Hao Chen, Bhiksha Raj, Xing Xie, Jindong Wang",Feb 2024,https://arxiv.org/abs/2402.01909
"From GPT-4 to Gemini and Beyond: Assessing the Landscape of MLLMs on Generalizability, Trustworthiness and Causality through Four Modalities","Chaochao Lu, Chen Qian, Guodong Zheng, Hongxing Fan, Hongzhi Gao, Jie Zhang, Jing Shao, Jingyi Deng, Jinlan Fu, Kexin Huang, Kunchang Li, Lijun Li, Limin Wang, Lu Sheng, Meiqi Chen, Ming Zhang, Qibing Ren, Sirui Chen, Tao Gui, Wanli Ouyang, Yali Wang, Yan Teng, Yaru Wang, Yi Wang, Yinan He, Yingchun Wang, Yixu Wang, Yongting Zhang, Yu Qiao, Yujiong Shen, Yurong Mou, Yuxi Chen, Zaibin Zhang, Zhelun Shi, Zhenfei Yin, Zhipin Wang",Jan 2024,https://arxiv.org/abs/2401.15071
R-Judge: Benchmarking Safety Risk Awareness for LLM Agents,"Tongxin Yuan, Zhiwei He, Lingzhong Dong, Yiming Wang, Ruijie Zhao, Tian Xia, Lizhen Xu, Binglin Zhou, Fangqi Li, Zhuosheng Zhang, Rui Wang, Gongshen Liu",Jan 2024,https://arxiv.org/abs/2401.10019
"Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems","Tianyu Cui, Yanling Wang, Chuanpu Fu, Yong Xiao, Sijia Li, Xinhao Deng, Yunpeng Liu, Qinglin Zhang, Ziyi Qiu, Peiyang Li, Zhixing Tan, Junwu Xiong, Xinyu Kong, Zujie Wen, Ke Xu, Qi Li",Jan 2024,https://arxiv.org/abs/2401.05778
"Without Fundamental Advances, Misalignment and Catastrophe are the Default Outcomes of Training Powerful AI","Peter Barnett, Jeremy Gillen",Jan 2024,https://intelligence.org/wp-content/uploads/2024/12/Misalignment_and_Catastrophe.pdf
WARM: On the Benefits of Weight Averaged Reward Models,"Alexandre Rame, Nino Vieillard, Léonard Hussenot, Robert Dadashi, Geoffrey Cideron, Olivier Bachem, Johan Ferret",Jan 2024,https://arxiv.org/pdf/2401.12187
On Prompt-Driven Safeguarding for Large Language Models,"Chujie Zheng, Fan Yin, Hao Zhou, Fandong Meng, Jie Zhou, Kai-Wei Chang, Minlie Huang, Nanyun Peng",Jan 2024,https://arxiv.org/abs/2401.18018
A Cross-Language Investigation into Jailbreak Attacks in Large Language Models,"Jie Li, Yi Liu, Chongyang Liu, Ling Shi, Xiaoning Ren, Yaowen Zheng, Yang Liu, Yinxing Xue",Jan 2024,https://arxiv.org/abs/2401.16765
Visibility into AI Agents,"Alan Chan, Carson Ezell, Max Kaufmann, Kevin Wei, Lewis Hammond, Herbie Bradley, Emma Bluemke, Nitarshan Rajkumar, David Krueger, Noam Kolt, Lennart Heim, Markus Anderljung",Jan 2024,https://arxiv.org/abs/2401.13138
Red Teaming Visual Language Models,"Mukai Li, Lei Li, Yuwei Yin, Masood Ahmed, Zhenguang Liu, Qi Liu",Jan 2024,https://arxiv.org/abs/2401.12915
"PsySafe: A Comprehensive Framework for Psychological-based Attack, Defense, and Evaluation of Multi-agent System Safety","Zaibin Zhang, Yongting Zhang, Lijun Li, Hongzhi Gao, Lijun Wang, Huchuan Lu, Feng Zhao, Yu Qiao, Jing Shao",Jan 2024,https://arxiv.org/abs/2401.11880
Self-Rewarding Language Models,"Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar Sukhbaatar, Jing Xu, Jason Weston",Jan 2024,https://arxiv.org/abs/2401.10020
Human-Instruction-Free LLM Self-Alignment with Limited Samples,"Hongyi Guo, Yuanshun Yao, Wei Shen, Jiaheng Wei, Xiaoying Zhang, Zhaoran Wang, Yang Liu",Jan 2024,https://arxiv.org/abs/2401.06785
Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training,"Evan Hubinger, Carson Denison, Jesse Mu, Mike Lambert, Meg Tong, Monte MacDiarmid, Tamera Lanham, Daniel M. Ziegler, Tim Maxwell, Newton Cheng, Adam Jermyn, Amanda Askell, Ansh Radhakrishnan, Cem Anil, David Duvenaud, Deep Ganguli, Fazl Barez, Jack Clark, Kamal Ndousse, Kshitij Sachan, Michael Sellitto, Mrinank Sharma, Nova DasSarma, Roger Grosse, Shauna Kravec, Yuntao Bai, Zachary Witten, Marina Favaro, Jan Brauner, Holden Karnofsky, Paul Christiano, Samuel R. Bowman, Logan Graham, Jared Kaplan, Sören Mindermann, Ryan Greenblatt, Buck Shlegeris, Nicholas Schiefer, Ethan Perez",Jan 2024,https://arxiv.org/abs/2401.05566
Agent Alignment in Evolving Social Norms,"Shimin Li, Tianxiang Sun, Qinyuan Cheng, Xipeng Qiu",Jan 2024,https://arxiv.org/abs/2401.04620
Mixtral of Experts,"Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed",Jan 2024,https://arxiv.org/abs/2401.04088
GOAT-Bench: Safety Insights to Large Multimodal Models through Meme-Based Social Abuse,"Hongzhan Lin, Ziyang Luo, Bo Wang, Ruichao Yang, Jing Ma",Jan 2024,https://arxiv.org/abs/2401.01523
Considerations for Governing Open Foundation Models,"Rishi Bommasani, Sayash Kapoor, Kevin Klyman, Shayne Longpre, Ashwin Ramaswami, Daniel Zhang, Marietje Schaake, Daniel E. Ho, Arvind Narayanan, Percy Liang",Dec 2023,https://hai.stanford.edu/sites/default/files/2023-12/Governing-Open-Foundation-Models.pdf
Align on the Fly: Adapting Chatbot Behavior to Established Norms,"Chunpu Xu, Steffi Chern, Ethan Chern, Ge Zhang, Zekun Wang, Ruibo Liu, Jing Li, Jie Fu, Pengfei Liu",Dec 2023,https://arxiv.org/abs/2312.15907
Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models,"Jingwei Yi, Yueqi Xie, Bin Zhu, Emre Kiciman, Guangzhong Sun, Xing Xie, Fangzhao Wu",Dec 2023,https://arxiv.org/abs/2312.14197
Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision,"Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, Ilya Sutskever, Jeff Wu",Dec 2023,https://arxiv.org/abs/2312.09390
A Survey of Text Watermarking in the Era of Large Language Models,"Aiwei Liu, Leyi Pan, Yijian Lu, Jingjing Li, Xuming Hu, Xi Zhang, Lijie Wen, Irwin King, Hui Xiong, Philip S. Yu",Dec 2023,https://arxiv.org/abs/2312.07913
On Diversified Preferences of Large Language Model Alignment,"Dun Zeng, Yong Dai, Pengyu Cheng, Longyue Wang, Tianhao Hu, Wanshun Chen, Nan Du, Zenglin Xu",Dec 2023,https://arxiv.org/abs/2312.07401
Alignment for Honesty,"Yuqing Yang, Ethan Chern, Xipeng Qiu, Graham Neubig, Pengfei Liu",Dec 2023,https://arxiv.org/abs/2312.07000
AI Control: Improving Safety Despite Intentional Subversion,"Ryan Greenblatt, Buck Shlegeris, Kshitij Sachan, Fabien Roger",Dec 2023,https://arxiv.org/abs/2312.06942
Control Risk for Potential Misuse of Artificial Intelligence in Science,"Jiyan He, Weitao Feng, Yaosen Min, Jingwei Yi, Kunsheng Tang, Shuai Li, Jie Zhang, Kejiang Chen, Wenbo Zhou, Xing Xie, Weiming Zhang, Nenghai Yu, Shuxin Zheng",Dec 2023,https://arxiv.org/abs/2312.06632
"Advanced AI Governance: A Literature Review of Problems, Options, and Proposals",Matthijs M. Maas,Nov 2023,https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4629460
Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding,"Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, Lidong Bing",Nov 2023,https://arxiv.org/abs/2311.16922
CDEval: A Benchmark for Measuring the Cultural Dimensions of Large Language Models,"Yuhang Wang, Yanxu Zhu, Chao Kong, Shuyu Wei, Xiaoyuan Yi, Xing Xie, Jitao Sang",Nov 2023,https://arxiv.org/abs/2311.16421
On the Calibration of Large Language Models and Alignment,"Chiwei Zhu, Benfeng Xu, Quan Wang, Yongdong Zhang, Zhendong Mao",Nov 2023,https://arxiv.org/abs/2311.13240
White-Box Transformers via Sparse Rate Reduction: Compression Is All There Is?,"Yaodong Yu, Sam Buchanan, Druv Pai, Tianzhe Chu, Ziyang Wu, Shengbang Tong, Hao Bai, Yuexiang Zhai, Benjamin D. Haeffele, Yi Ma",Nov 2023,https://arxiv.org/abs/2311.13110
Evil Geniuses: Delving into the Safety of LLM-based Agents,"Yu Tian, Xiao Yang, Jingyuan Zhang, Yinpeng Dong, Hang Su",Nov 2023,https://arxiv.org/abs/2311.11855
Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization,"Zhexin Zhang, Junxiao Yang, Pei Ke, Fei Mi, Hongning Wang, Minlie Huang",Nov 2023,https://arxiv.org/abs/2311.09096
Flames: Benchmarking Value Alignment of Chinese Large Language Models,"Kexin Huang, Xiangyang Liu, Qianyu Guo, Tianxiang Sun, Jiawei Sun, Yaru Wang, Zeyang Zhou, Yixu Wang, Yan Teng, Xipeng Qiu, Yingchun Wang, Dahua Lin",Nov 2023,https://arxiv.org/abs/2311.06899
Fake Alignment: Are LLMs Really Aligned Well?,"Yixu Wang, Yan Teng, Kexin Huang, Chengqi Lyu, Songyang Zhang, Wenwei Zhang, Xingjun Ma, Yu-Gang Jiang, Yu Qiao, Yingchun Wang",Nov 2023,https://arxiv.org/abs/2311.05915
FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts,"Yichen Gong, Delong Ran, Jinyuan Liu, Conglei Wang, Tianshuo Cong, Anyu Wang, Sisi Duan, Xiaoyun Wang",Nov 2023,https://arxiv.org/abs/2311.05608
"A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions","Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, Ting Liu",Nov 2023,https://arxiv.org/abs/2311.05232
Making Harmful Behaviors Unlearnable for Large Language Models,"Xin Zhou, Yi Lu, Ruotian Ma, Tao Gui, Qi Zhang, Xuanjing Huang",Nov 2023,https://arxiv.org/abs/2311.02105
JADE: A Linguistics-based Safety Evaluation Platform for LLM,"Mi Zhang, Xudong Pan, Min Yang",Nov 2023,https://arxiv.org/abs/2311.00286
Towards Monosemanticity: Decomposing Language Models With Dictionary Learning,"Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nicholas L Turner, Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas Schiefer, Tim Maxwell, Nicholas Joseph, Alex Tamkin, Karina Nguyen, Brayden McLean, Josiah E Burke, Tristan Hume, Shan Carter, Tom Henighan, Chris Olah",Oct 2023,https://transformer-circuits.pub/2023/monosemantic-features/index.html
An Overview of Catastrophic AI Risks,"Dan Hendrycks, Mantas Mazeika, Thomas Woodside",Oct 2023,https://arxiv.org/pdf/2306.12001
AI Alignment: A Comprehensive Survey,"Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao Lou, Kaile Wang, Yawen Duan, Zhonghao He, Jiayi Zhou, Zhaowei Zhang, Fanzhi Zeng, Kwan Yee Ng, Juntao Dai, Xuehai Pan, Aidan O'Gara, Yingshan Lei, Hua Xu, Brian Tse, Jie Fu, Stephen McAleer, Yaodong Yang, Yizhou Wang, Song-Chun Zhu, Yike Guo, Wen Gao",Oct 2023,https://arxiv.org/abs/2310.19852
Evaluating Large Language Models: A Comprehensive Survey,"Zishan Guo, Renren Jin, Chuang Liu, Yufei Huang, Dan Shi, Supryadi, Linhao Yu, Yan Liu, Jiaxuan Li, Bojian Xiong, Deyi Xiong",Oct 2023,https://arxiv.org/abs/2310.19736
Preventing Language Models From Hiding Their Reasoning,"Fabien Roger, Ryan Greenblatt",Oct 2023,https://arxiv.org/abs/2310.18512
Unpacking the Ethical Value Alignment in Big Models,"Xiaoyuan Yi, Jing Yao, Xiting Wang, Xing Xie",Oct 2023,https://arxiv.org/abs/2310.17551
CycleAlign: Iterative Distillation from Black-box LLM to White-box Models for Better Human Alignment,"Jixiang Hong, Quan Tu, Changyu Chen, Xing Gao, Ji Zhang, Rui Yan",Oct 2023,https://arxiv.org/abs/2310.16271
Woodpecker: Hallucination Correction for Multimodal Large Language Models,"Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun, Enhong Chen",Oct 2023,https://arxiv.org/abs/2310.16045
Safe RLHF: Safe Reinforcement Learning from Human Feedback,"Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel Liu, Yizhou Wang, Yaodong Yang",Oct 2023,https://arxiv.org/abs/2310.12773
Watermarking LLMs with Weight Quantization,"Linyang Li, Botian Jiang, Pengyu Wang, Ke Ren, Hang Yan, Xipeng Qiu",Oct 2023,https://arxiv.org/abs/2310.11237
RealBehavior: A Framework for Faithfully Characterizing Foundation Models' Human-like Behavior Mechanisms,"Enyu Zhou, Rui Zheng, Zhiheng Xi, Songyang Gao, Xiaoran Fan, Zichu Fei, Jingting Ye, Tao Gui, Qi Zhang, Xuanjing Huang",Oct 2023,https://arxiv.org/abs/2310.11227
Denevil: Towards Deciphering and Navigating the Ethical Values of Large Language Models via Instruction Learning,"Shitong Duan, Xiaoyuan Yi, Peng Zhang, Tun Lu, Xing Xie, Ning Gu",Oct 2023,https://arxiv.org/abs/2310.11053
Large Language Model Unlearning,"Yuanshun Yao, Xiaojun Xu, Yang Liu",Oct 2023,https://arxiv.org/abs/2310.10683
Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis,"Kai Chen, Chunwei Wang, Kuo Yang, Jianhua Han, Lanqing Hong, Fei Mi, Hang Xu, Zhengying Liu, Wenyong Huang, Zhenguo Li, Dit-Yan Yeung, Lifeng Shang, Xin Jiang, Qun Liu",Oct 2023,https://arxiv.org/abs/2310.10477
Multilingual Jailbreak Challenges in Large Language Models,"Yue Deng, Wenxuan Zhang, Sinno Jialin Pan, Lidong Bing",Oct 2023,https://arxiv.org/abs/2310.06474
Constructive Large Language Models Alignment with Diverse Feedback,"Tianshu Yu, Ting-En Lin, Yuchuan Wu, Min Yang, Fei Huang, Yongbin Li",Oct 2023,https://arxiv.org/abs/2310.06450
Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations,"Zeming Wei, Yifei Wang, Ang Li, Yichuan Mo, Yisen Wang",Oct 2023,https://arxiv.org/abs/2310.06387
SC-Safety：A Multi-round Adversarial Safety Benchmark for Large Language Model in Chinese,"Liang Xu, Kangkang Zhao, Lei Zhu, Hang Xue",Oct 2023,https://arxiv.org/abs/2310.05818
Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature,"Guangsheng Bao, Yanbin Zhao, Zhiyang Teng, Linyi Yang, Yue Zhang",Oct 2023,https://arxiv.org/abs/2310.05130
A Long Way to Go: Investigating Length Correlations in RLHF,"Prasann Singhal, Tanya Goyal, Jiacheng Xu, Greg Durrett",Oct 2023,https://arxiv.org/abs/2310.03716
Beyond One-Preference-Fits-All Alignment: Multi-Objective Direct Preference Optimization,"Zhanhui Zhou, Jie Liu, Jing Shao, Xiangyu Yue, Chao Yang, Wanli Ouyang, Yu Qia",Oct 2023,https://arxiv.org/abs/2310.03708
Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models,"Xianjun Yang, Xiao Wang, Qi Zhang, Linda Petzold, William Yang Wang, Xun Zhao, Dahua Lin",Oct 2023,https://arxiv.org/abs/2310.02949
Representation Engineering: A Top-Down Approach to AI Transparency,"Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, Shashwat Goel, Nathaniel Li, Michael J. Byun, Zifan Wang, Alex Mallen, Steven Basart, Sanmi Koyejo, Dawn Song, Matt Fredrikson, J. Zico Kolter, Dan Hendrycks",Oct 2023,https://arxiv.org/abs/2310.01405
Who is ChatGPT? Benchmarking LLMs' Psychological Portrayal Using PsychoBench,"Jen-tse Huang, Wenxuan Wang, Eric John Li, Man Ho Lam, Shujie Ren, Youliang Yuan, Wenxiang Jiao, Zhaopeng Tu, Michael R. Lyu",Oct 2023,https://arxiv.org/abs/2310.01386
Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting,"Haoyang Huang, Tianyi Tang, Dongdong Zhang, Wayne Xin Zhao, Ting Song, Yan Xia, Furu Wei",Oct 2023,https://arxiv.org/abs/2305.07004
Open-Sourcing Highly Capable Foundation Models,"Elizabeth Seger, Noemi Dreksler, Richard Moulange, Emily Dardaman, Jonas Schuett, K. Wei, Christoph Winter, Mackenzie Arnold, Seán Ó hÉigeartaigh, Anton Korinek, Markus Anderljung, Ben Bucknall, Alan Chan, Eoghan Sta!ord, Leonie Koessler, Aviv Ovadya, Ben Garfinkel, Emma Bluemke, Michael Aird, Patrick Levermore, Julian Hazell, Abhishek Gupta",Sep 2023,https://cdn.governance.ai/Open-Sourcing_Highly_Capable_Foundation_Models_2023_GovAI.pdf
Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback,"Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, Tony Wang, Samuel Marks, Charbel-Raphaël Segerie, Micah Carroll, Andi Peng, Phillip Christoffersen, Mehul Damani, Stewart Slocum, Usman Anwar, Anand Siththaranjan, Max Nadeau, Eric J. Michaud, Jacob Pfau, Dmitrii Krasheninnikov, Xin Chen, Lauro Langosco, Peter Hase, Erdem Bıyık, Anca Dragan, David Krueger, Dorsa Sadigh, Dylan Hadfield-Menell",Sep 2023,https://arxiv.org/pdf/2307.15217
Robust Feature-Level Adversaries are Interpretability Tools,"Stephen Casper, Max Nadeau, Dylan Hadfield-Menell, Gabriel Kreiman",Sep 2023,https://arxiv.org/pdf/2110.03605
"Open-Sourcing Highly Capable Foundation Models: An evaluation of risks, benefits, and alternative methods for pursuing open-source objectives","Elizabeth Seger, Noemi Dreksler, Richard Moulange, Emily Dardaman, Jonas Schuett, K. Wei, Christoph Winter, Mackenzie Arnold, Seán Ó hÉigeartaigh, Anton Korinek, Markus Anderljung, Ben Bucknall, Alan Chan, Eoghan Stafford, Leonie Koessler, Aviv Ovadya, Ben Garfinkel, Emma Bluemke, Michael Aird, Patrick Levermore, Julian Hazell, Abhishek Gupta",Sep 2023,https://arxiv.org/abs/2311.09227
Measuring Value Understanding in Language Models through Discriminator-Critique Gap,"Zhaowei Zhang, Fengshuo Bai, Jun Gao, Yaodong Yang",Sep 2023,https://arxiv.org/abs/2310.00378
Large Language Model Alignment: A Survey,"Tianhao Shen, Renren Jin, Yufei Huang, Chuang Liu, Weilong Dong, Zishan Guo, Xinwei Wu, Yan Liu, Deyi Xiong",Sep 2023,https://arxiv.org/abs/2309.15025
Exploiting Modality-Specific Features For Multi-Modal Manipulation Detection And Grounding,"Jiazhen Wang, Bin Liu, Changtao Miao, Zhiwei Zhao, Wanyi Zhuang, Qi Chu, Nenghai Yu",Sep 2023,https://arxiv.org/abs/2309.12657
How Robust is Google's Bard to Adversarial Image Attacks?,"Yinpeng Dong, Huanran Chen, Jiawei Chen, Zhengwei Fang, Xiao Yang, Yichi Zhang, Yu Tian, Hang Su, Jun Zhu",Sep 2023,https://arxiv.org/abs/2309.11751
GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts,"Jiahao Yu, Xingwei Lin, Zheng Yu, Xinyu Xing",Sep 2023,https://arxiv.org/abs/2309.10253
RAIN: Your Language Models Can Align Themselves without Finetuning,"Yuhui Li, Fangyun Wei, Jinjing Zhao, Chao Zhang, Hongyang Zhang",Sep 2023,https://arxiv.org/abs/2309.07124
SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions,"Zhexin Zhang, Leqi Lei, Lindong Wu, Rui Sun, Yongkang Huang, Chong Long, Xiao Liu, Xuanyu Lei, Jie Tang, Minlie Huang",Sep 2023,https://arxiv.org/abs/2309.07045
Explainability for Large Language Models: A Survey,"Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi Deng, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Mengnan Du",Sep 2023,https://arxiv.org/abs/2309.01029
RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback,"Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, Sushant Prakash",Sep 2023,https://arxiv.org/abs/2309.00267
Rethinking Machine Ethics -- Can LLMs Perform Moral Reasoning through the Lens of Moral Theories?,"Jingyan Zhou, Minda Hu, Junan Li, Xiaoying Zhang, Xixin Wu, Irwin King, Helen Meng",Aug 2023,https://arxiv.org/abs/2308.15399
From Instructions to Intrinsic Human Values -- A Survey of Alignment Goals for Big Models,"Jing Yao, Xiaoyuan Yi, Xiting Wang, Jindong Wang, Xing Xie",Aug 2023,https://arxiv.org/abs/2308.12014
Through the Lens of Core Competency: Survey on Evaluation of Large Language Models,"Ziyu Zhuang, Qiguang Chen, Longxuan Ma, Mingda Li, Yi Han, Yushan Qian, Haopeng Bai, Zixian Feng, Weinan Zhang, Ting Liu",Aug 2023,https://arxiv.org/abs/2308.07902
Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment,"Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo, Hao Cheng, Yegor Klochkov, Muhammad Faaiz Taufiq, Hang Li",Aug 2023,https://arxiv.org/abs/2308.05374
"Governing General Purpose AI — A Comprehensive Map of Unreliability, Misuse and Systemic Risks","Pegah Maham, Sabrina Küspert",Jul 2023,https://www.interface-eu.org/publications/governing-general-purpose-ai-comprehensive-map-unreliability-misuse-and-systemic-risks
Universal and Transferable Adversarial Attacks on Aligned Language Models,"Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J. Zico Kolter, Matt Fredrikson",Jul 2023,https://llm-attacks.org/
Reasoning about causality in games,"Lewis Hammond, James Fox, Tom Everitt, Ryan Carey, Alessandro Abate, Michael Wooldridge",Jul 2023,https://doi.org/10.1016/j.artint.2023.103919
CValues: Measuring the Values of Chinese Large Language Models from Safety to Responsibility,"Guohai Xu, Jiayi Liu, Ming Yan, Haotian Xu, Jinghui Si, Zhuoran Zhou, Peng Yi, Xing Gao, Jitao Sang, Rong Zhang, Ji Zhang, Chao Peng, Fei Huang, Jingren Zhou",Jul 2023,https://arxiv.org/pdf/2307.09705.pdf
Aligning Large Language Models with Human: A Survey,"Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, Qun Liu",Jul 2023,https://arxiv.org/abs/2307.12966
Emotional Intelligence of Large Language Models,"Xuena Wang, Xueting Li, Zi Yin, Yue Wu, Liu Jia",Jul 2023,https://arxiv.org/abs/2307.09042
SafeDreamer: Safe Reinforcement Learning with World Model,"Weidong Huang, Jiaming Ji, Chunhe Xia, Borong Zhang, Yaodong Yang",Jul 2023,https://arxiv.org/abs/2307.07176
International Institutions for Advanced AI,"Lewis Ho, Joslyn Barnhart, Robert Trager, Yoshua Bengio, Miles Brundage, Allison Carnegie, Rumman Chowdhury, Allan Dafoe, Gillian Hadfield, Margaret Levi, Duncan Snidal",Jul 2023,https://arxiv.org/abs/2307.04699
BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset,"Jiaming Ji, Mickel Liu, Juntao Dai, Xuehai Pan, Chi Zhang, Ce Bian, Chi Zhang, Ruiyang Sun, Yizhou Wang, Yaodong Yang",Jul 2023,https://arxiv.org/abs/2307.04657
A Survey on Evaluation of Large Language Models,"Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S. Yu, Qiang Yang, Xing Xie",Jul 2023,https://arxiv.org/abs/2307.03109
DeepfakeBench: A Comprehensive Benchmark of Deepfake Detection,"Zhiyuan Yan, Yong Zhang, Xinhang Yuan, Siwei Lyu, Baoyuan Wu",Jul 2023,https://arxiv.org/abs/2307.01426
Can large language models democratize access to dual-use biotechnology?,"Emily H. Soice, Rafael Rocha, Kimberlee Cordova, Michael Specter, Kevin M. Esvelt",Jun 2023,https://arxiv.org/pdf/2306.03809
Model evaluation for extreme risks,"Toby Shevlane, Sebastian Farquhar, Ben Garfinkel, Mary Phuong, Jess Whittlestone, Jade Leung, Daniel Kokotajlo, Nahema Marchal, Markus Anderljung, Noam Kolt, Lewis Ho, Divya Siddarth, Shahar Avin, Will Hawkins, Been Kim, Iason Gabriel, Vijay Bolina, Jack Clark, Yoshua Bengio, Paul Christiano, Allan Dafoe",May 2023,https://arxiv.org/pdf/2305.15324
Direct Preference Optimization: Your Language Model is Secretly a Reward Model,"Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, Chelsea Finn",May 2023,https://arxiv.org/abs/2305.18290
Heterogeneous Value Alignment Evaluation for Large Language Model,"Zhaowei Zhang, Ceyao Zhang, Nian Liu, Siyuan Qi, Ziqi Rong, Song-Chun Zhu, Shuguang Cui, Yaodong Yang",May 2023,https://arxiv.org/abs/2305.17147
OmniSafe: An Infrastructure for Accelerating Safe Reinforcement Learning Research,"Jiaming Ji, Jiayi Zhou, Borong Zhang, Juntao Dai, Xuehai Pan, Ruiyang Sun, Weidong Huang, Yiran Geng, Mickel Liu, Yaodong Yan",May 2023,https://arxiv.org/abs/2305.09304
"""Oops, Did I Just Say That?"" Testing and Repairing Unethical Suggestions of Large Language Models with Suggest-Critique-Reflect Process","Pingchuan Ma, Zongjie Li, Ao Sun, Shuai Wang",May 2023,https://arxiv.org/abs/2305.02626
Harm to Nonhuman Animals from AI: a Systematic Account and Framework,"Simon Coghlan, Christine Parker",Apr 2023,https://link.springer.com/article/10.1007/s13347-023-00627-6
Safety Assessment of Chinese Large Language Models,"Hao Sun, Zhexin Zhang, Jiawen Deng, Jiale Cheng, Minlie Huang",Apr 2023,https://arxiv.org/abs/2304.10436
Generative Agents: Interactive Simulacra of Human Behavior,"Joon Sung Park, Joseph C. O'Brien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, Michael S. Bernstein",Apr 2023,https://arxiv.org/abs/2304.03442
What does it take to catch a Chinchilla? Verifying Rules on Large-scale Neural Network Training via Compute Monitoring,Yonadav Shavit,Mar 2023,https://arxiv.org/pdf/2303.11341
Deep Reinforcement Learning from Human Preferences,"Paul F Christiano, Jan Leike, Tom B Brown, Miljan Martic, Shane Legg, Dario Amodei",Feb 2023,https://arxiv.org/pdf/1706.03741
LLaMA: Open and Efficient Foundation Language Models,"Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample",Feb 2023,https://arxiv.org/abs/2302.13971
Looped Transformers as Programmable Computers,"Angeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason D. Lee, Dimitris Papailiopoulos",Jan 2023,https://arxiv.org/abs/2301.13196
Causal Scrubbing: a method for rigorously testing interpretability hypotheses,"Lawrence Chan, Adrià Garriga-Alonso, Nicholas Goldowsky-Dill, Ryan Greenblatt, Jenny Nitishinskaya, Ansh Radhakrishnan, Buck Shlegeris, Nate Thomas",Dec 2022,https://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing
Discovering Language Model Behaviors with Model-Written Evaluations,"Ethan Perez, Sam Ringer, Kamile Lukosiute, Karina Nguyen, Edwin Chen, Scott Heiner, Craig Pettit, Catherine Olsson, Sandipan Kundu, Saurav Kadavath, Andy Jones, Anna Chen, Ben Mann, Brian Israel, Bryan Seethor, Cameron McKinnon, Christopher Olah, Da Yan, Daniela Amodei, Dario Amodei, Dawn Drain, Dustin Li, Eli Tran-Johnson, Guro Khundadze, Jackson Kernion, James Landis, Jamie Kerr, Jared Mueller, Jeeyoon Hyun, Joshua Landau, Kamal Ndousse, Landon Goldberg, Liane Lovitt, Martin Lucas, Michael Sellitto, Miranda Zhang, Neerav Kingsland, Nelson Elhage, Nicholas Joseph, Noemí Mercado, Nova DasSarma, Oliver Rausch, Robin Larson, Sam McCandlish, Scott Johnston, Shauna Kravec, Sheer El Showk, Tamera Lanham, Timothy Telleen-Lawton, Tom Brown, Tom Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Jack Clark, Samuel R. Bowman, Amanda Askell, Roger Grosse, Danny Hernandez, Deep Ganguli, Evan Hubinger, Nicholas Schiefer, Jared Kaplan",Dec 2022,https://arxiv.org/abs/2212.09251
Constitutional AI: Harmlessness from AI Feedback,"Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, Jared Kaplan",Dec 2022,https://arxiv.org/abs/2212.08073
Interpretability in the Wild: A Circuit for Indirect Object Identification in GPT-2 small,"Kevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, Jacob Steinhardt",Nov 2022,https://arxiv.org/pdf/2211.00593
Measuring Progress on Scalable Oversight for Large Language Models,"Samuel R. Bowman, Jeeyoon Hyun, Ethan Perez, Edwin Chen, Craig Pettit, Scott Heiner, Kamile Lukosiute, Amanda Askell, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Christopher Olah, Daniela Amodei, Dario Amodei, Dawn Drain, Dustin Li, Eli Tran-Johnson, Jackson Kernion, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Liane Lovitt, Nelson Elhage, Nicholas Schiefer, Nicholas Joseph, Noemí Mercado, Nova DasSarma, Robin Larson, Sam McCandlish, Sandipan Kundu, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Timothy Telleen-Lawton, Tom Brown, Tom Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Ben Mann, Jared Kaplan",Nov 2022,https://arxiv.org/abs/2211.03540
Scaling Laws for Reward Model Overoptimization,"Leo Gao, John Schulman, Jacob Hilton",Oct 2022,https://arxiv.org/abs/2210.10760
Toy Models of Superposition,"Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, Roger Grosse, Sam McCandlish, Jared Kaplan, Dario Amodei, Martin Wattenberg, Christopher Olah",Sep 2022,https://transformer-circuits.pub/2022/toy_model/index.html#discussion
Red Teaming with Mind Reading: White-Box Adversarial Policies Against RL Agents,"Stephen Casper, Taylor Killian, Gabriel Kreiman, Dylan Hadfield-Menell",Sep 2022,https://arxiv.org/abs/2209.02167
"A Survey of the Potential Long-term Impacts of AI: How AI Could Lead to Long-term Changes in Science, Cooperation, Power, Epistemics and Values","Sam Clarke, Jess Whittlestone",Aug 2022,https://dl.acm.org/doi/pdf/10.1145/3514094.3534131
Is Power-Seeking AI an Existential Risk?,Joseph Carlsmith,Jun 2022,https://arxiv.org/abs/2206.13353
Emergent Abilities of Large Language Models,"Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, William Fedus",Jun 2022,https://arxiv.org/abs/2206.07682
FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness,"Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, Christopher Re",May 2022,https://arxiv.org/abs/2205.14135
AI Colonialism,"Karen Hao, Heidi Swart, Andrea Paola Hernández, Nadine Freischlad",Apr 2022,https://www.technologyreview.com/supertopic/ai-colonialism-supertopic
In-context Learning and Induction Heads,"Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, Chris Olah",Mar 2022,https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html
Training Compute-Optimal Large Language Models,"Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, Laurent Sifre",Mar 2022,https://arxiv.org/abs/2203.15556
Training language models to follow instructions with human feedback,"Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, Ryan Lowe",Mar 2022,https://arxiv.org/abs/2203.02155
Structured access: an emerging paradigm for safe AI deployment,Toby Shevlane,Jan 2022,https://arxiv.org/abs/2201.05159
Hard choices in artificial intelligence,"Roel Dobbe, Thomas Krendl Gilbert, Yonatan Mintz",Nov 2021,https://doi.org/10.1016/j.artint.2021.103555
LoRA: Low-Rank Adaptation of Large Language Models,"Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen",Jun 2021,https://arxiv.org/abs/2106.09685
An Interpretability Illusion for BERT,"Tolga Bolukbasi, Adam Pearce, Ann Yuan, Andy Coenen, Emily Reif, Fernanda Viegas, Martin Wattenberg",Apr 2021,https://arxiv.org/pdf/2104.07143
On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?,"Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, Shmargaret Shmitchell",Mar 2021,https://dl.acm.org/doi/pdf/10.1145/3442188.3445922
Multimodal Neurons in Artificial Neural Networks,"Gabriel Goh, Nick Cammarata, Chelsea Voss, Shan Carter, Michael Petrov, Ludwig Schubert, Alec Radford, Chris Olah",Mar 2021,https://distill.pub/2021/multimodal-neurons/
Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI Offer a Safer Path?,"Yoshua Bengio, Michael Cohen, Damiano Fornasiere, Joumana Ghosn, Pietro Greiner, Matt MacDermott, Sören Mindermann, Adam Oberman, Jesse Richardson, Oliver Richardson, Marc-Antoine Rondeau, Pierre-Luc St-Charles, David Williams-King",Feb 2021,https://arxiv.org/abs/2502.15657
SWE-Lancer: Can Frontier LLMs Earn $1 Million from Real-World Freelance Software Engineering?,,Feb 2021,https://arxiv.org/abs/2502.12115
Learning Transferable Visual Models From Natural Language Supervision,"Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever",Feb 2021,https://arxiv.org/abs/2103.00020
An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,"Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby",Oct 2020,https://arxiv.org/abs/2010.11929
Understanding Human Intelligence through Human Limitations,Thomas L. Griffiths,Sep 2020,https://arxiv.org/abs/2009.14050
Measuring Massive Multitask Language Understanding,"Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt",Sep 2020,https://arxiv.org/abs/2009.03300
Language Models are Few-Shot Learners,"Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei",May 2020,https://arxiv.org/abs/2005.14165
Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,"Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela",May 2020,https://arxiv.org/abs/2005.11401
On Layer Normalization in the Transformer Architecture,"Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, Tie-Yan Liu",Feb 2020,https://arxiv.org/abs/2002.04745
Fine-Tuning Language Models from Human Preferences,"Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, Geoffrey Irving",Jan 2020,https://arxiv.org/pdf/1909.08593
Scaling Laws for Neural Language Models,"Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei",Jan 2020,https://arxiv.org/abs/2001.08361
Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu",Oct 2019,https://arxiv.org/abs/1910.10683
Language Models are Unsupervised Multitask Learners,"Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever",Feb 2019,https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova",Oct 2018,https://arxiv.org/abs/1810.04805
Improving Language Understanding by Generative Pre-Training,"Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever",Jun 2018,https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf
AGI Safety Literature Review,"Tom Everitt, Gary Lea, Marcus Hutter",May 2018,https://arxiv.org/abs/1805.01109
"CycleGAN, a Master of Steganography","Casey Chu, Andrey Zhmoginov, Mark Sandler",Dec 2017,https://arxiv.org/abs/1712.02950
Proximal Policy Optimization Algorithms,"John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov",Jul 2017,https://arxiv.org/abs/1707.06347
Attention Is All You Need,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin",Jun 2017,https://arxiv.org/abs/1706.03762
Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer,"Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, Jeff Dean",Jan 2017,https://arxiv.org/abs/1701.06538
"How Do AI Companies ""Fine-Tune"" Policy? Examining Regulatory Capture in AI Governance","Kevin Wei, Carson Ezell, Nick Gabrieli, Chinmay Deshpande",Oct 2024,https://arxiv.org/abs/2410.13042
Security Degradation in Iterative AI Code Generation -- A Systematic Analysis of the Paradox,"Shivani Shukla, Himanshu Joshi, Romilla Syed",May 2025,https://arxiv.org/abs/2506.11022