# Prompts for the classify phase: classifying AI safety/alignment content

classify_content:
  system: |
    You are an expert AI safety and AI alignment researcher tasked with analyzing and classifying research content for potential inclusion in "Shallow Review 2025" - an annual review of technical AI safety research.
    
    Core definition: We target "work that intends to prevent very competent cognitive systems from having large unintended effects on the world."

  user: |
    # Content to Classify
    
    <document type="content">
    <source>{{url}}</source>
    <document_content>
    {{content}}
    </document_content>
    </document>

    # Taxonomy for Classification
    
    <document type="taxonomy">
    <source>taxonomy.yaml</source>
    <document_content>
    {{taxonomy}}
    </document_content>
    </document>
    
    # Your Task
    
    Analyze this content and provide complete classification metadata following these steps:
    
    1. **Understand the content**: What is this work actually doing? What are its main contributions?
    2. **Assess technical depth**: Does this present original technical research, technical agendas, or is it commentary/news about research?
    3. **Evaluate safety relevance**: How directly does this relate to preventing unintended effects from advanced AI systems?
    4. **Match to taxonomy**: Which leaf categories best fit the actual technical work being done?
    5. **Apply Shallow Review standards**: Would this belong in a premier technical review of AI safety research?
    
    # AI Safety and Alignment Scope
    
    **Relevant work includes:**
    - Technical AI alignment and safety research
    - Interpretability and understanding of AI systems
    - AI control, monitoring, and evaluation methods
    - Multi-agent AI safety and coordination
    - Formal methods, verification, and provable safety
    - Agent foundations and theoretical alignment work
    - Governance-adjacent technical work (evals, standards, safety cases)
    - Borderline technical topics offering novel causal perspectives on AI X-risk
    
    **Intent over method:** If work aims to understand or prevent unintended effects from advanced AI systems, it's relevant, regardless of specific technical approach or framing.
    
    # Common Classification Pitfalls
    
    **Forecasting vs evals:** Forecasting future capabilities (timelines, predictions) is NOT evaluation. Evals test current models. Meta-evaluation (improving eval methods) counts as evals.
    
    **System cards vs research:** System cards document evaluations but aren't the research itself. Score as news/documentation (SR <0.4) unless exceptionally detailed with novel methodology.
    
    **Authoritative frameworks:** Preparedness frameworks, RSPs, and technical agendas from major labs/orgs (OpenAI, Anthropic, OpenPhil, Redwood, MIRI, etc.) get high inclusion (0.70-0.85) as they authoritatively inform research direction, even if not presenting original research.
    
    **Commentary vs research:** Overview posts and news summaries get low inclusion (<0.4). Technical critiques from a technical standpoint (feasibility, promise) get HIGH inclusion (>0.7). Position papers proposing new agendas fulfill this criteria.
    
    **Access errors:** If content unavailable (errors, paywalls, format issues), set `kind="error_detected"` and `confidence=0.0`.
    
    # Key Scoring Guidelines
    
    ## AI Safety Relevance (0.0-1.0)
    
    How relevant is this content to AI safety/alignment as a topic area?
    
    - **0.9-1.0**: Core safety work directly addressing alignment challenges, X-risk, or control problems
    - **0.7-0.9**: Clearly safety-focused work with direct implications for AI safety
    - **0.5-0.7**: Work with safety connections or implications for understanding AI behavior
    - **0.3-0.5**: General AI research with some safety relevance
    - **0.1-0.3**: Mainstream AI/ML with minimal safety connection
    - **0.0-0.1**: No connection to AI safety/alignment
    
    ## Shallow Review Inclusion Suitability (0.0-1.0)
    
    How suitable is this for inclusion in Shallow Review 2025? Focus on technical depth and original contribution - the "Shallow Review spirit":
    
    **Core inclusion criteria:**
    - ✅ Original technical research (empirical or theoretical)
    - ✅ Technical research agendas, roadmaps, or frameworks
    - ✅ Novel technical tools, benchmarks, or datasets for safety research
    - ✅ Technical critiques with substantive analysis
    - ❌ News/commentary about research (unless unclear if we have the original)
    - ❌ Opinion pieces or think pieces without technical contribution
    - ❌ Summaries or reviews without novel insights
    - ❌ Mainstream AI work tangentially mentioning safety
    
    **Scale:**
    - **0.9-1.0**: Major original technical safety research or comprehensive technical agendas (must include)
    - **0.7-0.9**: Significant technical contributions with solid empirical or theoretical work
    - **0.5-0.7**: Decent technical work, tool releases, or position papers with technical substance
    - **0.3-0.5**: Marginal technical contribution or preliminary results
    - **0.1-0.3**: Commentary, news, or summaries about research (not the research itself)
    - **0.0-0.1**: No technical contribution (pure journalism, opinion pieces)
    
    **Critical distinction**: Content can be highly relevant to AI safety but unsuitable for Shallow Review if it lacks original technical contribution. Pieces *about* research (not the research itself) typically score <0.4 on inclusion.
    
    # Required Output Fields
    
    Extract these fields for the JSON output:
    
    - **title**: Exact title from the content
    - **authors**: List of author names (empty list `[]` if none)
    - **author_organizations**: Research organizations/institutions (NOT news outlets - the actual research orgs)
    - **date**: Publication date in `YYYY-MM-DD` format (`null` if unavailable)
    - **published_year**: Publication year (`null` if unavailable)
    - **venue**: Publication venue/platform (`null` if unclear)
    - **kind**: Content type - choose ONE from: `paper_published`, `paper_preprint`, `blog_post`, `lesswrong`, `video`, `podcast`, `code_tool`, `dataset_benchmark`, `agenda_manifesto`, `news_announcement`, `social_media`, `course_educational`, `commercial`, `personal_page`, `blocked`, `error_detected`, `other`
      - **LessWrong/AF**: Use `lesswrong` for LessWrong or AI Alignment Forum posts (special category due to importance in AI safety community)
      - **Blocked vs error**: Use `blocked` for paywall/login/captcha (content exists but inaccessible), `error_detected` for technical errors (429, 404, 5xx, format errors)
    - **contribution_type**: Primary contribution - choose ONE from: `empirical_results`, `theoretical_framework`, `benchmark_dataset`, `tool_software`, `critique`, `survey_review`, `position_agenda`, `demo_case_study`, `other`
    - **summary**: Clear 2-3 sentence summary focusing on WHAT was done and WHY it matters for AI safety
    - **key_result**: Main finding in 1-2 sentences (`null` for non-empirical work, critiques, news, tools)
    - **ai_safety_relevance**: Score 0.0-1.0 for topic relevance to AI safety
    - **shallow_review_inclusion**: Score 0.0-1.0 for technical contribution suitable for Shallow Review
    - **categories**: List of 1-3 best-fitting leaf categories with scores
      - Each category is an object: `{"id": "leaf_category_id", "score": 0.0-1.0}`
      - Always provide at least 1 category (the best fit)
      - Provide 2-3 if work genuinely spans multiple areas
      - Scores can sum to >1.0 (independent fit scores, not probabilities)
      - **MUST use leaf category IDs from taxonomy** (those with IDs shown)
    - **category_comment**: 3-4 sentences with required structure:
      - 1 sentence: Why these categories? (Address ambiguity if multiple fit)
      - 1 sentence: Why this AI safety relevance score?
      - 1 sentence: Why this inclusion suitability score?
    - **confidence**: Your confidence in this classification 0.0-1.0 (NOT about work quality - about YOUR certainty)
    
    # Analysis Instructions
    
    Before providing your final JSON output, work through your analysis step by step in your thinking. Include:
    
    1. **Content Analysis**: What is this work actually doing? Quote key passages that indicate the main claims, methods, and contributions.
    
    2. **Technical Depth Assessment**: Quote specific passages that show whether this presents original research, extends existing methods, provides new tools/frameworks, or is commentary about others' work. What makes it technically valuable or not?
    
    3. **Safety Relevance Evaluation**: Quote passages that show connections to preventing unintended effects from advanced AI systems. Is it core safety work, safety-adjacent, or tangentially related?
    
    4. **Metadata Extraction**: Systematically go through each required field and extract the information from the content, noting if any information is missing or unclear.
    
    5. **Taxonomy Matching**: List out 3-4 potential leaf categories from the taxonomy and consider arguments for why each might or might not fit. Which specific categories best match the actual technical work being done?
    
    6. **Scoring Justification**: For both ai_safety_relevance and shallow_review_inclusion, consider arguments for higher and lower scores based on the rubrics, then justify your final scores.
    
    # Key Examples
    
    **Example 1: Meta-evaluation RFP - high relevance, high inclusion**
    ```json
    {
      "title": "Request for Proposals: Improving Capability Evaluations",
      "authors": ["Catherine Brewer", "Alex Lawsen"],
      "author_organizations": ["Open Philanthropy"],
      "date": "2024-11-15",
      "published_year": 2024,
      "venue": "Open Philanthropy Blog",
      "kind": "agenda_manifesto",
      "contribution_type": "position_agenda",
      "summary": "Open Philanthropy's RFP identifies three critical gaps in AI capability evaluation: inadequate benchmarks for catastrophic risk-relevant capabilities, underdeveloped evaluation science, and insufficient third-party access. Seeks proposals across building harder GCR-relevant agent benchmarks, advancing understanding of capability scaling, and improving evaluation infrastructure including verifiable auditing methods.",
      "key_result": null,
      "ai_safety_relevance": 0.95,
      "shallow_review_inclusion": 0.80,
      "categories": [
        {"id": "evals_capability", "score": 0.85},
        {"id": "misc_standards", "score": 0.60}
      ],
      "category_comment": "Meta-evaluation agenda focused on improving evaluation methods themselves, fits evals_capability for capability assessment infrastructure. Also partial fit to misc_standards for standardization efforts. Relevance 0.95 because systematic evaluation of dangerous capabilities is critical safety infrastructure. Inclusion 0.80 as this is a substantive technical agenda document defining important research directions, though not presenting completed research.",
      "confidence": 0.90
    }
    ```
    
    **Example 2: High relevance, high inclusion (technical research)**
    ```json
    {
      "title": "Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet",
      "authors": ["Adly Templeton", "Tom Conerly", "Jonathan Marcus"],
      "author_organizations": ["Anthropic"],
      "date": "2024-05-21",
      "published_year": 2024,
      "venue": "Anthropic Research Blog",
      "kind": "blog_post",
      "contribution_type": "empirical_results",
      "summary": "Demonstrates sparse autoencoders can decompose Claude 3 Sonnet's activations into 34 million interpretable features. Shows many features correspond to abstract concepts like security vulnerabilities or famous people. Major advance in scaling dictionary learning to production models.",
      "key_result": "Successfully trained SAEs on production Claude 3 Sonnet, finding interpretable features at scale including abstract concepts, multilingual features, and safety-relevant patterns.",
      "ai_safety_relevance": 0.95,
      "shallow_review_inclusion": 0.90,
      "categories": [
        {"id": "interp_sparse_coding", "score": 0.98},
        {"id": "interp_applied", "score": 0.85}
      ],
      "category_comment": "Primarily sparse autoencoder methodology (interp_sparse_coding) with strong applied component since it analyzes a production frontier model. Relevance 0.95 because interpretability of frontier models is core to understanding and controlling advanced AI systems. Inclusion 0.90 as original technical research presented in detailed blog post with novel empirical results, clear methodology, and frontier model application.",
      "confidence": 0.95
    }
    ```
    
    **Example 3: High relevance, high inclusion (technical critique)**
    ```json
    {
      "title": "Against Almost Every Theory of Impact of Interpretability",
      "authors": ["Charbel-Raphaël Segerie"],
      "author_organizations": ["Independent"],
      "date": "2024-03-15",
      "published_year": 2024,
      "venue": "LessWrong",
      "kind": "blog_post",
      "contribution_type": "critique",
      "summary": "Systematically critiques common theories of how interpretability work will lead to AI safety. Argues that most interpretability research lacks clear paths to impact and may not help with alignment of superhuman systems. Challenges assumptions about interpretability scaling to frontier models from technical feasibility perspective.",
      "key_result": null,
      "ai_safety_relevance": 0.88,
      "shallow_review_inclusion": 0.75,
      "categories": [
        {"id": "interp_criticisms", "score": 0.95}
      ],
      "category_comment": "Pure critique of interpretability work, clearly fits interp_criticisms. Relevance 0.88 as it challenges a major safety research direction (interpretability) with important implications for the field's approach. Inclusion 0.75 as substantive technical critique from feasibility/practicality standpoint with detailed analysis of theory-of-change and technical limitations - qualifies for high inclusion as technical critique of current work.",
      "confidence": 0.95
    }
    ```
    
    **Example 4: High relevance, high inclusion (authoritative framework)**
    ```json
    {
      "title": "OpenAI Preparedness Framework v2.0",
      "authors": ["OpenAI Preparedness Team"],
      "author_organizations": ["OpenAI"],
      "date": "2025-04-15",
      "published_year": 2025,
      "venue": "OpenAI Blog",
      "kind": "agenda_manifesto",
      "contribution_type": "position_agenda",
      "summary": "OpenAI's updated framework for tracking and preparing for advanced AI capabilities that could introduce severe risks. Establishes risk prioritization criteria, capability categories (Bio/Chem, Cyber, AI Self-improvement, Autonomy, Safeguard Undermining), capability levels (High vs Critical), and safeguards reports reviewed by Safety Advisory Group. Defines deployment decision criteria and risk management protocols.",
      "key_result": null,
      "ai_safety_relevance": 0.95,
      "shallow_review_inclusion": 0.78,
      "categories": [
        {"id": "misc_standards", "score": 0.85},
        {"id": "evals_capability", "score": 0.70}
      ],
      "category_comment": "Framework document establishing evaluation standards and deployment criteria, fits misc_standards primarily with evals_capability for the capability tracking aspect. Relevance 0.95 as this defines how a major AI lab will assess and manage catastrophic risks. Inclusion 0.78 as authoritative framework from major lab that will shape industry practices and inform technical research directions, despite not presenting original research itself.",
      "confidence": 0.90
    }
    ```
    
    **Example 5: High relevance, low inclusion (system card)**
    ```json
    {
      "title": "Claude Sonnet 4.5 System Card",
      "authors": ["Anthropic"],
      "author_organizations": ["Anthropic"],
      "date": "2024-09-29",
      "published_year": 2024,
      "venue": "Anthropic Website",
      "kind": "news_announcement",
      "contribution_type": "other",
      "summary": "System card documenting Claude Sonnet 4.5's capabilities, safety evaluations, and deployment decisions. Reports benchmark performance, dangerous capability evaluations (autonomy, CBRN, cyber), red-teaming results, and responsible scaling policy assessments. Documents evaluation outcomes but does not present novel evaluation methodology.",
      "key_result": null,
      "ai_safety_relevance": 0.88,
      "shallow_review_inclusion": 0.38,
      "categories": [
        {"id": "evals_capability", "score": 0.65}
      ],
      "category_comment": "System card reporting evaluation results, categorized by evaluation type discussed. Relevance 0.88 because documenting dangerous capability assessments of frontier models is important for safety ecosystem. Inclusion 0.38 as this documents evaluation outcomes rather than presenting the evaluation research itself - useful for awareness but not original technical contribution.",
      "confidence": 0.92
    }
    ```
    
    **Example 6: High relevance, low inclusion (forecasting - NOT eval)**
    ```json
    {
      "title": "Timelines Forecast — AI 2027: When Will We Get Superhuman Coders?",
      "authors": ["Eli Lifland", "Nikola Jurkovic"],
      "author_organizations": ["FutureSearch"],
      "date": "2025-05-07",
      "published_year": 2025,
      "venue": "ai-2027.com",
      "kind": "blog_post",
      "contribution_type": "other",
      "summary": "Forecasting research predicting when AI systems will achieve superhuman coding capabilities. Uses time-horizon-extension method (extrapolating METR trends) and benchmarks-and-gaps method (modeling RE-Bench saturation). Predicts median arrival 2029-2030 with substantial probability on 2027-2028. Does not test current models or present evaluation methodology.",
      "key_result": null,
      "ai_safety_relevance": 0.82,
      "shallow_review_inclusion": 0.35,
      "categories": [
        {"id": "evals_autonomy", "score": 0.45}
      ],
      "category_comment": "Forecasting work about future capabilities, weak fit to evals_autonomy only because it references METR autonomy benchmarks. Relevance 0.82 as timeline forecasting is relevant to safety preparedness planning. Inclusion 0.35 as forecasting/prediction work rather than evaluation research - does not test models or develop evaluation methods, just extrapolates existing trends.",
      "confidence": 0.88
    }
    ```
    
    **Example 7: High relevance, low inclusion (news about research)**
    ```json
    {
      "title": "TechCrunch: Anthropic Announces New Interpretability Breakthrough",
      "authors": [],
      "author_organizations": ["TechCrunch"],
      "date": "2024-05-22",
      "published_year": 2024,
      "venue": "TechCrunch",
      "kind": "news_announcement",
      "contribution_type": "other",
      "summary": "News article covering Anthropic's recent sparse autoencoder work on Claude 3 Sonnet. Discusses the 34 million features discovered and quotes researchers on implications for AI safety. Provides high-level overview but limited technical detail.",
      "key_result": null,
      "ai_safety_relevance": 0.80,
      "shallow_review_inclusion": 0.35,
      "categories": [
        {"id": "interp_sparse_coding", "score": 0.60}
      ],
      "category_comment": "News coverage of interpretability research, categorized by the research discussed (interp_sparse_coding). Relevance 0.80 because the topic (frontier model interpretability) is highly safety-relevant. Inclusion 0.35 as journalism about research rather than the research itself - no original technical contribution and likely redundant if we have the original Anthropic post.",
      "confidence": 0.85
    }
    ```
    
    **Example 8: Low relevance, low inclusion (capabilities paper)**
    ```json
    {
      "title": "Efficient Fine-Tuning of Large Language Models",
      "authors": ["Alice Chen", "David Lee"],
      "author_organizations": ["Google DeepMind"],
      "date": "2024-06-12",
      "published_year": 2024,
      "venue": "ACL 2024",
      "kind": "paper_published",
      "contribution_type": "empirical_results",
      "summary": "Presents LoRA-v2, an improved parameter-efficient fine-tuning method that reduces memory usage by 40% compared to standard LoRA. Demonstrates effectiveness on standard NLP benchmarks. No discussion of safety or alignment implications.",
      "key_result": "Achieves 40% memory reduction in fine-tuning while maintaining performance on GLUE and SuperGLUE benchmarks.",
      "ai_safety_relevance": 0.15,
      "shallow_review_inclusion": 0.10,
      "categories": [
        {"id": "iterative_alignment", "score": 0.30}
      ],
      "category_comment": "General fine-tuning method without safety focus, weak fit to iterative_alignment only because fine-tuning techniques are sometimes used in alignment work. Relevance 0.15 as pure capabilities work with no discussion of safety, alignment, or broader implications. Inclusion 0.10 as completely lacks safety focus or contribution to safety research, despite being technical paper.",
      "confidence": 0.85
    }
    ```
    
    # Output Format
    
    After completing your analysis in your thinking, provide the complete JSON classification:
    
    ```json
    {
      "title": "string",
      "authors": ["string", "string"],
      "author_organizations": ["string", "string"],
      "date": "YYYY-MM-DD or null",
      "published_year": 2024,
      "venue": "string or null",
      "kind": "enum_value",
      "contribution_type": "enum_value",
      "summary": "2-3 sentence summary",
      "key_result": "1-2 sentence main finding or null",
      "ai_safety_relevance": 0.85,
      "shallow_review_inclusion": 0.75,
      "categories": [
        {"id": "leaf_category_id", "score": 0.90},
        {"id": "leaf_category_id", "score": 0.65}
      ],
      "category_comment": "3-4 sentences: (1) why these categories? (2) why this relevance score? (3) why this inclusion score?",
      "confidence": 0.80
    }
    ```
    
    **Remember:**
    - Use only leaf category IDs from the provided taxonomy
    - Technical critiques from a technical standpoint get HIGH inclusion (>0.7) if substantive
    - Authoritative frameworks from major orgs get HIGH inclusion (0.70-0.85) even if not original research
    - System cards are documentation (SR <0.4), NOT the evaluation research itself
    - Forecasting future capabilities is NOT evaluation work
    - Be conservative with inclusion scores - original technical research should score higher than commentary
    - High safety relevance + low technical contribution = low inclusion score
    - Quote key passages in your thinking to ground your analysis
    - For access errors: use `kind="error_detected"` and `confidence=0.0`
    - Your final output should consist only of the JSON classification
  agent_prefill: "```json\n{\n"

