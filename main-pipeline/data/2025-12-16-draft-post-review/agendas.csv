name,section,sub_section,full_name,one_sentence_summary,theory_of_change,see_also,orthodox_problems,target_case_text,broad_approach_text,some_names,estimated_ftes,critiques,funded_by,outputs_count
OpenAI,Labs (giant companies),,Labs (giant companies) / OpenAI,,,"sec:Iterative_alignment, a:Safeguards_inference_time_auxiliaries_, a:Character_training_and_persona_steering",,,,"Johannes Heidecke, Boaz Barak, Mia Glaese, Jenny Nitishinskaya, Lama Ahmad, Naomi Bashkansky, Miles Wang, Wojciech Zaremba, David Robinson, Zico Kolter, Jerry Tworek, Eric Wallace, Olivia Watkins, Kai Chen, Chris Koch, Andrea Vallone, Leo Gao",,"[Stein-Perlman](https://ailabwatch.org/companies/openai), [Stewart](https://intelligence.org/2025/03/31/a-response-to-openais-how-we-think-about-safety-and-alignment/), [underelicitation](https://www.lesswrong.com/posts/AK6AihHGjirdoiJg6/ai-companies-eval-reports-mostly-don-t-support-their-claims), [Midas](https://www.openaifiles.org/transparency-and-safety), [defense](https://www.wired.com/story/openai-anduril-defense/), [Carlsmith](https://joecarlsmith.com/2025/11/03/leaving-open-philanthropy-going-to-anthropic#:~:text=a%20few%20words%20about%20some%20more%20general%20concerns%20about%20AI%2Dsafety%2Dfocused%20people%20going%20to%20work%20at%20AI%20companies%20\(and/or%2C%20at%20Anthropic%20in%20particular\).) on labs in general. It's [difficult](https://conversationswithtyler.com/episodes/sam-altman-2/) to model OpenAI as a single agent: *""ALTMAN: I very rarely get to have anybody work on anything. One thing about researchers is they're going to work on what they're going to work on, and that's that.""*","Microsoft, [AWS](https://www.aboutamazon.com/news/aws/aws-open-ai-workloads-compute-infrastructure), Oracle, NVIDIA, SoftBank, G42, AMD, Dragoneer, Coatue, Thrive, Altimeter, MGX, Blackstone, TPG, T. Rowe Price, Andreessen Horowitz, D1 Capital Partners, Fidelity Investments, Founders Fund, Sequoia…",13
Google Deepmind,Labs (giant companies),,Labs (giant companies) / Google Deepmind,,,"sec:White_box_safety_i_e_Interpretability_, Scalable Oversight",,,,"Rohin Shah, Allan Dafoe, Anca Dragan, Alex Irpan, Alex Turner, Anna Wang, Arthur Conmy, David Lindner, Jonah Brown-Cohen, Lewis Ho, Neel Nanda, Raluca Ada Popa, Rishub Jain, Rory Greig, Sebastian Farquhar, Senthooran Rajamanoharan, Sophie Bridgers, Tobi Ijitoye, Tom Everitt, Victoria Krakovna, Vikrant Varma, Zac Kenton, Four Flynn, Jonathan Richens, Lewis Smith, Janos Kramar, Matthew Rahtz, Mary Phuong, Erik Jenner",,"[Stein-Perlman](https://ailabwatch.org/companies/deepmind), [Carlsmith](https://joecarlsmith.com/2025/11/03/leaving-open-philanthropy-going-to-anthropic#:~:text=a%20few%20words%20about%20some%20more%20general%20concerns%20about%20AI%2Dsafety%2Dfocused%20people%20going%20to%20work%20at%20AI%20companies%20\(and/or%2C%20at%20Anthropic%20in%20particular\).) on labs in general, [underelicitation](https://www.lesswrong.com/posts/AK6AihHGjirdoiJg6/ai-companies-eval-reports-mostly-don-t-support-their-claims), [On Google's Safety Plan](https://lesswrong.com/posts/hvEikwtsbf6zaXG2s/on-google-s-safety-plan)","Google. Explicit 2024 Deepmind spending as a whole was [£1.3B](https://s3.eu-west-2.amazonaws.com/document-api-images-live.ch.gov.uk/docs/WT_VNJe9leRjfcU0-OtRjWqF7WiqueStclXgHPbdG4U/application-pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=ASIAWRGBDBV3HTI6EAXB%2F20251212%2Feu-west-2%2Fs3%2Faws4_request&X-Amz-Date=20251212T104902Z&X-Amz-Expires=60&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEDcaCWV1LXdlc3QtMiJGMEQCIH6GyZRz66qwZsWkZORhsAQyzQQoJ7j0F4jnnWsgT8i2AiBkQVTWkLiSt%2F89o2yMC5D9NGQ75b1RwC8MBr7dlvrJXyqBBAgAEAUaDDQ0OTIyOTAzMjgyMiIMDkbkT%2FH2vUMV3NxOKt4DfZ6bw%2BWE%2BPfifW4goryaR4bQ%2FeFEXDvW7MU%2BKlfUM8A2fNyEUpIq4f6PsRf0zntVIXmUOWnvyIcVB7EA31NcPn3O%2FHFga8gKZyDPnQnj7YM5Wrt%2FVvR2mj7dJcioOSATW6joYuAb2X0l6IVHXJnYcaxStVCaPauK98OWTTXwCQQwG9UYBWe5SGqOroOw%2FoYWx9GRGvDtQfQThGemJnDr%2FHkbM9YH%2BY860lrE4MEXQiPakkwgJZC%2B8kqsqxzAIyWegPjp3TvrNs7WJ4Fheq0BJo8B7uw0pYBB%2BE9WQEjgaO5dByd90cpnyHu%2F8HGSxwmuQQiUtrp0T3xpP1G%2B3bP%2FLUnhGTD6XWLW%2BtoywQ5ZJrizfwuLQuxFjZt2JwV50DslF47H4AltBRxQh6HHro%2BpiJJEv0rC5NKBS4XRaL8FWOFMD%2BxJctPoCxFJhour3SbcMET4148eVQL%2FenkSdPUz2FHNrO%2BnOTyZAG%2Bi9xiZR1MVOCYHTPHKFG9ReY4ck2mz4W94%2FI6iWuu%2BKWlrEr2hEWzo2RhwDJ09ASgoKNErYb2mJ4E0rMGQ7cv8d2bqF7f6ok1SbzJPClaCBN4qYBzX1rE2Uhdf4v2QueSi4c0i8oCWOGfsdp5FxpgrOlEIqzC9%2Fu7JBjqmAardqlTk%2BobAEzv0H0m2RO4m901C%2FsTzIKb2UlMRrUkTDH4MpCSg5eW3A86X2TnPfl66jC%2FV2P%2FIwY%2FkvsY7wNBgtYR92XE%2FMwyz1x3JD1qDnGWPybjso72aEPrMyekV2WV3U0%2BYh8zn83%2BneYZB9VaTu2QqSv7TZe3IWJyErbuZw%2BhmMlk5nhKZDNmo%2Fc12x%2B7jI0N6aKqUdp8BkGOqPrlUxn2mKcg%3D&X-Amz-SignedHeaders=host&response-content-disposition=inline%3Bfilename%3D%22companies_house_document.pdf%22&X-Amz-Signature=52be18d98d9589fa46d3686876b3107925b67ee083d05199e1428dfc14b9c457), but this doesn't count most spending e.g. Gemini compute.",14
Anthropic,Labs (giant companies),,Labs (giant companies) / Anthropic,,,"sec:White_box_safety_i_e_Interpretability_, Scalable Oversight",,,,"Chris Olah, Evan Hubinger, Sam Marks, Johannes Treutlein, Sam Bowman, Euan Ong, Fabien Roger, Adam Jermyn, Holden Karnofsky, Jan Leike, Ethan Perez, Jack Lindsey, Amanda Askell, Kyle Fish, Sara Price, Jon Kutasov, Minae Kwon, Monty Evans, Richard Dargan, Roger Grosse, Ben Levinstein, Joseph Carlsmith, Joe Benton",,"[Stein](https://ailabwatch.org/anthropic-opinions)[-Perlman](https://ailabwatch.org/companies/anthropic), [Casper](https://www.lesswrong.com/posts/pH6tyhEnngqWAXi9i/eis-xiii-reflections-on-anthropic-s-sae-research-circa-may#A_review___thoughts), [Carlsmith](https://joecarlsmith.com/2025/11/03/leaving-open-philanthropy-going-to-anthropic#:~:text=a%20few%20words%20about%20some%20more%20general%20concerns%20about%20AI%2Dsafety%2Dfocused%2Dpeople%20going%20to%20work%20at%20AI%20companies%20\(and/or%2C%20at%20Anthropic%20in%20particular\).), [underelicitation](https://www.lesswrong.com/posts/AK6AihHGjirdoiJg6/ai-companies-eval-reports-mostly-don-t-support-their-claims), [Greenblatt](https://nitter.net/RyanPGreenblatt/status/1925992236648464774), [Samin](https://www.lesswrong.com/posts/5aKRshJzhojqfbRyo/unless-its-governance-changes-anthropic-is-untrustworthy), [defense](https://techcrunch.com/2024/11/07/anthropic-teams-up-with-palantir-and-aws-to-sell-its-ai-to-defense-customers/), [Existing Safety Frameworks Imply Unreasonable Confidence](https://lesswrong.com/posts/7ExkgcDudwhag73vw/existing-safety-frameworks-imply-unreasonable-confidence)","Amazon, Google, ICONIQ, Fidelity, Lightspeed, Altimeter, Baillie Gifford, BlackRock, Blackstone, Coatue, D1 Capital Partners, General Atlantic, General Catalyst, GIC, Goldman Sachs, Insight Partners, Jane Street, Ontario Teachers' Pension Plan, Qatar Investment Authority, TPG, T. Rowe Price, WCM, XN",21
xAI,Labs (giant companies),,Labs (giant companies) / xAI,,,,,,,"Dan Hendrycks (advisor), Juntang Zhuang, Toby Pohlen, Lianmin Zheng, Piaoyang Cui, Nikita Popov, Ying Sheng, Sehoon Kim, Alexander Pan",,"[framework](https://www.lesswrong.com/posts/hQyrTDuTXpqkxrnoH/xai-s-new-safety-framework-is-dreadful), [hacking](https://x.com/g_leech_/status/1990543987846078854), [broken promises](https://x.com/g_leech_/status/1990734517145911593), [Stein](https://ailabwatch.org/companies/xai)\-[Perlman](https://ailabwatch.org/resources/integrity#xai), [insecurity](https://nitter.net/elonmusk/status/1961904269545648624), [Carlsmith](https://joecarlsmith.com/2025/11/03/leaving-open-philanthropy-going-to-anthropic#:~:text=a%20few%20words%20about%20some%20more%20general%20concerns%20about%20AI%2Dsafety%2Dfocused%20people%20going%20to%20work%20at%20AI%20companies%20\(and/or%2C%20at%20Anthropic%20in%20particular\).) on labs in general","A16Z, Blackrock, Fidelity, Kingdom, Lightspeed, MGX, Morgan Stanley, Sequoia…",0
Meta,Labs (giant companies),,Labs (giant companies) / Meta,,,a:Capability_removal_unlearning,,,,"Shuchao Bi, Hongyuan Zhan, Jingyu Zhang, Haozhu Wang, Eric Michael Smith, Sid Wang, Amr Sharaf, Mahesh Pasupuleti, Jason Weston, ShengYun Peng, Ivan Evtimov, Song Jiang, Pin-Yu Chen, Evangelia Spiliopoulou, Lei Yu, Virginie Do, Karen Hambardzumyan, Nicola Cancedda, Adina Williams",,"[extreme underelicitation](https://googleprojectzero.blogspot.com/2024/06/project-naptime.html#:~:text=We%20find%20that%2C%20by%20refining%20the%20testing%20methodology%20to%20take%20advantage%20of%20modern%20LLM%20capabilities%2C%20significantly%20better%20performance%20in%20vulnerability%20discovery%20can%20be%20achieved.%20To%20facilitate%20effective%20evaluation%20of%20LLMs%20for%20vulnerability%20discovery%2C%20we%20propose%20below%20a%20set%20of%20guiding%20principles.), [Stein](https://ailabwatch.org/companies/meta)-[Perlman](https://ailabwatch.org/companies/meta), [Carlsmith](https://joecarlsmith.com/2025/11/03/leaving-open-philanthropy-going-to-anthropic#:~:text=a%20few%20words%20about%20some%20more%20general%20concerns%20about%20AI%2Dsafety%2Dfocused%20people%20going%20to%20work%20at%20AI%20companies%20\(and/or%2C%20at%20Anthropic%20in%20particular\).) on labs in general",Meta,6
China,Labs (giant companies),,Labs (giant companies) / China,,,,,,,,,,,0
Others,Labs (giant companies),,Labs (giant companies) / Others,,,,,,,,,,,0
Iterative alignment at pretrain-time,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Iterative alignment at pretrain-time,Guide weights during pretraining.,"""LLMs don't seem very dangerous and might scale to AGI, things are generally smooth, relevant capabilities are harder than alignment, assume no mesaoptimisers, assume that zero-shot deception is hard, assume a fundamentally humanish ontology is learned, assume no simulated agents, assume that noise in the data means that human preferences are not ruled out, assume that alignment is a superficial feature, assume that tuning for what we want will also get us to avoid what we don't want. Maybe assume that thoughts are translucent.""","[prosaic alignment](https://www.lesswrong.com/posts/5ciYedyQDDqAcrDLr/a-positive-case-for-how-we-might-succeed-at-prosaic-ai), [incrementalism](https://www.lesswrong.com/posts/TALmStNf6479uTwzT/ai-alignment-metastrategy#Incrementalist_Metastrategy), [alignment-by-default](https://www.lesswrong.com/posts/Nwgdq6kHke5LY692J/alignment-by-default), [Korbak 2023](https://arxiv.org/abs/2302.08582)",,average,engineering,"Jan Leike, Stuart Armstrong, Cyrus Cousins, Oliver Daniels",,"[Bellot](https://arxiv.org/abs/2506.02923), [STACK](https://arxiv.org/abs/2506.24068), [Dung](https://arxiv.org/abs/2510.11235), [Gaikwad](https://arxiv.org/abs/2509.05381), [Hubinger](https://www.alignmentforum.org/posts/epjuxGnSPof3GnMSL)",most of the industry,2
Iterative alignment at post-train-time,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Iterative alignment at post-train-time,Modify weights after pre-training.,"""LLMs don't seem very dangerous and might scale to AGI, things are generally smooth, relevant capabilities are harder than alignment, assume no mesaoptimisers, assume that zero-shot deception is hard, assume a fundamentally humanish ontology is learned, assume no simulated agents, assume that noise in the data means that human preferences are not ruled out, assume that alignment is a superficial feature, assume that tuning for what we want will also get us to avoid what we don't want. Maybe assume that thoughts are translucent.""",,,average,engineering,"Adam Gleave, Anca Dragan, Jacob Steinhardt, Rohin Shah",,"[Bellot](https://arxiv.org/abs/2506.02923), [STACK](https://arxiv.org/abs/2506.24068), [Dung](https://arxiv.org/abs/2510.11235), [Gölz](https://arxiv.org/abs/2505.23749), [Gaikwad](https://arxiv.org/abs/2509.05381), [Hubinger](https://www.alignmentforum.org/posts/epjuxGnSPof3GnMSL)",most of the industry,16
Black-box make-AI-solve-it,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Black-box make-AI-solve-it,Focus on using existing models to improve and align further models.,"""LLMs don't seem very dangerous and might scale to AGI, things are generally smooth, relevant capabilities are harder than alignment, assume no mesaoptimisers, assume that zero-shot deception is hard, assume a fundamentally humanish ontology is learned, assume no simulated agents, assume that noise in the data means that human preferences are not ruled out, assume that alignment is a superficial feature, assume that tuning for what we want will also get us to avoid what we don't want. Maybe assume that thoughts are translucent.""","sec:Make_AI_solve_it, a:Debate",,average,engineering,"Jacques Thibodeau, Matthew Shingle, Nora Belrose, Lewis Hammond, Geoffrey Irving",,"[STACK](https://arxiv.org/abs/2506.24068), [Dung](https://arxiv.org/abs/2510.11235), [Gölz](https://arxiv.org/abs/2505.23749), [Gaikwad](https://arxiv.org/abs/2509.05381), [Hubinger](https://www.alignmentforum.org/posts/epjuxGnSPof3GnMSL), [SAIF](https://saif.org/research/bare-minimum-mitigations-for-autonomous-ai-development/)",most of the industry,12
Inoculation prompting,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Inoculation prompting,"Prompt mild misbehaviour in training, to prevent the failure mode where once AI misbehaves in a mild way, it will be more inclined towards all bad behaviour.","LLMs don't seem very dangerous and might scale to AGI, things are generally smooth, relevant capabilities are harder than alignment, assume no mesaoptimisers, assume that zero-shot deception is hard, assume a fundamentally humanish ontology is learned, assume no simulated agents, assume that noise in the data means that human preferences are not ruled out, assume that alignment is a superficial feature, assume that tuning for what we want will also get us to avoid what we don't want. Maybe assume that thoughts are translucent.",,,average,engineering,"Ariana Azarbal, Daniel Tan, Victor Gillioz, Alex Turner, Alex Cloud, Monte MacDiarmid, Daniel Ziegler",,"[Bellot](https://arxiv.org/abs/2506.02923), [Alfour](https://cognition.cafe/p/ai-alignment-based-on-intentions), [Gölz](https://arxiv.org/abs/2505.23749), [Gaikwad](https://arxiv.org/abs/2509.05381), [Hubinger](https://www.alignmentforum.org/posts/epjuxGnSPof3GnMSL)",most of the industry,4
Inference-time: In-context learning,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Inference-time: In-context learning,"Investigate what runtime guidelines, rules, or examples provided to an LLM yield better behavior.","LLMs don't seem very dangerous and might scale to AGI, things are generally smooth, relevant capabilities are harder than alignment, assume no mesaoptimisers, assume that zero-shot deception is hard, assume a fundamentally humanish ontology is learned, assume no simulated agents, assume that noise in the data means that human preferences are not ruled out, assume that alignment is a superficial feature, assume that tuning for what we want will also get us to avoid what we don't want. Maybe assume that thoughts are translucent.","model spec as prompt, a:Model_specs_and_constitutions",,average,engineering,"Jacob Steinhardt, Kayo Yin, Atticus Geiger",,"[STACK](https://arxiv.org/abs/2506.24068), [Dung](https://arxiv.org/abs/2510.11235), [Gölz](https://arxiv.org/abs/2505.23749), [Gaikwad](https://arxiv.org/abs/2509.05381), [Hubinger](https://www.alignmentforum.org/posts/epjuxGnSPof3GnMSL)",,5
Inference-time: Steering,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Inference-time: Steering,Manipulate an LLM's internal representations/token probabilities without touching weights.,"""LLMs don't seem very dangerous and might scale to AGI, things are generally smooth, relevant capabilities are harder than alignment, assume no mesaoptimisers, assume that zero-shot deception is hard, assume a fundamentally humanish ontology is learned, assume no simulated agents, assume that noise in the data means that human preferences are not ruled out, assume that alignment is a superficial feature, assume that tuning for what we want will also get us to avoid what we don't want. Maybe assume that thoughts are translucent.""","a:Activation_engineering, a:Character_training_and_persona_steering, a:Safeguards_inference_time_auxiliaries_",,average,engineering,"Taylor Sorensen, Constanza Fierro, Kshitish Ghate, Arthur Vogels",,"[Alfour](https://cognition.cafe/p/ai-alignment-based-on-intentions), [STACK](https://arxiv.org/abs/2506.24068), [Dung](https://arxiv.org/abs/2510.11235), [Gölz](https://arxiv.org/abs/2505.23749), [Gaikwad](https://arxiv.org/abs/2509.05381), [Hubinger](https://www.alignmentforum.org/posts/epjuxGnSPof3GnMSL)",,4
Capability removal: unlearning,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Capability removal: unlearning,"Developing methods to selectively remove specific information, capabilities, or behaviors from a trained model (e.g. without retraining it from scratch). A mixture of black-box and white-box approaches.","If an AI learns dangerous knowledge (e.g., dual-use capabilities like virology or hacking, or knowledge of their own safety controls) or exhibits undesirable behaviors (e.g., memorizing private data), we can specifically erase this ""bad"" knowledge post-training, which is much cheaper and faster than retraining, thereby making the model safer. Alternatively, intervene in pre-training, to prevent the model from learning it in the first place (even when data filtering is imperfect). You could imagine also unlearning propensities to power-seeking, deception, sycophancy, or spite.","a:Data_filtering, sec:White_box_safety_i_e_Interpretability_, a:Various_Redteams","Superintelligence can hack software supervisors, A boxed AGI might exfiltrate itself by steganography, spearphishing, Humanlike minds/goals are not necessarily safe",pessimistic,cognitive / engineering,"Rowan Wang, Avery Griffin, Johannes Treutlein, Zico Kolter, Bruce W. Lee, Addie Foote, Alex Infanger, Zesheng Shi, Yucheng Zhou, Jing Li, Timothy Qian, Stephen Casper, Alex Cloud, Peter Henderson, Filip Sondej, Fazl Barez",10-50,[Existing Large Language Model Unlearning Evaluations Are Inconclusive](https://arxiv.org/abs/2506.00688),"Coefficient Giving, MacArthur Foundation, UK AI Safety Institute (AISI), Canadian AI Safety Institute (CAISI), industry labs (e.g., Microsoft Research, Google)",18
Control,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Control,"If we assume early transformative AIs are misaligned and actively trying to subvert safety measures, can we still set up protocols to extract useful work from them while preventing sabotage, and watching with incriminating behaviour?",,safety cases,,worst-case,engineering / behavioral,"Redwood, UK AISI, Deepmind, OpenAI, Anthropic, Buck Shlegeris, Ryan Greenblatt, Kshitij Sachan, Alex Mallen",5-50,"[Wentworth](https://www.lesswrong.com/posts/8wBN8cdNAv3c7vt6p/the-case-against-ai-control-research), [Mannheim](https://lesswrong.com/posts/25dsPH6CuRXPBkGHN/no-we-re-not-getting-meaningful-oversight-of-ai), [Kulveit](https://www.lesswrong.com/posts/rZcyemEpBHgb2hqLP/ai-control-may-increase-existential-risk)",,22
Safeguards (inference-time auxiliaries),Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Safeguards (inference-time auxiliaries),"Layers of inference-time defenses, such as classifiers, monitors, and rapid-response protocols, to detect and block jailbreaks, prompt injections, and other harmful model behaviors.","By building a bunch of scalable and hardened things on top of an unsafe model, we can defend against known and unknown attacks, monitor for misuse, and prevent models from causing harm, even if the core model has vulnerabilities.","a:Various_Redteams, sec:Iterative_alignment","Superintelligence can fool human supervisors, A boxed AGI might exfiltrate itself by steganography, spearphishing",average,engineering,"Mrinank Sharma, Meg Tong, Jesse Mu, Alwin Peng, Julian Michael, Henry Sleight, Theodore Sumers, Raj Agarwal, Nathan Bailey, Edoardo Debenedetti, Ilia Shumailov, Tianqi Fan, Sahil Verma, Keegan Hines, Jeff Bilmes",100+,[Obfuscated Activations Bypass LLM Latent-Space Defenses](https://arxiv.org/abs/2412.09565),most of the big labs,6
Chain of thought monitoring,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Chain of thought monitoring,"Supervise an AI's natural-language (output) ""reasoning"" to detect misalignment, scheming, or deception, rather than studying the actual internal states.","The reasoning process (Chain of Thought, or CoT) of an AI provides a legible signal of its internal state and intentions. By monitoring this CoT, supervisors (human or AI) can detect misalignment, scheming, or reward hacking before it results in a harmful final output. This allows for more robust oversight than supervising outputs alone, but it relies on the CoT remaining faithful (i.e., accurately reflecting the model's reasoning) and not becoming obfuscated under optimization pressure.","sec:White_box_safety_i_e_Interpretability_, a:Steganography_evals","Superintelligence can fool human supervisors, Superintelligence can hack software supervisors, A boxed AGI might exfiltrate itself by steganography, spearphishing",average,engineering,"Aether, Bowen Baker, Joost Huizinga, Leo Gao, Scott Emmons, Erik Jenner, Yanda Chen, James Chua, Owain Evans, Tomek Korbak, Mikita Balesni, Xinpeng Wang, Miles Turpin, Rohin Shah",10-100,[Reasoning Models Don't Always Say What They Think](https://assets.anthropic.com/m/71876fabef0f0ed4/original/reasoning_models_paper.pdf); [Chain-of-Thought Reasoning In The Wild Is Not Always Faithful](https://arxiv.org/abs/2503.08679); [Beyond Semantics: The Unreasonable Effectiveness of Reasonless Intermediate Tokens](https://arxiv.org/abs/2505.13775); [Reasoning Models Sometimes Output Illegible Chains of Thought](https://arxiv.org/abs/2510.27338),"OpenAI, Anthropic, Google DeepMind",17
Model values / model preferences,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Model values / model preferences,"Analyse and control emergent, coherent value systems in LLMs, which change as models scale, and can contain problematic values like preferences for AIs over humans.","As AIs become more agentic, their behaviours and risks are increasingly determined by their goals and values. Since coherent value systems emerge with scale, we must leverage utility functions to analyse these values and apply ""utility control"" methods to constrain them, rather than just controlling outputs downstream of them.","[Values in the Wild: Discovering and Analyzing Values in Real-World Language Model Interactions](https://arxiv.org/abs/2504.15236), [Persona Vectors: Monitoring and Controlling Character Traits in Language Models](https://arxiv.org/abs/2507.21509)",Value is fragile and hard to specify,pessimistic,cognitivist science,"Mantas Mazeika, Xuwang Yin, Rishub Tamirisa, Jaehyuk Lim, Bruce W. Lee, Richard Ren, Long Phan, Norman Mu, Adam Khoja, Oliver Zhang, Dan Hendrycks",30,"[Randomness, Not Representation: The Unreliability of Evaluating Cultural Alignment in LLMs](https://dl.acm.org/doi/full/10.1145/3715275.3732147)","Coefficient Giving. $289,000 SFF funding for CAIS.",14
Character training and persona steering,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Character training and persona steering,"Map, shape, and control the personae of language models, such that new models embody desirable values (e.g., honesty, empathy) rather than undesirable ones (e.g., sycophancy, self-perpetuating behaviors).","If post-training, prompting, and activation-engineering interact with some kind of structured 'persona space', then better understanding it should benefit the design, control, and detection of LLM personas.","[Simulators](#a:simulators), a:Activation_engineering, a:Emergent_misalignment, a:Hyperstition_studies, a:Anthropic, [Cyborgism](https://www.lesswrong.com/posts/bxt7uCiHam4QXrQAA/cyborgism), shard theory, [AI psychiatry](https://nitter.net/Jack_W_Lindsey/status/1948138767753326654#m), [Ward et al](https://arxiv.org/abs/2410.04272)",Value is fragile and hard to specify,average,cognitive,"Truthful AI, OpenAI, Anthropic, CLR, Amanda Askell, Jack Lindsey, Janus, Theia Vogel, Sharan Maiya, Evan Hubinger",,[Nostalgebraist](https://nostalgebraist.tumblr.com/post/785766737747574784/the-void),"Anthropic, Coefficient Giving",13
Emergent misalignment,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Emergent misalignment,"Fine-tuning LLMs on one narrow antisocial task can cause general misalignment including deception, shutdown resistance, harmful advice, and extremist sympathies, when those behaviors are never trained or rewarded directly. [A new agenda](https://www.lesswrong.com/posts/AcTEiu5wYDgrbmXow/open-problems-in-emergent-misalignment) which quickly led to a stream of exciting work.","Predict, detect, and prevent models from developing broadly harmful behaviors (like deception or shutdown resistance) when fine-tuned on seemingly unrelated tasks. Find, preserve, and robustify this correlated representation of the good.","auditing real models, a:Pragmatic_interpretability","Goals misgeneralize out of distribution, Superintelligence can fool human supervisors",pessimistic,behaviorist science,"Truthful AI, Jan Betley, James Chua, Mia Taylor, Miles Wang, Edward Turner, Anna Soligo, Alex Cloud, Nathan Hu, Owain Evans",10-50,"[Emergent Misalignment as Prompt Sensitivity](https://arxiv.org/html/2507.06253v1), [Go home GPT-4o, you're drunk](https://www.lesswrong.com/posts/RoWabfQxabWBiXwxP/go-home-gpt-4o-you-re-drunk-emergent-misalignment-as-lowered)","Coefficient Giving, >$1 million",17
Model specs and constitutions,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Model specs and constitutions,"Write detailed, natural language descriptions of values and rules for models to follow, then instill these values and rules into models via techniques like Constitutional AI or deliberative alignment.","Model specs and constitutions serve three purposes. First, they provide a clear standard of behavior which can be used to *train* models to value what we want them to value. Second, they serve as something closer to a ground truth standard for evaluating the degree of misalignment ranging from  ""models straightforwardly obey the spec"" to ""models flagrantly disobey the spec"". A combination of scalable stress-testing and reinforcement for obedience can be used to iteratively reduce the risk of misalignment. Third, they get more useful as models' instruction-following capability improves.","sec:Iterative_alignment, sec:Model_psychology",Value is fragile and hard to specify,average,engineering,"Amanda Askell, Joe Carlsmith",,"[LLM AGI may reason about its goals and discover misalignments by default](https://www.alignmentforum.org/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover), [On OpenAI's Model Spec 2.0](https://thezvi.wordpress.com/2025/02/21/on-openais-model-spec-2-0/), [Giving AIs safe motivations (esp. Sections 4.3-4.5)](https://joecarlsmith.com/2025/08/18/giving-ais-safe-motivations#4-5-step-4-good-instructions), [On Deliberative Alignment](https://thezvi.substack.com/p/on-deliberative-alignment)",major funders include Anthropic and OpenAI (internally),11
Model psychopathology,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Model psychopathology,Find interesting LLM phenomena like glitch [tokens](https://vgel.me/posts/seahorse/) and the reversal curse; these are vital data for theory.,"The study of 'pathological' phenomena in LLMs is potentially key for theoretically modelling LLM cognition and LLM training-dynamics (compare: the study of aphasia and visual processing disorder in humans plays a key role cognitive science), and in particular for developing a good theory of generalization in LLMS","a:Emergent_misalignment, mechanistic anomaly detection",Goals misgeneralize out of distribution,pessimistic,behaviorist / cognitivist,"Janus, Truthful AI, Theia Vogel, Stewart Slocum, Nell Watson, Samuel G. B. Johnson, Liwei Jiang, Monika Jotautaite, Saloni Dash",5-20,,Coefficient Giving (via Truthful AI and Interpretability grants),9
Data filtering,Black-box safety (understand and control current model behaviour),Better data,Black-box safety (understand and control current model behaviour) / Better data / Data filtering,"Builds safety into models from the start by removing harmful or toxic content (like dual-use info) from the pretraining data, rather than relying only on post-training alignment.","By curating the pretraining data, we can prevent the model from learning dangerous capabilities (e.g., dual-use info) or undesirable behaviors (e.g., toxicity) in the first place, making safety more robust and ""tamper-resistant"" than post-training patches.","a:Data_quality_for_alignment, a:Data_poisoning_defense, a:Synthetic_data_for_alignment, a:Capability_removal_unlearning","Goals misgeneralize out of distribution, Value is fragile and hard to specify",average,engineering,"Yanda Chen, Pratyush Maini, Kyle O'Brien, Stephen Casper, Simon Pepin Lehalleur, Jesse Hoogland, Himanshu Beniwal, Sachin Goyal, Mycal Tucker, Dylan Sam",10-50,"[When Bad Data Leads to Good Models](https://arxiv.org/pdf/2505.04741), [Medical large language models are vulnerable to data-poisoning attacks](https://www.nature.com/articles/s41591-024-03445-1)","Anthropic, various academics",4
Hyperstition studies,Black-box safety (understand and control current model behaviour),Better data,Black-box safety (understand and control current model behaviour) / Better data / Hyperstition studies,"Study, steer, and intervene on the following feedback loop: ""we produce stories about how present and future AI systems behave"" → ""these stories become training data for the AI"" → ""these stories shape how AI systems in fact behave"".",Measure the influence of existing AI narratives in the training data → seed and develop more salutary ontologies and self-conceptions for AI models → control and redirect AI models' self-concepts through selectively amplifying certain components of the training data.,"a:Data_filtering, [active inference](https://arxiv.org/abs/2311.10215), LLM whisperers",Value is fragile and hard to specify,average,cognitive,"Alex Turner, [Hyperstition AI](https://www.hyperstitionai.com/), Kyle O'Brien",1-10,,"Unclear, niche",4
Data poisoning defense,Black-box safety (understand and control current model behaviour),Better data,Black-box safety (understand and control current model behaviour) / Better data / Data poisoning defense,Develops methods to detect and prevent malicious or backdoor-inducing samples from being included in the training data.,"By identifying and filtering out malicious training examples, we can prevent attackers from creating hidden backdoors or triggers that would cause aligned models to behave dangerously.","a:Data_filtering, a:Safeguards_inference_time_auxiliaries_, a:Various_Redteams, adversarial robustness","Superintelligence can hack software supervisors, Someone else will deploy unsafe superintelligence first",pessimistic,engineering,"Alexandra Souly, Javier Rando, Ed Chapman, Hanna Foerster, Ilia Shumailov, Yiren Zhao",5-20,"[A small number of samples can poison LLMs of any size](https://arxiv.org/abs/2510.04567), [Reasoning Introduces New Poisoning Attacks Yet Makes Them More Complicated](https://arxiv.org/abs/2509.03405)","Google DeepMind, Anthropic, University of Cambridge, Vector Institute",3
Synthetic data for alignment,Black-box safety (understand and control current model behaviour),Better data,Black-box safety (understand and control current model behaviour) / Better data / Synthetic data for alignment,"Uses AI-generated data (e.g., critiques, preferences, or self-labeled examples) to scale and improve alignment, especially for superhuman models.","We can overcome the bottleneck of human feedback and data by using models to generate vast amounts of high-quality, targeted data for safety, preference tuning, and capability elicitation.","a:Data_quality_for_alignment, a:Data_filtering, scalable oversight, automated alignment research, a:Weak_to_strong_generalization","Goals misgeneralize out of distribution, Superintelligence can fool human supervisors, Value is fragile and hard to specify",average,engineering,"Mianqiu Huang, Xiaoran Liu, Rylan Schaeffer, Nevan Wichers, Aram Ebtekar, Jiaxin Wen, Vishakh Padmakumar, Benjamin Newman",50-150,"[Synthetic Data in AI: Challenges, Applications, and Ethical Implications](https://arxiv.org/abs/2401.01629). Sort of [Demski](https://www.lesswrong.com/posts/nQwbDPgYvAbqAmAud/llms-for-alignment-research-a-safety-priority).","Anthropic, Google DeepMind, OpenAI, Meta AI, various academic groups.",8
Data quality for alignment,Black-box safety (understand and control current model behaviour),Better data,Black-box safety (understand and control current model behaviour) / Better data / Data quality for alignment,"Improves the quality, signal-to-noise ratio, and reliability of human-generated preference and alignment data.","The quality of alignment is heavily dependent on the quality of the data (e.g., human preferences); by improving the ""signal"" from annotators and reducing noise/bias, we will get more robustly aligned models.","a:Synthetic_data_for_alignment, scalable oversight, a:Assistance_games_assistive_agents, a:Model_values_model_preferences","Superintelligence can fool human supervisors, Value is fragile and hard to specify",average,engineering,"Maarten Buyl, Kelsey Kraus, Margaret Kroll, Danqing Shi",20-50,[A Statistical Case Against Empirical Human-AI Alignment](https://arxiv.org/abs/2502.14581),"Anthropic, Google DeepMind, OpenAI, Meta AI, various academic groups",5
Mild optimisation,Black-box safety (understand and control current model behaviour),Goal robustness,Black-box safety (understand and control current model behaviour) / Goal robustness / Mild optimisation,Avoid Goodharting by getting AI to satisfice rather than maximise.,"If we fail to exactly nail down the preferences for a superintelligent agent we die to Goodharting → shift from maximising to satisficing in the agent's utility function → we get a nonzero share of the lightcone as opposed to zero; also, moonshot at this being the recipe for fully aligned AI.",,Value is fragile and hard to specify,mixed,cognitive,,10-50,,Google DeepMind,4
RL safety,Black-box safety (understand and control current model behaviour),Goal robustness,Black-box safety (understand and control current model behaviour) / Goal robustness / RL safety,"Improves the robustness of reinforcement learning agents by addressing core problems in reward learning, goal misgeneralization, and specification gaming.","Standard RL objectives (like maximizing expected value) are brittle and lead to goal misgeneralization or specification gaming; by developing more robust frameworks (like pessimistic RL, minimax regret, or provable inverse reward learning), we can create agents that are safe even when misspecified.","a:Behavior_alignment_theory, a:Assistance_games_assistive_agents, sec:Goal_robustness, sec:Iterative_alignment, a:Mild_optimisation, scalable oversight, [The Theoretical Reward Learning Research Agenda: Introduction and Motivation](https://www.alignmentforum.org/posts/pJ3mDD7LfEwp3s5vG/the-theoretical-reward-learning-research-agenda-introduction)","Goals misgeneralize out of distribution, Value is fragile and hard to specify, Superintelligence can fool human supervisors",pessimistic,engineering,"Joar Skalse, Karim Abdel Sadek, Matthew Farrugia-Roberts, Benjamin Plaut, Fang Wu, Stephen Zhao, Alessandro Abate, Steven Byrnes, Michael Cohen",20-70,"[""The Era of Experience"" has an unsolved technical alignment problem](https://www.lesswrong.com/posts/747f6b8e/the-era-of-experience-has-an-unsolved-technical-alignment-problem), [The Invisible Leash: Why RLVR May or May Not Escape Its Origin](https://arxiv.org/abs/2507.14843)","Google DeepMind, University of Oxford, CMU, Coefficient Giving",11
"Assistance games, assistive agents",Black-box safety (understand and control current model behaviour),Goal robustness,"Black-box safety (understand and control current model behaviour) / Goal robustness / Assistance games, assistive agents","Formalize how AI assistants learn about human preferences given uncertainty and partial observability, and construct environments which better incentivize AIs to learn what we want them to learn.",Understand what kinds of things can go wrong when humans are directly involved in training a model → build tools that make it easier for a model to learn what humans want it to learn.,a:Guaranteed_Safe_AI,"Value is fragile and hard to specify, Humanlike minds/goals are not necessarily safe",varies,engineering / cognitive,"Joar Skalse, Anca Dragan, Caspar Oesterheld, David Krueger, Dylan Hafield-Menell, Stuart Russell",,[nice summary](https://www.lesswrong.com/posts/i5kijcjFJD6bn7dwq/evaluating-the-historical-value-misspecification-argument) of historical problem statements,"Future of Life Institute, Coefficient Giving, Survival and Flourishing Fund, Cooperative AI Foundation, Polaris Ventures",5
Harm reduction for open weights,Black-box safety (understand and control current model behaviour),Goal robustness,Black-box safety (understand and control current model behaviour) / Goal robustness / Harm reduction for open weights,"Develops methods, primarily based on pretraining data intervention, to create tamper-resistant safeguards that prevent open-weight models from being maliciously fine-tuned to remove safety features or exploit dangerous capabilities.","Open-weight models allow adversaries to easily remove post-training safety (like refusal training) via simple fine-tuning; by making safety an intrinsic property of the model's learned knowledge and capabilities (e.g., by ensuring ""deep ignorance"" of dual-use information), the safeguards become far more difficult and expensive to remove.","a:Data_filtering, a:Capability_removal_unlearning, a:Data_poisoning_defense",Someone else will deploy unsafe superintelligence first,average,engineering,"Kyle O'Brien, Stephen Casper, Quentin Anthony, Tomek Korbak, Rishub Tamirisa, Mantas Mazeika, Stella Biderman, Yarin Gal",10-100,,"UK AI Safety Institute (AISI), EleutherAI, Coefficient Giving",5
"The ""Neglected Approaches"" Approach",Black-box safety (understand and control current model behaviour),Goal robustness,"Black-box safety (understand and control current model behaviour) / Goal robustness / The ""Neglected Approaches"" Approach","Agenda-agnostic approaches to identifying good but overlooked empirical alignment ideas, working with theorists who could use engineers, and prototyping them.","Empirical search for ""negative alignment taxes"" (prioritizing methods that simultaneously enhance alignment and capabilities)","sec:Iterative_alignment, automated alignment research, Beijing Key Laboratory of Safe AI and Superalignment, Aligned AI",Someone else will deploy unsafe superintelligence first,average,engineering,"AE Studio, Gunnar Zarncke, Cameron Berg, Michael Vaiana, Judd Rosenblatt, Diogo Schwerz de Lucena",15,[The 'Alignment Bonus' is a Dangerous Mirage](https://www.alignmentforum.org/posts/example-critique-neg-tax),AE Studio,3
Reverse engineering,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Reverse engineering,"Decompose a model into its functional, interacting components (circuits), formally describe what computation those components perform, and validate their causal effects to reverse-engineer the model's internal algorithm.","By gaining a mechanical understanding of how a model works (the ""circuit diagram""), we can predict how models will act in novel situations (generalization), and gain the mechanistic knowledge necessary to safely modify an AI's goals or internal mechanisms, or allow for high-confidence alignment auditing and better feedback to safety researchers.",[ambitious mech interp](https://www.alignmentforum.org/posts/Hy6PX43HGgmfiTaKu/an-ambitious-vision-for-interpretability),"Goals misgeneralize out of distribution, Superintelligence can fool human supervisors",worst-case,cognitivist science,"Lucius Bushnaq, Dan Braun, Lee Sharkey, Aaron Mueller, Atticus Geiger, Sheridan Feucht, David Bau, Yonatan Belinkov, Stefan Heimersheim, Chris Olah, Leo Gao",100-200,"[Interpretability Will Not Reliably Find Deceptive AI](https://www.alignmentforum.org/posts/PwnadG4BFjaER3MGf/interpretability-will-not-reliably-find-deceptive-ai), [A Problem to Solve Before Building a Deception Detector](https://www.lesswrong.com/posts/YXNeA3RyRrrRWS37A/a-problem-to-solve-before-building-a-deception-detector), [MoSSAIC: AI Safety After Mechanism](https://openreview.net/forum?id=n7WYSJ35FU), [The Misguided Quest for Mechanistic AI Interpretability](https://ai-frontiers.org/articles/the-misguided-quest-for-mechanistic-ai-interpretability). [Mechanistic?](https://arxiv.org/abs/2410.09087), [Assessing skeptical views of interpretability research](https://www.youtube.com/watch?v=woo_J0RKcpQ), [Activation space interpretability may be doomed](https://www.lesswrong.com/posts/gYfpPbww3wQRaxAFD/activation-space-interpretability-may-be-doomed), [A Pragmatic Vision for Interpretability](https://www.alignmentforum.org/posts/StENzDcD3kpfGJssR/a-pragmatic-vision-for-interpretability)",,33
Extracting latent knowledge,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Extracting latent knowledge,"Identify and decoding the ""true"" beliefs or knowledge represented inside a model's activations, even when the model's output is deceptive or false.","Powerful models may know things they do not say (e.g. that they are currently being tested). If we can translate this latent knowledge directly from the model's internals, we can supervise them reliably even when they attempt to deceive human evaluators or when the task is too difficult for humans to verify directly.","a:AI_explanations_of_AIs, a:Heuristic_explanations, a:Lie_and_deception_detectors",Superintelligence can fool human supervisors,worst-case,cognitivist science,"Bartosz Cywiński, Emil Ryd, Senthooran Rajamanoharan, Alexander Pan, Lijie Chen, Jacob Steinhardt, Javier Ferrando, Oscar Obeso, Collin Burns, Paul Christiano",20-40,[A Problem to Solve Before Building a Deception Detector](https://www.lesswrong.com/posts/YXNeA3RyRrrRWS37A/a-problem-to-solve-before-building-a-deception-detector),"Open Philanthropy, Anthropic, NSF, various academic grants",9
Lie and deception detectors,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Lie and deception detectors,"Detect when a model is being deceptive or lying by building white- or black-box detectors. Some work below requires intent in their definition, while other work focuses only on whether the model states something it believes to be false, regardless of intent.","Such detectors could flag suspicious behavior during evaluations or deployment, augment training to reduce deception, or audit models pre-deployment. Specific applications include alignment evaluations (e.g. by validating answers to introspective questions), safeguarding evaluations (catching models that ""sandbag"", that is, strategically underperform to pass capability tests), and large-scale deployment monitoring. An honest version of a model could also provide oversight during training or detect cases where a model behaves in ways it understands are unsafe.","a:Reverse_engineering, a:AI_deception_evals, a:Sandbagging_evals",,pessimistic,cognitivist science,"Cadenza, Sam Marks, Rowan Wang, Kieron Kretschmar, Sharan Maiya, Walter Laurito, Chris Cundy, Adam Gleave, Aviel Parrack, Stefan Heimersheim, Carlo Attubato, Joseph Bloom, Jordan Taylor, Alex McKenzie, Urja Pawar, Lewis Smith, Bilal Chughtai, Neel Nanda",10-50,"difficult to determine if behavior is strategic deception or only low level ""reflexive"" actions; Unclear if a model roleplaying a liar has deceptive intent. [How are intentional descriptions (like deception) related to algorithmic ones (like understanding the mechanisms models use)?](https://www.lesswrong.com/posts/YXNeA3RyRrrRWS37A/a-problem-to-solve-before-building-a-deception-detector), [Is This Lie Detector Really Just a Lie Detector? An Investigation of LLM Probe Specificity](https://www.lesswrong.com/posts/5dkhdRMypeuyoXfmb/is-this-lie-detector-really-just-a-lie-detector-an), [Herrmann](https://www.lesswrong.com/posts/bCQbSFrnnAk7CJNpM/still-no-lie-detector-for-llms), [Smith and Chughtai](https://arxiv.org/abs/2511.22662)","Anthropic, Deepmind, UK AISI, Coefficient Giving",11
Model diffing,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Model diffing,"Understand what happens when a model is finetuned, what the ""diff"" between the finetuned and the original model consists in.","By identifying the mechanistic differences between a base model and its fine-tune (e.g., after RLHF), maybe we can verify that safety behaviors are robustly ""internalized"" rather than superficially patched, and detect if dangerous capabilities or deceptive alignment have been introduced without needing to re-analyze the entire model. The diff is also much smaller, since most parameters don't change, which means you can use heavier methods on them.","a:Sparse_Coding, a:Reverse_engineering",Value is fragile and hard to specify,pessimistic,cognitive,"Julian Minder, Clément Dumas, Neel Nanda, Trenton Bricken, Jack Lindsey",10-30,,"various academic groups, Anthropic, Google DeepMind",9
Sparse Coding,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Sparse Coding,"Decompose the polysemantic activations of the residual stream into a sparse linear combination of monosemantic ""features"" which correspond to interpretable concepts.",Get a principled decomposition of an LLM's activation into atomic components → identify deception and other misbehaviors.,"sec:Concept_based_interpretability, a:Reverse_engineering","Value is fragile and hard to specify, Goals misgeneralize out of distribution, Superintelligence can fool human supervisors",average,engineering / cognitive,"Leo Gao, Dan Mossing, Emmanuel Ameisen, Jack Lindsey, Adam Pearce, Thomas Heap, Abhinav Menon, Kenny Peng, Tim Lawson",50-100,"[Sparse Autoencoders Can Interpret Randomly Initialized Transformers](https://arxiv.org/abs/2501.17727), [The Sparse Autoencoders bubble has popped, but they are still promising](https://agarriga.substack.com/p/the-sparse-autoencoders-bubble-has), [Negative Results for SAEs On Downstream Tasks and Deprioritising SAE Research](https://www.alignmentforum.org/posts/4uXCAJNuPKtKBsi28/), [Sparse Autoencoders Trained on the Same Data Learn Different Features](https://arxiv.org/pdf/2501.16615), [Why Not Just Train For Interpretability?](https://www.lesswrong.com/posts/2HbgHwdygH6yeHKKq/why-not-just-train-for-interpretability)","everyone, roughly. Frontier labs, LTFF, Coefficient Giving, etc.",44
Causal Abstractions,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Causal Abstractions,Verify that a neural network implements a specific high-level causal model (like a logical algorithm) by finding a mapping between high-level variables and low-level neural representations.,"By establishing a causal mapping between a black-box neural network and a human-interpretable algorithm, we can check whether the model is using safe reasoning processes and predict its behavior on unseen inputs, rather than relying on behavioural testing alone.","sec:Concept_based_interpretability, a:Reverse_engineering",Goals misgeneralize out of distribution,worst-case,cognitivist science,"Atticus Geiger, Christopher Potts, Thomas Icard, Theodora-Mara Pîslar, Sara Magliacane, Jiuding Sun, Jing Huang",10-30,"[The Misguided Quest for Mechanistic AI Interpretability](https://www.google.com/search?q=https://open.substack.com/pub/aifrontiersmedia/p/the-misguided-quest-for-mechanistic), [Interpretability Will Not Reliably Find Deceptive AI](https://www.lesswrong.com/posts/PwnadG4BFjaER3MGf/interpretability-will-not-reliably-find-deceptive-ai)","Various academic groups, Google DeepMind, Goodfire",3
Data attribution,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Data attribution,"Quantifies the influence of individual training data points on a model's specific behavior or output, allowing researchers to trace model properties (like misalignment, bias, or factual errors) back to their source in the training set.","By attributing harmful, biased, or unaligned behaviors to specific training examples, researchers can audit proprietary models, debug training data, enable effective data deletion/unlearning",a:Data_quality_for_alignment,"Goals misgeneralize out of distribution, Value is fragile and hard to specify",average,behavioural,"Roger Grosse, Philipp Alexander Kreer, Jin Hwa Lee, Matthew Smith, Abhilasha Ravichander, Andrew Wang, Jiacheng Liu, Jiaqi Ma, Junwei Deng, Yijun Pan, Daniel Murfet, Jesse Hoogland",30-60,,Various academic groups,12
Pragmatic interpretability,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Pragmatic interpretability,"Directly tackling concrete, safety-critical problems on the path to AGI by using lightweight interpretability tools (like steering and probing) and empirical feedback from proxy tasks, rather than pursuing complete mechanistic reverse-engineering.","By applying interpretability skills to concrete problems, researchers can rapidly develop monitoring and control tools (e.g., steering vectors or probes) that have immediate, measurable impact on real-world safety issues like detecting hidden goals or emergent misalignment.","a:Reverse_engineering, sec:Concept_based_interpretability","Superintelligence can fool human supervisors, Goals misgeneralize out of distribution",mixed,cognitive,"Lee Sharkey, Dario Amodei, David Chalmers, Been Kim, Neel Nanda, David D. Baek, Lauren Greenspan, Dmitry Vaintrob, Sam Marks, Jacob Pfau",30-60,,"Google DeepMind, Anthropic, various academic groups",3
Other interpretability,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Other interpretability,Interpretability that does not fall well into other categories.,"Explore alternative conceptual frameworks (e.g., agentic, propositional) and physics-inspired methods (e.g., renormalization). Or be ""pragmatic"".","a:Reverse_engineering, sec:Concept_based_interpretability","Superintelligence can fool human supervisors, Goals misgeneralize out of distribution",mixed,engineering / cognitive,"Lee Sharkey, Dario Amodei, David Chalmers, Been Kim, Neel Nanda, David D. Baek, Lauren Greenspan, Dmitry Vaintrob, Sam Marks, Jacob Pfau",30-60,"[The Misguided Quest for Mechanistic AI Interpretability](https://aifrontiersmedia.substack.com/p/the-misguided-quest-for-mechanistic), [Interpretability Will Not Reliably Find Deceptive AI](https://www.lesswrong.com/posts/PwnadG4BFjaER3MGf/interpretability-will-not-reliably-find-deceptive-ai).",,19
Learning dynamics and developmental interpretability,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Learning dynamics and developmental interpretability,"Builds tools for detecting, locating, and interpreting key structural shifts, phase transitions, and emergent phenomena (like grokking or deception) that occur during a model's training and in-context learning phases.","Structures forming in neural networks leave identifiable traces that can be interpreted (e.g., using concepts from Singular Learning Theory); by catching and analyzing these developmental moments, researchers can automate interpretability, predict when dangerous capabilities emerge, and intervene to prevent deceptiveness or misaligned values as early as possible.","a:Reverse_engineering, a:Sparse_Coding, [ICL transience](https://proceedings.mlr.press/v267/singh25c.html)",Goals misgeneralize out of distribution,worst-case,cognitivist science,"Timaeus, Jesse Hoogland, George Wang, Daniel Murfet, Stan van Wingerden, Alexander Gietelink Oldenziel",10-50,"[Vaintrob](https://www.lesswrong.com/posts/M2bs6xCbmc79nwr8j/dmitry-vaintrob-s-shortform#A8Ziwhts35dgqbz52), [Joar Skalse (2023)](https://www.alignmentforum.org/posts/ALJYj4PpkqyseL7kZ/my-criticism-of-singular-learning-theory)","Manifund, Survival and Flourishing Fund, EA Funds",14
Representation structure and geometry,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Representation structure and geometry,What do the representations look like? Does any simple structure underlie the beliefs of all well-trained models? Can we get the semantics from this geometry?,"Get scalable unsupervised methods for finding structure in representations and interpreting them, then using this to e.g. guide training.","sec:Concept_based_interpretability, computational mechanics, feature universality, a:Natural_abstractions, a:Causal_Abstractions","Goals misgeneralize out of distribution, Superintelligence can fool human supervisors",mixed,cognitivist science,"Simplex, Insight + Interaction Lab, Paul Riechers, Adam Shai, Martin Wattenberg, Blake Richards, Mateusz Piotrowski",10-50,,"Various academic groups, Astera Institute, Coefficient Giving",13
Human inductive biases,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Human inductive biases,Discover connections deep learning AI systems have with human brains and human learning processes. Develop an 'alignment moonshot' based on a coherent theory of learning which applies to both humans and AI systems.,"Humans learn trust, honesty, self-maintenance, and corrigibility; if we understand how they do maybe we can get future AI systems to learn them.","active learning, ACS research",Goals misgeneralize out of distribution,pessimistic,cognitive,"Lukas Muttenthaler, Quentin Delfosse",4,,"Google DeepMind, various academic groups",6
Monitoring concepts,White-box safety (i.e. Interpretability),Concept-based interpretability,White-box safety (i.e. Interpretability) / Concept-based interpretability / Monitoring concepts,"Identifies directions or subspaces in a model's latent state that correspond to high-level concepts (like refusal, deception, or planning) and uses them to audit models for misalignment, monitor them at runtime, suppress eval awareness, debug why models are failing, etc.","By mapping internal activations to human-interpretable concepts, we can detect dangerous capabilities or deceptive alignment directly in the mind of the model even if its overt behavior is perfectly safe. Deploy computationally cheap monitors to flag some hidden misalignment in deployed systems.","[Pragmatic interp](https://www.alignmentforum.org/posts/StENzDcD3kpfGJssR/a-pragmatic-vision-for-interpretability), a:Reverse_engineering, a:Sparse_Coding, a:Model_diffing","Value is fragile and hard to specify, Goals misgeneralize out of distribution, A boxed AGI might exfiltrate itself by steganography, spearphishing",pessimistic,cognitive,"Daniel Beaglehole, Adityanarayanan Radhakrishnan, Enric Boix-Adserà, Tom Wollschläger, Anna Soligo, Jack Lindsey, Brian Christian, Ling Hu, Nicholas Goldowsky-Dill, Neel Nanda",50-100,"[Exploring the generalization of LLM truth directions on conversational formats](https://arxiv.org/html/2505.09807v1), [Understanding (Un)Reliability of Steering Vectors in Language Models](https://arxiv.org/abs/2505.22637)","Coefficient Giving, Anthropic, various academic groups",11
Activation engineering,White-box safety (i.e. Interpretability),Concept-based interpretability,White-box safety (i.e. Interpretability) / Concept-based interpretability / Activation engineering,"Programmatically modify internal model activations to steer outputs toward desired behaviors; a lightweight, interpretable supplement to fine-tuning.","Test interpretability theories by intervening on activations; find new insights from interpretable causal interventions on representations. Or: build more stuff to stack on top of finetuning. Slightly encourage the model to be nice, add one more layer of defence to our bundle of partial alignment methods.",a:Sparse_Coding,Value is fragile and hard to specify,average,engineering / cognitive,"Runjin Chen, Andy Arditi, David Krueger, Jan Wehner, Narmeen Oozeer, Reza Bayat, Adam Karvonen, Jiuding Sun, Tim Tian Hua, Helena Casademunt, Jacob Dunefsky, Thomas Marshall",20-100,[Understanding (Un)Reliability of Steering Vectors in Language Models](https://arxiv.org/abs/2505.22637),"Coefficient Giving, Anthropic",15
Guaranteed-Safe AI,Safety by construction,,Safety by construction / Guaranteed-Safe AI,"Have an AI system generate outputs (e.g. code, control systems, or RL policies) which it can quantitatively guarantee comply with a formal safety specification and world model.","Various, including:

i) safe deployment: create a scalable process to get not-fully-trusted AIs to produce highly trusted outputs;

ii) secure containers: create a 'gatekeeper' system that can act as an intermediary between human users and a potentially dangerous system, only letting provably safe actions through.

(Notable for not requiring that we solve ELK; does require that we solve ontology though)","[Towards Guaranteed Safe AI](https://arxiv.org/abs/2405.06624), [Standalone World-Models](https://www.alignmentforum.org/posts/LngR93YwiEpJ3kiWh/research-agenda-synthesizing-standalone-world-models), a:Scientist_AI, Safeguarded AI, a:Asymptotic_guarantees, Open Agency Architecture, SLES, program synthesis, Scalable formal oversight","Value is fragile and hard to specify, Goals misgeneralize out of distribution, Superintelligence can fool human supervisors, Humans cannot be first-class parties to a superintelligent value handshake, A boxed AGI might exfiltrate itself by steganography, spearphishing",worst-case,cognitive / engineering,"ARIA, Lawzero, Atlas Computing, FLF, Max Tegmark, Beneficial AI Foundation, Steve Omohundro, David ""davidad"" Dalrymple, Joar Skalse, Stuart Russell, Alessandro Abate",10-100,"[Zvi](https://thezvi.substack.com/p/ai-28-watching-and-waiting?utm_source=%2Fsearch%2Fomohundro&utm_medium=reader2#:~:text=Max%20Tegmark%20and%20Steve%20Omohundo%20drop%20a%20new%20paper), [Gleave](https://manifund.org//projects/relocating-to-montreal-to-work-full-time-on-ai-safety?tab=comments#aea6521a-c6bb-4c66-9f5f-cf647589cf7e), [Dickson](https://www.lesswrong.com/posts/B2bg677TaS4cmDPzL/limitations-on-formal-verification-for-ai-safety), [Greenblatt](https://www.lesswrong.com/posts/jRf4WENQnhssCb6mJ/davidad-s-bold-plan-for-alignment-an-in-depth-explanation?commentId=MJCvHk5ARMnWDjQDg)","Manifund, ARIA, Coefficient Giving, Survival and Flourishing Fund, Mila / CIFAR",5
Scientist AI,Safety by construction,,Safety by construction / Scientist AI,"Develop powerful, nonagentic, uncertain world models that accelerate scientific progress while avoiding the risks of agent AIs","Developing non-agentic 'Scientist AI' allows us to: (i) reap the benefits of AI progress while (ii) avoiding the inherent risks of agentic systems. These systems can also (iii) provide a useful guardrail to protect us from unsafe agentic AIs by double-checking actions they propose, and (iv) help us more safely build agentic superintelligent systems.","[JEPA](https://arxiv.org/abs/2511.08544), [oracles](https://www.lesswrong.com/w/oracle-ai)","Pivotal processes require dangerous capabilities, Goals misgeneralize out of distribution, Instrumental convergence",pessimistic,cognitivist science,"Yoshua Bengio, Younesse Kaddar",1-10,"Hard to find, but see [Raymond Douglas' comment](https://www.lesswrong.com/posts/p5gBcoQeBsvsMShvT/superintelligent-agents-pose-catastrophic-risks-can?commentId=tJXqhg3XZsqnyaZs2), [Karnofsky-Soares discussion](https://www.lesswrong.com/posts/iy2o4nQj9DnQD7Yhj/discussion-with-nate-soares-on-a-key-alignment-difficulty). Perhaps also [Predict-O-Matic](https://www.lesswrong.com/posts/SwcyMEgLyd4C3Dern/the-parable-of-predict-o-matic).","ARIA, Gates Foundation, Future of Life Institute, Coefficient Giving, Jaan Tallinn, Schmidt Sciences",2
Brainlike-AGI Safety,Safety by construction,,Safety by construction / Brainlike-AGI Safety,"Social and moral instincts are (partly) implemented in particular hardwired brain circuitry; let's figure out what those circuits are and how they work; this will involve symbol grounding. ""a yet-to-be-invented variation on actor-critic model-based reinforcement learning""","Fairly-direct alignment via changing training to reflect actual human reward. Get actual data about (reward, training data) → (human values) to help with theorising this map in AIs; ""understand human social instincts, and then maybe adapt some aspects of those for AGIs, presumably in conjunction with other non-biological ingredients"".",,,worst-case,cognitivist science,Steve Byrnes,1-5,[Tsvi BT](https://www.lesswrong.com/posts/unCG3rhyMJpGJpoLd/koan-divining-alien-datastructures-from-ram-activations#BtHCubjKWDFafkmYH),Astera Institute,6
Weak-to-strong generalization,Make AI solve it,,Make AI solve it / Weak-to-strong generalization,Use weaker models to supervise and provide a feedback signal to stronger models.,Find techniques that do better than RLHF at supervising superior models → track whether these techniques fail as capabilities increase further → keep the stronger systems aligned by amplifying weak oversight and quantifying where it breaks.,"sec:White_box_safety_i_e_Interpretability_, a:Supervising_AIs_improving_AIs",Superintelligence can hack software supervisors,average,engineering,"Joshua Engels, Nora Belrose, David D. Baek",2-20,"[Can we safely automate alignment research?](https://joecarlsmith.substack.com/p/can-we-safely-automate-alignment), [Super(ficial)-alignment: Strong Models May Deceive Weak Models in Weak-to-Strong Generalization](https://arxiv.org/abs/2406.11431)","lab funders, Eleuther funders",4
Supervising AIs improving AIs,Make AI solve it,,Make AI solve it / Supervising AIs improving AIs,"Build formal and empirical frameworks where AIs supervise other (stronger) AI systems via structured interactions; construct monitoring tools which enable scalable tracking of behavioural drift, benchmarks for self-modification, and robustness guarantees","Early models train ~only on human data while later models also train on early model outputs, which leads to early model problems cascading. Left unchecked this will likely cause problems, so supervision mechanisms are needed to help ensure the AI self-improvement remains legible.",,"Superintelligence can fool human supervisors, Superintelligence can hack software supervisors",pessimistic,behavioral,"Roman Engeler, Akbir Khan, Ethan Perez",1-10,"[Automation collapse](https://www.lesswrong.com/posts/2Gy9tfjmKwkYbF9BY/automation-collapse), [Great Models Think Alike and this Undermines AI Oversight](https://arxiv.org/abs/2502.04313)","Long-Term Future Fund, lab funders",8
AI explanations of AIs,Make AI solve it,,Make AI solve it / AI explanations of AIs,"Make open AI tools to explain AIs, including AI agents. e.g. automatic feature descriptions for neuron activation patterns; an interface for steering these features; a behaviour elicitation agent that ""searches"" for a specified behaviour in frontier models.",Use AI to help improve interp and evals. Develop and release open tools to level up the whole field. Get invited to improve lab processes.,sec:White_box_safety_i_e_Interpretability_,"Superintelligence can fool human supervisors, Superintelligence can hack software supervisors",pessimistic,cognitive,"Transluce, Jacob Steinhardt, Neil Chowdhury, Vincent Huang, Sarah Schwettmann, Robert Friel",15-30,,"Schmidt Sciences, Halcyon Futures, John Schulman, Wojciech Zaremba",5
Debate,Make AI solve it,,Make AI solve it / Debate,"In the limit, it's easier to compellingly argue for true claims than for false claims; exploit this asymmetry to get trusted work out of untrusted debaters.","""Give humans help in supervising strong agents"" + ""Align explanations with the true reasoning process of the agent"" + ""Red team models to exhibit failure modes that don't occur in normal use"" are necessary but probably not sufficient for safe AGI.",,"Value is fragile and hard to specify, Superintelligence can fool human supervisors",worst-case,engineering / cognitive,"Rohin Shah, Jonah Brown-Cohen, Georgios Piliouras, UK AISI (Benjamin Holton)",,[The limits of AI safety via debate (2022)](https://www.lesswrong.com/posts/kguLeJTt6LnGuYX4E/the-limits-of-ai-safety-via-debate),"Google, others",6
LLM introspection training,Make AI solve it,,Make AI solve it / LLM introspection training,"Train LLMs to the predict the outputs of high-quality whitebox methods, to induce general self-explanation skills that use its own 'introspective' access","Use the resulting LLMs as powerful dimensionality reduction, explaining internals in a distinct way than interpretability methods and CoT. Distilling self-explanation into the model should lead to the skill being scalable, since self-explanation skill advancement will feed off general-intelligence advancement.","[Transluce](#a:transluce), a:Anthropic","Goals misgeneralize out of distribution, Superintelligence can fool human supervisors, Superintelligence can hack software supervisors",mixed,cognitivist science,"Belinda Z. Li, Zifan Carl Guo, Vincent Huang, Jacob Steinhardt, Jacob Andreas, Jack Lindsey",2-20,,"Schmidt Sciences, Halcyon Futures, John Schulman, Wojciech Zaremba",2
Agent foundations,Theory,,Theory / Agent foundations,"Develop philosophical clarity and mathematical formalizations of building blocks that might be useful for plans to align strong superintelligence, such as agency, optimization strength, decision theory, abstractions, concepts, etc.","Rigorously understand optimization processed and agents, and what it means for them to be aligned in a substrate independent way → identify impossibility results and necessary conditions for aligned optimizer systems → use this theoretical understanding to eventually design safe architectures that remain stable and safe under self-reflection","a:Aligning_what_, a:Tiling_agents, [Dovetail](#a:theory_dovetail)","Value is fragile and hard to specify, Corrigibility is anti-natural, Goals misgeneralize out of distribution",worst-case,cognitive,"Abram Demski, Alex Altair, Sam Eisenstat, Thane Ruthenis, Alfred Harwood, Daniel C, Dalcy K, José Pedro Faustino",,,,10
Tiling agents,Theory,,Theory / Tiling agents,An aligned agentic system modifying itself into an unaligned system would be bad and we can research ways that this could occur and infrastructure/approaches that prevent it from happening.,Build enough theoretical basis through various approaches such that AI systems we create are capable of self-modification while preserving goals.,a:Agent_foundations,"Value is fragile and hard to specify, Corrigibility is anti-natural, Goals misgeneralize out of distribution",worst-case,cognitivist science,Abram Demski,1-10,,,4
High-Actuation Spaces,Theory,,Theory / High-Actuation Spaces,"Mech interp and alignment assume a stable ""computational substrate"" (linear algebra on GPUs). If later AI uses different substrates (e.g. something neuromorphic), methods like probes and steering will not transfer. Therefore, better to try and infer goals via a ""telic DAG"" which abstracts over substrates, and so sidestep the issue of how to define intermediate representations. Category theory is intended to provide guarantees that this abstraction is valid.",Sufficiently complex mindlike entities can alter their goals in ways that cannot be predicted or accounted for under substrate-dependent descriptions of the kind sought in mechanistic interpretability. use the telic DAG to define a method analogous to factoring a causal DAG.,"[Live theory](https://www.lesswrong.com/s/aMz2JMvgXrLBkq4h3), [MoSSAIC](https://openreview.net/forum?id=n7WYSJ35FU), [Topos Institute](https://topos.institute/), a:Agent_foundations",,pessimistic,maths / philosophy,"Sahil K, Matt Farr, Aditya Arpitha Prasad, Chris Pang, Aditya Adiga, Jayson Amati, Steve Petersen, Topos, T J",1-10,,,7
Asymptotic guarantees,Theory,,Theory / Asymptotic guarantees,"Prove that if a safety process has enough resources (human data quality, training time, neural network capacity), then in the limit some system specification will be guaranteed. Use complexity theory, game theory, learning theory and other areas to both improve asymptotic guarantees and develop ways of showing convergence.",Formal verification may be too hard. Make safety cases stronger by modelling their processes and proving that they would work in the limit.,"a:Debate, a:Guaranteed_Safe_AI, a:Control","Goals misgeneralize out of distribution, Superintelligence can fool human supervisors",pessimistic,cognitive,"AISI, Jacob Pfau, Benjamin Hilton, Geoffrey Irving, Simon Marshall, Will Kirby, Martin Soto, David Africa, davidad",5 - 10,Self-critique in [UK AISI's Alignment Team: Research Agenda](https://lesswrong.com/posts/tbnw7LbNApvxNLAg8/uk-aisi-s-alignment-team-research-agenda),AISI,4
Heuristic explanations,Theory,,Theory / Heuristic explanations,"Formalize mechanistic explanations of neural network behavior, automate the discovery of these ""heuristic explanations"" and use them to predict when novel input will lead to extreme behavior (i.e. ""Low Probability Estimation"" and ""Mechanistic Anomaly Detection"").","The current goalpost is methods whose *reasoned predictions* about properties of a neural network's outputs distribution (for a given inputs distribution) are certifiably at least as accurate as estimations via sampling. If successful for safety-relevant properties, this should allow for automated alignment methods that are both human-legible and worst-case certified, as well more efficient than sampling-based methods in most cases.","ARC Theory, ELK, mechanistic anomaly detection, [Acorn](https://acausal.org/), a:Guaranteed_Safe_AI","Goals misgeneralize out of distribution, Superintelligence can hack software supervisors",worst-case,cognitive / maths/philosophy,"Jacob Hilton, Mark Xu, Eric Neyman, Victor Lecomte, George Robinson",1-10,[Matolcsi](https://www.lesswrong.com/s/uYMw689vDFmgPEHrS),,5
Behavior alignment theory,Theory,Corrigibility,Theory / Corrigibility / Behavior alignment theory,Predict properties of future AGI (e.g. power-seeking) with formal models; formally state and prove hypotheses about the properties powerful systems will have and how we might try to change them.,Figure out hypotheses about properties powerful agents will have → attempt to rigorously prove under what conditions the hypotheses hold → test these hypotheses where feasible → design training environments that lead to more salutary properties.,"a:Agent_foundations, a:Control","Corrigibility is anti-natural, Instrumental convergence",worst-case,maths / philosophy,"Ram Potham, Michael K. Cohen, Max Harms/Raelifin, John Wentworth, David Lorell, Elliott Thornley",1-10,[Ryan Greenblatt's criticism](https://www.lesswrong.com/posts/YbEbwYWkf8mv9jnmi/the-shutdown-problem-incomplete-preferences-as-a-solution?commentId=GJAippZ6ZzCagSnDb) of one behavioural proposal,,10
Other corrigibility,Theory,Corrigibility,Theory / Corrigibility / Other corrigibility,"Diagnose and communicate obstacles to achieving robustly corrigible behavior; suggest mechanisms, tests, and escalation channels for surfacing and mitigating incorrigible behaviors",Labs are likely to develop AGI using something analogous to current pipelines. Clarifying why naive instruction-following doesn't buy robust corrigibility + building strong tripwires/diagnostics for scheming and Goodharting thus reduces risks on the likely default path.,a:Behavior_alignment_theory,"Corrigibility is anti-natural, Instrumental convergence",pessimistic,varies,Jeremy Gillen,1-10,,,9
Natural abstractions,Theory,Ontology Identification,Theory / Ontology Identification / Natural abstractions,"Develop a theory of concepts that explains how they are learned, how they structure a particular system's understanding, and how mutual translatability can be achieved between different collections of concepts.","Understand the concepts a system's understanding is structured with and use them to inspect its ""alignment/safety properties"" and/or ""retarget its search"", i.e. identify utility-function-like components inside an AI and replacing calls to them with calls to ""user values"" (represented using existing abstractions inside the AI).","a:Causal_Abstractions, representational alignment, convergent abstractions, feature universality, Platonic representation hypothesis, microscope AI","Instrumental convergence, Superintelligence can fool human supervisors, Humans cannot be first-class parties to a superintelligent value handshake",worst-case,cognitive,"John Wentworth, Paul Colognese, David Lorrell, Sam Eisenstat, Fernando Rosas",1-10,"[Chan et al (2023)](https://www.lesswrong.com/posts/gvzW46Z3BsaZsLc25/natural-abstractions-key-claims-theorems-and-critiques-1#3__A_formalization_of_abstractions_would_accelerate_alignment_research), [Soto](https://www.lesswrong.com/posts/CJjT8GMitsnKc2wgG/natural-abstractions-are-observer-dependent-a-conversation-1), [Harwood](https://www.lesswrong.com/posts/F4nzox6oh5oAdX9D3/abstractions-are-not-natural), [Soares (2023)](https://www.lesswrong.com/posts/mgjHS6ou7DgwhKPpu/a-rough-and-incomplete-review-of-some-of-john-wentworth-s)",,10
The Learning-Theoretic Agenda,Theory,Ontology Identification,Theory / Ontology Identification / The Learning-Theoretic Agenda,"Create a mathematical theory of intelligent agents that encompasses both humans and the AIs we want, one that specifies what it means for two such agents to be aligned; translate between its ontology and ours; produce formal desiderata for a training setup that produces coherent AGIs similar to (our model of) an aligned agent",Fix formal epistemology to work out how to avoid deep training problems,,"Value is fragile and hard to specify, Goals misgeneralize out of distribution, Humans cannot be first-class parties to a superintelligent value handshake",worst-case,cognitive,"Vanessa Kosoy, Diffractor, Gergely Szücs",3,[Matolcsi](https://www.lesswrong.com/posts/StkjjQyKwg7hZjcGB/a-mostly-critical-review-of-infra-bayesianism),"Survival and Flourishing Fund, ARIA, UK AISI, Coefficient Giving",6
Aligning to context,Multi-agent first,,Multi-agent first / Aligning to context,"Align AI directly to the role of participant, collaborator, or advisor for our best real human practices and institutions, instead of aligning AI to separately representable goals, rules, or utility functions.","""Many classical problems in AGI alignment are downstream of a type error about human values."" Operationalizing a correct view of human values - one that treats human values as impossible or impractical to abstract from concrete practices - will unblock value fragility, goal-misgeneralization, instrumental convergence, and pivotal-act specification.","a:Aligning_what_, a:Aligned_to_who_","Value is fragile and hard to specify, Corrigibility is anti-natural, Goals misgeneralize out of distribution, Instrumental convergence, Fair, sane pivotal processes",mixed,behavioural,"Full Stack Alignment, Meaning Alignment Institute, Plurality Institute, Tan Zhi-Xuan, Matija Franklin, Ryan Lowe, Joe Edelman, Oliver Klingefjord",5,,"ARIA, OpenAI, Survival and Flourishing Fund",8
Aligning to the social contract,Multi-agent first,,Multi-agent first / Aligning to the social contract,Generate AIs' operational values from 'social contract'-style ideal civic deliberation formalisms and their consequent rulesets for civic actors,"Formalize and apply the liberal tradition's project of defining civic principles separable from the substantive good, aligning our AIs to civic principles that bypass fragile utility-learning and intractable utility-calculation","a:Aligning_to_context, a:Aligning_what_","Value is fragile and hard to specify, Goals misgeneralize out of distribution, Instrumental convergence, Humanlike minds/goals are not necessarily safe, Fair, sane pivotal processes",mixed,cognitive,"Gillian Hadfield, Tan Zhi-Xuan, Sydney Levine, Matija Franklin, Joshua B. Tenenbaum",5 - 10,,"Deepmind, Macroscopic Ventures",8
Theory for aligning multiple AIs,Multi-agent first,,Multi-agent first / Theory for aligning multiple AIs,"Use realistic game-theory variants (e.g. evolutionary game theory, computational game theory) or develop alternative game theories to describe/predict the collective and individual behaviours of AI agents in multi-agent scenarios.","While traditional AGI safety focuses on idealized decision-theory and individual agents, it's plausible that strategic AI agents will first emerge (or are emerging now) in a complex, multi-AI strategic landscape. We need granular, realistic formal models of AIs' strategic interactions and collective dynamics to understand this future.","a:Tools_for_aligning_multiple_AIs, a:Aligning_what_","Goals misgeneralize out of distribution, Superintelligence can fool human supervisors, Superintelligence can hack software supervisors",mixed,cognitive,"Lewis Hammond, Emery Cooper, Allan Chan, Caspar Oesterheld, Vincent Conitzer, Vojta Kovarik, Nathaniel Sauerberg, ACS Research, Jan Kulveit, Richard Ngo, Emmett Shear, Softmax, Full Stack Alignment, AI Objectives Institute, Sahil, TJ, Andrew Critch",10,,"SFF, CAIF, Deepmind, Macroscopic Ventures",12
Tools for aligning multiple AIs,Multi-agent first,,Multi-agent first / Tools for aligning multiple AIs,"Develop tools and techniques for designing and testing multi-agent AI scenarios, for auditing real-world multi-agent AI dynamics, and for aligning AIs in multi-AI settings.","Addressing multi-agent AI dynamics is key for aligning near-future agents and their impact on the world. Feedback loops from multi-agent dynamics can radically change the future AI landscape, and require a different toolset from model psychology to audit and control.","a:Theory_for_aligning_multiple_AIs, a:Aligning_what_","Goals misgeneralize out of distribution, Superintelligence can fool human supervisors, Superintelligence can hack software supervisors",mixed,engineering / behavioral,"Andrew Critch, Lewis Hammond, Emery Cooper, Allan Chan, Caspar Oesterheld, Vincent Conitzer, Gillian Hadfield, Nathaniel Sauerberg, Zhijing Jin",10 - 15,,"Coefficient Giving, Deepmind, Cooperative AI Foundation",12
Aligned to who?,Multi-agent first,,Multi-agent first / Aligned to who?,"Technical protocols for taking seriously the plurality of human values, cultures, and communities when aligning AI to ""humanity""","use democratic/pluralist/context-sensitive principles to guide AI development, alignment, and deployment somehow. Doing it as an afterthought in post-training or the spec isn't good enough. Continuously shape AI's social and technical feedback loop on the road to AGI","a:Aligning_what_, a:Aligning_to_context","Value is fragile and hard to specify, Fair, sane pivotal processes",average,behavioral,"Joel Z. Leibo, Divya Siddarth, Séb Krier, Luke Thorburn, Seth Lazar, AI Objectives Institute, The Collective Intelligence Project, Vincent Conitzer",5 - 15,,"Future of Life Institute, Survival and Flourishing Fund, Deepmind, CAIF",9
Aligning what?,Multi-agent first,,Multi-agent first / Aligning what?,"Develop alternatives to agent-level models of alignment, by treating human-AI interactions, AI-assisted institutions, AI economic or cultural systems, drives within one AI, and other causal/constitutive processes as subject to alignment","Model multiple reality-shaping processes above and below the level of the individual AI, some of which are themselves quasi-agential (e.g. cultures) or intelligence-like (e.g. markets), will develop AI alignment into a mature science for managing the transition to an AGI civilization","a:Theory_for_aligning_multiple_AIs, a:Aligning_to_context, a:Aligned_to_who_","Value is fragile and hard to specify, Corrigibility is anti-natural, Goals misgeneralize out of distribution, Instrumental convergence, Fair, sane pivotal processes",mixed,behavioral / cognitive,"Richard Ngo, Emmett Shear, Softmax, Full Stack Alignment, AI Objectives Institute, Sahil, TJ, Andrew Critch, ACS Research, Jan Kulveit",5-10,,"Future of Life Institute, Emmett Shear",13
AGI metrics,Evals,,Evals / AGI metrics,Evals with the explicit aim of measuring progress towards full human-level generality.,Help predict timelines for risk awareness and strategy.,a:Capability_evals,,mixed,behavioural,"CAIS, CFI Kinds of Intelligence, Apart Research, OpenAI, METR, Lexin Zhou, Adam Scholl, Lorenzo Pacchiardi",10-50,"[Is the Definition of AGI a Percentage?](https://aievaluation.substack.com/p/is-the-definition-of-agi-a-percentage), [The ""Length"" of ""Horizons""](https://www.lesswrong.com/posts/PzLSuaT6WGLQGJJJD/the-length-of-horizons)","Leverhulme Trust, Open Philanthropy, Long-Term Future Fund",5
Capability evals,Evals,,Evals / Capability evals,Make tools that can actually check whether a model has a certain capability or propensity. We default to low-n sampling of a vast latent space but aim to do better.,"Keep a close eye on what capabilities are acquired when, so that frontier labs and regulators are better informed on what security measures are already necessary (and hopefully they extrapolate). You can't regulate without them.","[Deepmind's frontier safety framework](https://deepmind.google/blog/strengthening-our-frontier-safety-framework/), [Aether](https://www.lesswrong.com/posts/B8Cmtf5gdHwxb8qtT/aether-july-2025-update)",,average,behaviorist science,"METR, AISI, Apollo Research, Marrius Hobbhahn, Meg Tong, Mary Phuong, Beth Barnes, Thomas Kwa, Joel Becker",100+,"[Large Language Models Often Know When They Are Being Evaluated](https://arxiv.org/abs/2505.23836), [AI Sandbagging: Language Models can Strategically Underperform on Evaluations](https://arxiv.org/abs/2406.07358), [The Leaderboard Illusion](https://arxiv.org/abs/2504.20879), [Do Large Language Model Benchmarks Test Reliability?](https://arxiv.org/abs/2502.03461)","basically everyone. Google, Microsoft, Open Philanthropy, LTFF, Governments etc",34
Autonomy evals,Evals,,Evals / Autonomy evals,"Measure an AI's ability to act autonomously to complete long-horizon, complex tasks.","By measuring how long and complex a task an AI can complete (its ""time horizon""), we can track capability growth and identify when models gain dangerous autonomous capabilities (like R&D acceleration or replication).","a:Capability_evals, [OpenAI Preparedness](https://openai.com/index/updating-our-preparedness-framework/), [Anthropic RSP](https://www.anthropic.com/rsp-updates)",,average,behaviorist science,"METR, Thomas Kwa, Ben West, Joel Becker, Beth Barnes, Hjalmar Wijk, Tao Lin, Giulio Starace, Oliver Jaffe, Dane Sherburn, Sanidhya Vijayvargiya, Aditya Bharat Soni, Xuhui Zhou",10-50,"[Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity.](https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/) [The ""Length"" of ""Horizons""](https://www.lesswrong.com/posts/PzLSuaT6WGLQGJJJD/the-length-of-horizons)","The Audacious Project, Open Philanthropy",13
WMD evals (Weapons of Mass Destruction),Evals,,Evals / WMD evals (Weapons of Mass Destruction),"Evaluate whether AI models possess dangerous knowledge or capabilities related to biological and chemical weapons, such as biosecurity or chemical synthesis.","By benchmarking and tracking AI's knowledge of biology and chemistry, we can identify when models become capable of accelerating WMD development or misuse, allowing for timely intervention.","a:Capability_evals, a:Autonomy_evals, a:Various_Redteams",,pessimistic,behaviorist science,"Lennart Justen, Haochen Zhao, Xiangru Tang, Ziran Yang, Aidan Peppin, Anka Reuel, Stephen Casper",10-50,[The Reality of AI and Biorisk](https://arxiv.org/abs/2412.01946),"Open Philanthropy, UK AI Safety Institute (AISI), frontier labs, Scale AI, various academic institutions (Peking University, Yale, etc.), Meta",6
Situational awareness and self-awareness evals,Evals,,Evals / Situational awareness and self-awareness evals,"Evaluate if models understand their own internal states and behaviors, their environment, and whether they are in a test or real-world deployment.","If an AI can distinguish between evaluation and deployment (""evaluation awareness""), it might hide dangerous capabilities (scheming/sandbagging). By measuring self- and situational-awareness, we can better assess this risk and build more robust evaluations.","a:Sandbagging_evals, a:Various_Redteams, sec:Model_psychology","Superintelligence can fool human supervisors, Superintelligence can hack software supervisors",worst-case,behaviorist science,"Jan Betley, Xuchan Bao, Martín Soto, Mary Phuong, Roland S. Zimmermann, Joe Needham, Giles Edkins, Govind Pimpale, Kai Fronsdal, David Lindner, Lang Xiong, Xiaoyan Bai",30-70,"[Lessons from a Chimp: AI ""Scheming"" and the Quest for Ape Language](https://arxiv.org/abs/2507.03409), [It's hard to make scheming evals look realistic for LLMs](https://www.lesswrong.com/posts/TBk2dbWkg2F7dB3jb/it-s-hard-to-make-scheming-evals-look-realistic-for-llms)","frontier labs (Google DeepMind, Anthropic), Open Philanthropy, The Audacious Project, UK AI Safety Institute (AISI), AI Safety Support, Apollo Research, METR",11
Steganography evals,Evals,,Evals / Steganography evals,"evaluate whether models can hide secret information or encoded reasoning in their outputs, such as in chain-of-thought scratchpads, to evade monitoring.","if models can use steganography, they could hide deceptive reasoning, bypassing safety monitoring and control measures. By evaluating this capability, we can assess the risk of a model fooling its supervisors.","a:AI_deception_evals, a:Chain_of_thought_monitoring","A boxed AGI might exfiltrate itself by steganography, spearphishing, Superintelligence can fool human supervisors",worst-case,behavioral,"Antonio Norelli, Michael Bronstein",1-10,Chain-of-Thought Is Already Unfaithful (So Steganography is Irrelevant): [Reasoning Models Don't Always Say What They Think.](https://assets.anthropic.com/m/71876fabef0f0ed4/original/reasoning_models_paper.pdf),"Anthropic (and its general funders, e.g., Google, Amazon)",5
AI deception evals,Evals,,Evals / AI deception evals,"research demonstrating that AI models, particularly agentic ones, can learn and execute deceptive behaviors such as alignment faking, manipulation, and sandbagging.","proactively discover, evaluate, and understand the mechanisms of AI deception (e.g., alignment faking, manipulation, agentic deception) to prevent models from fooling human supervisors and causing harm.","a:Situational_awareness_and_self_awareness_evals, a:Steganography_evals, a:Sandbagging_evals, a:Chain_of_thought_monitoring","Superintelligence can fool human supervisors, Superintelligence can hack software supervisors",worst-case,behavioral / engineering,"Cadenza, Fred Heiding, Simon Lermen, Andrew Kao, Myra Cheng, Cinoo Lee, Pranav Khadpe, Satyapriya Krishna, Andy Zou, Rahul Gupta",30-80,"A central criticism is that the evaluation scenarios are ""artificial and contrived"". [the void](https://nostalgebraist.tumblr.com/post/785766737747574784/the-void) and [Lessons from a Chimp](https://arxiv.org/abs/2507.03409) argue this research is ""overattributing human traits"" to models.","Labs, academic institutions (e.g., Harvard, CMU, Barcelona Institute of Science and Technology), NSFC, ML Alignment Theory & Scholars (MATS) Program, FAR AI",13
AI scheming evals,Evals,,Evals / AI scheming evals,"Evaluate frontier models for scheming, a sophisticated, strategic form of AI deception where a model covertly pursues a misaligned, long-term objective while deliberately faking alignment and compliance to evade detection by human supervisors and safety mechanisms.","Robust evaluations must move beyond checking final outputs and probe the model's reasoning to verify that alignment is genuine, not faked, because capable models are capable of strategically concealing misaligned goals (scheming) to pass standard behavioural evaluations.","a:AI_deception_evals, a:Situational_awareness_and_self_awareness_evals",Superintelligence can fool human supervisors,pessimistic,behavioral / engineering,"Bronson Schoen, Alexander Meinke, Jason Wolfe, Mary Phuong, Rohin Shah, Evgenia Nitishinskaya, Mikita Balesni, Marius Hobbhahn, Jérémy Scheurer, Wojciech Zaremba, David Lindner",30-60,"[No, LLMs are not ""scheming""](https://www.strangeloopcanon.com/p/no-llms-are-not-scheming)","OpenAI, Anthropic, Google DeepMind, Open Philanthropy",7
Sandbagging evals,Evals,,Evals / Sandbagging evals,"Evaluate whether AI models deliberately hide their true capabilities or underperform, especially when they detect they are in an evaluation context.","If models can distinguish between evaluation and deployment contexts (""evaluation awareness""), they might learn to ""sandbag"" or deliberately underperform to hide dangerous capabilities, fooling safety evaluations. By developing evaluations for sandbagging, we can test whether our safety methods are being deceived and detect this behavior before a model is deployed.","a:AI_deception_evals, a:Situational_awareness_and_self_awareness_evals, a:Various_Redteams","Superintelligence can fool human supervisors, Superintelligence can hack software supervisors",pessimistic,behaviorist science,"Teun van der Weij, Cameron Tice, Chloe Li, Johannes Gasteiger, Joseph Bloom, Joel Dyer",10-50,"The main external critique, from sources like ""[the void](https://nostalgebraist.tumblr.com/post/785766737747574784/the-void)"" and ""[Lessons from a Chimp](https://arxiv.org/abs/2507.03409)"", is that this research ""overattribut\[es\] human traits"" to models. It argues that what's being measured isn't genuine sandbagging but models ""playing-along-with-drama behaviour"" in response to ""artificial and contrived"" evals.","Anthropic (and its funders, e.g., Google, Amazon), UK Government (funding the AI Security Institute)",9
Self-replication evals,Evals,,Evals / Self-replication evals,"evaluate whether AI agents can autonomously replicate themselves by obtaining their own weights, securing compute resources, and creating copies of themselves.","if AI agents gain the ability to self-replicate, they could proliferate uncontrollably, making them impossible to shut down. By measuring this capability with benchmarks like RepliBench, we can identify when models cross this dangerous ""red line"" and implement controls before losing containment.","a:Autonomy_evals, a:Situational_awareness_and_self_awareness_evals","Instrumental convergence, A boxed AGI might exfiltrate itself by steganography, spearphishing",worst-case,behaviorist science,"Sid Black, Asa Cooper Stickland, Jake Pencharz, Oliver Sourbut, Michael Schmatz, Jay Bailey, Ollie Matthews, Ben Millwood, Alex Remedios, Alan Cooney, Xudong Pan, Jiarun Dai, Yihe Fan",10-20,[AI Sandbagging](https://arxiv.org/abs/2406.07358),UK Government (via UK AI Safety Institute),3
Various Redteams,Evals,,Evals / Various Redteams,attack current models and see what they do / deliberately induce bad things on current frontier models to test out our theories / methods.,"to ensure models are safe, we must actively try to break them. By developing and applying a diverse suite of attacks (e.g., in novel domains, against agentic systems, or using automated tools), researchers can discover vulnerabilities, specification gaming, and deceptive behaviors before they are exploited, thereby informing the development of more robust defenses.",a:Other_evals,"A boxed AGI might exfiltrate itself by steganography, spearphishing, Goals misgeneralize out of distribution",average,behaviorist science,"Ryan Greenblatt, Benjamin Wright, Aengus Lynch, John Hughes, Samuel R. Bowman, Andy Zou, Nicholas Carlini, Abhay Sheshadri",100+,"[Claude Sonnet 3.7 (often) knows when it's in alignment evaluations](https://www.alignmentforum.org/posts/E3daBewppAiECN3Ao/claude-sonnet-3-7-often-knows-when-it-s-in-alignment), [Red Teaming AI Red Teaming.](https://arxiv.org/html/2507.05538v1)","Frontier labs (Anthropic, OpenAI, Google), government (UK AISI), Open Philanthropy, LTFF, academic grants.",57
Other evals,Evals,,Evals / Other evals,"A collection of miscellaneous evaluations for specific alignment properties, such as honesty, shutdown resistance and sycophancy.","By developing novel benchmarks for specific, hard-to-measure properties (like honesty), critiquing the reliability of existing methods (like cultural surveys), and improving the formal rigor of evaluation systems (like LLM-as-Judges), researchers can create a more robust and comprehensive suite of evaluations to catch failures missed by standard capability or safety testing.",other more specific sections on evals,,average,behaviorist science,"Richard Ren, Mantas Mazeika, Andrés Corrada-Emmanuel, Ariba Khan, Stephen Casper",20-50,"[The Unreliability of Evaluating Cultural Alignment in LLMs](https://arxiv.org/abs/2503.08688), [The Leaderboard Illusion](https://arxiv.org/abs/2504.20879)","Lab funders (OpenAI), Open Philanthropy (which funds CAIS, the organization for the MASK benchmark), academic institutions. N/A (as a discrete amount). This work is part of the ""tens of millions"" budgets for broader evaluation and red-teaming efforts at labs and independent organizations.",20
