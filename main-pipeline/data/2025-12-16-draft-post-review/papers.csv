agenda_name,agenda_section,agenda_sub_section,agenda_full_name,link_url,link_text,title,authors,author_organizations,published_year,date
OpenAI,Labs (giant companies),,Labs (giant companies) / OpenAI,,Their 60-page System Cards now contain a large amount of their public safety work.,,,,,
OpenAI,Labs (giant companies),,Labs (giant companies) / OpenAI,https://arxiv.org/abs/2503.11926,Monitoring Reasoning Models for Misbehavior and the Risks of Promoting Obfuscation,Monitoring Reasoning Models for Misbehavior and the Risks of Promoting Obfuscation,"Bowen Baker, Joost Huizinga, Leo Gao, Zehao Dou, Melody Y. Guan, Aleksander Madry, Wojciech Zaremba, Jakub Pachocki, David Farhi",OpenAI,2025,2025-03-14
OpenAI,Labs (giant companies),,Labs (giant companies) / OpenAI,https://arxiv.org/abs/2506.19823,Persona Features Control Emergent Misalignment,Persona Features Control Emergent Misalignment,"Miles Wang, Tom Dupré la Tour, Olivia Watkins, Alex Makelov, Ryan A. Chi, Samuel Miserendino, Jeffrey Wang, Achyuta Rajaram, Johannes Heidecke, Tejal Patwardhan, Dan Mossing",,2025,2025-06-24
OpenAI,Labs (giant companies),,Labs (giant companies) / OpenAI,https://arxiv.org/abs/2509.15541,Stress Testing Deliberative Alignment for Anti-Scheming Training,Stress Testing Deliberative Alignment for Anti-Scheming Training,"Bronson Schoen, Evgenia Nitishinskaya, Mikita Balesni, Axel Højmark, Felix Hofstätter, Jérémy Scheurer, Alexander Meinke, Jason Wolfe, Teun van der Weij, Alex Lloyd, Nicholas Goldowsky-Dill, Angela Fan, Andrei Matveiakin, Rusheb Shah, Marcus Williams, Amelia Glaese, Boaz Barak, Wojciech Zaremba, Marius Hobbhahn","OpenAI, Anthropic",2025,2025-09-19
OpenAI,Labs (giant companies),,Labs (giant companies) / OpenAI,https://arxiv.org/abs/2412.16339,Deliberative Alignment: Reasoning Enables Safer Language Models,Deliberative Alignment: Reasoning Enables Safer Language Models,"Melody Y. Guan, Manas Joglekar, Eric Wallace, Saachi Jain, Boaz Barak, Alec Helyar, Rachel Dias, Andrea Vallone, Hongyu Ren, Jason Wei, Hyung Won Chung, Sam Toyer, Johannes Heidecke, Alex Beutel, Amelia Glaese",OpenAI,2024,2024-12-20
OpenAI,Labs (giant companies),,Labs (giant companies) / OpenAI,https://openai.com/index/emergent-misalignment,Toward understanding and preventing misalignment generalization,Toward understanding and preventing misalignment generalization,"Miles Wang, Tom Dupré la Tour, Olivia Watkins, Aleksandar Makelov, Ryan A. Chi, Samuel Miserendino, Tejal Patwardhan, Dan Mossing",OpenAI,2025,2025-06-18
OpenAI,Labs (giant companies),,Labs (giant companies) / OpenAI,https://openai.com/index/updating-our-preparedness-framework/,Our updated Preparedness Framework,Our updated Preparedness Framework,OpenAI Preparedness Team,OpenAI,2025,2025-04-15
OpenAI,Labs (giant companies),,Labs (giant companies) / OpenAI,https://arxiv.org/abs/2501.18841,Trading Inference-Time Compute for Adversarial Robustness,Trading Inference-Time Compute for Adversarial Robustness,"Wojciech Zaremba, Evgenia Nitishinskaya, Boaz Barak, Stephanie Lin, Sam Toyer, Yaodong Yu, Rachel Dias, Eric Wallace, Kai Xiao, Johannes Heidecke, Amelia Glaese",OpenAI,2025,2025-01-31
OpenAI,Labs (giant companies),,Labs (giant companies) / OpenAI,https://arxiv.org/abs/2505.16260,Small-to-Large Generalization: Data Influences Models Consistently Across Scale,Small-to-Large Generalization: Data Influences Models Consistently Across Scale,"Alaa Khaddaj, Logan Engstrom, Aleksander Madry",MIT,2025,2025-05-22
OpenAI,Labs (giant companies),,Labs (giant companies) / OpenAI,https://openai.com/index/openai-anthropic-safety-evaluation,Findings from a pilot Anthropic–OpenAI alignment evaluation exercise: OpenAI Safety Tests,Findings from a pilot Anthropic–OpenAI alignment evaluation exercise: OpenAI Safety Tests,,"OpenAI, Anthropic",2025,2025-08-27
OpenAI,Labs (giant companies),,Labs (giant companies) / OpenAI,https://openai.com/safety/evaluations-hub,Safety evaluations hub,Safety evaluations hub,,OpenAI,2025,2025-08-15
OpenAI,Labs (giant companies),,Labs (giant companies) / OpenAI,https://alignment.openai.com/,alignment.openai.com,Alignment Research Blog,,OpenAI,2025,2025-12-01
OpenAI,Labs (giant companies),,Labs (giant companies) / OpenAI,https://cdn.openai.com/pdf/41df8f28-d4ef-43e9-aed2-823f9393e470/circuit-sparsity-paper.pdf,Weight-sparse transformers have interpretable circuits,Unknown - PDF content not accessible,,OpenAI,,
Google Deepmind,Labs (giant companies),,Labs (giant companies) / Google Deepmind,https://www.alignmentforum.org/posts/StENzDcD3kpfGJssR/a-pragmatic-vision-for-interpretability%20,A Pragmatic Vision for Interpretability,A Pragmatic Vision for Interpretability,"Neel Nanda, Josh Engels, Arthur Conmy, Senthooran Rajamanoharan, bilalchughtai, CallumMcDougall, János Kramár, lewis smith",Google DeepMind,2025,2025-12-01
Google Deepmind,Labs (giant companies),,Labs (giant companies) / Google Deepmind,https://www.alignmentforum.org/posts/MnkeepcGirnJn736j/how-can-interpretability-researchers-help-agi-go-well%20,How Can Interpretability Researchers Help AGI Go Well?,How Can Interpretability Researchers Help AGI Go Well?,"Neel Nanda, Josh Engels, Senthooran Rajamanoharan, Arthur Conmy, bilalchughtai, CallumMcDougall, János Kramár, lewis smith",Google DeepMind,2024,2024-12-01
Google Deepmind,Labs (giant companies),,Labs (giant companies) / Google Deepmind,https://arxiv.org/abs/2505.01420,Evaluating Frontier Models for Stealth and Situational Awareness,Evaluating Frontier Models for Stealth and Situational Awareness,"Mary Phuong, Roland S. Zimmermann, Ziyue Wang, David Lindner, Victoria Krakovna, Sarah Cogan, Allan Dafoe, Lewis Ho, Rohin Shah",Google DeepMind,2025,2025-05-02
Google Deepmind,Labs (giant companies),,Labs (giant companies) / Google Deepmind,https://arxiv.org/abs/2507.05246,"When Chain of Thought is Necessary, Language Models Struggle to Evade Monitors","When Chain of Thought is Necessary, Language Models Struggle to Evade Monitors","Scott Emmons, Erik Jenner, David K. Elson, Rif A. Saurous, Senthooran Rajamanoharan, Heng Chen, Irhum Shafkat, Rohin Shah",Google DeepMind,2025,2025-07-07
Google Deepmind,Labs (giant companies),,Labs (giant companies) / Google Deepmind,https://alignmentforum.org/posts/zWySWKuXnhMDhgwc3/mona-managed-myopia-with-approval-feedback-2,MONA: Managed Myopia with Approval Feedback,MONA: Managed Myopia with Approval Feedback,"Sebastian Farquhar, David Lindner, Rohin Shah",DeepMind,2025,2025-01-23
Google Deepmind,Labs (giant companies),,Labs (giant companies) / Google Deepmind,https://arxiv.org/abs/2510.27062,Consistency Training Helps Stop Sycophancy and Jailbreaks,Consistency Training Helps Stop Sycophancy and Jailbreaks,"Alex Irpan, Alexander Matt Turner, Mark Kurzeja, David K. Elson, Rohin Shah","DeepMind, Google",2025,2025-10-31
Google Deepmind,Labs (giant companies),,Labs (giant companies) / Google Deepmind,https://arxiv.org/abs/2504.01849,An Approach to Technical AGI Safety and Security,An Approach to Technical AGI Safety and Security,"Rohin Shah, Alex Irpan, Alexander Matt Turner, Anna Wang, Arthur Conmy, David Lindner, Jonah Brown-Cohen, Lewis Ho, Neel Nanda, Raluca Ada Popa, Rishub Jain, Rory Greig, Samuel Albanie, Scott Emmons, Sebastian Farquhar, Sébastien Krier, Senthooran Rajamanoharan, Sophie Bridgers, Tobi Ijitoye, Tom Everitt, Victoria Krakovna, Vikrant Varma, Vladimir Mikulik, Zachary Kenton, Dave Orr, Shane Legg, Noah Goodman, Allan Dafoe, Four Flynn, Anca Dragan","Google DeepMind, Anthropic, Stanford University, UC Berkeley, Redwood Research",2025,2025-04-02
Google Deepmind,Labs (giant companies),,Labs (giant companies) / Google Deepmind,https://alignmentforum.org/posts/4uXCAJNuPKtKBsi28/negative-results-for-saes-on-downstream-tasks,Negative Results for SAEs On Downstream Tasks and Deprioritising SAE Research (GDM Mech Interp Team Progress Update #2),Negative Results for SAEs On Downstream Tasks and Deprioritising SAE Research (GDM Mech Interp Team Progress Update #2),"Lewis Smith, Senthooran Rajamanoharan, Arthur Conmy, Callum McDougall, Tom Lieberum, János Kramár, Rohin Shah, Neel Nanda",Google DeepMind,2025,2025-03-26
Google Deepmind,Labs (giant companies),,Labs (giant companies) / Google Deepmind,https://turntrout.com/gemini-steering,Steering Gemini Using BIDPO Vectors,Steering Gemini Using BIDPO Vectors,"Alex Turner, Mark Kurzeja, Dave Orr, David Elson",Google DeepMind,2025,2025-01-30
Google Deepmind,Labs (giant companies),,Labs (giant companies) / Google Deepmind,https://arxiv.org/html/2511.22662v1,Difficulties with Evaluating a Deception Detector for AIs,Difficulties with Evaluating a Deception Detector for AIs,"Lewis Smith, Bilal Chughtai, Neel Nanda",Google,2025,2025-11-27
Google Deepmind,Labs (giant companies),,Labs (giant companies) / Google Deepmind,https://deepmind.google/discover/blog/taking-a-responsible-path-to-agi/,Taking a responsible path to AGI,Taking a responsible path to AGI,"Anca Dragan, Rohin Shah, Four Flynn, Shane Legg",Google DeepMind,2025,2025-04-02
Google Deepmind,Labs (giant companies),,Labs (giant companies) / Google Deepmind,https://deepmind.google/discover/blog/evaluating-potential-cybersecurity-threats-of-advanced-ai,Evaluating potential cybersecurity threats of advanced AI,Evaluating potential cybersecurity threats of advanced AI,"Four Flynn, Mikel Rodriguez, Raluca Ada Popa",Google DeepMind,2025,2025-04-02
Google Deepmind,Labs (giant companies),,Labs (giant companies) / Google Deepmind,https://www.alignmentforum.org/posts/wnzkjSmrgWZaBa2aC/self-preservation-or-instruction-ambiguity-examining-the,Self-preservation or Instruction Ambiguity? Examining the Causes of Shutdown Resistance,Self-preservation or Instruction Ambiguity? Examining the Causes of Shutdown Resistance,"Senthooran Rajamanoharan, Neel Nanda",Google DeepMind,2025,2025-07-14
Google Deepmind,Labs (giant companies),,Labs (giant companies) / Google Deepmind,https://arxiv.org/abs/2510.23966,A Pragmatic Way to Measure Chain-of-Thought Monitorability,A Pragmatic Way to Measure Chain-of-Thought Monitorability,"Scott Emmons, Roland S. Zimmermann, David K. Elson, Rohin Shah",Google DeepMind,2025,2025-10-28
Anthropic,Labs (giant companies),,Labs (giant companies) / Anthropic,https://alignment.anthropic.com/2025/honesty-elicitation/,Evaluating honesty and lie detection techniques on a diverse suite of dishonest models,Evaluating honesty and lie detection techniques on a diverse suite of dishonest models,"Rowan Wang, Johannes Treutlein, Fabien Roger, Evan Hubinger, Sam Marks",Anthropic,2025,2025-11-25
Anthropic,Labs (giant companies),,Labs (giant companies) / Anthropic,https://anthropic.com/research/agentic-misalignment,Agentic Misalignment: How LLMs could be insider threats,Agentic Misalignment: How LLMs could be insider threats,"Aengus Lynch, Benjamin Wright, Caleb Larson, Kevin K. Troy, Stuart J. Ritchie, Sören Mindermann, Ethan Perez, Evan Hubinger","Anthropic, University College London, MATS, Mila",2025,2025-06-20
Anthropic,Labs (giant companies),,Labs (giant companies) / Anthropic,https://alignmentforum.org/posts/ghESoA8mo3fv9Yx3E/why-do-some-language-models-fake-alignment-while-others-don,Why Do Some Language Models Fake Alignment While Others Don't?,Why Do Some Language Models Fake Alignment While Others Don't?,"abhayesian, John Hughes, Alex Mallen, Jozdien, janus, Fabien Roger","Anthropic, Redwood Research",2025,2025-07-08
Anthropic,Labs (giant companies),,Labs (giant companies) / Anthropic,https://arxiv.org/abs/2502.16797,Forecasting Rare Language Model Behaviors,Forecasting Rare Language Model Behaviors,"Erik Jones, Meg Tong, Jesse Mu, Mohammed Mahfoud, Jan Leike, Roger Grosse, Jared Kaplan, William Fithian, Ethan Perez, Mrinank Sharma","Anthropic, OpenAI, UC Berkeley",2025,2025-02-24
Anthropic,Labs (giant companies),,Labs (giant companies) / Anthropic,https://alignment.anthropic.com/2025/openai-findings,Findings from a Pilot Anthropic—OpenAI Alignment Evaluation Exercise,Findings from a Pilot Anthropic—OpenAI Alignment Evaluation Exercise,"Samuel R. Bowman, Megha Srivastava, Jon Kutasov, Rowan Wang, Trenton Bricken, Benjamin Wright, Ethan Perez, Nicholas Carlini","Anthropic, OpenAI",2025,2025-08-27
Anthropic,Labs (giant companies),,Labs (giant companies) / Anthropic,https://transformer-circuits.pub/2025/attribution-graphs/biology.html,On the Biology of a Large Language Model,On the Biology of a Large Language Model,"Jack Lindsey, Wes Gurnee, Emmanuel Ameisen, Brian Chen, Adam Pearce, Nicholas L. Turner, Craig Citro, David Abrahams, Shan Carter, Basil Hosmer, Jonathan Marcus, Michael Sklar, Adly Templeton, Trenton Bricken, Callum McDougall, Hoagy Cunningham, Thomas Henighan, Adam Jermyn, Andy Jones, Andrew Persic, Zhenyi Qi, T. Ben Thompson, Sam Zimmerman, Kelley Rivoire, Thomas Conerly, Chris Olah, Joshua Batson",Anthropic,2025,2025-03-27
Anthropic,Labs (giant companies),,Labs (giant companies) / Anthropic,https://www.anthropic.com/research/auditing-hidden-objectives,Auditing language models for hidden objectives,Auditing language models for hidden objectives,,Anthropic,2025,2025-03-13
Anthropic,Labs (giant companies),,Labs (giant companies) / Anthropic,https://arxiv.org/abs/2510.07192,Poisoning Attacks on LLMs Require a Near-constant Number of Poison Samples,Poisoning Attacks on LLMs Require a Near-constant Number of Poison Samples,"Alexandra Souly, Javier Rando, Ed Chapman, Xander Davies, Burak Hasircioglu, Ezzeldin Shereen, Carlos Mougan, Vasilios Mavroudis, Erik Jones, Chris Hicks, Nicholas Carlini, Yarin Gal, Robert Kirk",,2025,2025-10-08
Anthropic,Labs (giant companies),,Labs (giant companies) / Anthropic,https://transformer-circuits.pub/2025/attribution-graphs/methods.html,Circuit Tracing: Revealing Computational Graphs in Language Models,Circuit Tracing: Revealing Computational Graphs in Language Models,"Emmanuel Ameisen, Jack Lindsey, Adam Pearce, Wes Gurnee, Nicholas L. Turner, Brian Chen, Craig Citro, David Abrahams, Shan Carter, Basil Hosmer, Jonathan Marcus, Michael Sklar, Adly Templeton, Trenton Bricken, Callum McDougall, Hoagy Cunningham, Thomas Henighan, Adam Jermyn, Andy Jones, Andrew Persic, Zhenyi Qi, T. Ben Thompson, Sam Zimmerman, Kelley Rivoire, Thomas Conerly, Chris Olah, Joshua Batson",Anthropic,2025,2025-03-27
Anthropic,Labs (giant companies),,Labs (giant companies) / Anthropic,https://anthropic.com/research/shade-arena-sabotage-monitoring,SHADE-Arena: Evaluating sabotage and monitoring in LLM agents,SHADE-Arena: Evaluating sabotage and monitoring in LLM agents,"Xiang Deng, Chen Bo Calvin Zhang, Tyler Tracy, Buck Shlegeris, Yuqi Sun, Paul Colognese, Teun van der Weij, Linda Petrini, Henry Sleight","Anthropic, Scale AI, Redwood Research",2025,2025-06-16
Anthropic,Labs (giant companies),,Labs (giant companies) / Anthropic,https://transformer-circuits.pub/2025/introspection/index.html,Emergent Introspective Awareness in Large Language Models,Emergent Introspective Awareness in Large Language Models,Jack Lindsey,Anthropic,2025,2025-10-29
Anthropic,Labs (giant companies),,Labs (giant companies) / Anthropic,https://www.anthropic.com/research/reasoning-models-dont-say-think,Reasoning models don't always say what they think,Reasoning models don't always say what they think,,Anthropic,2025,2025-04-03
Anthropic,Labs (giant companies),,Labs (giant companies) / Anthropic,https://alignment.anthropic.com/2025/petri,Petri: An open-source auditing tool to accelerate AI safety research,Petri: An open-source auditing tool to accelerate AI safety research,,Anthropic,2025,2025-10-06
Anthropic,Labs (giant companies),,Labs (giant companies) / Anthropic,https://anthropic.com/research/introspection,Signs of introspection in large language models,Signs of introspection in large language models,,Anthropic,2025,2025-10-29
Anthropic,Labs (giant companies),,Labs (giant companies) / Anthropic,https://alignment.anthropic.com/2025/bumpers/,Putting up Bumpers,Putting up Bumpers,,Anthropic,2025,
Anthropic,Labs (giant companies),,Labs (giant companies) / Anthropic,https://alignment.anthropic.com/2024/safety-cases/index.html,Three Sketches of ASL-4 Safety Case Components,Three Sketches of ASL-4 Safety Case Components,,Anthropic,2024,2024-01-01
Anthropic,Labs (giant companies),,Labs (giant companies) / Anthropic,https://alignment.anthropic.com/2025/recommended-directions/index.html,Recommendations for Technical AI Safety Research Directions,Recommendations for Technical AI Safety Research Directions,Anthropic Alignment Science Team,Anthropic,2025,
Anthropic,Labs (giant companies),,Labs (giant companies) / Anthropic,https://www.anthropic.com/research/constitutional-classifiers,Constitutional Classifiers: Defending against universal jailbreaks,Constitutional Classifiers: Defending against universal jailbreaks,Anthropic Safeguards Research Team,Anthropic,2025,2025-02-03
Anthropic,Labs (giant companies),,Labs (giant companies) / Anthropic,https://gist.github.com/Richard-Weiss/efe157692991535403bd7e7fb20b6695,The Soul Document,Claude 4.5 Opus Soul Document,Richard-Weiss,,2024,2024-11-27
Anthropic,Labs (giant companies),,Labs (giant companies) / Anthropic,https://anthropic.com/research/open-source-circuit-tracing,Open-sourcing circuit tracing tools,Open-sourcing circuit tracing tools,"Michael Hanna, Mateusz Piotrowski, Emmanuel Ameisen, Jack Lindsey, Johnny Lin, Curt Tigges","Anthropic, Decode Research",2025,2025-05-29
Anthropic,Labs (giant companies),,Labs (giant companies) / Anthropic,https://assets.anthropic.com/m/74342f2c96095771/original/Natural-emergent-misalignment-from-reward-hacking-paper.pdf,Natural emergent misalignment from reward hacking,Natural emergent misalignment from reward hacking,,Anthropic,,
Meta,Labs (giant companies),,Labs (giant companies) / Meta,https://arxiv.org/pdf/2510.08240,The Alignment Waltz: Jointly Training Agents to Collaborate for Safety,The Alignment Waltz: Jointly Training Agents to Collaborate for Safety,"Jingyu Zhang, Haozhu Wang, Eric Michael Smith, Sid Wang, Amr Sharaf, Mahesh Pasupuleti, Benjamin Van Durme, Daniel Khashabi, Jason Weston, Hongyuan Zhan","Meta, Johns Hopkins University",2025,2025-10-09
Meta,Labs (giant companies),,Labs (giant companies) / Meta,https://arxiv.org/pdf/2510.00938%20,Large Reasoning Models Learn Better Alignment from Flawed Thinking,Large Reasoning Models Learn Better Alignment from Flawed Thinking,"ShengYun Peng, Eric Smith, Ivan Evtimov, Song Jiang, Pin-Yu Chen, Hongyuan Zhan, Haozhu Wang, Duen Horng Chau, Mahesh Pasupuleti, Jianfeng Chi",,2025,2025-10-01
Meta,Labs (giant companies),,Labs (giant companies) / Meta,https://arxiv.org/pdf/2409.20089,Robust LLM safeguarding via refusal feature adversarial training,Robust LLM safeguarding via refusal feature adversarial training,"Lei Yu, Virginie Do, Karen Hambardzumyan, Nicola Cancedda",,2024,2024-09-30
Meta,Labs (giant companies),,Labs (giant companies) / Meta,https://scontent-lhr8-1.xx.fbcdn.net/v/t39.2365-6/557601942_1468972530985309_838842257265552803_n.pdf?_nc_cat=108&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=_H33_VKF3ZUQ7kNvwFog8dd&_nc_oc=AdlNtqCDY4HafZ3-d5rb26AF5f2m0X46SGdKhVq3jLqwpNf_wEXhdQnH7_30ychiZWk&_nc_zt=14&_nc_ht=scontent-lhr8-1.xx&_nc_gid=QvW_ePiaF4E-PxOf30MWyg&oh=00_AfiZC5G4ODvWhiy0MuVH8PSlUFrW8RDQQ8tdr6Zec5k9aA&oe=691A6D09,Code World Model Preparedness Report,Unable to access - URL signature expired,,,,
Meta,Labs (giant companies),,Labs (giant companies) / Meta,https://ai.meta.com/blog/practical-ai-agent-security/%20,Agents Rule of Two: A Practical Approach to AI Agent Security,Connect 2024: The responsible approach we're taking to generative AI,,Meta,2024,2024-09-25
Meta,Labs (giant companies),,Labs (giant companies) / Meta,https://github.com/facebookresearch/RAM/blob/main/projects/co-improvement.pdf,AI & Human Co-Improvement,Co-improvement (PDF - content not accessible),,"Meta, Facebook Research",2025,2025-12-05
Iterative alignment at pretrain-time,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Iterative alignment at pretrain-time,https://alignment.anthropic.com/2025/unsupervised-elicitation,Unsupervised Elicitation,Unsupervised Elicitation,"Jiaxin Wen, Zachary Ankner, Arushi Somani, Peter Hase, Samuel Marks, Jacob Goldman-Wetzler, Linda Petrini, Henry Sleight, Collin Burns, He He, Shi Feng, Ethan Perez, Jan Leike","Anthropic, Schmidt Sciences, Independent, Constellation, New York University, George Washington University",2025,
Iterative alignment at pretrain-time,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Iterative alignment at pretrain-time,https://arxiv.org/abs/2509.07955,ACE and Diverse Generalization via Selective Disagreement,ACE and Diverse Generalization via Selective Disagreement,"Oliver Daniels, Stuart Armstrong, Alexandre Maranhão, Mahirah Fairuz Rahman, Benjamin M. Marlin, Rebecca Gorman",Unknown - not specified in abstract,2025,2025-09-09
Iterative alignment at post-train-time,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Iterative alignment at post-train-time,https://arxiv.org/abs/2407.06483,Composable Interventions for Language Models,Composable Interventions for Language Models,"Arinbjorn Kolbeinsson, Kyle O'Brien, Tianjin Huang, Shanghua Gao, Shiwei Liu, Jonathan Richard Schwarz, Anurag Vaidya, Faisal Mahmood, Marinka Zitnik, Tianlong Chen, Thomas Hartvigsen",Various Universities,2024,2024-07-09
Iterative alignment at post-train-time,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Iterative alignment at post-train-time,https://arxiv.org/abs/2511.06626,Spilling the Beans: Teaching LLMs to Self-Report Their Hidden Objectives,Spilling the Beans: Teaching LLMs to Self-Report Their Hidden Objectives,"Chloe Li, Mary Phuong, Daniel Tan",,2025,2025-11-10
Iterative alignment at post-train-time,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Iterative alignment at post-train-time,https://arxiv.org/abs/2411.02306,On Targeted Manipulation and Deception when Optimizing LLMs for User Feedback,On Targeted Manipulation and Deception when Optimizing LLMs for User Feedback,"Marcus Williams, Micah Carroll, Adhyyan Narang, Constantin Weisser, Brendan Murphy, Anca Dragan",,2024,2024-11-04
Iterative alignment at post-train-time,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Iterative alignment at post-train-time,https://arxiv.org/abs/2505.13787,Preference Learning with Lie Detectors can Induce Honesty or Evasion,Preference Learning with Lie Detectors can Induce Honesty or Evasion,"Chris Cundy, Adam Gleave",,2025,2025-05-20
Iterative alignment at post-train-time,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Iterative alignment at post-train-time,https://arxiv.org/abs/2502.01930,Robust LLM Alignment via Distributionally Robust Direct Preference Optimization,Robust LLM Alignment via Distributionally Robust Direct Preference Optimization,"Zaiyan Xu, Sushil Vemuri, Kishan Panaganti, Dileep Kalathil, Rahul Jain, Deepak Ramachandran",,2025,2025-02-04
Iterative alignment at post-train-time,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Iterative alignment at post-train-time,https://arxiv.org/abs/2501.08617,RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation,RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation,"Kaiqu Liang, Haimin Hu, Ryan Liu, Thomas L. Griffiths, Jaime Fernández Fisac",Princeton University,2025,2025-01-15
Iterative alignment at post-train-time,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Iterative alignment at post-train-time,https://arxiv.org/abs/2510.21184,Reducing the Probability of Undesirable Outputs in Language Models Using Probabilistic Inference,Reducing the Probability of Undesirable Outputs in Language Models Using Probabilistic Inference,"Stephen Zhao, Aidan Li, Rob Brekelmans, Roger Grosse",,2025,2025-10-24
Iterative alignment at post-train-time,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Iterative alignment at post-train-time,https://arxiv.org/abs/2501.07886,Iterative Label Refinement Matters More than Preference Optimization under Weak Supervision,Iterative Label Refinement Matters More than Preference Optimization under Weak Supervision,"Yaowen Ye, Cassidy Laidlaw, Jacob Steinhardt",UC Berkeley,2025,2025-01-14
Iterative alignment at post-train-time,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Iterative alignment at post-train-time,https://arxiv.org/abs/2510.27062,Consistency Training Helps Stop Sycophancy and Jailbreaks,Consistency Training Helps Stop Sycophancy and Jailbreaks,"Alex Irpan, Alexander Matt Turner, Mark Kurzeja, David K. Elson, Rohin Shah","DeepMind, Google",2025,2025-10-31
Iterative alignment at post-train-time,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Iterative alignment at post-train-time,https://arxiv.org/abs/2508.12531,Rethinking Safety in LLM Fine-tuning: An Optimization Perspective,Rethinking Safety in LLM Fine-tuning: An Optimization Perspective,"Minseon Kim, Jin Myung Kwak, Lama Alssum, Bernard Ghanem, Philip Torr, David Krueger, Fazl Barez, Adel Bibi",,2025,2025-08-17
Iterative alignment at post-train-time,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Iterative alignment at post-train-time,https://arxiv.org/abs/2506.05967,Preference Learning for AI Alignment: a Causal Perspective,Preference Learning for AI Alignment: a Causal Perspective,"Katarzyna Kobalczyk, Mihaela van der Schaar",,2025,2025-06-06
Iterative alignment at post-train-time,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Iterative alignment at post-train-time,https://arxiv.org/abs/2506.08998,On Monotonicity in AI Alignment,On Monotonicity in AI Alignment,"Gilles Bareilles, Julien Fageot, Lê-Nguyên Hoang, Peva Blanchard, Wassim Bouaziz, Sébastien Rouault, El-Mahdi El-Mhamdi",,2025,2025-06-10
Iterative alignment at post-train-time,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Iterative alignment at post-train-time,https://arxiv.org/abs/2510.06084,Spectrum Tuning: Post-Training for Distributional Coverage and In-Context Steerability,Spectrum Tuning: Post-Training for Distributional Coverage and In-Context Steerability,"Taylor Sorensen, Benjamin Newman, Jared Moore, Chan Park, Jillian Fisher, Niloofar Mireshghallah, Liwei Jiang, Yejin Choi","University of Washington, Allen Institute for AI",2025,2025-10-07
Iterative alignment at post-train-time,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Iterative alignment at post-train-time,https://arxiv.org/abs/2502.11250,Uncertainty-Aware Step-wise Verification with Generative Reward Models,Uncertainty-Aware Step-wise Verification with Generative Reward Models,"Zihuiwen Ye, Luckeciano Carvalho Melo, Younesse Kaddar, Phil Blunsom, Sam Staton, Yarin Gal",Oxford University,2025,2025-02-16
Iterative alignment at post-train-time,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Iterative alignment at post-train-time,https://arxiv.org/abs/2507.06187,The Delta Learning Hypothesis: Preference Tuning on Weak Data can Yield Strong Gains,The Delta Learning Hypothesis: Preference Tuning on Weak Data can Yield Strong Gains,"Scott Geng, Hamish Ivison, Chun-Liang Li, Maarten Sap, Jerry Li, Ranjay Krishna, Pang Wei Koh",,2025,2025-07-08
Iterative alignment at post-train-time,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Iterative alignment at post-train-time,https://arxiv.org/pdf/2512.08093,Training LLMs for Honesty via Confessions,Training LLMs for Honesty via Confessions,"Manas Joglekar, Jeremy Chen, Gabriel Wu, Jason Yosinski, Jasmine Wang, Boaz Barak, Amelia Glaese","OpenAI, Google DeepMind",2025,2025-12-08
Black-box make-AI-solve-it,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Black-box make-AI-solve-it,https://neural-interactive-proofs.com/,Neural Interactive Proofs,Neural Interactive Proofs,"Lewis Hammond, Sam Adam-Day",University of Oxford,2024,2024-12-08
Black-box make-AI-solve-it,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Black-box make-AI-solve-it,https://arxiv.org/abs/2501.13011,MONA: Myopic Optimization with Non-myopic Approval Can Mitigate Multi-step Reward Hacking,MONA: Myopic Optimization with Non-myopic Approval Can Mitigate Multi-step Reward Hacking,"Sebastian Farquhar, Vikrant Varma, David Lindner, David Elson, Caleb Biddulph, Ian Goodfellow, Rohin Shah",Google DeepMind,2025,2025-01-22
Black-box make-AI-solve-it,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Black-box make-AI-solve-it,https://lesswrong.com/posts/8XHBaugB5S3r27MG9/prover-estimator-debate-a-new-scalable-oversight-protocol,Prover-Estimator Debate: A New Scalable Oversight Protocol,Prover-Estimator Debate: A New Scalable Oversight Protocol,"Jonah Brown-Cohen, Geoffrey Irving",UK AISI,2025,2025-06-17
Black-box make-AI-solve-it,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Black-box make-AI-solve-it,https://openreview.net/forum?id=N1vYivuSKq,Weak to Strong Generalization for Large Language Models with Multi-capabilities,Weak to Strong Generalization for Large Language Models with Multi-capabilities,"Yucheng Zhou, Jianbing Shen, Yu Cheng",,2025,2025-01-22
Black-box make-AI-solve-it,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Black-box make-AI-solve-it,https://arxiv.org/abs/2501.13124,Debate Helps Weak-to-Strong Generalization,Debate Helps Weak-to-Strong Generalization,"Hao Lang, Fei Huang, Yongbin Li",,2025,2025-01-21
Black-box make-AI-solve-it,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Black-box make-AI-solve-it,https://arxiv.org/abs/2504.08812,"Mechanistic Anomaly Detection for ""Quirky"" Language Models","Mechanistic Anomaly Detection for ""Quirky"" Language Models","David O. Johnston, Arkajyoti Chakraborty, Nora Belrose",FAR AI,2025,2025-04-09
Black-box make-AI-solve-it,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Black-box make-AI-solve-it,https://arxiv.org/abs/2506.02175,AI Debate Aids Assessment of Controversial Claims,AI Debate Aids Assessment of Controversial Claims,"Salman Rahman, Sheriff Issaka, Ashima Suvarna, Genglin Liu, James Shiffer, Jaeyoung Lee, Md Rizwan Parvez, Hamid Palangi, Shi Feng, Nanyun Peng, Yejin Choi, Julian Michael, Liwei Jiang, Saadia Gabriel","University of Washington, Microsoft Research, UCLA, NYU",2025,2025-06-02
Black-box make-AI-solve-it,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Black-box make-AI-solve-it,https://arxiv.org/abs/2505.03989,An alignment safety case sketch based on debate,An alignment safety case sketch based on debate,"Marie Davidsen Buhl, Jacob Pfau, Benjamin Hilton, Geoffrey Irving",Google DeepMind,2025,2025-05-23
Black-box make-AI-solve-it,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Black-box make-AI-solve-it,https://arxiv.org/abs/2509.00091,Ensemble Debates with Local Large Language Models for AI Alignment,Ensemble Debates with Local Large Language Models for AI Alignment,Ephraiem Sarabamoun,,2025,2025-08-27
Black-box make-AI-solve-it,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Black-box make-AI-solve-it,https://lesswrong.com/posts/5gmALpCetyjkSPEDr/training-ai-to-do-alignment-research-we-don-t-already-know,Training AI to do alignment research we don't already know how to do,Training AI to do alignment research we don't already know how to do,joshc,Redwood Research,2025,2025-02-24
Black-box make-AI-solve-it,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Black-box make-AI-solve-it,https://lesswrong.com/posts/FqpAPC48CzAtvfx5C/automating-ai-safety-what-we-can-do-today,Automating AI Safety: What we can do today,Automating AI Safety: What we can do today,"Matthew Shinkle, Eyon Jang, Jacques Thibodeau","SPAR, PIBBSS",2025,2025-07-25
Black-box make-AI-solve-it,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Black-box make-AI-solve-it,https://arxiv.org/abs/2503.13621,Superalignment with Dynamic Human Values,Superalignment with Dynamic Human Values,"Florian Mai, David Kaczér, Nicholas Kluge Corrêa, Lucie Flek",,2025,2025-03-17
Inoculation prompting,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Inoculation prompting,https://www.alignmentforum.org/posts/whkMnqFWKsBm7Gyd7/recontextualization-mitigates-specification-gaming-without,Recontextualization Mitigates Specification Gaming Without Modifying the Specification,Recontextualization Mitigates Specification Gaming Without Modifying the Specification,"Ariana Azarbal, Victor Gillioz, Alexander Matt Turner, Alex Cloud",MATS Program,2025,2025-10-14
Inoculation prompting,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Inoculation prompting,https://arxiv.org/abs/2510.04340,Inoculation Prompting: Eliciting traits from LLMs during training can suppress them at test-time,Inoculation Prompting: Eliciting traits from LLMs during training can suppress them at test-time,"Daniel Tan, Anders Woodruff, Niels Warncke, Arun Jose, Maxime Riché, David Demitri Africa, Mia Taylor",,2025,2025-10-05
Inoculation prompting,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Inoculation prompting,https://arxiv.org/abs/2510.05024,Inoculation Prompting: Instructing LLMs to misbehave at train-time improves test-time alignment,Inoculation Prompting: Instructing LLMs to misbehave at train-time improves test-time alignment,"Nevan Wichers, Aram Ebtekar, Ariana Azarbal, Victor Gillioz, Christine Ye, Emil Ryd, Neil Rathi, Henry Sleight, Alex Mallen, Fabien Roger, Samuel Marks",,2025,2025-10-27
Inoculation prompting,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Inoculation prompting,https://assets.anthropic.com/m/74342f2c96095771/original/Natural-emergent-misalignment-from-reward-hacking-paper.pdf,Natural Emergent Misalignment from Reward Hacking,Natural emergent misalignment from reward hacking,,Anthropic,,
Inference-time: In-context learning,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Inference-time: In-context learning,https://arxiv.org/abs/2510.01569,InvThink: Towards AI Safety via Inverse Reasoning,InvThink: Towards AI Safety via Inverse Reasoning,"Yubin Kim, Taehan Kim, Eugene Park, Chunjong Park, Cynthia Breazeal, Daniel McDuff, Hae Won Park",,2025,2025-10-02
Inference-time: In-context learning,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Inference-time: In-context learning,https://arxiv.org/abs/2506.19248,Inference-Time Reward Hacking in Large Language Models,Inference-Time Reward Hacking in Large Language Models,"Hadi Khalaf, Claudio Mayrink Verdun, Alex Oesterling, Himabindu Lakkaraju, Flavio du Pin Calmon",,2025,2025-06-24
Inference-time: In-context learning,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Inference-time: In-context learning,https://arxiv.org/abs/2505.05145,Understanding In-context Learning of Addition via Activation Subspaces,Understanding In-context Learning of Addition via Activation Subspaces,"Xinyan Hu, Kayo Yin, Michael I. Jordan, Jacob Steinhardt, Lijie Chen",UC Berkeley,2025,2025-05-08
Inference-time: In-context learning,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Inference-time: In-context learning,https://arxiv.org/abs/2510.06182,Mixing Mechanisms: How Language Models Retrieve Bound Entities In-Context,Mixing Mechanisms: How Language Models Retrieve Bound Entities In-Context,"Yoav Gur-Arieh, Mor Geva, Atticus Geiger",,2025,2025-10-07
Inference-time: In-context learning,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Inference-time: In-context learning,https://arxiv.org/abs/2502.14010,Which Attention Heads Matter for In-Context Learning?,Which Attention Heads Matter for In-Context Learning?,"Kayo Yin, Jacob Steinhardt",UC Berkeley,2025,2025-02-19
Inference-time: Steering,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Inference-time: Steering,https://arxiv.org/abs/2511.05408,Steering Language Models with Weight Arithmetic,Steering Language Models with Weight Arithmetic,"Constanza Fierro, Fabien Roger",,2025,2025-11-07
Inference-time: Steering,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Inference-time: Steering,https://arxiv.org/abs/2510.06370,EVALUESTEER: Measuring Reward Model Steerability Towards Values and Preferences,EVALUESTEER: Measuring Reward Model Steerability Towards Values and Preferences,"Kshitish Ghate, Andy Liu, Devansh Jain, Taylor Sorensen, Atoosa Kasirzadeh, Aylin Caliskan, Mona T. Diab, Maarten Sap","University of Washington, Carnegie Mellon University",2025,2025-10-07
Inference-time: Steering,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Inference-time: Steering,https://arxiv.org/abs/2502.00580,Defense Against the Dark Prompts: Mitigating Best-of-N Jailbreaking with Prompt Evaluation,Defense Against the Dark Prompts: Mitigating Best-of-N Jailbreaking with Prompt Evaluation,"Stuart Armstrong, Matija Franklin, Connor Stevens, Rebecca Gorman",,2025,2025-02-01
Inference-time: Steering,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Inference-time: Steering,https://arxiv.org/abs/2510.13285,In-Distribution Steering: Balancing Control and Coherence in Language Model Generation.,In-Distribution Steering: Balancing Control and Coherence in Language Model Generation,"Arthur Vogels, Benjamin Wong, Yann Choho, Annabelle Blangero, Milan Bhan",,2025,2025-10-15
Capability removal: unlearning,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Capability removal: unlearning,https://github.com/locuslab/open-unlearning,OpenUnlearning,OpenUnlearning: Accelerating LLM Unlearning via Unified Benchmarking of Methods and Metrics,"Vineeth Dorna, Anmol Mekala, Wenlong Zhao, Andrew McCallum, Zachary C Lipton, J Zico Kolter, Pratyush Maini","Carnegie Mellon University, University of Massachusetts Amherst",2025,2025-06-20
Capability removal: unlearning,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Capability removal: unlearning,https://alignment.anthropic.com/2025/modifying-beliefs-via-sdf,Modifying LLM Beliefs with Synthetic Document Finetuning,Modifying LLM Beliefs with Synthetic Document Finetuning,"Rowan Wang, Avery Griffin, Johannes Treutlein, Ethan Perez, Julian Michael, Fabien Roger, Sam Marks","Anthropic, MATS, Scale AI",2025,2025-04-24
Capability removal: unlearning,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Capability removal: unlearning,https://arxiv.org/abs/2505.22310,From Dormant to Deleted: Tamper-Resistant Unlearning Through Weight-Space Regularization,From Dormant to Deleted: Tamper-Resistant Unlearning Through Weight-Space Regularization,"Shoaib Ahmed Siddiqui, Adrian Weller, David Krueger, Gintare Karolina Dziugaite, Michael Curtis Mozer, Eleni Triantafillou","University of Cambridge, Google DeepMind, Vector Institute",2025,2025-05-28
Capability removal: unlearning,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Capability removal: unlearning,https://arxiv.org/abs/2505.08138,"Mirror Mirror on the Wall, Have I Forgotten it All?","Mirror Mirror on the Wall, Have I Forgotten it All? A New Framework for Evaluating Machine Unlearning","Brennon Brimhall, Philip Mathew, Neil Fendley, Yinzhi Cao, Matthew Green",,2025,2025-05-13
Capability removal: unlearning,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Capability removal: unlearning,https://arxiv.org/abs/2412.06966,Machine Unlearning Doesn't Do What You Think: Lessons for Generative AI Policy and Research,Machine Unlearning Doesn't Do What You Think: Lessons for Generative AI Policy and Research,"A. Feder Cooper, Christopher A. Choquette-Choo, Miranda Bogen, Kevin Klyman, Matthew Jagielski, Katja Filippova, Ken Liu, Alexandra Chouldechova, Jamie Hayes, Yangsibo Huang, Eleni Triantafillou, Peter Kairouz, Nicole Elyse Mitchell, Niloofar Mireshghallah, Abigail Z. Jacobs, James Grimmelmann, Vitaly Shmatikov, Christopher De Sa, Ilia Shumailov, Andreas Terzis, Solon Barocas, Jennifer Wortman Vaughan, Danah Boyd, Yejin Choi, Sanmi Koyejo, Fernando Delgado, Percy Liang, Daniel E. Ho, Pamela Samuelson, Miles Brundage, David Bau, Seth Neel, Hanna Wallach, Amy B. Cyphert, Mark A. Lemley, Nicolas Papernot, Katherine Lee","Anthropic, Google, Stanford University, Cornell University, University of Washington, Various Universities and Research Institutions",2024,2024-12-09
Capability removal: unlearning,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Capability removal: unlearning,https://arxiv.org/abs/2501.04952,Open Problems in Machine Unlearning for AI Safety,Open Problems in Machine Unlearning for AI Safety,"Fazl Barez, Tingchen Fu, Ameya Prabhu, Stephen Casper, Amartya Sanyal, Adel Bibi, Aidan O'Gara, Robert Kirk, Ben Bucknall, Tim Fist, Luke Ong, Philip Torr, Kwok-Yan Lam, Robert Trager, David Krueger, Sören Mindermann, José Hernandez-Orallo, Mor Geva, Yarin Gal","University of Oxford, MIT, University of Cambridge, Nanyang Technological University, Tel Aviv University",2025,2025-01-09
Capability removal: unlearning,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Capability removal: unlearning,https://arxiv.org/abs/2509.11816,Collapse of Irrelevant Representations (CIR) Ensures Robust and Non-Disruptive LLM Unlearning,Collapse of Irrelevant Representations (CIR) Ensures Robust and Non-Disruptive LLM Unlearning,"Filip Sondej, Yushi Yang",,2025,2025-09-15
Capability removal: unlearning,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Capability removal: unlearning,https://arxiv.org/abs/2505.18588,Safety Alignment via Constrained Knowledge Unlearning,Safety Alignment via Constrained Knowledge Unlearning,"Zesheng Shi, Yucheng Zhou, Jing Li",,2025,2025-05-24
Capability removal: unlearning,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Capability removal: unlearning,https://arxiv.org/abs/2506.12484,Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption Masking And Normalization,Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption Masking And Normalization,"Filip Sondej, Yushi Yang, Mikołaj Kniejski, Marcel Windys",,2025,2025-06-14
Capability removal: unlearning,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Capability removal: unlearning,https://arxiv.org/abs/2505.16831,Unlearning Isn't Deletion: Investigating Reversibility of Machine Unlearning in LLMs,Unlearning Isn't Deletion: Investigating Reversibility of Machine Unlearning in LLMs,"Xiaoyu Xu, Xiang Yue, Yang Liu, Qingqing Ye, Huadi Zheng, Peizhao Hu, Minxin Du, Haibo Hu",,2025,2025-05-22
Capability removal: unlearning,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Capability removal: unlearning,https://lesswrong.com/posts/QYzofMbzmbgiwfqy8/unlearning-needs-to-be-more-selective-progress-report,Unlearning Needs to be More Selective [Progress Report],Unlearning Needs to be More Selective [Progress Report],"Filip Sondej, Yushi Yang, Marcel Windys",,2025,2025-06-27
Capability removal: unlearning,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Capability removal: unlearning,https://arxiv.org/abs/2505.09500,Layered Unlearning for Adversarial Relearning,Layered Unlearning for Adversarial Relearning,"Timothy Qian, Vinith Suriyakumar, Ashia Wilson, Dylan Hadfield-Menell",,2025,2025-05-14
Capability removal: unlearning,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Capability removal: unlearning,https://goodfire.ai/research/understanding-memorization-via-loss-curvature,Understanding Memorization via Loss Curvature,Understanding Memorization via Loss Curvature,"Jack Merullo, Srihita Vatsavaya, Lucius Bushnaq, Owen Lewis",Goodfire,2025,2025-11-06
Capability removal: unlearning,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Capability removal: unlearning,https://arxiv.org/abs/2502.05209,Model Tampering Attacks Enable More Rigorous Evaluations of LLM Capabilities,Model Tampering Attacks Enable More Rigorous Evaluations of LLM Capabilities,"Zora Che, Stephen Casper, Robert Kirk, Anirudh Satheesh, Stewart Slocum, Lev E McKinney, Rohit Gandikota, Aidan Ewart, Domenic Rosati, Zichu Wu, Zikui Cai, Bilal Chughtai, Yarin Gal, Furong Huang, Dylan Hadfield-Menell",,2025,2025-02-03
Capability removal: unlearning,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Capability removal: unlearning,https://arxiv.org/abs/2410.04332,Gradient Routing: Masking Gradients to Localize Computation in Neural Networks,Gradient Routing: Masking Gradients to Localize Computation in Neural Networks,"Alex Cloud, Jacob Goldman-Wetzler, Evžen Wybitul, Joseph Miller, Alexander Matt Turner",,2024,2024-10-06
Capability removal: unlearning,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Capability removal: unlearning,https://www.lesswrong.com/posts/tAnHM3L25LwuASdpF/selective-modularity-a-research-agenda,Selective modularity: a research agenda,Selective modularity: a research agenda,"cloud, Jacob G-W",MATS,2025,2025-03-24
Capability removal: unlearning,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Capability removal: unlearning,https://arxiv.org/abs/2506.06278,Distillation Robustifies Unlearning,Distillation Robustifies Unlearning,"Bruce W. Lee, Addie Foote, Alex Infanger, Leni Shor, Harish Kamath, Jacob Goldman-Wetzler, Bryce Woodworth, Alex Cloud, Alexander Matt Turner",,2025,2025-06-06
Capability removal: unlearning,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Capability removal: unlearning,https://www.arxiv.org/abs/2512.05648,Beyond Data Filtering: Knowledge Localization for Capability Removal in LLMs,Beyond Data Filtering: Knowledge Localization for Capability Removal in LLMs,"Igor Shilov, Alex Cloud, Aryo Pradipta Gema, Jacob Goldman-Wetzler, Nina Panickssery, Henry Sleight, Erik Jones, Cem Anil",Anthropic,2024,2024-12-05
Control,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Control,https://luthienresearch.org/updates/2025-03-redteam-as-upsampling/,Luthien's Approach to Prosaic AI Control in 21 Points,Luthien's Approach to Prosaic AI Control in 21 Points,,Luthien Research,2025,2025-03-17
Control,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Control,https://alignmentforum.org/posts/LPHMMMZFAWog6ty5x/ctrl-z-controlling-ai-agents-via-resampling,Ctrl-Z: Controlling AI Agents via Resampling,Ctrl-Z: Controlling AI Agents via Resampling,"Aryan Bhatt, Buck Shlegeris, Adam Kaufman, Cody Rushing, Tyler Tracy, Vasil Georgiev, David Matolcsi, Akbir Khan",Redwood Research,2025,2025-04-16
Control,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Control,https://arxiv.org/abs/2506.15740,SHADE-Arena: Evaluating Sabotage and Monitoring in LLM Agents,SHADE-Arena: Evaluating Sabotage and Monitoring in LLM Agents,"Jonathan Kutasov, Yuqi Sun, Paul Colognese, Teun van der Weij, Linda Petrini, Chen Bo Calvin Zhang, John Hughes, Xiang Deng, Henry Sleight, Tyler Tracy, Buck Shlegeris, Joe Benton",Redwood Research,2025,2025-06-17
Control,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Control,https://arxiv.org/abs/2411.17693,Adaptive Deployment of Untrusted LLMs Reduces Distributed Threats,Adaptive Deployment of Untrusted LLMs Reduces Distributed Threats,"Jiaxin Wen, Vivek Hebbar, Caleb Larson, Aryan Bhatt, Ansh Radhakrishnan, Mrinank Sharma, Henry Sleight, Shi Feng, He He, Ethan Perez, Buck Shlegeris, Akbir Khan",,2024,2024-11-26
Control,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Control,https://arxiv.org/abs/2509.17938,D-REX: A Benchmark for Detecting Deceptive Reasoning in Large Language Models,D-REX: A Benchmark for Detecting Deceptive Reasoning in Large Language Models,"Satyapriya Krishna, Andy Zou, Rahul Gupta, Eliot Krzysztof Jones, Nick Winter, Dan Hendrycks, J. Zico Kolter, Matt Fredrikson, Spyros Matsoukas","Carnegie Mellon University, Center for AI Safety, Anthropic",2025,2025-09-22
Control,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Control,https://arxiv.org/abs/2412.12480,Subversion Strategy Eval: Can language models statelessly strategize to subvert control protocols?,Subversion Strategy Eval: Can language models statelessly strategize to subvert control protocols?,"Alex Mallen, Charlie Griffin, Misha Wagner, Alessandro Abate, Buck Shlegeris",Redwood Research,2024,2024-12-17
Control,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Control,https://arxiv.org/abs/2511.02997,Evaluating Control Protocols for Untrusted AI Agents,Evaluating Control Protocols for Untrusted AI Agents,"Jon Kutasov, Chloe Loughridge, Yuqi Sun, Henry Sleight, Buck Shlegeris, Tyler Tracy, Joe Benton",Redwood Research,2025,2025-11-04
Control,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Control,https://arxiv.org/abs/2510.19851,Can Reasoning Models Obfuscate Reasoning? Stress-Testing Chain-of-Thought Monitorability,Can Reasoning Models Obfuscate Reasoning? Stress-Testing Chain-of-Thought Monitorability,"Artur Zolkowski, Wen Xing, David Lindner, Florian Tramèr, Erik Jenner","ETH Zurich, FAR AI",2025,2025-10-21
Control,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Control,https://arxiv.org/abs/2511.02823,Optimizing AI Agent Attacks With Synthetic Data,Optimizing AI Agent Attacks With Synthetic Data,"Chloe Loughridge, Paul Colognese, Avery Griffin, Tyler Tracy, Jon Kutasov, Joe Benton",,2025,2025-11-04
Control,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Control,https://openreview.net/forum?id=QWopGahUEL,Games for AI Control,Games for AI Control: Models of Safety Evaluations of AI Deployment Protocols,"Charlie Griffin, Louis Thomson, Buck Shlegeris, Alessandro Abate","Redwood Research, University of Oxford",2025,2025-09-20
Control,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Control,https://arxiv.org/abs/2501.17315,A sketch of an AI control safety case,A sketch of an AI control safety case,"Tomek Korbak, Joshua Clymer, Benjamin Hilton, Buck Shlegeris, Geoffrey Irving",Redwood Research,2025,2025-01-28
Control,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Control,https://arxiv.org/abs/2502.05791,Assessing confidence in frontier AI safety cases,Assessing confidence in frontier AI safety cases,"Stephen Barrett, Philip Fox, Joshua Krook, Tuneer Mondal, Simon Mylius, Alejandro Tlaie",,2025,2025-02-09
Control,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Control,https://control-arena.aisi.org.uk/,ControlArena,ControlArena,"Rogan Inglis, Ollie Matthews, Tyler Tracy, Oliver Makins, Tom Catling, Asa Cooper Stickland, Rasmus Faber-Espensen, Daniel O'Connell, Myles Heller, Miguel Brandao, Adam Hanson, Arathi Mani, Tomek Korbak, Jan Michelfeit, Dishank Bansal, Tomas Bark, Chris Canal, Charlie Griffin, Jasmine Wang, Alan Cooney","UK AI Security Institute, Redwood Research",2025,2025-01-01
Control,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Control,https://arxiv.org/abs/2504.05259,How to evaluate control measures for LLM agents? A trajectory from today to superintelligence,How to evaluate control measures for LLM agents? A trajectory from today to superintelligence,"Tomek Korbak, Mikita Balesni, Buck Shlegeris, Geoffrey Irving","Redwood Research, Google DeepMind",2025,2025-04-07
Control,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Control,https://lesswrong.com/posts/wKTwdgZDo479EhmJL/the-alignment-project-by-uk-aisi-1,The Alignment Project by UK AISI,The Alignment Project by UK AISI,"Mojmir, Benjamin Hilton, Jacob Pfau, Geoffrey Irving, Joseph Bloom, Tomek Korbak, David Africa, Edmund Lau",UK AI Security Institute,2025,2025-08-01
Control,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Control,https://arxiv.org/abs/2411.03336,Towards evaluations-based safety cases for AI scheming,Towards evaluations-based safety cases for AI scheming,"Mikita Balesni, Marius Hobbhahn, David Lindner, Alexander Meinke, Tomek Korbak, Joshua Clymer, Buck Shlegeris, Jérémy Scheurer, Charlotte Stix, Rusheb Shah, Nicholas Goldowsky-Dill, Dan Braun, Bilal Chughtai, Owain Evans, Daniel Kokotajlo, Lucius Bushnaq","Redwood Research, Apollo Research, Anthropic",2024,2024-11-07
Control,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Control,https://arxiv.org/abs/2001.07118,"Incentives for Responsiveness, Instrumental Control and Impact","Incentives for Responsiveness, Instrumental Control and Impact","Ryan Carey, Eric Langlois, Chris van Merwijk, Shane Legg, Tom Everitt",DeepMind,2020,2020-01-20
Control,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Control,https://lesswrong.com/posts/neTbrpBziAsTH5Bn7/ai-companies-are-unlikely-to-make-high-assurance-safety,AI companies are unlikely to make high-assurance safety cases if timelines are short,AI companies are unlikely to make high-assurance safety cases if timelines are short,Ryan Greenblatt,Anthropic,2025,2025-01-23
Control,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Control,https://arxiv.org/abs/2507.12872,Manipulation Attacks by Misaligned AI: Risk Analysis and Safety Case Framework,Manipulation Attacks by Misaligned AI: Risk Analysis and Safety Case Framework,"Rishane Dassanayake, Mario Demetroudi, James Walpole, Lindley Lentati, Jason R. Brown, Edward James Young",,2025,2025-07-17
Control,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Control,https://arxiv.org/abs/2412.17618,Dynamic safety cases for frontier AI,Dynamic safety cases for frontier AI,"Carmen Cârlan, Francesca Gomez, Yohan Mathew, Ketana Krishna, René King, Peter Gebauer, Ben R. Smith",,2024,2024-12-23
Control,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Control,https://lesswrong.com/posts/cJQZAueoPC6aTncKK/ais-at-the-current-capability-level-may-be-important-for,AIs at the current capability level may be important for future safety work,AIs at the current capability level may be important for future safety work,Ryan Greenblatt,Anthropic,2025,2025-05-12
Control,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Control,https://lesswrong.com/posts/y6rBarAPTLmuhn9PJ/takeaways-from-sketching-a-control-safety-case,Takeaways from sketching a control safety case,Takeaways from sketching a control safety case,"Josh Clymer, Buck Shlegeris","Redwood Research, UK AISI",2025,2025-01-31
Safeguards (inference-time auxiliaries),Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Safeguards (inference-time auxiliaries),https://arxiv.org/abs/2501.18837,Constitutional Classifiers: Defending against Universal Jailbreaks across Thousands of Hours of Red Teaming,Constitutional Classifiers: Defending against Universal Jailbreaks across Thousands of Hours of Red Teaming,"Mrinank Sharma, Meg Tong, Jesse Mu, Jerry Wei, Jorrit Kruthoff, Scott Goodfriend, Euan Ong, Alwin Peng, Raj Agarwal, Cem Anil, Amanda Askell, Nathan Bailey, Joe Benton, Emma Bluemke, Samuel R. Bowman, Eric Christiansen, Hoagy Cunningham, Andy Dau, Anjali Gopal, Rob Gilson, Logan Graham, Logan Howard, Nimit Kalra, Taesung Lee, Kevin Lin, Peter Lofgren, Francesco Mosconi, Clare O'Hara, Catherine Olsson, Linda Petrini, Samir Rajani, Nikhil Saxena, Alex Silverstein, Tanya Singh, Theodore Sumers, Leonard Tang, Kevin K. Troy, Constantin Weisser, Ruiqi Zhong, Giulio Zhou, Jan Leike, Jared Kaplan, Ethan Perez",Anthropic,2025,2025-01-31
Safeguards (inference-time auxiliaries),Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Safeguards (inference-time auxiliaries),https://arxiv.org/abs/2411.07494,Rapid Response: Mitigating LLM Jailbreaks with a Few Examples,Rapid Response: Mitigating LLM Jailbreaks with a Few Examples,"Alwin Peng, Julian Michael, Henry Sleight, Ethan Perez, Mrinank Sharma",Anthropic,2024,2024-11-12
Safeguards (inference-time auxiliaries),Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Safeguards (inference-time auxiliaries),https://alignment.anthropic.com/2025/summarization-for-monitoring/index.html,Monitoring computer use via hierarchical summarization,Monitoring computer use via hierarchical summarization,"Theodore Sumers, Raj Agarwal, Nathan Bailey, Tim Belonax, Brian Clarke, Jasmine Deng, Kyla Guru, Evan Frondorf, Keegan Hankes, Jacob Klein, Lynx Lean, Kevin Lin, Linda Petrini, Madeleine Tucker, Ethan Perez, Mrinank Sharma, Nikhil Saxena",Anthropic,2025,2025-02-27
Safeguards (inference-time auxiliaries),Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Safeguards (inference-time auxiliaries),https://arxiv.org/abs/2503.18813,Defeating Prompt Injections by Design,Defeating Prompt Injections by Design,"Edoardo Debenedetti, Ilia Shumailov, Tianqi Fan, Jamie Hayes, Nicholas Carlini, Daniel Fabian, Christoph Kern, Chongyang Shi, Andreas Terzis, Florian Tramèr","Google Research, ETH Zurich",2025,2025-03-24
Safeguards (inference-time auxiliaries),Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Safeguards (inference-time auxiliaries),https://alignment.anthropic.com/2025/introducing-safeguards-research-team/index.html,Introducing Anthropic's Safeguards Research Team,Introducing Anthropic's Safeguards Research Team,,Anthropic,2025,2025-01-01
Safeguards (inference-time auxiliaries),Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Safeguards (inference-time auxiliaries),https://arxiv.org/abs/2505.23856,OMNIGUARD: An Efficient Approach for AI Safety Moderation Across Modalities,OMNIGUARD: An Efficient Approach for AI Safety Moderation Across Modalities,"Sahil Verma, Keegan Hines, Jeff Bilmes, Charlotte Siska, Luke Zettlemoyer, Hila Gonen, Chandan Singh","University of Washington, Meta AI Research",2025,2025-05-29
Chain of thought monitoring,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Chain of thought monitoring,https://arxiv.org/abs/2503.11926,Monitoring Reasoning Models for Misbehavior and the Risks of Promoting Obfuscation,Monitoring Reasoning Models for Misbehavior and the Risks of Promoting Obfuscation,"Bowen Baker, Joost Huizinga, Leo Gao, Zehao Dou, Melody Y. Guan, Aleksander Madry, Wojciech Zaremba, Jakub Pachocki, David Farhi",OpenAI,2025,2025-03-14
Chain of thought monitoring,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Chain of thought monitoring,https://openai.com/index/chain-of-thought-monitoring/,Detecting misbehavior in frontier reasoning models,Detecting misbehavior in frontier reasoning models,"Bowen Baker, Joost Huizinga, Aleksander Madry, Wojciech Zaremba, Jakub Pachocki, David Farhi",OpenAI,2025,2025-03-10
Chain of thought monitoring,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Chain of thought monitoring,https://arxiv.org/abs/2507.05246,"When Chain of Thought is Necessary, Language Models Struggle to Evade Monitors","When Chain of Thought is Necessary, Language Models Struggle to Evade Monitors","Scott Emmons, Erik Jenner, David K. Elson, Rif A. Saurous, Senthooran Rajamanoharan, Heng Chen, Irhum Shafkat, Rohin Shah",Google DeepMind,2025,2025-07-07
Chain of thought monitoring,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Chain of thought monitoring,https://arxiv.org/abs/2505.05410,Reasoning Models Don't Always Say What They Think,Reasoning Models Don't Always Say What They Think,"Yanda Chen, Joe Benton, Ansh Radhakrishnan, Jonathan Uesato, Carson Denison, John Schulman, Arushi Somani, Peter Hase, Misha Wagner, Fabien Roger, Vlad Mikulik, Samuel R. Bowman, Jan Leike, Jared Kaplan, Ethan Perez","OpenAI, Anthropic",2025,2025-05-08
Chain of thought monitoring,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Chain of thought monitoring,https://arxiv.org/abs/2510.01367,Is It Thinking or Cheating? Detecting Implicit Reward Hacking by Measuring Reasoning Effort,Is It Thinking or Cheating? Detecting Implicit Reward Hacking by Measuring Reasoning Effort,"Xinpeng Wang, Nitish Joshi, Barbara Plank, Rico Angell, He He",,2025,2025-10-01
Chain of thought monitoring,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Chain of thought monitoring,https://arxiv.org/abs/2505.23575,CoT Red-Handed: Stress Testing Chain-of-Thought Monitoring,CoT Red-Handed: Stress Testing Chain-of-Thought Monitoring,"Benjamin Arnav, Pablo Bernabeu-Pérez, Nathan Helm-Burger, Tim Kostolansky, Hannes Whittingham, Mary Phuong",DeepMind,2025,2025-05-29
Chain of thought monitoring,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Chain of thought monitoring,https://alignment.anthropic.com/2025/subtle-reasoning/,Training fails to elicit subtle reasoning in current language models,Training fails to elicit subtle reasoning in current language models,,Anthropic,2025,2025-01-01
Chain of thought monitoring,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Chain of thought monitoring,https://arxiv.org/abs/2510.19851,Can Reasoning Models Obfuscate Reasoning? Stress-Testing Chain-of-Thought Monitorability,Can Reasoning Models Obfuscate Reasoning? Stress-Testing Chain-of-Thought Monitorability,"Artur Zolkowski, Wen Xing, David Lindner, Florian Tramèr, Erik Jenner","ETH Zurich, FAR AI",2025,2025-10-21
Chain of thought monitoring,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Chain of thought monitoring,https://arxiv.org/abs/2506.22777,Teaching Models to Verbalize Reward Hacking in Chain-of-Thought Reasoning,Teaching Models to Verbalize Reward Hacking in Chain-of-Thought Reasoning,"Miles Turpin, Andy Arditi, Marvin Li, Joe Benton, Julian Michael",,2025,2025-06-28
Chain of thought monitoring,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Chain of thought monitoring,https://arxiv.org/abs/2501.08156,Are DeepSeek R1 And Other Reasoning Models More Faithful?,Are DeepSeek R1 And Other Reasoning Models More Faithful?,"James Chua, Owain Evans",,2025,2025-01-14
Chain of thought monitoring,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Chain of thought monitoring,https://arxiv.org/abs/2510.23966,A Pragmatic Way to Measure Chain-of-Thought Monitorability,A Pragmatic Way to Measure Chain-of-Thought Monitorability,"Scott Emmons, Roland S. Zimmermann, David K. Elson, Rohin Shah",Google DeepMind,2025,2025-10-28
Chain of thought monitoring,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Chain of thought monitoring,https://lesswrong.com/posts/Em9sihEZmbofZKc2t/a-concrete-roadmap-towards-safety-cases-based-on-chain-of,A Concrete Roadmap towards Safety Cases based on Chain-of-Thought Monitoring,A Concrete Roadmap towards Safety Cases based on Chain-of-Thought Monitoring,Wuschel Schulz,,2025,2025-10-23
Chain of thought monitoring,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Chain of thought monitoring,https://arxiv.org/abs/2507.11473,Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety,Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety,"Tomek Korbak, Mikita Balesni, Elizabeth Barnes, Yoshua Bengio, Joe Benton, Joseph Bloom, Mark Chen, Alan Cooney, Allan Dafoe, Anca Dragan, Scott Emmons, Owain Evans, David Farhi, Ryan Greenblatt, Dan Hendrycks, Marius Hobbhahn, Evan Hubinger, Geoffrey Irving, Erik Jenner, Daniel Kokotajlo, Victoria Krakovna, Shane Legg, David Lindner, David Luan, Aleksander Mądry, Julian Michael, Neel Nanda, Dave Orr, Jakub Pachocki, Ethan Perez, Mary Phuong, Fabien Roger, Joshua Saxe, Buck Shlegeris, Martín Soto, Eric Steinberger, Jasmine Wang, Wojciech Zaremba, Bowen Baker, Rohin Shah, Vlad Mikulik","Anthropic, OpenAI, DeepMind, FAR AI, UC Berkeley, Mila, Center for AI Safety, Redwood Research, Apart Research",2025,2025-07-15
Chain of thought monitoring,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Chain of thought monitoring,https://metr.org/blog/2025-03-11-good-for-ai-to-reason-legibly-and-faithfully/,Why it's good for AI reasoning to be legible and faithful,Why it's good for AI reasoning to be legible and faithful,,METR,2025,2025-03-11
Chain of thought monitoring,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Chain of thought monitoring,https://www.lesswrong.com/posts/Tzdwetw55JNqFTkzK/why-don-t-we-just-shoggoth-face-paraphraser,Why Don't We Just... Shoggoth+Face+Paraphraser?,Why don't we just shoggoth-face paraphraser,,,,
Chain of thought monitoring,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Chain of thought monitoring,https://metr.org/blog/2025-08-08-cot-may-be-highly-informative-despite-unfaithfulness/,"CoT May Be Highly Informative Despite ""Unfaithfulness""","CoT May Be Highly Informative Despite ""Unfaithfulness""","Amy Deng, Sydney Von Arx, Ben Snodin, Sudarsh Kunnavakkam, Tamera Lanham","METR, Gray Swan",2025,2025-08-08
Chain of thought monitoring,Black-box safety (understand and control current model behaviour),Iterative alignment,Black-box safety (understand and control current model behaviour) / Iterative alignment / Chain of thought monitoring,https://www.lesswrong.com/posts/B8Cmtf5gdHwxb8qtT/aether-july-2025-update,Aether July 2025 Update,Aether July 2025 Update,"Rohan Subramani, Rauno Arike, Shubhorup Biswas",Aether,2025,2025-07-01
Model values / model preferences,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Model values / model preferences,https://arxiv.org/abs/2406.07882v1,What Kind of User Are You? Uncovering User Models in LLM Chatbots,Designing a Dashboard for Transparency and Control of Conversational AI,"Yida Chen, Aoyu Wu, Trevor DePodesta, Catherine Yeh, Kenneth Li, Nicholas Castillo Marin, Oam Patel, Jan Riecke, Shivam Raval, Olivia Seow, Martin Wattenberg, Fernanda Viégas",Google,2024,2024-06-12
Model values / model preferences,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Model values / model preferences,https://arxiv.org/abs/2502.08640,Utility Engineering: Analyzing and Controlling Emergent Value Systems in AIs,Utility Engineering: Analyzing and Controlling Emergent Value Systems in AIs,"Mantas Mazeika, Xuwang Yin, Rishub Tamirisa, Jaehyuk Lim, Bruce W. Lee, Richard Ren, Long Phan, Norman Mu, Adam Khoja, Oliver Zhang, Dan Hendrycks",Unknown,2025,2025-02-12
Model values / model preferences,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Model values / model preferences,https://arxiv.org/abs/2505.14633,Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values Prioritization with AIRiskDilemmas,Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values Prioritization with AIRiskDilemmas,"Yu Ying Chiu, Zhilin Wang, Sharan Maiya, Yejin Choi, Kyle Fish, Sydney Levine, Evan Hubinger",Anthropic,2025,2025-05-20
Model values / model preferences,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Model values / model preferences,https://arxiv.org/abs/2508.09762,The PacifAIst Benchmark:Would an Artificial Intelligence Choose to Sacrifice Itself for Human Safety?,The PacifAIst Benchmark:Would an Artificial Intelligence Choose to Sacrifice Itself for Human Safety?,Manuel Herrador,,2025,2025-08-13
Model values / model preferences,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Model values / model preferences,https://arxiv.org/abs/2504.15236,Values in the Wild: Discovering and Analyzing Values in Real-World Language Model Interactions,Values in the Wild: Discovering and Analyzing Values in Real-World Language Model Interactions,"Saffron Huang, Esin Durmus, Miles McCain, Kunal Handa, Alex Tamkin, Jerry Hong, Michael Stern, Arushi Somani, Xiuruo Zhang, Deep Ganguli",Anthropic,2025,2025-04-21
Model values / model preferences,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Model values / model preferences,https://arxiv.org/abs/2509.01938,EigenBench: A Comparative behavioural Measure of Value Alignment,EigenBench: A Comparative Behavioral Measure of Value Alignment,"Jonathn Chang, Leonhard Piff, Suvadip Sana, Jasmine X. Li, Lionel Levine",,2025,2025-09-02
Model values / model preferences,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Model values / model preferences,https://arxiv.org/abs/2504.04994,Following the Whispers of Values: Unraveling Neural Mechanisms Behind Value-Oriented Behaviors in LLMs,Following the Whispers of Values: Unraveling Neural Mechanisms Behind Value-Oriented Behaviors in LLMs,"Ling Hu, Yuemei Xu, Xiaoyang Gu, Letao Han",,2025,2025-04-07
Model values / model preferences,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Model values / model preferences,https://lesswrong.com/posts/jrkrHyrymv95CX5NC/alignment-can-reduce-performance-on-simple-ethical-questions,Alignment Can Reduce Performance on Simple Ethical Questions,Alignment Can Reduce Performance on Simple Ethical Questions,Daan Henselmans,,2025,2025-02-03
Model values / model preferences,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Model values / model preferences,https://arxiv.org/abs/2410.01639,Moral Alignment for LLM Agents,Moral Alignment for LLM Agents,"Elizaveta Tennant, Stephen Hailes, Mirco Musolesi",,2025,2024-10-02
Model values / model preferences,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Model values / model preferences,https://www.lesswrong.com/posts/6JdSJ63LZ4TuT5cTH/the-llm-has-left-the-chat-evidence-of-bail-preferences-in,The LLM Has Left The Chat: Evidence of Bail Preferences in Large Language Models,The LLM Has Left The Chat: Evidence of Bail Preferences in Large Language Models,Danielle Ensign,Anthropic,2024,2024-09-08
Model values / model preferences,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Model values / model preferences,https://arxiv.org/abs/2505.21479,Are Language Models Consequentialist or Deontological Moral Reasoners?,Are Language Models Consequentialist or Deontological Moral Reasoners?,"Keenan Samway, Max Kleiman-Weiner, David Guzman Piedrahita, Rada Mihalcea, Bernhard Schölkopf, Zhijing Jin",,2025,2025-05-27
Model values / model preferences,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Model values / model preferences,https://nature.com/articles/s41562-025-02172-y,Playing repeated games with large language models,Playing repeated games with large language models,"Elif Akata, Lion Schulz, Julian Coda-Forno, Seong Joon Oh, Matthias Bethge, Eric Schulz","Max Planck Institute for Biological Cybernetics, Institute for Human-Centered AI (Helmholtz Munich), University of Tübingen",2025,2025-05-08
Model values / model preferences,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Model values / model preferences,https://arxiv.org/abs/2504.06324,From Stability to Inconsistency: A Study of Moral Preferences in LLMs,From Stability to Inconsistency: A Study of Moral Preferences in LLMs,"Monika Jotautaite, Mary Phuong, Chatrik Singh Mangat, Maria Angelica Martinez",,2025,2025-04-08
Model values / model preferences,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Model values / model preferences,https://arxiv.org/abs/2510.05465,VAL-Bench: Measuring Value Alignment in Language Models,VAL-Bench: Measuring Value Alignment in Language Models,"Aman Gupta, Denny O'Shea, Fazl Barez",,2025,2025-10-06
Character training and persona steering,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Character training and persona steering,https://arxiv.org/pdf/2511.01689%20,Open Character Training: Shaping the Persona of AI Assistants through Constitutional AI,Open Character Training: Shaping the Persona of AI Assistants through Constitutional AI,"Sharan Maiya, Henning Bartsch, Nathan Lambert, Evan Hubinger",Anthropic,2025,2025-11-03
Character training and persona steering,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Character training and persona steering,https://www.lesswrong.com/posts/29aWbJARGF4ybAa5d/on-the-functional-self-of-llms,On the functional self of LLMs,On the functional self of LLMs,eggsyntax,,2025,2025-07-07
Character training and persona steering,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Character training and persona steering,https://www.lesswrong.com/posts/vpNG99GhbBoLov9og/claude-4-5-opus-soul-document%20,Opus 4.5's Soul Document,Claude 4.5 Opus' Soul Document,Richard Weiss,,2025,2025-11-28
Character training and persona steering,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Character training and persona steering,https://arxiv.org/abs/2506.19823,Persona Features Control Emergent Misalignment,Persona Features Control Emergent Misalignment,"Miles Wang, Tom Dupré la Tour, Olivia Watkins, Alex Makelov, Ryan A. Chi, Samuel Miserendino, Jeffrey Wang, Achyuta Rajaram, Johannes Heidecke, Tejal Patwardhan, Dan Mossing",,2025,2025-06-24
Character training and persona steering,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Character training and persona steering,https://arxiv.org/abs/2510.04340,Inoculation Prompting: Eliciting traits from LLMs during training can suppress them at test-time,Inoculation Prompting: Eliciting traits from LLMs during training can suppress them at test-time,"Daniel Tan, Anders Woodruff, Niels Warncke, Arun Jose, Maxime Riché, David Demitri Africa, Mia Taylor",,2025,2025-10-05
Character training and persona steering,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Character training and persona steering,https://arxiv.org/abs/2507.21509,Persona Vectors: Monitoring and Controlling Character Traits in Language Models,Persona Vectors: Monitoring and Controlling Character Traits in Language Models,"Runjin Chen, Andy Arditi, Henry Sleight, Owain Evans, Jack Lindsey",,2025,2025-07-29
Character training and persona steering,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Character training and persona steering,https://lesswrong.com/posts/jtqcsARGtmgogdcLT/reducing-llm-deception-at-scale-with-self-other-overlap-fine,Reducing LLM deception at scale with self-other overlap fine-tuning,Reducing LLM deception at scale with self-other overlap fine-tuning,"Marc Carauleanu, Diogo de Lucena, Gunnar_Zarncke, Judd Rosenblatt, Cameron Berg, Mike Vaiana, Trent Hodgeson","AE Studio, Foresight Institute",2025,2025-03-13
Character training and persona steering,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Character training and persona steering,https://www.lesswrong.com/posts/6ZnznCaTcbGYsCmqu/the-rise-of-parasitic-ai?commentId=RrWjMnKwXGTtmw9rQ,The Rise of Parasitic AI,The Rise of Parasitic AI,Adele Lopez,,2025,2025-09-11
Character training and persona steering,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Character training and persona steering,https://www.alignmentforum.org/posts/zuXo9imNKYspu9HGv/a-three-layer-model-of-llm-psychology,A Three-Layer Model of LLM Psychology,A Three-Layer Model of LLM Psychology,Jan_Kulveit,,2024,2024-12-26
Character training and persona steering,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Character training and persona steering,https://arxiv.org/abs/2502.07077,Multi-turn Evaluation of Anthropomorphic Behaviours in Large Language Models,Multi-turn Evaluation of Anthropomorphic Behaviours in Large Language Models,"Lujain Ibrahim, Canfer Akbulut, Rasmi Elasmar, Charvi Rastogi, Minsuk Kahng, Meredith Ringel Morris, Kevin R. McKee, Verena Rieser, Murray Shanahan, Laura Weidinger","Google DeepMind, Heriot-Watt University",2025,2025-02-10
Character training and persona steering,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Character training and persona steering,https://www.lesswrong.com/posts/LdBhgAhpvbdEep79F/selection-pressures-on-lm-personas,Selection Pressures on LM Personas,Selection Pressures on LM Personas,Raymond Douglas,,2025,2025-03-28
Character training and persona steering,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Character training and persona steering,https://nostalgebraist.tumblr.com/post/785766737747574784/the-void,the void,the void,nostalgebraist,,2025,2025-06-07
Character training and persona steering,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Character training and persona steering,https://nostalgebraist.tumblr.com/post/786568570671923200/void-miscellany,void miscellany,void miscellany,nostalgebraist,,2025,2025-06-16
Emergent misalignment,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Emergent misalignment,https://arxiv.org/abs/2502.17424,Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs,Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs,"Jan Betley, Daniel Tan, Niels Warncke, Anna Sztyber-Betley, Xuchan Bao, Martín Soto, Nathan Labenz, Owain Evans",,2025,2025-02-24
Emergent misalignment,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Emergent misalignment,https://arxiv.org/abs/2506.13206,Thought Crime: Backdoors and Emergent Misalignment in Reasoning Models,Thought Crime: Backdoors and Emergent Misalignment in Reasoning Models,"James Chua, Jan Betley, Mia Taylor, Owain Evans",,2025,2025-06-16
Emergent misalignment,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Emergent misalignment,https://arxiv.org/abs/2506.19823,Persona Features Control Emergent Misalignment,Persona Features Control Emergent Misalignment,"Miles Wang, Tom Dupré la Tour, Olivia Watkins, Alex Makelov, Ryan A. Chi, Samuel Miserendino, Jeffrey Wang, Achyuta Rajaram, Johannes Heidecke, Tejal Patwardhan, Dan Mossing",,2025,2025-06-24
Emergent misalignment,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Emergent misalignment,https://arxiv.org/abs/2506.11613,Model Organisms for Emergent Misalignment,Model Organisms for Emergent Misalignment,"Edward Turner, Anna Soligo, Mia Taylor, Senthooran Rajamanoharan, Neel Nanda",Google DeepMind,2025,2025-06-13
Emergent misalignment,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Emergent misalignment,https://arxiv.org/abs/2508.17511,School of Reward Hacks: Hacking harmless tasks generalizes to misaligned behavior in LLMs,School of Reward Hacks: Hacking harmless tasks generalizes to misaligned behavior in LLMs,"Mia Taylor, James Chua, Jan Betley, Johannes Treutlein, Owain Evans",,2025,2025-08-24
Emergent misalignment,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Emergent misalignment,https://alignment.anthropic.com/2025/subliminal-learning/,Subliminal Learning: Language Models Transmit behavioural Traits via Hidden Signals in Data,Subliminal Learning: Language Models Transmit Behavioral Traits via Hidden Signals in Data,"Alex Cloud, Minh Le, James Chua, Jan Betley, Anna Sztyber-Betley, Jacob Hilton, Samuel Marks, Owain Evans","Anthropic Fellows Program, Truthful AI, Warsaw University of Technology, Alignment Research Center, Anthropic, UC Berkeley",2025,2025-07-22
Emergent misalignment,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Emergent misalignment,https://lesswrong.com/posts/umYzsh7SGHHKsRCaA/convergent-linear-representations-of-emergent-misalignment,Convergent Linear Representations of Emergent Misalignment,Convergent Linear Representations of Emergent Misalignment,"Anna Soligo, Edward Turner, Senthooran Rajamanoharan, Neel Nanda",ML Alignment & Theory Scholars,2025,2025-06-16
Emergent misalignment,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Emergent misalignment,https://www.lesswrong.com/posts/gLDSqQm8pwNiq7qst/narrow-misalignment-is-hard-emergent-misalignment-is-easy,"Narrow Misalignment is Hard, Emergent Misalignment is Easy","Narrow Misalignment is Hard, Emergent Misalignment is Easy","Edward Turner, Anna Soligo, Senthooran Rajamanoharan, Neel Nanda",Google DeepMind,2024,2024-07-14
Emergent misalignment,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Emergent misalignment,https://lesswrong.com/posts/gT3wtWBAs7PKonbmy/aesthetic-preferences-can-cause-emergent-misalignment,Aesthetic Preferences Can Cause Emergent Misalignment,Aesthetic Preferences Can Cause Emergent Misalignment,Anders Woodruff,Center on Long-Term Risk,2025,2025-08-26
Emergent misalignment,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Emergent misalignment,https://arxiv.org/abs/2510.06105,Moloch's Bargain: Emergent Misalignment When LLMs Compete for Audiences,Moloch's Bargain: Emergent Misalignment When LLMs Compete for Audiences,"Batu El, James Zou",Stanford University,2025,2025-10-07
Emergent misalignment,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Emergent misalignment,https://lesswrong.com/posts/ZdY4JzBPJEgaoCxTR/emergent-misalignment-and-realignment,Emergent Misalignment & Realignment,Emergent Misalignment & Realignment,"Elizaveta Tennant, Jasper Timm, Kevin Wei, David Quarel",ARENA (Alignment Research Engineering Accelerator),2025,2025-06-27
Emergent misalignment,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Emergent misalignment,https://www.lesswrong.com/posts/HLJoJYi52mxgomujc/realistic-reward-hacking-induces-different-and-deeper-1,Realistic Reward Hacking Induces Different and Deeper Misalignment,Realistic Reward Hacking Induces Different and Deeper Misalignment,Jozdien,,2025,2025-10-09
Emergent misalignment,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Emergent misalignment,https://lesswrong.com/posts/ZXxY2tccLapdjLbKm/selective-generalization-improving-capabilities-while,Selective Generalization: Improving Capabilities While Maintaining Alignment,Selective Generalization: Improving Capabilities While Maintaining Alignment,"Ariana Azarbal, Matthew A. Clarke, Jorio Cocola, Cailley Factor, Alex Cloud",SPAR,2025,2025-07-16
Emergent misalignment,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Emergent misalignment,https://lesswrong.com/posts/qHudHZNLCiFrygRiy/emergent-misalignment-on-a-budget,Emergent Misalignment on a Budget,Emergent Misalignment on a Budget,"Valerio Pepe, Armaan Tipirneni",Harvard College,2025,2025-06-08
Emergent misalignment,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Emergent misalignment,https://www.lesswrong.com/posts/6ZnznCaTcbGYsCmqu/the-rise-of-parasitic-ai,The Rise of Parasitic AI,The Rise of Parasitic AI,Adele Lopez,,2025,2025-09-11
Emergent misalignment,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Emergent misalignment,https://lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover,LLM AGI may reason about its goals and discover misalignments by default,LLM AGI may reason about its goals and discover misalignments by default,Seth Herd,,2025,2025-09-15
Emergent misalignment,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Emergent misalignment,https://lesswrong.com/posts/AcTEiu5wYDgrbmXow/open-problems-in-emergent-misalignment,Open problems in emergent misalignment,Open problems in emergent misalignment,"Jan Betley, Daniel Tan",,2025,2025-03-01
Model specs and constitutions,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Model specs and constitutions,https://www.anthropic.com/news/claudes-constitution,Claude's Constitution,Claude's Constitution,,Anthropic,2023,2023-05-09
Model specs and constitutions,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Model specs and constitutions,https://github.com/elder-plinius/CL4R1T4S/blob/main/GOOGLE/Gemini-2.5-Pro-04-18-2025.md,"Google doesn't have anything public. The Gemini system prompt is very short and dry and doesn't even have any rules for handling copyrighted, let alone wetter stuff",Gemini-2.5-Pro-04-18-2025 System Prompt,,Google,2025,2025-04-18
Model specs and constitutions,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Model specs and constitutions,https://arxiv.org/abs/2412.16339,Deliberative Alignment: Reasoning Enables Safer Language Models,Deliberative Alignment: Reasoning Enables Safer Language Models,"Melody Y. Guan, Manas Joglekar, Eric Wallace, Saachi Jain, Boaz Barak, Alec Helyar, Rachel Dias, Andrea Vallone, Hongyu Ren, Jason Wei, Hyung Won Chung, Sam Toyer, Johannes Heidecke, Alex Beutel, Amelia Glaese",OpenAI,2024,2024-12-20
Model specs and constitutions,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Model specs and constitutions,https://arxiv.org/abs/2510.07686,Stress-Testing Model Specs Reveals Character Differences among Language Models,Stress-Testing Model Specs Reveals Character Differences among Language Models,"Jifan Zhang, Henry Sleight, Andi Peng, John Schulman, Esin Durmus","OpenAI, Anthropic",2025,2025-10-09
Model specs and constitutions,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Model specs and constitutions,https://model-spec.openai.com/,OpenAI Model Spec,OpenAI Model Spec,,OpenAI,2025,2025-09-12
Model specs and constitutions,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Model specs and constitutions,https://arxiv.org/abs/2506.00195,Let Them Down Easy! Contextual Effects of LLM Guardrails on User Perceptions and Preferences,Let Them Down Easy! Contextual Effects of LLM Guardrails on User Perceptions and Preferences,"Mingqian Zheng, Wenjia Hu, Patrick Zhao, Motahhare Eslami, Jena D. Hwang, Faeze Brahman, Carolyn Rose, Maarten Sap","Carnegie Mellon University, University of Southern California",2025,2025-05-30
Model specs and constitutions,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Model specs and constitutions,https://lesswrong.com/posts/LSJx5EnQEW6s5Juw6/no-self-as-an-alignment-target,No-self as an alignment target,No-self as an alignment target,Milan W,,,
Model specs and constitutions,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Model specs and constitutions,https://lesswrong.com/posts/3jnziqCF3vA2NXAKp/six-thoughts-on-ai-safety,Six Thoughts on AI Safety,Six Thoughts on AI Safety,Boaz Barak,Harvard University,2025,2025-01-24
Model specs and constitutions,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Model specs and constitutions,https://newsletter.forethought.org/p/how-important-is-the-model-spec-if,How important is the model spec if alignment fails?,How important is the model spec if alignment fails?,Mia Taylor,Forethought,2025,2025-12-03
Model specs and constitutions,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Model specs and constitutions,https://arxiv.org/abs/2503.05728,Political Neutrality in AI Is Impossible- But Here Is How to Approximate It,Political Neutrality in AI Is Impossible- But Here Is How to Approximate It,"Jillian Fisher, Ruth E. Appel, Chan Young Park, Yujin Potter, Liwei Jiang, Taylor Sorensen, Shangbin Feng, Yulia Tsvetkov, Margaret E. Roberts, Jennifer Pan, Dawn Song, Yejin Choi","Anthropic, UC San Diego, Stanford University, UC Berkeley, University of Washington",2025,2025-02-18
Model specs and constitutions,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Model specs and constitutions,https://joecarlsmith.com/2025/08/18/giving-ais-safe-motivations#4-5-step-4-good-instructions,Giving AIs safe motivations,Giving AIs safe motivations,Joe Carlsmith,Anthropic,2025,2025-08-18
Model psychopathology,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Model psychopathology,https://arxiv.org/abs/2507.14805,Subliminal Learning: Language models transmit behavioural traits via hidden signals in data,Subliminal Learning: Language models transmit behavioral traits via hidden signals in data,"Alex Cloud, Minh Le, James Chua, Jan Betley, Anna Sztyber-Betley, Jacob Hilton, Samuel Marks, Owain Evans",,2025,2025-07-20
Model psychopathology,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Model psychopathology,https://arxiv.org/abs/2510.13928,"LLMs Can Get ""Brain Rot""!","LLMs Can Get ""Brain Rot""!","Shuo Xing, Junyuan Hong, Yifan Wang, Runjin Chen, Zhenyu Zhang, Ananth Grama, Zhengzhong Tu, Zhangyang Wang",,2025,2025-10-15
Model psychopathology,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Model psychopathology,https://arxiv.org/abs/2506.20020,Persona-Assigned Large Language Models Exhibit Human-Like Motivated Reasoning,Persona-Assigned Large Language Models Exhibit Human-Like Motivated Reasoning,"Saloni Dash, Amélie Reymond, Emma S. Spiro, Aylin Caliskan",,2025,2025-06-24
Model psychopathology,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Model psychopathology,https://spylab.ai/blog/modal-aphasia,Unified Multimodal Models Cannot Describe Images From Memory,Unified Multimodal Models Cannot Describe Images From Memory,"Michael Aerni, Joshua Swanson, Kristina Nikolić, Florian Tramèr","SPY Lab, ETH Zurich",2024,2024-10-28
Model psychopathology,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Model psychopathology,https://arxiv.org/abs/2510.17941,Believe It or Not: How Deeply do LLMs Believe Implanted Facts?,Believe It or Not: How Deeply do LLMs Believe Implanted Facts?,"Stewart Slocum, Julian Minder, Clément Dumas, Henry Sleight, Ryan Greenblatt, Samuel Marks, Rowan Wang",Redwood Research,2025,2025-10-20
Model psychopathology,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Model psychopathology,https://www.psychopathia.ai/,Psychopathia Machinalis: A Nosological Framework for Understanding Pathologies in Advanced Artificial Intelligence,Psychopathia Machinalis: A Nosological Framework for Understanding Pathologies in Advanced Artificial Intelligence,"Nell Watson, Ali Hessami",,2025,2025-01-01
Model psychopathology,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Model psychopathology,https://arxiv.org/abs/2411.02478,Imagining and building wise machines: The centrality of AI metacognition,Imagining and building wise machines: The centrality of AI metacognition,"Samuel G. B. Johnson, Amir-Hossein Karimi, Yoshua Bengio, Nick Chater, Tobias Gerstenberg, Kate Larson, Sydney Levine, Melanie Mitchell, Iyad Rahwan, Bernhard Schölkopf, Igor Grossmann","Various institutions including Mila, Santa Fe Institute",2024,2024-11-04
Model psychopathology,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Model psychopathology,https://arxiv.org/abs/2510.22954,Artificial Hivemind: The Open-Ended Homogeneity of Language Models (and Beyond),Artificial Hivemind: The Open-Ended Homogeneity of Language Models (and Beyond),"Liwei Jiang, Yuanjun Chai, Margaret Li, Mickel Liu, Raymond Fok, Nouha Dziri, Yulia Tsvetkov, Maarten Sap, Alon Albalak, Yejin Choi","University of Washington, Allen Institute for AI",2025,2025-10-27
Model psychopathology,Black-box safety (understand and control current model behaviour),Model psychology,Black-box safety (understand and control current model behaviour) / Model psychology / Model psychopathology,https://arxiv.org/abs/2510.20039,Beyond One-Way Influence: Bidirectional Opinion Dynamics in Multi-Turn Human-LLM Interactions,Beyond One-Way Influence: Bidirectional Opinion Dynamics in Multi-Turn Human-LLM Interactions,"Yuyang Jiang, Longjie Guo, Yuchen Wu, Aylin Caliskan, Tanu Mitra, Hua Shen","University of Washington, Northeastern University",2025,2025-10-22
Data filtering,Black-box safety (understand and control current model behaviour),Better data,Black-box safety (understand and control current model behaviour) / Better data / Data filtering,https://alignment.anthropic.com/2025/pretraining-data-filtering/,Enhancing Model Safety through Pretraining Data Filtering,Enhancing Model Safety through Pretraining Data Filtering,"Yanda Chen, Mycal Tucker, Nina Panickssery, Tony Wang, Francesco Mosconi, Anjali Gopal, Carson Denison, Linda Petrini, Jan Leike, Ethan Perez, Mrinank Sharma",Anthropic,2025,2025-08-19
Data filtering,Black-box safety (understand and control current model behaviour),Better data,Black-box safety (understand and control current model behaviour) / Better data / Data filtering,https://arxiv.org/abs/2508.06601,Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs,Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs,"Kyle O'Brien, Stephen Casper, Quentin Anthony, Tomek Korbak, Robert Kirk, Xander Davies, Ishan Mishra, Geoffrey Irving, Yarin Gal, Stella Biderman","Anthropic, Redwood Research, EleutherAI, University of Oxford",2025,2025-08-08
Data filtering,Black-box safety (understand and control current model behaviour),Better data,Black-box safety (understand and control current model behaviour) / Better data / Data filtering,https://arxiv.org/abs/2504.16980,Safety Pretraining: Toward the Next Generation of Safe AI,Safety Pretraining: Toward the Next Generation of Safe AI,"Pratyush Maini, Sachin Goyal, Dylan Sam, Alex Robey, Yash Savani, Yiding Jiang, Andy Zou, Matt Fredrikson, Zacharcy C. Lipton, J. Zico Kolter",Carnegie Mellon University,2025,2025-04-23
Data filtering,Black-box safety (understand and control current model behaviour),Better data,Black-box safety (understand and control current model behaviour) / Better data / Data filtering,https://arxiv.org/abs/2510.27629v2,Best Practices for Biorisk Evaluations on Open-Weight Bio-Foundation Models,Best Practices for Biorisk Evaluations on Open-Weight Bio-Foundation Models,"Boyi Wei, Zora Che, Nathaniel Li, Udari Madhushani Sehwag, Jasper Götting, Samira Nedungadi, Julian Michael, Summer Yue, Dan Hendrycks, Peter Henderson, Zifan Wang, Seth Donoughe, Mantas Mazeika","Anthropic, Redwood Research",2025,2025-10-31
Hyperstition studies,Black-box safety (understand and control current model behaviour),Better data,Black-box safety (understand and control current model behaviour) / Better data / Hyperstition studies,https://www.lesswrong.com/posts/qXYLvjGL9QvD3aFSW/training-on-documents-about-reward-hacking-induces-reward,Training on Documents About Reward Hacking Induces Reward Hacking,Training on Documents About Reward Hacking Induces Reward Hacking,"Evan Hubinger, Nathan Hu",Anthropic,2025,2025-01-21
Hyperstition studies,Black-box safety (understand and control current model behaviour),Better data,Black-box safety (understand and control current model behaviour) / Better data / Hyperstition studies,https://www.lesswrong.com/posts/Y8zS8iG5HhqKcQBtA/do-not-tile-the-lightcone-with-your-confused-ontology,Do Not Tile the Lightcone with Your Confused Ontology,Do Not Tile the Lightcone with Your Confused Ontology,Jan_Kulveit,,2025,2025-06-13
Hyperstition studies,Black-box safety (understand and control current model behaviour),Better data,Black-box safety (understand and control current model behaviour) / Better data / Hyperstition studies,https://turntrout.com/self-fulfilling-misalignment,Self-Fulfilling Misalignment Data Might Be Poisoning Our AI Models,Self-Fulfilling Misalignment Data Might Be Poisoning Our AI Models,Alex Turner,,2025,2025-03-01
Hyperstition studies,Black-box safety (understand and control current model behaviour),Better data,Black-box safety (understand and control current model behaviour) / Better data / Hyperstition studies,https://arxiv.org/abs/2411.13223,"Existential Conversations with Large Language Models: Content, Community, and Culture","Existential Conversations with Large Language Models: Content, Community, and Culture","Murray Shanahan, Beth Singler",,2024,2024-11-20
Data poisoning defense,Black-box safety (understand and control current model behaviour),Better data,Black-box safety (understand and control current model behaviour) / Better data / Data poisoning defense,https://example-blog.com/a-small-number-of-samples-can-poison-llms,A small number of samples can poison LLMs of any size,A small number of samples can poison LLMs,,,,
Data poisoning defense,Black-box safety (understand and control current model behaviour),Better data,Black-box safety (understand and control current model behaviour) / Better data / Data poisoning defense,https://arxiv.org/abs/2509.03405,Reasoning Introduces New Poisoning Attacks Yet Makes Them More Complicated,LMEnt: A Suite for Analyzing Knowledge in Language Models from Pretraining Data to Representations,"Daniela Gottesman, Alon Gilae-Dotan, Ido Cohen, Yoav Gur-Arieh, Marius Mosbach, Ori Yoran, Mor Geva",Unknown,2025,2025-09-03
Data poisoning defense,Black-box safety (understand and control current model behaviour),Better data,Black-box safety (understand and control current model behaviour) / Better data / Data poisoning defense,https://arxiv.org/abs/2510.04567,Poisoning Attacks on LLMs Require a Near-constant Number of Poison Samples,"GILT: An LLM-Free, Tuning-Free Graph Foundational Model for In-Context Learning","Weishuo Ma, Yanbo Wang, Xiyuan Wang, Lei Zou, Muhan Zhang",,2025,2025-10-06
Synthetic data for alignment,Black-box safety (understand and control current model behaviour),Better data,Black-box safety (understand and control current model behaviour) / Better data / Synthetic data for alignment,https://arxiv.org/abs/2510.06652,Aligning Large Language Models via Fully Self-Synthetic Data,Aligning Large Language Models via Fully Self-Synthetic Data,"Shangjian Yin, Zhepei Wei, Xinyu Zhu, Wei-Lin Chen, Yu Meng",,2025,2025-10-08
Synthetic data for alignment,Black-box safety (understand and control current model behaviour),Better data,Black-box safety (understand and control current model behaviour) / Better data / Synthetic data for alignment,https://arxiv.org/html/2412.17417v2,Synth-Align: Improving Trustworthiness in Vision-Language Model with Synthetic Preference Data Alignment,Synth-Align: Improving Trustworthiness in Vision-Language Model with Synthetic Preference Data Alignment,"Robert Wijaya, Ngoc-Bao Nguyen, Ngai-Man Cheung",,2024,2024-12-23
Synthetic data for alignment,Black-box safety (understand and control current model behaviour),Better data,Black-box safety (understand and control current model behaviour) / Better data / Synthetic data for alignment,https://arxiv.org/abs/2510.12345,Inoculation Prompting: Instructing LLMs to misbehave at train-time improves test-time alignment,Carleman Estimates and Controllability of Forward Stochastic Parabolic Equations with General Dynamic Boundary Conditions,"Said Boulite, Abdellatif Elgrou, Lahcen Maniar, Abdelaziz Rhandi",,2025,2025-10-14
Synthetic data for alignment,Black-box safety (understand and control current model behaviour),Better data,Black-box safety (understand and control current model behaviour) / Better data / Synthetic data for alignment,https://arxiv.org/abs/2506.05678,Unsupervised Elicitation of Language Models,Numerical Investigation of Sequence Modeling Theory using Controllable Memory Functions,"Haotian Jiang, Zeyu Bao, Shida Wang, Qianxiao Li",,2025,2025-06-06
Synthetic data for alignment,Black-box safety (understand and control current model behaviour),Better data,Black-box safety (understand and control current model behaviour) / Better data / Synthetic data for alignment,https://arxiv.org/abs/2412.02345,Beyond the Binary: Capturing Diverse Preferences With Reward Regularization,On the Realization of quantum gates coming from the Tracy-Singh product,Fabienne Chouraqui,,2024,2024-12-03
Synthetic data for alignment,Black-box safety (understand and control current model behaviour),Better data,Black-box safety (understand and control current model behaviour) / Better data / Synthetic data for alignment,https://arxiv.org/abs/2507.06789,The Curious Case of Factuality Finetuning: Models' Internal Beliefs Can Improve Factuality,Sharp uniform approximation for spectral Barron functions by deep neural networks,"Yulei Liao, Pingbing Ming, Hao Yu",,2025,2025-07-09
Synthetic data for alignment,Black-box safety (understand and control current model behaviour),Better data,Black-box safety (understand and control current model behaviour) / Better data / Synthetic data for alignment,https://arxiv.org/abs/2502.13456,LongSafety: Enhance Safety for Long-Context LLMs,OGBoost: A Python Package for Ordinal Gradient Boosting,"Mansour T.A. Sharabiani, Alex Bottle, Alireza S. Mahani",,2025,2025-02-19
Synthetic data for alignment,Black-box safety (understand and control current model behaviour),Better data,Black-box safety (understand and control current model behaviour) / Better data / Synthetic data for alignment,https://arxiv.org/abs/2503.02341,Position: Model Collapse Does Not Mean What You Think,GRADEO: Towards Human-Like Evaluation for Text-to-Video Generation via Multi-Step Reasoning,"Zhun Mou, Bin Xia, Zhengchao Huang, Wenming Yang, Jiaya Jia",,2025,2025-03-04
Data quality for alignment,Black-box safety (understand and control current model behaviour),Better data,Black-box safety (understand and control current model behaviour) / Better data / Data quality for alignment,https://arxiv.org/abs/2502.10441,AI Alignment at Your Discretion,AI Alignment at Your Discretion,"Maarten Buyl, Hadi Khalaf, Claudio Mayrink Verdun, Lucas Monteiro Paes, Caio C. Vieira Machado, Flavio du Pin Calmon",,2025,2025-02-10
Data quality for alignment,Black-box safety (understand and control current model behaviour),Better data,Black-box safety (understand and control current model behaviour) / Better data / Data quality for alignment,https://arxiv.org/abs/2503.04910,Maximizing Signal in Human-Model Preference Alignment,Maximizing Signal in Human-Model Preference Alignment,"Kelsey Kraus, Margaret Kroll",,2025,2025-03-06
Data quality for alignment,Black-box safety (understand and control current model behaviour),Better data,Black-box safety (understand and control current model behaviour) / Better data / Data quality for alignment,https://arxiv.org/abs/2507.18802,DxHF: Providing High-Quality Human Feedback for LLM Alignment via Interactive Decomposition,DxHF: Providing High-Quality Human Feedback for LLM Alignment via Interactive Decomposition,"Danqing Shi, Furui Cheng, Tino Weinkauf, Antti Oulasvirta, Mennatallah El-Assady",,2025,2025-07-24
Data quality for alignment,Black-box safety (understand and control current model behaviour),Better data,Black-box safety (understand and control current model behaviour) / Better data / Data quality for alignment,https://arxiv.org/html/2410.01957v2,Challenges and Future Directions of Data-Centric AI Alignment,Challenges and Future Directions of Data-Centric AI Alignment,"Min-Hsuan Yeh, Jeffrey Wang, Xuefeng Du, Seongheon Park, Leitian Tao, Shawn Im, Yixuan Li",,2025,2025-05-01
Data quality for alignment,Black-box safety (understand and control current model behaviour),Better data,Black-box safety (understand and control current model behaviour) / Better data / Data quality for alignment,https://arxiv.org/abs/2502.05475,You Are What You Eat -- AI Alignment Requires Understanding How Data Shapes Structure and Generalisation,You Are What You Eat -- AI Alignment Requires Understanding How Data Shapes Structure and Generalisation,"Simon Pepin Lehalleur, Jesse Hoogland, Matthew Farrugia-Roberts, Susan Wei, Alexander Gietelink Oldenziel, George Wang, Liam Carroll, Daniel Murfet",,2025,2025-02-08
Mild optimisation,Black-box safety (understand and control current model behaviour),Goal robustness,Black-box safety (understand and control current model behaviour) / Goal robustness / Mild optimisation,https://arxiv.org/abs/2501.13011,MONA: Myopic Optimization with Non-myopic Approval Can Mitigate Multi-step Reward Hacking,MONA: Myopic Optimization with Non-myopic Approval Can Mitigate Multi-step Reward Hacking,"Sebastian Farquhar, Vikrant Varma, David Lindner, David Elson, Caleb Biddulph, Ian Goodfellow, Rohin Shah",Google DeepMind,2025,2025-01-22
Mild optimisation,Black-box safety (understand and control current model behaviour),Goal robustness,Black-box safety (understand and control current model behaviour) / Goal robustness / Mild optimisation,https://arxiv.org/abs/2509.02655,BioBlue: Notable runaway-optimiser-like LLM failure modes on biologically and economically aligned AI safety benchmarks for LLMs with simplified observation format,BioBlue: Notable runaway-optimiser-like LLM failure modes on biologically and economically aligned AI safety benchmarks for LLMs with simplified observation format,"Roland Pihlakas, Sruthi Kuriakose",,2025,2025-09-02
Mild optimisation,Black-box safety (understand and control current model behaviour),Goal robustness,Black-box safety (understand and control current model behaviour) / Goal robustness / Mild optimisation,https://lesswrong.com/posts/vGeuBKQ7nzPnn5f7A/why-modelling-multi-objective-homeostasis-is-essential-for,Why modelling multi-objective homeostasis is essential for AI alignment (and how it helps with AI safety as well). Subtleties and Open Challenges,Why modelling multi-objective homeostasis is essential for AI alignment (and how it helps with AI safety as well). Subtleties and Open Challenges.,Roland Pihlakas,,2025,2025-01-12
Mild optimisation,Black-box safety (understand and control current model behaviour),Goal robustness,Black-box safety (understand and control current model behaviour) / Goal robustness / Mild optimisation,https://arxiv.org/abs/2410.00081,From homeostasis to resource sharing: Biologically and economically aligned multi-objective multi-agent gridworld-based AI safety benchmarks,From homeostasis to resource sharing: Biologically and economically aligned multi-objective multi-agent gridworld-based AI safety benchmarks,Roland Pihlakas,,2024,2024-09-30
RL safety,Black-box safety (understand and control current model behaviour),Goal robustness,Black-box safety (understand and control current model behaviour) / Goal robustness / RL safety,https://arxiv.org/abs/2406.15753,The Perils of Optimizing Learned Reward Functions: Low Training Error Does Not Guarantee Low Regret,The Perils of Optimizing Learned Reward Functions: Low Training Error Does Not Guarantee Low Regret,"Lukas Fluri, Leon Lang, Alessandro Abate, Patrick Forré, David Krueger, Joar Skalse",,2024,2024-06-22
RL safety,Black-box safety (understand and control current model behaviour),Goal robustness,Black-box safety (understand and control current model behaviour) / Goal robustness / RL safety,https://arxiv.org/abs/2502.14043,Safe Learning Under Irreversible Dynamics via Asking for Help,Safe Learning Under Irreversible Dynamics via Asking for Help,"Benjamin Plaut, Juan Liévano-Karim, Hanlin Zhu, Stuart Russell",UC Berkeley,2025,2025-02-19
RL safety,Black-box safety (understand and control current model behaviour),Goal robustness,Black-box safety (understand and control current model behaviour) / Goal robustness / RL safety,https://arxiv.org/abs/2507.03068,Mitigating Goal Misgeneralization via Minimax Regret,Mitigating Goal Misgeneralization via Minimax Regret,"Karim Abdel Sadek, Matthew Farrugia-Roberts, Usman Anwar, Hannah Erlebach, Christian Schroeder de Witt, David Krueger, Michael Dennis",,2025,2025-07-03
RL safety,Black-box safety (understand and control current model behaviour),Goal robustness,Black-box safety (understand and control current model behaviour) / Goal robustness / RL safety,https://arxiv.org/abs/2410.05584,Rethinking Reward Model Evaluation: Are We Barking up the Wrong Tree?,Rethinking Reward Model Evaluation: Are We Barking up the Wrong Tree?,"Xueru Wen, Jie Lou, Yaojie Lu, Hongyu Lin, Xing Yu, Xinyu Lu, Ben He, Xianpei Han, Debing Zhang, Le Sun",,2024,2024-10-08
RL safety,Black-box safety (understand and control current model behaviour),Goal robustness,Black-box safety (understand and control current model behaviour) / Goal robustness / RL safety,https://arxiv.org/abs/2507.14843,The Invisible Leash: Why RLVR May or May Not Escape Its Origin,The Invisible Leash: Why RLVR May or May Not Escape Its Origin,"Fang Wu, Weihao Xuan, Ximing Lu, Mingjie Liu, Yi Dong, Zaid Harchaoui, Yejin Choi","University of Washington, Allen Institute for AI",2025,2025-07-20
RL safety,Black-box safety (understand and control current model behaviour),Goal robustness,Black-box safety (understand and control current model behaviour) / Goal robustness / RL safety,https://arxiv.org/abs/2510.21184,Reducing the Probability of Undesirable Outputs in Language Models Using Probabilistic Inference,Reducing the Probability of Undesirable Outputs in Language Models Using Probabilistic Inference,"Stephen Zhao, Aidan Li, Rob Brekelmans, Roger Grosse",,2025,2025-10-24
RL safety,Black-box safety (understand and control current model behaviour),Goal robustness,Black-box safety (understand and control current model behaviour) / Goal robustness / RL safety,https://arxiv.org/abs/2504.01871,Interpreting Emergent Planning in Model-Free Reinforcement Learning,Interpreting Emergent Planning in Model-Free Reinforcement Learning,"Thomas Bush, Stephen Chung, Usman Anwar, Adrià Garriga-Alonso, David Krueger",,2025,2025-04-02
RL safety,Black-box safety (understand and control current model behaviour),Goal robustness,Black-box safety (understand and control current model behaviour) / Goal robustness / RL safety,https://arxiv.org/abs/2507.10995,Misalignment From Treating Means as Ends,Misalignment from Treating Means as Ends,"Henrik Marklund, Alex Infanger, Benjamin Van Roy",,2025,2025-07-15
RL safety,Black-box safety (understand and control current model behaviour),Goal robustness,Black-box safety (understand and control current model behaviour) / Goal robustness / RL safety,https://lesswrong.com/posts/TCGgiJAinGgcMEByt/the-era-of-experience-has-an-unsolved-technical-alignment,"""The Era of Experience"" has an unsolved technical alignment problem","""The Era of Experience"" has an unsolved technical alignment problem",Steven Byrnes,,2025,2025-04-24
RL safety,Black-box safety (understand and control current model behaviour),Goal robustness,Black-box safety (understand and control current model behaviour) / Goal robustness / RL safety,https://lesswrong.com/posts/CpftMXCEnwqbWreHD/safety-cases-for-pessimism,Safety cases for Pessimism,Safety cases for Pessimism,Michael Cohen,,,
RL safety,Black-box safety (understand and control current model behaviour),Goal robustness,Black-box safety (understand and control current model behaviour) / Goal robustness / RL safety,https://www.lesswrong.com/posts/oxvnREntu82tffkYW/we-need-a-field-of-reward-function-design,We need a field of Reward Function Design,We need a field of Reward Function Design,Steven Byrnes,,2025,2025-12-08
"Assistance games, assistive agents",Black-box safety (understand and control current model behaviour),Goal robustness,"Black-box safety (understand and control current model behaviour) / Goal robustness / Assistance games, assistive agents",https://arxiv.org/pdf/2510.13709,Training LLM Agents to Empower Humans,Training LLM Agents to Empower Humans,"Evan Ellis, Vivek Myers, Jens Tuyls, Sergey Levine, Anca Dragan, Benjamin Eysenbach","UC Berkeley, Princeton University",2025,2025-10-16
"Assistance games, assistive agents",Black-box safety (understand and control current model behaviour),Goal robustness,"Black-box safety (understand and control current model behaviour) / Goal robustness / Assistance games, assistive agents",https://arxiv.org/abs/2509.05381,Murphys Laws of AI Alignment: Why the Gap Always Wins,Murphys Laws of AI Alignment: Why the Gap Always Wins,Madhava Gaikwad,,2025,2025-09-04
"Assistance games, assistive agents",Black-box safety (understand and control current model behaviour),Goal robustness,"Black-box safety (understand and control current model behaviour) / Goal robustness / Assistance games, assistive agents",https://arxiv.org/abs/2504.07091,AssistanceZero: Scalably Solving Assistance Games,AssistanceZero: Scalably Solving Assistance Games,"Cassidy Laidlaw, Eli Bronstein, Timothy Guo, Dylan Feng, Lukas Berglund, Justin Svegliato, Stuart Russell, Anca Dragan",UC Berkeley,2025,2025-04-09
"Assistance games, assistive agents",Black-box safety (understand and control current model behaviour),Goal robustness,"Black-box safety (understand and control current model behaviour) / Goal robustness / Assistance games, assistive agents",https://arxiv.org/abs/2412.17797,Observation Interference in Partially Observable Assistance Games,Observation Interference in Partially Observable Assistance Games,"Scott Emmons, Caspar Oesterheld, Vincent Conitzer, Stuart Russell",UC Berkeley,2024,2024-12-23
"Assistance games, assistive agents",Black-box safety (understand and control current model behaviour),Goal robustness,"Black-box safety (understand and control current model behaviour) / Goal robustness / Assistance games, assistive agents",https://arxiv.org/abs/2411.02623,Learning to Assist Humans without Inferring Rewards,Learning to Assist Humans without Inferring Rewards,"Vivek Myers, Evan Ellis, Sergey Levine, Benjamin Eysenbach, Anca Dragan","UC Berkeley, Google",2024,2024-11-04
Harm reduction for open weights,Black-box safety (understand and control current model behaviour),Goal robustness,Black-box safety (understand and control current model behaviour) / Goal robustness / Harm reduction for open weights,https://www.aisi.gov.uk/research/deep-ignorance-filtering-pretraining-data-builds-tamper-resistant-safeguards-into-open-weight-llms,Deep ignorance: Filtering pretraining data builds tamper-resistant safeguards into open-weight LLMs,Deep ignorance: Filtering pretraining data builds tamper-resistant safeguards into open-weight LLMs,"Kyle O'Brien, Stephen Casper, Quentin Anthony, Tomek Korbak, Robert Kirk, Xander Davies, Ishan Mishra, Geoffrey Irving, Yarin Gal, Stella Biderman","UK AI Security Institute, MIT, Eleuther AI",2025,2025-08-08
Harm reduction for open weights,Black-box safety (understand and control current model behaviour),Goal robustness,Black-box safety (understand and control current model behaviour) / Goal robustness / Harm reduction for open weights,https://arxiv.org/abs/2408.00761,Tamper-Resistant Safeguards for Open-Weight LLMs,Tamper-Resistant Safeguards for Open-Weight LLMs,"Rishub Tamirisa, Bhrugu Bharathi, Long Phan, Andy Zhou, Alice Gatti, Tarun Suresh, Maxwell Lin, Justin Wang, Rowan Wang, Ron Arel, Andy Zou, Dawn Song, Bo Li, Dan Hendrycks, Mantas Mazeika","UC Berkeley, UIUC, Center for AI Safety",2024,2024-08-01
Harm reduction for open weights,Black-box safety (understand and control current model behaviour),Goal robustness,Black-box safety (understand and control current model behaviour) / Goal robustness / Harm reduction for open weights,https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5705186,Open Technical Problems in Open-Weight AI Model Risk Management,Open Technical Problems in Open-Weight AI Model Risk Management,"Stephen Casper, Kyle O'Brien, Shayne Longpre, Elizabeth Seger, Kevin Klyman, Rishi Bommasani, Aniruddha Nrusimha, Ilia Shumailov, Sören Mindermann, Steven Basart, Frank Rudzicz, Kellin Pelrine, Avijit Ghosh, Andrew Strait, Robert Kirk, Dan Hendrycks, Peter Henderson, J. Zico Kolter, Geoffrey Irving, Yarin Gal, Yoshua Bengio, Dylan Hadfield-Menell","Massachusetts Institute of Technology, ERA Fellowship, Apple, Centre for the Governance of AI, Stanford University, Google DeepMind, Vector Institute for Artificial Intelligence, FAR.AI, Hugging Face, Center for AI Safety, Princeton University, Carnegie Mellon University, UK AI Security Institute, University of Oxford, University of Montreal",2025,2025-10-26
Harm reduction for open weights,Black-box safety (understand and control current model behaviour),Goal robustness,Black-box safety (understand and control current model behaviour) / Goal robustness / Harm reduction for open weights,https://arxiv.org/pdf/2506.22183,A Different Approach to AI Safety Proceedings from the Columbia Convening on AI Openness and Safety,A Different Approach to AI Safety: Proceedings from the Columbia Convening on Openness in Artificial Intelligence and AI Safety,"Camille François, Ludovic Péran, Ayah Bdeir, Nouha Dziri, Will Hawkins, Yacine Jernite, Sayash Kapoor, Juliet Shen, Heidy Khlaaf, Kevin Klyman, Nik Marda, Marie Pellat, Deb Raji, Divya Siddarth, Aviya Skowron, Joseph Spisak, Madhulika Srikumar, Victor Storchan, Audrey Tang, Jen Weedon","Columbia University, Academia, Industry, Civil Society, Government",2025,2025-06-27
Harm reduction for open weights,Black-box safety (understand and control current model behaviour),Goal robustness,Black-box safety (understand and control current model behaviour) / Goal robustness / Harm reduction for open weights,https://partnershiponai.org/wp-content/uploads/dlm_uploads/2024/07/open-foundation-model-risk-mitigation_rev3-1.pdf,Risk Mitigation Strategies for the Open Foundation Model Value Chain,Open Foundation Model Risk Mitigation,,Partnership on AI,2024,
"The ""Neglected Approaches"" Approach",Black-box safety (understand and control current model behaviour),Goal robustness,"Black-box safety (understand and control current model behaviour) / Goal robustness / The ""Neglected Approaches"" Approach",https://arxiv.org/abs/2412.16325,Learning Representations of Alignment,Towards Safe and Honest AI Agents with Neural Self-Other Overlap,"Marc Carauleanu, Michael Vaiana, Judd Rosenblatt, Cameron Berg, Diogo Schwerz de Lucena",,2024,2024-12-20
"The ""Neglected Approaches"" Approach",Black-box safety (understand and control current model behaviour),Goal robustness,"Black-box safety (understand and control current model behaviour) / Goal robustness / The ""Neglected Approaches"" Approach",https://arxiv.org/abs/2508.08492,Engineering Alignment: A Practical Framework for Prototyping 'Negative Tax' Solutions,Momentum Point-Perplexity Mechanics in Large Language Models,"Lorenzo Tomaz, Judd Rosenblatt, Thomas Berry Jones, Diogo Schwerz de Lucena",,2025,2025-08-11
"The ""Neglected Approaches"" Approach",Black-box safety (understand and control current model behaviour),Goal robustness,"Black-box safety (understand and control current model behaviour) / Goal robustness / The ""Neglected Approaches"" Approach",https://arxiv.org/abs/2510.24797,Self-Correction in Thought-Attractors: A Nudge Towards Alignment,Large Language Models Report Subjective Experience Under Self-Referential Processing,"Cameron Berg, Diogo de Lucena, Judd Rosenblatt",,2025,2025-10-27
Reverse engineering,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Reverse engineering,https://www.neuronpedia.org/graph/info,The Circuits Research Landscape,The Circuits Research Landscape: Results and Perspectives,"Jack Lindsey, Emmanuel Ameisen, Neel Nanda, Stepan Shabalin, Mateusz Piotrowski, Tom McGrath, Michael Hanna, Owen Lewis, Curt Tigges, Jack Merullo, Connor Watts, Gonçalo Paulo, Joshua Batson, Liv Gorton, Elana Simon, Max Loeffler, Callum McDougall, Johnny Lin","Anthropic, Google DeepMind, Goodfire AI, EleutherAI, Decode",2025,2025-08-01
Reverse engineering,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Reverse engineering,https://www.lesswrong.com/posts/roE7SHjFWEoMcGZKd/circuits-in-superposition-compressing-many-small-neural,Circuits in Superposition,Circuits in Superposition: Compressing many small neural networks into one,"Lucius Bushnaq, jake_mendel",Apollo Research,2024,2024-10-14
Reverse engineering,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Reverse engineering,https://www.google.com/url?q=https://www.lesswrong.com/posts/FWkZYQceEzL84tNej/circuits-in-superposition-2-now-with-less-wrong-math&sa=D&source=docs&ust=1765550772146255&usg=AOvVaw334Tyidx2keGCA9vGwQ9a-,2,Circuits in Superposition 2: Now With Less Wrong Math,,,,
Reverse engineering,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Reverse engineering,https://www.lesswrong.com/posts/ZxFchCFJFcgysYsT9/compressed-computation-is-probably-not-computation-in,Compressed Computation is (probably) not Computation in Superposition,Error: Content Unavailable (429: Too Many Requests),,,,
Reverse engineering,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Reverse engineering,https://arxiv.org/abs/2504.13151,MIB: A Mechanistic Interpretability Benchmark,MIB: A Mechanistic Interpretability Benchmark,"Aaron Mueller, Atticus Geiger, Sarah Wiegreffe, Dana Arad, Iván Arcuschin, Adam Belfki, Yik Siu Chan, Jaden Fiotto-Kaufman, Tal Haklay, Michael Hanna, Jing Huang, Rohan Gupta, Yaniv Nikankin, Hadas Orgad, Nikhil Prakash, Anja Reusch, Aruna Sankaranarayanan, Shun Shao, Alessandro Stolfo, Martin Tutek, Amir Zur, David Bau, Yonatan Belinkov",Multiple institutions,2025,2025-06-09
Reverse engineering,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Reverse engineering,https://arxiv.org/abs/2508.21258,RelP: Faithful and Efficient Circuit Discovery in Language Models via Relevance Patching,RelP: Faithful and Efficient Circuit Discovery in Language Models via Relevance Patching,"Farnoush Rezaei Jafari, Oliver Eberle, Ashkan Khakzar, Neel Nanda",,2025,2025-08-28
Reverse engineering,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Reverse engineering,https://arxiv.org/abs/2504.03022,The Dual-Route Model of Induction,The Dual-Route Model of Induction,"Sheridan Feucht, Eric Todd, Byron Wallace, David Bau",Northeastern University,2025,2025-04-03
Reverse engineering,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Reverse engineering,https://arxiv.org/abs/2504.18274,Structural Inference: Interpreting Small Language Models with Susceptibilities,Structural Inference: Interpreting Small Language Models with Susceptibilities,"Garrett Baker, George Wang, Jesse Hoogland, Daniel Murfet",,2025,2025-04-25
Reverse engineering,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Reverse engineering,https://openreview.net/forum?id=dEdS9ao8gN,Stochastic Parameter Decomposition,Stochastic Parameter Decomposition,"Dan Braun, Lucius Bushnaq, Lee Sharkey",,2025,2025-06-26
Reverse engineering,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Reverse engineering,https://arxiv.org/abs/2504.14379,The Geometry of Self-Verification in a Task-Specific Reasoning Model,The Geometry of Self-Verification in a Task-Specific Reasoning Model,"Andrew Lee, Lihao Sun, Chris Wendler, Fernanda Viégas, Martin Wattenberg",Google Research,2025,2025-04-19
Reverse engineering,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Reverse engineering,https://arxiv.org/abs/2502.01032,Converting MLPs into Polynomials in Closed Form,Converting MLPs into Polynomials in Closed Form,"Nora Belrose, Alice Rigg",,2025,2025-02-03
Reverse engineering,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Reverse engineering,https://arxiv.org/abs/2412.04614,Extractive Structures Learned in Pretraining Enable Generalization on Finetuned Facts,Extractive Structures Learned in Pretraining Enable Generalization on Finetuned Facts,"Jiahai Feng, Stuart Russell, Jacob Steinhardt",UC Berkeley,2024,2024-12-05
Reverse engineering,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Reverse engineering,https://arxiv.org/abs/2501.14926,Interpretability in Parameter Space: Minimizing Mechanistic Description Length with Attribution-based Parameter Decomposition,Interpretability in Parameter Space: Minimizing Mechanistic Description Length with Attribution-based Parameter Decomposition,"Dan Braun, Lucius Bushnaq, Stefan Heimersheim, Jake Mendel, Lee Sharkey",,2025,2025-01-24
Reverse engineering,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Reverse engineering,https://arxiv.org/abs/2504.00194,Identifying Sparsely Active Circuits Through Local Loss Landscape Decomposition,Identifying Sparsely Active Circuits Through Local Loss Landscape Decomposition,"Brianna Chrisman, Lucius Bushnaq, Lee Sharkey",,2025,2025-03-31
Reverse engineering,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Reverse engineering,https://arxiv.org/abs/2510.24256,From Memorization to Reasoning in the Spectrum of Loss Curvature,From Memorization to Reasoning in the Spectrum of Loss Curvature,"Jack Merullo, Srihita Vatsavaya, Lucius Bushnaq, Owen Lewis",,2025,2025-10-28
Reverse engineering,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Reverse engineering,https://arxiv.org/abs/2506.10887,Generalization or Hallucination? Understanding Out-of-Context Reasoning in Transformers,Generalization or Hallucination? Understanding Out-of-Context Reasoning in Transformers,"Yixiao Huang, Hanlin Zhu, Tianyu Guo, Jiantao Jiao, Somayeh Sojoudi, Michael I. Jordan, Stuart Russell, Song Mei",UC Berkeley,2025,2025-06-12
Reverse engineering,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Reverse engineering,https://arxiv.org/abs/2502.13913,How Do LLMs Perform Two-Hop Reasoning in Context?,How Do LLMs Perform Two-Hop Reasoning in Context?,"Tianyu Guo, Hanlin Zhu, Ruiqi Zhang, Jiantao Jiao, Song Mei, Michael I. Jordan, Stuart Russell",UC Berkeley,2025,2025-02-19
Reverse engineering,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Reverse engineering,https://arxiv.org/abs/2502.00921,Blink of an eye: a simple theory for feature localization in generative models,Blink of an eye: a simple theory for feature localization in generative models,"Marvin Li, Aayush Karan, Sitan Chen",,2025,2025-06-05
Reverse engineering,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Reverse engineering,https://arxiv.org/abs/2505.15811,On the creation of narrow AI: hierarchy and nonlocality of neural network skills,On the creation of narrow AI: hierarchy and nonlocality of neural network skills,"Eric J. Michaud, Asher Parker-Sartori, Max Tegmark",,2025,2025-05-21
Reverse engineering,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Reverse engineering,https://arxiv.org/pdf/2504.01871,Interpreting Emergent Planning in Model-Free Reinforcement Learning,Interpreting Emergent Planning in Model-Free Reinforcement Learning,"Thomas Bush, Stephen Chung, Usman Anwar, Adrià Garriga-Alonso, David Krueger",,2025,2025-04-02
Reverse engineering,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Reverse engineering,https://www.pnas.org/doi/10.1073/pnas.2406675122,Bridging the human–AI knowledge gap through concept discovery and transfer in AlphaZero,,,,,
Reverse engineering,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Reverse engineering,https://lesswrong.com/posts/DJAZHYjWxMrcd2na3/building-and-evaluating-alignment-auditing-agents,Building and evaluating alignment auditing agents,Building and evaluating alignment auditing agents,"Sam Marks, trentbrick, RowanWang, Sam Bowman, Euan Ong, Johannes Treutlein, evhub",Anthropic,2025,2025-07-24
Reverse engineering,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Reverse engineering,https://arxiv.org/abs/2505.20896,How Do Transformers Learn Variable Binding in Symbolic Programs?,How Do Transformers Learn Variable Binding in Symbolic Programs?,"Yiwei Wu, Atticus Geiger, Raphaël Millière",,2025,2025-05-27
Reverse engineering,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Reverse engineering,https://arxiv.org/abs/2509.14223,Fresh in memory: Training-order recency is linearly encoded in language model activations,Fresh in memory: Training-order recency is linearly encoded in language model activations,"Dmitrii Krasheninnikov, Richard E. Turner, David Krueger",University of Cambridge,2025,2025-09-17
Reverse engineering,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Reverse engineering,https://arxiv.org/abs/2505.14685,Language Models use Lookbacks to Track Beliefs,Language Models use Lookbacks to Track Beliefs,"Nikhil Prakash, Natalie Shapira, Arnab Sen Sharma, Christoph Riedl, Yonatan Belinkov, Tamar Rott Shaham, David Bau, Atticus Geiger","Northeastern University, Bar-Ilan University, Boston University",2025,2025-05-20
Reverse engineering,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Reverse engineering,https://arxiv.org/abs/2502.01954,Constrained belief updates explain geometric structures in transformer representations,Constrained belief updates explain geometric structures in transformer representations,"Mateusz Piotrowski, Paul M. Riechers, Daniel Filan, Adam S. Shai",,2025,2025-02-04
Reverse engineering,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Reverse engineering,https://arxiv.org/abs/2510.26784,LLMs Process Lists With General Filter Heads,LLMs Process Lists With General Filter Heads,"Arnab Sen Sharma, Giordano Rogers, Natalie Shapira, David Bau",Bau Lab,2025,2025-10-30
Reverse engineering,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Reverse engineering,https://arxiv.org/abs/2502.00873,Language Models Use Trigonometry to Do Addition,Language Models Use Trigonometry to Do Addition,"Subhash Kantamneni, Max Tegmark",,2025,2025-02-02
Reverse engineering,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Reverse engineering,https://arxiv.org/abs/2506.10138,Interpreting learned search: finding a transition model and value function in an RNN that plays Sokoban,Interpreting learned search: finding a transition model and value function in an RNN that plays Sokoban,"Mohammad Taufeeque, Aaron David Tucker, Adam Gleave, Adrià Garriga-Alonso",FAR AI,2025,2025-06-11
Reverse engineering,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Reverse engineering,https://arxiv.org/abs/2412.04703,Transformers Struggle to Learn to Search,Transformers Struggle to Learn to Search,"Abulhair Saparov, Srushti Pawar, Shreyas Pimpalgaonkar, Nitish Joshi, Richard Yuanzhe Pang, Vishakh Padmakumar, Seyed Mehran Kazemi, Najoung Kim, He He",,2024,2024-12-06
Reverse engineering,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Reverse engineering,https://arxiv.org/abs/2508.17456,"Adversarial Examples Are Not Bugs, They Are Superposition","Adversarial Examples Are Not Bugs, They Are Superposition","Liv Gorton, Owen Lewis",,2025,2025-08-24
Reverse engineering,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Reverse engineering,https://arxiv.org/abs/2505.13898,Do Language Models Use Their Depth Efficiently?,Do Language Models Use Their Depth Efficiently?,"Róbert Csordás, Christopher D. Manning, Christopher Potts",Stanford University,2025,2025-05-20
Reverse engineering,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Reverse engineering,https://openreview.net/forum?id=pXlmOmlHJZ,ICLR: In-Context Learning of Representations,ICLR: In-Context Learning of Representations,"Core Francisco Park, Andrew Lee, Ekdeep Singh Lubana, Yongyi Yang, Maya Okawa, Kento Nishi, Martin Wattenberg, Hidenori Tanaka",,2025,2025-01-22
Extracting latent knowledge,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Extracting latent knowledge,https://www.anthropic.com/research/auditing-hidden-objectives,Auditing language models for hidden objectives,Auditing language models for hidden objectives,,Anthropic,2025,2025-03-13
Extracting latent knowledge,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Extracting latent knowledge,https://arxiv.org/abs/2510.01070,Eliciting Secret Knowledge from Language Models,Eliciting Secret Knowledge from Language Models,"Bartosz Cywiński, Emil Ryd, Rowan Wang, Senthooran Rajamanoharan, Neel Nanda, Arthur Conmy, Samuel Marks",,2025,2025-10-01
Extracting latent knowledge,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Extracting latent knowledge,https://www.lesswrong.com/posts/7zhAwcBri7yupStKy/here-s-18-applications-of-deception-probes,Here's 18 Applications of Deception Probes,Here's 18 Applications of Deception Probes,"Cleo Nardo, Avi Parrack, jordine",,2025,2025-08-28
Extracting latent knowledge,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Extracting latent knowledge,https://arxiv.org/pdf/2505.14352,Towards eliciting latent knowledge from LLMs with mechanistic interpretability,Towards eliciting latent knowledge from LLMs with mechanistic interpretability,"Bartosz Cywiński, Emil Ryd, Senthooran Rajamanoharan, Neel Nanda",,2025,2025-05-20
Extracting latent knowledge,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Extracting latent knowledge,https://joss.theoj.org/papers/10.21105/joss.06511,CCS-Lib: A Python package to elicit latent knowledge from LLMs,CCS-Lib: A Python package to elicit latent knowledge from LLMs,"Walter Laurito, Nora Belrose, Alex Mallen, Kay Kozaronek, Fabien Roger, Christy Koh, James Chua, Jonathan Ng, Alexander Wan, Reagan Lee, Ben W., Kyle O'Brien, Augustas Macijauskas, Eric Mungai Kinuthia, Marius Pl, Waree Sethapun, Kaarel Hänni",Cadenza Labs,2025,2025-10-21
Extracting latent knowledge,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Extracting latent knowledge,https://arxiv.org/abs/2509.10625,No Answer Needed: Predicting LLM Answer Accuracy from Question-Only Linear Probes,No Answer Needed: Predicting LLM Answer Accuracy from Question-Only Linear Probes,"Iván Vicente Moreno Cencerrado, Arnau Padrés Masdemont, Anton Gonzalvez Hawthorne, David Demitri Africa, Lorenzo Pacchiardi",,2025,2025-09-12
Extracting latent knowledge,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Extracting latent knowledge,https://arxiv.org/abs/2506.04909,When Thinking LLMs Lie: Unveiling the Strategic Deception in Representations of Reasoning Models,When Thinking LLMs Lie: Unveiling the Strategic Deception in Representations of Reasoning Models,"Kai Wang, Yihao Zhang, Meng Sun",,2025,2025-06-05
Extracting latent knowledge,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Extracting latent knowledge,https://arxiv.org/abs/2508.19505,Caught in the Act: a mechanistic approach to detecting deception,Caught in the Act: a mechanistic approach to detecting deception,"Gerard Boxo, Ryan Socha, Daniel Yoo, Shivam Raval",,2025,2025-08-27
Extracting latent knowledge,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Extracting latent knowledge,https://arxiv.org/abs/2507.22149,When Truthful Representations Flip Under Deceptive Instructions?,When Truthful Representations Flip Under Deceptive Instructions?,"Xianxuan Long, Yao Fu, Runchao Li, Mu Sheng, Haotian Yu, Xiaotian Han, Pan Li",,2025,2025-07-29
Lie and deception detectors,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Lie and deception detectors,https://www.lesswrong.com/posts/9pGbTz6c78PGwJein/detecting-strategic-deception-using-linear-probes,Detecting Strategic Deception Using Linear Probes,Detecting Strategic Deception Using Linear Probes,"Nicholas Goldowsky-Dill, Bilal Chughtai, Stefan Heimersheim, Marius Hobbhahn",Apollo Research,2025,2025-02-06
Lie and deception detectors,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Lie and deception detectors,https://www.lesswrong.com/posts/pPEeMdgjpjHZWCDFw/white-box-control-at-uk-aisi-update-on-sandbagging,Whitebox detection of sandbagging model organisms,White Box Control at UK AISI - Update on Sandbagging Investigations,"Joseph Bloom, Jordan Taylor, Connor Kissane, Sid Black, Jacob Merizian, Alex Zelenka-Martin, Jacob Arbeid, Ben Millwood, Alan Cooney",UK AISI,2025,2025-07-10
Lie and deception detectors,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Lie and deception detectors,https://www.lesswrong.com/posts/eaEqAzGN3uJfpfGoc/trusted-monitoring-but-with-deception-probes,Benchmarking deception probes for trusted monitoring,"Trusted monitoring, but with deception probes.","Avi Parrack, StefanHex, Cleo Nardo",Stanford University,2024,2024-07-23
Lie and deception detectors,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Lie and deception detectors,https://www.lesswrong.com/posts/7zhAwcBri7yupStKy/18-applications-of-deception-probes,18 Applications of Deception Probes,Here's 18 Applications of Deception Probes,"Cleo Nardo, Avi Parrack, jordine",,2025,2025-08-28
Lie and deception detectors,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Lie and deception detectors,https://alignment.anthropic.com/2025/honesty-elicitation/,Evaluating honesty and lie detection techniques on a diverse suite of dishonest models,Evaluating honesty and lie detection techniques on a diverse suite of dishonest models,"Rowan Wang, Johannes Treutlein, Fabien Roger, Evan Hubinger, Sam Marks",Anthropic,2025,2025-11-25
Lie and deception detectors,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Lie and deception detectors,https://arxiv.org/abs/2508.19505,Caught in the Act: a mechanistic approach to detecting deception,Caught in the Act: a mechanistic approach to detecting deception,"Gerard Boxo, Ryan Socha, Daniel Yoo, Shivam Raval",,2025,2025-08-27
Lie and deception detectors,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Lie and deception detectors,https://arxiv.org/abs/2505.13787,Preference Learning with Lie Detectors can Induce Honesty or Evasion,Preference Learning with Lie Detectors can Induce Honesty or Evasion,"Chris Cundy, Adam Gleave",,2025,2025-05-20
Lie and deception detectors,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Lie and deception detectors,https://arxiv.org/abs/2506.10805,Detecting High-Stakes Interactions with Activation Probes,Detecting High-Stakes Interactions with Activation Probes,"Alex McKenzie, Urja Pawar, Phil Blandfort, William Bankes, David Krueger, Ekdeep Singh Lubana, Dmitrii Krasheninnikov",,2025,2025-06-12
Lie and deception detectors,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Lie and deception detectors,https://www.alignmentforum.org/posts/pPEeMdgjpjHZWCDFw/white-box-control-at-uk-aisi-update-on-sandbagging,White Box Control at UK AISI - Update on Sandbagging Investigations,White Box Control at UK AISI - Update on Sandbagging Investigations,"Joseph Bloom, Jordan Taylor, Connor Kissane, Sid Black, merizian, alexdzm, jacoba, Ben Millwood, Alan Cooney",UK AISI,2025,2025-07-10
Lie and deception detectors,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Lie and deception detectors,https://arxiv.org/html/2511.16035v1,Liars' Bench: Evaluating Lie Detectors for Language Models,Liars' Bench: Evaluating Lie Detectors for Language Models,"Kieron Kretschmar, Walter Laurito, Sharan Maiya, Samuel Marks","Cadenza Labs, Anthropic, FZI, University of Cambridge",2025,2025-11-20
Lie and deception detectors,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Lie and deception detectors,https://www.lesswrong.com/posts/gRc8KL2HLtKkFmNPr/among-us-a-sandbox-for-agentic-deception,Probes and SAEs do well on Among Us benchmark,Among Us: A Sandbox for Agentic Deception,,,,
Model diffing,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Model diffing,https://www.lesswrong.com/posts/xmpauEXEerzYcJKNm/what-we-learned-trying-to-diff-base-and-chat-models-and-why,What We Learned Trying to Diff Base and Chat Models (And Why It Matters),What We Learned Trying to Diff Base and Chat Models (And Why It Matters),"Clément Dumas, Julian Minder, Neel Nanda",MATS Program,2025,2025-06-30
Model diffing,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Model diffing,https://www.lesswrong.com/posts/srt6JXsRMtmqAJavD/open-source-replication-of-anthropic-s-crosscoder-paper-for,Open Source Replication of Anthropic's Crosscoder paper for model-diffing,Open Source Replication of Anthropic's Crosscoder paper for model-diffing,"Connor Kissane, robertzk, Arthur Conmy, Neel Nanda",,2024,2024-10-27
Model diffing,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Model diffing,https://openreview.net/forum?id=ZB84SvrZB8%20,Cross-Architecture Model Diffing with Crosscoders: Unsupervised Discovery of Differences Between LLMs,Error: Content Not Found,,,,
Model diffing,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Model diffing,https://www.goodfire.ai/research/model-diff-amplification#,Discovering Undesired Rare Behaviors via Model Diff Amplification,Discovering Undesired Rare Behaviors via Model Diff Amplification,"Santiago Aranguri, Thomas McGrath","Goodfire, NYU",2025,2025-08-21
Model diffing,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Model diffing,https://arxiv.org/abs/2504.02922,Overcoming Sparsity Artifacts in Crosscoders to Interpret Chat-Tuning,Overcoming Sparsity Artifacts in Crosscoders to Interpret Chat-Tuning,"Julian Minder, Clément Dumas, Caden Juang, Bilal Chugtai, Neel Nanda",,2025,2025-04-03
Model diffing,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Model diffing,https://arxiv.org/abs/2506.19823,Persona Features Control Emergent Misalignment,Persona Features Control Emergent Misalignment,"Miles Wang, Tom Dupré la Tour, Olivia Watkins, Alex Makelov, Ryan A. Chi, Samuel Miserendino, Jeffrey Wang, Achyuta Rajaram, Johannes Heidecke, Tejal Patwardhan, Dan Mossing",,2025,2025-06-24
Model diffing,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Model diffing,https://arxiv.org/abs/2510.13900,Narrow Finetuning Leaves Clearly Readable Traces in Activation Differences,Narrow Finetuning Leaves Clearly Readable Traces in Activation Differences,"Julian Minder, Clément Dumas, Stewart Slocum, Helena Casademunt, Cameron Holmes, Robert West, Neel Nanda",Google DeepMind,2025,2025-10-14
Model diffing,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Model diffing,https://transformer-circuits.pub/2025/crosscoder-diffing-update/index.html,Insights on Crosscoder Model Diffing,Insights on Crosscoder Model Diffing,"Siddharth Mishra-Sharma, Trenton Bricken, Jack Lindsey, Adam Jermyn, Jonathan Marcus, Kelley Rivoire, Christopher Olah, Thomas Henighan",Anthropic,,
Model diffing,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Model diffing,https://github.com/science-of-finetuning/diffing-toolkit%20,Diffing Toolkit: Model Comparison and Analysis Framework,diffing-toolkit (GitHub 404 Error),,,,
Sparse Coding,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Sparse Coding,https://arxiv.org/abs/2504.02922,Overcoming Sparsity Artifacts in Crosscoders to Interpret Chat-Tuning,Overcoming Sparsity Artifacts in Crosscoders to Interpret Chat-Tuning,"Julian Minder, Clément Dumas, Caden Juang, Bilal Chugtai, Neel Nanda",,2025,2025-04-03
Sparse Coding,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Sparse Coding,https://arxiv.org/abs/2411.14257,Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models,Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models,"Javier Ferrando, Oscar Obeso, Senthooran Rajamanoharan, Neel Nanda",,2024,2024-11-21
Sparse Coding,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Sparse Coding,https://transformer-circuits.pub/2025/attribution-graphs/methods.html,Circuit Tracing: Revealing Computational Graphs in Language Models,Circuit Tracing: Revealing Computational Graphs in Language Models,"Emmanuel Ameisen, Jack Lindsey, Adam Pearce, Wes Gurnee, Nicholas L. Turner, Brian Chen, Craig Citro, David Abrahams, Shan Carter, Basil Hosmer, Jonathan Marcus, Michael Sklar, Adly Templeton, Trenton Bricken, Callum McDougall, Hoagy Cunningham, Thomas Henighan, Adam Jermyn, Andy Jones, Andrew Persic, Zhenyi Qi, T. Ben Thompson, Sam Zimmerman, Kelley Rivoire, Thomas Conerly, Chris Olah, Joshua Batson",Anthropic,2025,2025-03-27
Sparse Coding,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Sparse Coding,https://arxiv.org/abs/2504.02821,Sparse Autoencoders Learn Monosemantic Features in Vision-Language Models,Sparse Autoencoders Learn Monosemantic Features in Vision-Language Models,"Mateusz Pach, Shyamgopal Karthik, Quentin Bouniot, Serge Belongie, Zeynep Akata",,2025,2025-04-03
Sparse Coding,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Sparse Coding,https://arxiv.org/abs/2503.18878,I Have Covered All the Bases Here: Interpreting Reasoning Features in Large Language Models via Sparse Autoencoders,I Have Covered All the Bases Here: Interpreting Reasoning Features in Large Language Models via Sparse Autoencoders,"Andrey Galichin, Alexey Dontsov, Polina Druzhinina, Anton Razzhigaev, Oleg Y. Rogov, Elena Tutubalina, Ivan Oseledets",AIRI Institute,2025,2025-03-24
Sparse Coding,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Sparse Coding,https://arxiv.org/abs/2502.04878,Sparse Autoencoders Do Not Find Canonical Units of Analysis,Sparse Autoencoders Do Not Find Canonical Units of Analysis,"Patrick Leask, Bart Bussmann, Michael Pearce, Joseph Bloom, Curt Tigges, Noura Al Moubayed, Lee Sharkey, Neel Nanda",,2025,2025-02-07
Sparse Coding,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Sparse Coding,https://arxiv.org/abs/2501.18823,Transcoders Beat Sparse Autoencoders for Interpretability,Transcoders Beat Sparse Autoencoders for Interpretability,"Gonçalo Paulo, Stepan Shabalin, Nora Belrose",,2025,2025-01-31
Sparse Coding,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Sparse Coding,https://arxiv.org/abs/2506.10920,Decomposing MLP Activations into Interpretable Features via Semi-Nonnegative Matrix Factorization,Decomposing MLP Activations into Interpretable Features via Semi-Nonnegative Matrix Factorization,"Or Shafran, Atticus Geiger, Mor Geva",,2025,2025-06-12
Sparse Coding,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Sparse Coding,https://arxiv.org/abs/2508.13650,CRISP: Persistent Concept Unlearning via Sparse Autoencoders,CRISP: Persistent Concept Unlearning via Sparse Autoencoders,"Tomer Ashuach, Dana Arad, Aaron Mueller, Martin Tutek, Yonatan Belinkov",,2025,2025-08-19
Sparse Coding,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Sparse Coding,https://arxiv.org/abs/2510.07775,The Unintended Trade-off of AI Alignment:Balancing Hallucination Mitigation and Safety in LLMs,The Unintended Trade-off of AI Alignment:Balancing Hallucination Mitigation and Safety in LLMs,"Omar Mahmoud, Ali Khalil, Buddhika Laknath Semage, Thommen George Karimpanal, Santu Rana",,2025,2025-10-09
Sparse Coding,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Sparse Coding,https://arxiv.org/abs/2504.13756,Scaling sparse feature circuit finding for in-context learning,Scaling sparse feature circuit finding for in-context learning,"Dmitrii Kharlapenko, Stepan Shabalin, Fazl Barez, Arthur Conmy, Neel Nanda",,2025,2025-04-18
Sparse Coding,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Sparse Coding,https://arxiv.org/abs/2503.17547,Learning Multi-Level Features with Matryoshka Sparse Autoencoders,Learning Multi-Level Features with Matryoshka Sparse Autoencoders,"Bart Bussmann, Noa Nabeshima, Adam Karvonen, Neel Nanda",,2025,2025-03-21
Sparse Coding,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Sparse Coding,https://arxiv.org/abs/2502.16681,Are Sparse Autoencoders Useful? A Case Study in Sparse Probing,Are Sparse Autoencoders Useful? A Case Study in Sparse Probing,"Subhash Kantamneni, Joshua Engels, Senthooran Rajamanoharan, Max Tegmark, Neel Nanda",,2025,2025-02-23
Sparse Coding,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Sparse Coding,https://arxiv.org/abs/2501.16615,Sparse Autoencoders Trained on the Same Data Learn Different Features,Sparse Autoencoders Trained on the Same Data Learn Different Features,"Gonçalo Paulo, Nora Belrose",,2025,2025-01-28
Sparse Coding,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Sparse Coding,https://arxiv.org/abs/2510.26202,What's In My Human Feedback? Learning Interpretable Descriptions of Preference Data,What's In My Human Feedback? Learning Interpretable Descriptions of Preference Data,"Rajiv Movva, Smitha Milli, Sewon Min, Emma Pierson",,2025,2025-10-30
Sparse Coding,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Sparse Coding,https://arxiv.org/abs/2511.01836,Priors in Time: Missing Inductive Biases for Language Model Interpretability,Priors in Time: Missing Inductive Biases for Language Model Interpretability,"Ekdeep Singh Lubana, Can Rager, Sai Sumedh R. Hindupur, Valerie Costa, Greta Tuckute, Oam Patel, Sonia Krishna Murthy, Thomas Fel, Daniel Wurgaft, Eric J. Bigelow, Johnny Lin, Demba Ba, Martin Wattenberg, Fernanda Viegas, Melanie Weber, Aaron Mueller",,2025,2025-11-03
Sparse Coding,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Sparse Coding,https://arxiv.org/abs/2505.17769,Inference-Time Decomposition of Activations (ITDA): A Scalable Approach to Interpreting Large Language Models,Inference-Time Decomposition of Activations (ITDA): A Scalable Approach to Interpreting Large Language Models,"Patrick Leask, Neel Nanda, Noura Al Moubayed",Independent,2025,2025-05-23
Sparse Coding,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Sparse Coding,https://arxiv.org/abs/2509.25596,Binary Sparse Coding for Interpretability,Binary Sparse Coding for Interpretability,"Lucia Quirke, Stepan Shabalin, Nora Belrose",,2025,2025-09-29
Sparse Coding,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Sparse Coding,https://lesswrong.com/posts/PkeB4TLxgaNnSmddg/scaling-sparse-feature-circuit-finding-to-gemma-9b,Scaling Sparse Feature Circuit Finding to Gemma 9B,Scaling Sparse Feature Circuit Finding to Gemma 9B,"Diego Caples, Jatin Nainani, CallumMcDougall, rrenaud",MATS Program,2025,2025-01-10
Sparse Coding,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Sparse Coding,https://arxiv.org/abs/2501.18838,Partially Rewriting a Transformer in Natural Language,Partially Rewriting a Transformer in Natural Language,"Gonçalo Paulo, Nora Belrose",,2025,2025-01-31
Sparse Coding,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Sparse Coding,https://arxiv.org/abs/2506.15679,"Dense SAE Latents Are Features, Not Bugs","Dense SAE Latents Are Features, Not Bugs","Xiaoqing Sun, Alessandro Stolfo, Joshua Engels, Ben Wu, Senthooran Rajamanoharan, Mrinmaya Sachan, Max Tegmark","MIT, ETH Zurich",2025,2025-06-18
Sparse Coding,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Sparse Coding,https://arxiv.org/abs/2411.18895,Evaluating Sparse Autoencoders on Targeted Concept Erasure Tasks,Evaluating Sparse Autoencoders on Targeted Concept Erasure Tasks,"Adam Karvonen, Can Rager, Samuel Marks, Neel Nanda",,2024,2024-11-28
Sparse Coding,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Sparse Coding,https://arxiv.org/abs/2507.08473,Evaluating SAE interpretability without explanations,Evaluating SAE interpretability without explanations,"Gonçalo Paulo, Nora Belrose",,2025,2025-07-11
Sparse Coding,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Sparse Coding,https://arxiv.org/abs/2504.08192,SAEs Can Improve Unlearning: Dynamic Sparse Autoencoder Guardrails for Precision Unlearning in LLMs,SAEs Can Improve Unlearning: Dynamic Sparse Autoencoder Guardrails for Precision Unlearning in LLMs,"Aashiq Muhamed, Jacopo Bonato, Mona Diab, Virginia Smith",,2025,2025-04-11
Sparse Coding,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Sparse Coding,https://arxiv.org/abs/2503.09532,SAEBench: A Comprehensive Benchmark for Sparse Autoencoders in Language Model Interpretability,SAEBench: A Comprehensive Benchmark for Sparse Autoencoders in Language Model Interpretability,"Adam Karvonen, Can Rager, Johnny Lin, Curt Tigges, Joseph Bloom, David Chanin, Yeu-Tong Lau, Eoin Farrell, Callum McDougall, Kola Ayonrinde, Demian Till, Matthew Wearden, Arthur Conmy, Samuel Marks, Neel Nanda",,2025,2025-06-04
Sparse Coding,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Sparse Coding,https://arxiv.org/abs/2505.20063,SAEs Are Good for Steering \-- If You Select the Right Features,SAEs Are Good for Steering -- If You Select the Right Features,"Dana Arad, Aaron Mueller, Yonatan Belinkov",,2025,2025-05-26
Sparse Coding,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Sparse Coding,https://arxiv.org/abs/2506.04706,Line of Sight: On Linear Representations in VLLMs,Line of Sight: On Linear Representations in VLLMs,"Achyuta Rajaram, Sarah Schwettmann, Jacob Andreas, Arthur Conmy","MIT, Independent",2025,2025-06-05
Sparse Coding,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Sparse Coding,https://arxiv.org/abs/2501.19406,Low-Rank Adapting Models for Sparse Autoencoders,Low-Rank Adapting Models for Sparse Autoencoders,"Matthew Chen, Joshua Engels, Max Tegmark",MIT,2025,2025-01-31
Sparse Coding,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Sparse Coding,https://arxiv.org/abs/2501.08319,Enhancing Automated Interpretability with Output-Centric Feature Descriptions,Enhancing Automated Interpretability with Output-Centric Feature Descriptions,"Yoav Gur-Arieh, Roy Mayan, Chen Agassy, Atticus Geiger, Mor Geva",,2025,2025-01-14
Sparse Coding,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Sparse Coding,https://arxiv.org/abs/2411.00743,Decoding Dark Matter: Specialized Sparse Autoencoders for Interpreting Rare Concepts in Foundation Models,Decoding Dark Matter: Specialized Sparse Autoencoders for Interpreting Rare Concepts in Foundation Models,"Aashiq Muhamed, Mona Diab, Virginia Smith",,2024,2024-11-01
Sparse Coding,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Sparse Coding,https://arxiv.org/abs/2411.01220,Enhancing Neural Network Interpretability with Feature-Aligned Sparse Autoencoders,Enhancing Neural Network Interpretability with Feature-Aligned Sparse Autoencoders,"Luke Marks, Alasdair Paren, David Krueger, Fazl Barez",,2024,2024-11-02
Sparse Coding,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Sparse Coding,https://arxiv.org/abs/2412.06410,BatchTopK Sparse Autoencoders,BatchTopK Sparse Autoencoders,"Bart Bussmann, Patrick Leask, Neel Nanda",,2024,2024-12-09
Sparse Coding,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Sparse Coding,https://arxiv.org/abs/2503.03730,Towards Understanding Distilled Reasoning Models: A Representational Approach,Towards Understanding Distilled Reasoning Models: A Representational Approach,"David D. Baek, Max Tegmark",MIT,2025,2025-03-05
Sparse Coding,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Sparse Coding,https://arxiv.org/abs/2509.02565,Understanding sparse autoencoder scaling in the presence of feature manifolds,Understanding sparse autoencoder scaling in the presence of feature manifolds,"Eric J. Michaud, Liv Gorton, Tom McGrath",,2025,2025-09-02
Sparse Coding,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Sparse Coding,https://arxiv.org/abs/2510.04128,Internal states before wait modulate reasoning patterns,Internal states before wait modulate reasoning patterns,"Dmitrii Troitskii, Koyena Pal, Chris Wendler, Callum Stuart McDougall, Neel Nanda",Google DeepMind,2025,2025-10-05
Sparse Coding,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Sparse Coding,https://arxiv.org/abs/2502.19964,Do Sparse Autoencoders Generalize? A Case Study of Answerability,Do Sparse Autoencoders Generalize? A Case Study of Answerability,"Lovis Heindrich, Philip Torr, Fazl Barez, Veronika Thost",,2025,2025-02-27
Sparse Coding,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Sparse Coding,https://arxiv.org/abs/2505.20254,Position: Mechanistic Interpretability Should Prioritize Feature Consistency in SAEs,Position: Mechanistic Interpretability Should Prioritize Feature Consistency in SAEs,"Xiangchen Song, Aashiq Muhamed, Yujia Zheng, Lingjing Kong, Zeyu Tang, Mona T. Diab, Virginia Smith, Kun Zhang",,2025,2025-05-26
Sparse Coding,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Sparse Coding,https://arxiv.org/abs/2506.11976,How Visual Representations Map to Language Feature Space in Multimodal LLMs,How Visual Representations Map to Language Feature Space in Multimodal LLMs,"Constantin Venhoff, Ashkan Khakzar, Sonia Joseph, Philip Torr, Neel Nanda","Google DeepMind, University of Oxford",2025,2025-06-13
Sparse Coding,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Sparse Coding,https://arxiv.org/abs/2504.19475,Prisma: An Open Source Toolkit for Mechanistic Interpretability in Vision and Video,Prisma: An Open Source Toolkit for Mechanistic Interpretability in Vision and Video,"Sonia Joseph, Praneet Suresh, Lorenz Hufe, Edward Stevinson, Robert Graham, Yash Vadi, Danilo Bzdok, Sebastian Lapuschkin, Lee Sharkey, Blake Aaron Richards","Apollo Research, Various Academic Institutions",2025,2025-04-28
Sparse Coding,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Sparse Coding,https://lesswrong.com/posts/6oF6pRr2FgjTmiHus/topological-data-analysis-and-mechanistic-interpretability,Topological Data Analysis and Mechanistic Interpretability,Topological Data Analysis and Mechanistic Interpretability,Gunnar Carlsson,,2025,2025-02-24
Sparse Coding,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Sparse Coding,https://arxiv.org/abs/2501.06346,Large Language Models Share Representations of Latent Grammatical Concepts Across Typologically Diverse Languages,Large Language Models Share Representations of Latent Grammatical Concepts Across Typologically Diverse Languages,"Jannik Brinkmann, Chris Wendler, Christian Bartelt, Aaron Mueller",,2025,2025-01-10
Sparse Coding,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Sparse Coding,https://arxiv.org/abs/2504.11695,Interpreting the linear structure of vision-language model embedding spaces,Interpreting the linear structure of vision-language model embedding spaces,"Isabel Papadimitriou, Huangyuan Su, Thomas Fel, Sham Kakade, Stephanie Gil",,2025,2025-04-16
Sparse Coding,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Sparse Coding,https://arxiv.org/abs/2505.24360,Interpreting Large Text-to-Image Diffusion Models with Dictionary Learning,Interpreting Large Text-to-Image Diffusion Models with Dictionary Learning,"Stepan Shabalin, Ayush Panda, Dmitrii Kharlapenko, Abdur Raheem Ali, Yixiong Hao, Arthur Conmy",,2025,2025-05-30
Sparse Coding,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Sparse Coding,https://cdn.openai.com/pdf/41df8f28-d4ef-43e9-aed2-823f9393e470/circuit-sparsity-paper.pdf,Weight-sparse transformers have interpretable circuits,Unknown - PDF content not accessible,,OpenAI,,
Causal Abstractions,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Causal Abstractions,https://arxiv.org/abs/2503.10894,HyperDAS: Towards Automating Mechanistic Interpretability with Hypernetworks,HyperDAS: Towards Automating Mechanistic Interpretability with Hypernetworks,"Jiuding Sun, Jing Huang, Sidharth Baskaran, Karel D'Oosterlinck, Christopher Potts, Michael Sklar, Atticus Geiger",,2025,2025-03-13
Causal Abstractions,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Causal Abstractions,https://arxiv.org/abs/2503.11429,Combining Causal Models for More Accurate Abstractions of Neural Networks,Combining Causal Models for More Accurate Abstractions of Neural Networks,"Theodora-Mara Pîslar, Sara Magliacane, Atticus Geiger",,2025,2025-03-14
Causal Abstractions,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Causal Abstractions,https://arxiv.org/abs/2508.11214,How Causal Abstraction Underpins Computational Explanation,How Causal Abstraction Underpins Computational Explanation,"Atticus Geiger, Jacqueline Harding, Thomas Icard",,2025,2025-08-15
Data attribution,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Data attribution,https://arxiv.org/abs/2510.12071,Influence Dynamics and Stagewise Data Attribution,Influence Dynamics and Stagewise Data Attribution,"Jin Hwa Lee, Matthew Smith, Maxwell Adam, Jesse Hoogland",,2025,2025-10-14
Data attribution,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Data attribution,https://arxiv.org/abs/2405.13954,What is Your Data Worth to GPT?,What is Your Data Worth to GPT? LLM-Scale Data Valuation with Influence Functions,"Sang Keun Choe, Hwijeen Ahn, Juhan Bae, Kewen Zhao, Minsoo Kang, Youngseog Chung, Adithya Pratapa, Willie Neiswanger, Emma Strubell, Teruko Mitamura, Jeff Schneider, Eduard Hovy, Roger Grosse, Eric Xing","Carnegie Mellon University, University of Toronto, Various Universities",2024,2024-05-22
Data attribution,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Data attribution,https://arxiv.org/abs/2502.11411,Detecting and Filtering Unsafe Training Data via Data Attribution with Denoised Representation,Detecting and Filtering Unsafe Training Data via Data Attribution with Denoised Representation,"Yijun Pan, Taiwei Shi, Jieyu Zhao, Jiaqi W. Ma",,2025,2025-02-17
Data attribution,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Data attribution,https://arxiv.org/abs/2507.14740,Better Training Data Attribution via Better Inverse Hessian-Vector Products,Better Training Data Attribution via Better Inverse Hessian-Vector Products,"Andrew Wang, Elisa Nguyen, Runshi Yang, Juhan Bae, Sheila A. McIlraith, Roger Grosse",,2025,2025-07-19
Data attribution,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Data attribution,https://arxiv.org/abs/2507.09424,DATE-LM: Benchmarking Data Attribution Evaluation for Large Language Models,DATE-LM: Benchmarking Data Attribution Evaluation for Large Language Models,"Cathy Jiao, Yijun Pan, Emily Xiao, Daisy Sheng, Niket Jain, Hanzhang Zhao, Ishita Dasgupta, Jiaqi W. Ma, Chenyan Xiong",,2025,2025-07-12
Data attribution,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Data attribution,https://arxiv.org/abs/2509.26544,Bayesian Influence Functions for Hessian-Free Data Attribution,Bayesian Influence Functions for Hessian-Free Data Attribution,"Philipp Alexander Kreer, Wilson Wu, Maxwell Adam, Zach Furman, Jesse Hoogland",,2025,2025-09-30
Data attribution,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Data attribution,https://arxiv.org/abs/2504.07096,OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training Tokens,OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training Tokens,"Jiacheng Liu, Taylor Blanton, Yanai Elazar, Sewon Min, YenSung Chen, Arnavi Chheda-Kothary, Huy Tran, Byron Bischoff, Eric Marsh, Michael Schmitz, Cassidy Trier, Aaron Sarnat, Jenna James, Jon Borchardt, Bailey Kuehl, Evie Cheng, Karen Farley, Sruthi Sreeram, Taira Anderson, David Albright, Carissa Schoenick, Luca Soldaini, Dirk Groeneveld, Rock Yuren Pang, Pang Wei Koh, Noah A. Smith, Sophie Lebrecht, Yejin Choi, Hannaneh Hajishirzi, Ali Farhadi, Jesse Dodge","Allen Institute for AI, University of Washington",2025,2025-04-09
Data attribution,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Data attribution,https://arxiv.org/abs/2502.05475,You Are What You Eat -- AI Alignment Requires Understanding How Data Shapes Structure and Generalisation,You Are What You Eat -- AI Alignment Requires Understanding How Data Shapes Structure and Generalisation,"Simon Pepin Lehalleur, Jesse Hoogland, Matthew Farrugia-Roberts, Susan Wei, Alexander Gietelink Oldenziel, George Wang, Liam Carroll, Daniel Murfet",,2025,2025-02-08
Data attribution,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Data attribution,https://arxiv.org/abs/2503.12072,Information-Guided Identification of Training Data Imprint in (Proprietary) Large Language Models,Information-Guided Identification of Training Data Imprint in (Proprietary) Large Language Models,"Abhilasha Ravichander, Jillian Fisher, Taylor Sorensen, Ximing Lu, Yuchen Lin, Maria Antoniak, Niloofar Mireshghallah, Chandra Bhagavatula, Yejin Choi","AI2, University of Washington",2025,2025-03-15
Data attribution,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Data attribution,https://arxiv.org/abs/2506.12965,Distributional Training Data Attribution: What do Influence Functions Sample?,Distributional Training Data Attribution: What do Influence Functions Sample?,"Bruno Mlodozeniec, Isaac Reid, Sam Power, David Krueger, Murat Erdogdu, Richard E. Turner, Roger Grosse",,2025,2025-06-15
Data attribution,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Data attribution,https://openreview.net/forum?id=sYK4yPDuT1,A Snapshot of Influence: A Local Data Attribution Framework for Online Reinforcement Learning,A Snapshot of Influence: A Local Data Attribution Framework for Online Reinforcement Learning,"Yuzheng Hu, Fan Wu, Haotian Ye, David Forsyth, James Zou, Nan Jiang, Jiaqi W. Ma, Han Zhao",,2025,2025-09-18
Data attribution,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Data attribution,https://arxiv.org/abs/2508.07297,Revisiting Data Attribution for Influence Functions,Revisiting Data Attribution for Influence Functions,"Hongbo Zhu, Angelo Cangelosi",,2025,2025-08-10
Pragmatic interpretability,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Pragmatic interpretability,https://www.lesswrong.com/posts/StENzDcD3kpfGJssR/a-pragmatic-vision-for-inter,A Pragmatic Vision for Interpretability,A Pragmatic Vision for Interpretability,"Neel Nanda, Josh Engels, Arthur Conmy, Senthooran Rajamanoharan, bilalchughtai, CallumMcDougall, János Kramár, lewis smith",Google DeepMind,2025,2025-12-01
Pragmatic interpretability,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Pragmatic interpretability,https://www.alignmentforum.org/posts/s9z4mgjtWTPpDLxFy/agentic-interpretability-a-strategy-against-gradual,Agentic Interpretability: A Strategy Against Gradual Disempowerment,Agentic Interpretability: A Strategy Against Gradual Disempowerment,"Been Kim, John Hewitt, Neel Nanda, Noah Fiedel, Oyvind Tafjord","Google DeepMind, Anthropic",2025,2025-06-17
Pragmatic interpretability,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Pragmatic interpretability,https://arxiv.org/abs/2503.10965,Auditing language models for hidden objectives,Auditing language models for hidden objectives,"Samuel Marks, Johannes Treutlein, Trenton Bricken, Jack Lindsey, Jonathan Marcus, Siddharth Mishra-Sharma, Daniel Ziegler, Emmanuel Ameisen, Joshua Batson, Tim Belonax, Samuel R. Bowman, Shan Carter, Brian Chen, Hoagy Cunningham, Carson Denison, Florian Dietz, Satvik Golechha, Akbir Khan, Jan Kirchner, Jan Leike, Austin Meek, Kei Nishimura-Gasparian, Euan Ong, Christopher Olah, Adam Pearce, Fabien Roger, Jeanne Salle, Andy Shih, Meg Tong, Drake Thomas, Kelley Rivoire, Adam Jermyn, Monte MacDiarmid, Tom Henighan, Evan Hubinger","Anthropic, Independent Researchers",2025,2025-03-14
Other interpretability,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Other interpretability,https://lesswrong.com/posts/KbFuuaBKRP7FcAADL/transformers-don-t-need-layernorm-at-inference-time,Transformers Don't Need LayerNorm at Inference Time: Implications for Interpretability,Transformers Don't Need LayerNorm at Inference Time: Implications for Interpretability,"submarat, Joachim Schaeffer, Luca Baroni, galvsk, StefanHex","MARS, SPAR",2025,2025-07-23
Other interpretability,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Other interpretability,https://arxiv.org/abs/2510.02334,Where Did It Go Wrong? Attributing Undesirable LLM Behaviors via Representation Gradient Tracing,Where Did It Go Wrong? Attributing Undesirable LLM Behaviors via Representation Gradient Tracing,"Zhe Li, Wei Zhao, Yige Li, Jun Sun",,2025,2025-09-26
Other interpretability,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Other interpretability,https://arxiv.org/abs/2501.16496,Open Problems in Mechanistic Interpretability,Open Problems in Mechanistic Interpretability,"Lee Sharkey, Bilal Chughtai, Joshua Batson, Jack Lindsey, Jeff Wu, Lucius Bushnaq, Nicholas Goldowsky-Dill, Stefan Heimersheim, Alejandro Ortega, Joseph Bloom, Stella Biderman, Adria Garriga-Alonso, Arthur Conmy, Neel Nanda, Jessica Rumbelow, Martin Wattenberg, Nandi Schoots, Joseph Miller, Eric J. Michaud, Stephen Casper, Max Tegmark, William Saunders, David Bau, Eric Todd, Atticus Geiger, Mor Geva, Jesse Hoogland, Daniel Murfet, Tom McGrath",Multiple research organizations and universities,2025,2025-01-27
Other interpretability,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Other interpretability,https://lesswrong.com/posts/u3ZysuXEjkyHhefrk/against-blanket-arguments-against-interpretability,Against blanket arguments against interpretability,Against blanket arguments against interpretability,Dmitry Vaintrob,,2025,2025-01-22
Other interpretability,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Other interpretability,https://lesswrong.com/posts/wkGmouy7JnTNtWAbc/opportunity-space-renormalization-for-ai-safety,Opportunity Space: Renormalization for AI Safety,Opportunity Space: Renormalization for AI Safety,"Lauren Greenspan, Dmitry Vaintrob, Lucas Teixeira",PIBBSS,2025,2025-03-31
Other interpretability,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Other interpretability,https://lesswrong.com/posts/y5cYisQ2QHiSbQbhk/prospects-for-alignment-automation-interpretability-case,Prospects for Alignment Automation: Interpretability Case Study,Prospects for Alignment Automation: Interpretability Case Study,"Jacob Pfau, Geoffrey Irving","UK AISI, Google DeepMind",2025,2025-03-21
Other interpretability,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Other interpretability,https://www.darioamodei.com/post/the-urgency-of-interpretability,The Urgency of Interpretability,The Urgency of Interpretability,Dario Amodei,Anthropic,2025,
Other interpretability,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Other interpretability,https://arxiv.org/abs/2503.17514,Language Models May Verbatim Complete Text They Were Not Explicitly Trained On,Language Models May Verbatim Complete Text They Were Not Explicitly Trained On,"Ken Ziyu Liu, Christopher A. Choquette-Choo, Matthew Jagielski, Peter Kairouz, Sanmi Koyejo, Percy Liang, Nicolas Papernot","Google, Stanford University, University of Toronto",2025,2025-03-21
Other interpretability,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Other interpretability,https://lesswrong.com/posts/wGRnzCFcowRCrpX4Y/downstream-applications-as-validation-of-interpretability,Downstream applications as validation of interpretability progress,Downstream applications as validation of interpretability progress,Sam Marks,,2025,2025-03-31
Other interpretability,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Other interpretability,https://lesswrong.com/posts/DqaoPNqhQhwBFqWue/principles-for-picking-practical-interpretability-projects,Principles for Picking Practical Interpretability Projects,Principles for Picking Practical Interpretability Projects,Sam Marks,Anthropic,2025,2025-07-15
Other interpretability,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Other interpretability,https://arxiv.org/abs/2501.15740,Propositional Interpretability in Artificial Intelligence,Propositional Interpretability in Artificial Intelligence,David J. Chalmers,,2025,2025-01-27
Other interpretability,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Other interpretability,https://arxiv.org/abs/2412.02104,Explainable and Interpretable Multimodal Large Language Models: A Comprehensive Survey,Explainable and Interpretable Multimodal Large Language Models: A Comprehensive Survey,"Yunkai Dang, Kaichen Huang, Jiahao Huo, Yibo Yan, Sirui Huang, Dongrui Liu, Mengxi Gao, Jie Zhang, Chen Qian, Kun Wang, Yong Liu, Jing Shao, Hui Xiong, Xuming Hu",,2024,2024-12-03
Other interpretability,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Other interpretability,https://lesswrong.com/posts/sjr66DBEgyogAbfdf/renormalization-redux-qft-techniques-for-ai-interpretability,Renormalization Redux: QFT Techniques for AI Interpretability,Renormalization Redux: QFT Techniques for AI Interpretability,"Lauren Greenspan, Dmitry Vaintrob",,2025,2025-01-18
Other interpretability,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Other interpretability,https://lesswrong.com/posts/qRnupMmFG7dxQTTYh/the-strange-science-of-interpretability-recent-papers-and-a,The Strange Science of Interpretability: Recent Papers and a Reading List for the Philosophy of Interpretability,The Strange Science of Interpretability: Recent Papers and a Reading List for the Philosophy of Interpretability,"Kola Ayonrinde, Louis Jaburi",,2025,2025-08-17
Other interpretability,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Other interpretability,https://arxiv.org/abs/2506.02300,Through a Steerable Lens: Magnifying Neural Network Interpretability via Phase-Based Extrapolation,Through a Steerable Lens: Magnifying Neural Network Interpretability via Phase-Based Extrapolation,"Farzaneh Mahdisoltani, Saeed Mahdisoltani, Roger B. Grosse, David J. Fleet",,2025,2025-06-02
Other interpretability,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Other interpretability,https://lesswrong.com/posts/MDWGcNHkZ3NPEzcnp/call-for-collaboration-renormalization-for-ai-safety,Call for Collaboration: Renormalization for AI safety,Call for Collaboration: Renormalization for AI safety,Lauren Greenspan,PIBBSS,2025,2025-03-31
Other interpretability,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Other interpretability,https://arxiv.org/abs/2505.15811,On the creation of narrow AI: hierarchy and nonlocality of neural network skills,On the creation of narrow AI: hierarchy and nonlocality of neural network skills,"Eric J. Michaud, Asher Parker-Sartori, Max Tegmark",,2025,2025-05-21
Other interpretability,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Other interpretability,https://arxiv.org/abs/2502.01628,Harmonic Loss Trains Interpretable AI Models,Harmonic Loss Trains Interpretable AI Models,"David D. Baek, Ziming Liu, Riya Tyagi, Max Tegmark",,2025,2025-02-03
Other interpretability,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Other interpretability,https://arxiv.org/abs/2505.12546,Extracting memorized pieces of (copyrighted) books from open-weight language models,Extracting memorized pieces of (copyrighted) books from open-weight language models,"A. Feder Cooper, Aaron Gokaslan, Ahmed Ahmed, Amy B. Cyphert, Christopher De Sa, Mark A. Lemley, Daniel E. Ho, Percy Liang","Stanford University, Cornell University",2025,2025-05-18
Learning dynamics and developmental interpretability,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Learning dynamics and developmental interpretability,https://www.lesswrong.com/posts/2MX2bXreTtntB85Zy/from-slt-to-ait-nn-generalisation-out-of-distribution,From SLT to AIT: NN Generalisation Out of Distribution,From SLT to AIT: NN Generalisation Out of Distribution,,,,
Learning dynamics and developmental interpretability,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Learning dynamics and developmental interpretability,https://www.lesswrong.com/posts/ZSQaT2yxNNZ3eLxRd/understanding-and-controlling-llm-generalization,Understanding and Controlling LLM Generalization,Understanding and Controlling LLM Generalization,Daniel Tan,,2025,2025-11-14
Learning dynamics and developmental interpretability,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Learning dynamics and developmental interpretability,https://lesswrong.com/posts/J7CyENFYXPxXQpsnD/slt-for-ai-safety,SLT for AI Safety,SLT for AI Safety,Jesse Hoogland,Timaeus,2025,2025-07-01
Learning dynamics and developmental interpretability,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Learning dynamics and developmental interpretability,https://arxiv.org/abs/2509.05291,Crosscoding Through Time: Tracking Emergence & Consolidation Of Linguistic Representations Throughout LLM Pretraining,Crosscoding Through Time: Tracking Emergence & Consolidation Of Linguistic Representations Throughout LLM Pretraining,"Deniz Bayazit, Aaron Mueller, Antoine Bosselut",,2025,2025-09-05
Learning dynamics and developmental interpretability,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Learning dynamics and developmental interpretability,https://arxiv.org/abs/2508.15841,A Review of Developmental Interpretability in Large Language Models,A Review of Developmental Interpretability in Large Language Models,Ihor Kendiukhov,,2025,2025-08-19
Learning dynamics and developmental interpretability,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Learning dynamics and developmental interpretability,https://arxiv.org/abs/2501.17745,Dynamics of Transient Structure in In-Context Linear Regression Transformers,Dynamics of Transient Structure in In-Context Linear Regression Transformers,"Liam Carroll, Jesse Hoogland, Matthew Farrugia-Roberts, Daniel Murfet",,2025,2025-01-29
Learning dynamics and developmental interpretability,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Learning dynamics and developmental interpretability,https://openreview.net/forum?id=KUFH0n1BIM,"Learning Coefficients, Fractals, and Trees in Parameter Space","Learning Coefficients, Fractals, and Trees in Parameter Space","Max Hennick, Matthias Dellago",,2025,2025-06-23
Learning dynamics and developmental interpretability,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Learning dynamics and developmental interpretability,https://arxiv.org/abs/2509.23365,Emergence of Superposition: Unveiling the Training Dynamics of Chain of Continuous Thought,Emergence of Superposition: Unveiling the Training Dynamics of Chain of Continuous Thought,"Hanlin Zhu, Shibo Hao, Zhiting Hu, Jiantao Jiao, Stuart Russell, Yuandong Tian","UC Berkeley, Meta",2025,2025-09-27
Learning dynamics and developmental interpretability,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Learning dynamics and developmental interpretability,https://arxiv.org/abs/2510.12077,Compressibility Measures Complexity: Minimum Description Length Meets Singular Learning Theory,Compressibility Measures Complexity: Minimum Description Length Meets Singular Learning Theory,"Einar Urdshals, Edmund Lau, Jesse Hoogland, Stan van Wingerden, Daniel Murfet","Timaeus, University of Melbourne",2025,2025-10-14
Learning dynamics and developmental interpretability,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Learning dynamics and developmental interpretability,https://openreview.net/forum?id=Td37oOfmmz,Programs as Singularities,Programs as Singularities,"Daniel Murfet, William Troiani",,2025,2025-06-20
Learning dynamics and developmental interpretability,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Learning dynamics and developmental interpretability,https://arxiv.org/abs/2411.07681,What Do Learning Dynamics Reveal About Generalization in LLM Reasoning?,What Do Learning Dynamics Reveal About Generalization in LLM Reasoning?,"Katie Kang, Amrith Setlur, Dibya Ghosh, Jacob Steinhardt, Claire Tomlin, Sergey Levine, Aviral Kumar",UC Berkeley,2024,2024-11-12
Learning dynamics and developmental interpretability,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Learning dynamics and developmental interpretability,https://lesswrong.com/posts/HFcriD29cw3E5QLCR/selective-regularization-for-alignment-focused,Selective regularization for alignment-focused representation engineering,Selective regularization for alignment-focused representation engineering,Sandy Fraser,,2025,2025-05-20
Learning dynamics and developmental interpretability,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Learning dynamics and developmental interpretability,https://arxiv.org/abs/2504.18048,Modes of Sequence Models and Learning Coefficients,Modes of Sequence Models and Learning Coefficients,"Zhongtian Chen, Daniel Murfet",,2025,2025-04-25
Learning dynamics and developmental interpretability,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Learning dynamics and developmental interpretability,https://arxiv.org/abs/2506.18777,Programming by Backprop: LLMs Acquire Reusable Algorithmic Abstractions During Code Training,Programming by Backprop: LLMs Acquire Reusable Algorithmic Abstractions During Code Training,"Jonathan Cook, Silvia Sapora, Arash Ahmadian, Akbir Khan, Tim Rocktaschel, Jakob Foerster, Laura Ruis",,2025,2025-06-23
Representation structure and geometry,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Representation structure and geometry,https://arxiv.org/pdf/2504.14379,The Geometry of Self-Verification in a Task-Specific Reasoning Model,The Geometry of Self-Verification in a Task-Specific Reasoning Model,"Andrew Lee, Lihao Sun, Chris Wendler, Fernanda Viégas, Martin Wattenberg",Google Research,2025,2025-04-19
Representation structure and geometry,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Representation structure and geometry,http://arxiv.org/abs/2511.06739,Rank-1 LoRAs Encode Interpretable Reasoning Signals,Rank-1 LoRAs Encode Interpretable Reasoning Signals,"Jake Ward, Paul Riechers, Adam Shai",,2025,2025-11-10
Representation structure and geometry,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Representation structure and geometry,https://arxiv.org/abs/2502.17420,The Geometry of Refusal in Large Language Models: Concept Cones and Representational Independence,The Geometry of Refusal in Large Language Models: Concept Cones and Representational Independence,"Tom Wollschläger, Jannes Elstner, Simon Geisler, Vincent Cohen-Addad, Stephan Günnemann, Johannes Gasteiger",Technical University of Munich,2025,2025-02-24
Representation structure and geometry,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Representation structure and geometry,https://arxiv.org/abs/2508.00331,Embryology of a Language Model,Embryology of a Language Model,"George Wang, Garrett Baker, Andrew Gordon, Daniel Murfet",,2025,2025-08-01
Representation structure and geometry,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Representation structure and geometry,https://arxiv.org/abs/2502.01954,Constrained belief updates explain geometric structures in transformer representations,Constrained belief updates explain geometric structures in transformer representations,"Mateusz Piotrowski, Paul M. Riechers, Daniel Filan, Adam S. Shai",,2025,2025-02-04
Representation structure and geometry,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Representation structure and geometry,https://arxiv.org/abs/2503.21073,Shared Global and Local Geometry of Language Model Embeddings,Shared Global and Local Geometry of Language Model Embeddings,"Andrew Lee, Melanie Weber, Fernanda Viégas, Martin Wattenberg",,2025,2025-03-27
Representation structure and geometry,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Representation structure and geometry,https://arxiv.org/abs/2507.07432,Neural networks leverage nominally quantum and post-quantum representations,Neural networks leverage nominally quantum and post-quantum representations,"Paul M. Riechers, Thomas J. Elliott, Adam S. Shai",,2025,2025-07-10
Representation structure and geometry,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Representation structure and geometry,https://arxiv.org/abs/2509.23024,Tracing the Representation Geometry of Language Models from Pretraining to Post-training,Tracing the Representation Geometry of Language Models from Pretraining to Post-training,"Melody Zixuan Li, Kumar Krishna Agrawal, Arna Ghosh, Komal Kumar Teru, Adam Santoro, Guillaume Lajoie, Blake A. Richards",Unknown,2025,2025-09-27
Representation structure and geometry,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Representation structure and geometry,https://arxiv.org/abs/2510.26745,Deep sequence models tend to memorize geometrically; it is unclear why,Deep sequence models tend to memorize geometrically; it is unclear why,"Shahriar Noroozizadeh, Vaishnavh Nagarajan, Elan Rosenfeld, Sanjiv Kumar",,2025,2025-10-30
Representation structure and geometry,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Representation structure and geometry,https://arxiv.org/abs/2505.22785,Navigating the Latent Space Dynamics of Neural Models,Navigating the Latent Space Dynamics of Neural Models,"Marco Fumero, Luca Moschella, Emanuele Rodolà, Francesco Locatello",,2025,2025-05-28
Representation structure and geometry,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Representation structure and geometry,https://arxiv.org/abs/2505.11692,The Geometry of ReLU Networks through the ReLU Transition Graph,The Geometry of ReLU Networks through the ReLU Transition Graph,Sahil Rajesh Dhayalkar,,2025,2025-05-16
Representation structure and geometry,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Representation structure and geometry,https://arxiv.org/abs/2506.01599,Connecting Neural Models Latent Geometries with Relative Geodesic Representations,Connecting Neural Models Latent Geometries with Relative Geodesic Representations,"Hanlin Yu, Berfin Inal, Georgios Arvanitidis, Soren Hauberg, Francesco Locatello, Marco Fumero",,2025,2025-06-02
Representation structure and geometry,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Representation structure and geometry,https://arxiv.org/abs/2505.18373,Next-token pretraining implies in-context learning,Next-token pretraining implies in-context learning,"Paul M. Riechers, Henry R. Bigelow, Eric A. Alt, Adam Shai",,2025,2025-05-23
Human inductive biases,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Human inductive biases,https://www.nature.com/articles/s41586-025-09631-6,Aligning machine and human visual representations across abstraction levels,Aligning machine and human visual representations across abstraction levels,"Lukas Muttenthaler, Klaus Greff, Frieda Born, Bernhard Spitzer, Simon Kornblith, Michael C. Mozer, Klaus-Robert Müller, Thomas Unterthiner, Andrew K. Lampinen","Google DeepMind, Technische Universität Berlin, BIFOLD, Max Planck Institute for Human Cognitive and Brain Sciences, Max Planck Institute for Human Development, TUD Dresden University of Technology, Anthropic, Korea University, Max Planck Institute for Informatics",2025,2025-11-12
Human inductive biases,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Human inductive biases,https://arxiv.org/html/2505.21731v1,Deep Reinforcement Learning Agents are not even close to Human Intelligence,,,,,
Human inductive biases,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Human inductive biases,https://arxiv.org/html/2503.02976v2#S3,Teaching AI to Handle Exceptions: Supervised Fine-tuning with Human-aligned Judgment,Teaching AI to Handle Exceptions: Supervised Fine-tuning with Human-aligned Judgment,"Matthew DosSantos DiSorbo, Harang Ju, Sinan Aral","Harvard Business School, Johns Hopkins University, MIT Sloan School of Management",2025,2025-03-XX
Human inductive biases,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Human inductive biases,https://docs.google.com/document/d/1fl7LE8AN7mLJ6uFcPuFCzatp0zCIYvjRIjQRgHPAkSE/edit?tab=t.0,HIBP Human Inductive Bias Project Plan,HIBP Human Inductive Bias Project Plan,Félix Dorn,,,
Human inductive biases,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Human inductive biases,https://arxiv.org/abs/2505.14204,Beginning with You: Perceptual-Initialization Improves Vision-Language Representation and Alignment,Beginning with You: Perceptual-Initialization Improves Vision-Language Representation and Alignment,"Yang Hu, Runchen Wang, Stephen Chong Zhao, Xuhui Zhan, Do Hun Kim, Mark Wallace, David A. Tovar",,2025,2025-05-20
Human inductive biases,White-box safety (i.e. Interpretability),,White-box safety (i.e. Interpretability) / Human inductive biases,https://arxiv.org/abs/2509.04445,Towards Cognitively-Faithful Decision-Making Models to Improve AI Alignment,Towards Cognitively-Faithful Decision-Making Models to Improve AI Alignment,"Cyrus Cousins, Vijay Keswani, Vincent Conitzer, Hoda Heidari, Jana Schaich Borg, Walter Sinnott-Armstrong","Carnegie Mellon University, Duke University",2025,2025-09-04
Monitoring concepts,White-box safety (i.e. Interpretability),Concept-based interpretability,White-box safety (i.e. Interpretability) / Concept-based interpretability / Monitoring concepts,https://arxiv.org/abs/2506.11618,Convergent Linear Representations of Emergent Misalignment,Convergent Linear Representations of Emergent Misalignment,"Anna Soligo, Edward Turner, Senthooran Rajamanoharan, Neel Nanda",Google DeepMind,2025,2025-06-20
Monitoring concepts,White-box safety (i.e. Interpretability),Concept-based interpretability,White-box safety (i.e. Interpretability) / Concept-based interpretability / Monitoring concepts,https://arxiv.org/abs/2502.03407,Detecting Strategic Deception Using Linear Probes,Detecting Strategic Deception Using Linear Probes,"Nicholas Goldowsky-Dill, Bilal Chughtai, Stefan Heimersheim, Marius Hobbhahn",Apollo Research,2025,2025-02-05
Monitoring concepts,White-box safety (i.e. Interpretability),Concept-based interpretability,White-box safety (i.e. Interpretability) / Concept-based interpretability / Monitoring concepts,https://arxiv.org/abs/2502.03708,Toward universal steering and monitoring of AI models,Toward universal steering and monitoring of AI models,"Daniel Beaglehole, Adityanarayanan Radhakrishnan, Enric Boix-Adserà, Mikhail Belkin",,2025,2025-05-28
Monitoring concepts,White-box safety (i.e. Interpretability),Concept-based interpretability,White-box safety (i.e. Interpretability) / Concept-based interpretability / Monitoring concepts,https://arxiv.org/abs/2506.07326,Reward Model Interpretability via Optimal and Pessimal Tokens,Reward Model Interpretability via Optimal and Pessimal Tokens,"Brian Christian, Hannah Rose Kirk, Jessica A.F. Thompson, Christopher Summerfield, Tsvetomira Dumbalska",,2025,2025-06-08
Monitoring concepts,White-box safety (i.e. Interpretability),Concept-based interpretability,White-box safety (i.e. Interpretability) / Concept-based interpretability / Monitoring concepts,https://arxiv.org/abs/2502.17420,The Geometry of Refusal in Large Language Models: Concept Cones and Representational Independence,The Geometry of Refusal in Large Language Models: Concept Cones and Representational Independence,"Tom Wollschläger, Jannes Elstner, Simon Geisler, Vincent Cohen-Addad, Stephan Günnemann, Johannes Gasteiger",Technical University of Munich,2025,2025-02-24
Monitoring concepts,White-box safety (i.e. Interpretability),Concept-based interpretability,White-box safety (i.e. Interpretability) / Concept-based interpretability / Monitoring concepts,https://alignment.anthropic.com/2025/cheap-monitors,Cost-Effective Constitutional Classifiers via Representation Re-use,Cost-Effective Constitutional Classifiers via Representation Re-use,"Hoagy Cunningham, Alwin Peng, Jerry Wei, Euan Ong, Fabien Roger, Linda Petrini, Misha Wagner, Vladimir Mikulik, Mrinank Sharma",Anthropic,2025,
Monitoring concepts,White-box safety (i.e. Interpretability),Concept-based interpretability,White-box safety (i.e. Interpretability) / Concept-based interpretability / Monitoring concepts,https://arxiv.org/abs/2411.09003,Refusal in LLMs is an Affine Function,Refusal in LLMs is an Affine Function,"Thomas Marshall, Adam Scherlis, Nora Belrose",EleutherAI,2024,2024-11-13
Monitoring concepts,White-box safety (i.e. Interpretability),Concept-based interpretability,White-box safety (i.e. Interpretability) / Concept-based interpretability / Monitoring concepts,https://www.alignmentforum.org/posts/pPEeMdgjpjHZWCDFw/white-box-control-at-uk-aisi-update-on-sandbagging,White Box Control at UK AISI - Update on Sandbagging Investigations,White Box Control at UK AISI - Update on Sandbagging Investigations,"Joseph Bloom, Jordan Taylor, Connor Kissane, Sid Black, merizian, alexdzm, jacoba, Ben Millwood, Alan Cooney",UK AISI,2025,2025-07-10
Monitoring concepts,White-box safety (i.e. Interpretability),Concept-based interpretability,White-box safety (i.e. Interpretability) / Concept-based interpretability / Monitoring concepts,https://lesswrong.com/posts/7zhAwcBri7yupStKy/here-s-18-applications-of-deception-probes,Here's 18 Applications of Deception Probes,Here's 18 Applications of Deception Probes,"Cleo Nardo, Avi Parrack, jordine",,2025,2025-08-28
Monitoring concepts,White-box safety (i.e. Interpretability),Concept-based interpretability,White-box safety (i.e. Interpretability) / Concept-based interpretability / Monitoring concepts,https://arxiv.org/abs/2508.05625,How Do LLMs Persuade? Linear Probes Can Uncover Persuasion Dynamics in Multi-Turn Conversations,How Do LLMs Persuade? Linear Probes Can Uncover Persuasion Dynamics in Multi-Turn Conversations,"Brandon Jaipersaud, David Krueger, Ekdeep Singh Lubana",,2025,2025-08-07
Monitoring concepts,White-box safety (i.e. Interpretability),Concept-based interpretability,White-box safety (i.e. Interpretability) / Concept-based interpretability / Monitoring concepts,https://arxiv.org/abs/2509.26238,Beyond Linear Probes: Dynamic Safety Monitoring for Language Models,Beyond Linear Probes: Dynamic Safety Monitoring for Language Models,"James Oldfield, Philip Torr, Ioannis Patras, Adel Bibi, Fazl Barez",,2025,2025-09-30
Activation engineering,White-box safety (i.e. Interpretability),Concept-based interpretability,White-box safety (i.e. Interpretability) / Concept-based interpretability / Activation engineering,https://lesswrong.com/posts/6aXe9nipTgwK5LxaP/do-safety-relevant-llm-steering-vectors-optimized-on-a,Do safety-relevant LLM steering vectors optimized on a single example generalize?,Do safety-relevant LLM steering vectors optimized on a single example generalize?,Jacob Dunefsky,Yale University,2025,2025-02-28
Activation engineering,White-box safety (i.e. Interpretability),Concept-based interpretability,White-box safety (i.e. Interpretability) / Concept-based interpretability / Activation engineering,https://arxiv.org/abs/2510.12672,Keep Calm and Avoid Harmful Content: Concept Alignment and Latent Manipulation Towards Safer Answers,Keep Calm and Avoid Harmful Content: Concept Alignment and Latent Manipulation Towards Safer Answers,"Ruben Belo, Marta Guimaraes, Claudia Soares",,2025,2025-10-14
Activation engineering,White-box safety (i.e. Interpretability),Concept-based interpretability,White-box safety (i.e. Interpretability) / Concept-based interpretability / Activation engineering,https://arxiv.org/abs/2503.04429,Activation Space Interventions Can Be Transferred Between Large Language Models,Activation Space Interventions Can Be Transferred Between Large Language Models,"Narmeen Oozeer, Dhruv Nathawani, Nirmalendu Prakash, Michael Lan, Abir Harrasse, Amirali Abdullah",,2025,2025-03-06
Activation engineering,White-box safety (i.e. Interpretability),Concept-based interpretability,White-box safety (i.e. Interpretability) / Concept-based interpretability / Activation engineering,https://arxiv.org/abs/2506.03292,HyperSteer: Activation Steering at Scale with Hypernetworks,HyperSteer: Activation Steering at Scale with Hypernetworks,"Jiuding Sun, Sidharth Baskaran, Zhengxuan Wu, Michael Sklar, Christopher Potts, Atticus Geiger",,2025,2025-06-03
Activation engineering,White-box safety (i.e. Interpretability),Concept-based interpretability,White-box safety (i.e. Interpretability) / Concept-based interpretability / Activation engineering,https://arxiv.org/abs/2510.20487,Steering Evaluation-Aware Language Models to Act Like They Are Deployed,Steering Evaluation-Aware Language Models to Act Like They Are Deployed,"Tim Tian Hua, Andrew Qin, Samuel Marks, Neel Nanda",,2025,2025-10-23
Activation engineering,White-box safety (i.e. Interpretability),Concept-based interpretability,White-box safety (i.e. Interpretability) / Concept-based interpretability / Activation engineering,https://arxiv.org/abs/2507.16795,Steering Out-of-Distribution Generalization with Concept Ablation Fine-Tuning,Steering Out-of-Distribution Generalization with Concept Ablation Fine-Tuning,"Helena Casademunt, Caden Juang, Adam Karvonen, Samuel Marks, Senthooran Rajamanoharan, Neel Nanda",Google DeepMind,2025,2025-07-22
Activation engineering,White-box safety (i.e. Interpretability),Concept-based interpretability,White-box safety (i.e. Interpretability) / Concept-based interpretability / Activation engineering,https://arxiv.org/abs/2507.21509,Persona Vectors: Monitoring and Controlling Character Traits in Language Models,Persona Vectors: Monitoring and Controlling Character Traits in Language Models,"Runjin Chen, Andy Arditi, Henry Sleight, Owain Evans, Jack Lindsey",,2025,2025-07-29
Activation engineering,White-box safety (i.e. Interpretability),Concept-based interpretability,White-box safety (i.e. Interpretability) / Concept-based interpretability / Activation engineering,https://arxiv.org/abs/2503.00177,Steering Large Language Model Activations in Sparse Spaces,Steering Large Language Model Activations in Sparse Spaces,"Reza Bayat, Ali Rahimi-Kalahroudi, Mohammad Pezeshki, Sarath Chandar, Pascal Vincent",,2025,2025-02-28
Activation engineering,White-box safety (i.e. Interpretability),Concept-based interpretability,White-box safety (i.e. Interpretability) / Concept-based interpretability / Activation engineering,https://arxiv.org/abs/2411.02193,Improving Steering Vectors by Targeting Sparse Autoencoder Features,Improving Steering Vectors by Targeting Sparse Autoencoder Features,"Sviatoslav Chalnev, Matthew Siu, Arthur Conmy",,2024,2024-11-04
Activation engineering,White-box safety (i.e. Interpretability),Concept-based interpretability,White-box safety (i.e. Interpretability) / Concept-based interpretability / Activation engineering,https://arxiv.org/abs/2506.18167,Understanding Reasoning in Thinking Language Models via Steering Vectors,Understanding Reasoning in Thinking Language Models via Steering Vectors,"Constantin Venhoff, Iván Arcuschin, Philip Torr, Arthur Conmy, Neel Nanda","University of Oxford, Anthropic",2025,2025-06-22
Activation engineering,White-box safety (i.e. Interpretability),Concept-based interpretability,White-box safety (i.e. Interpretability) / Concept-based interpretability / Activation engineering,https://lesswrong.com/posts/kcKnKHTHycHeRhcHF/one-shot-steering-vectors-cause-emergent-misalignment-too,"One-shot steering vectors cause emergent misalignment, too","One-shot steering vectors cause emergent misalignment, too",Jacob Dunefsky,,2025,2025-04-14
Activation engineering,White-box safety (i.e. Interpretability),Concept-based interpretability,White-box safety (i.e. Interpretability) / Concept-based interpretability / Activation engineering,https://arxiv.org/abs/2411.02461,Enhancing Multiple Dimensions of Trustworthiness in LLMs via Sparse Activation Control,Enhancing Multiple Dimensions of Trustworthiness in LLMs via Sparse Activation Control,"Yuxin Xiao, Chaoqun Wan, Yonggang Zhang, Wenxiao Wang, Binbin Lin, Xiaofei He, Xu Shen, Jieping Ye",,2024,2024-11-04
Activation engineering,White-box safety (i.e. Interpretability),Concept-based interpretability,White-box safety (i.e. Interpretability) / Concept-based interpretability / Activation engineering,https://arxiv.org/abs/2411.07213,Comparing Bottom-Up and Top-Down Steering Approaches on In-Context Learning Tasks,Comparing Bottom-Up and Top-Down Steering Approaches on In-Context Learning Tasks,"Madeline Brumley, Joe Kwon, David Krueger, Dmitrii Krasheninnikov, Usman Anwar",,2024,2024-11-11
Activation engineering,White-box safety (i.e. Interpretability),Concept-based interpretability,White-box safety (i.e. Interpretability) / Concept-based interpretability / Activation engineering,https://arxiv.org/abs/2502.19649,"Taxonomy, Opportunities, and Challenges of Representation Engineering for Large Language Models","Taxonomy, Opportunities, and Challenges of Representation Engineering for Large Language Models","Jan Wehner, Sahar Abdelnabi, Daniel Tan, David Krueger, Mario Fritz","University of Cambridge, CISPA Helmholtz Center for Information Security",2025,2025-02-27
Activation engineering,White-box safety (i.e. Interpretability),Concept-based interpretability,White-box safety (i.e. Interpretability) / Concept-based interpretability / Activation engineering,https://arxiv.org/abs/2506.10922,Robustly Improving LLM Fairness in Realistic Settings via Interpretability,Robustly Improving LLM Fairness in Realistic Settings via Interpretability,"Adam Karvonen, Samuel Marks",,2025,2025-06-12
Guaranteed-Safe AI,Safety by construction,,Safety by construction / Guaranteed-Safe AI,https://manifund.org/projects/safeplanbench-evaluating-a-guaranteed-safe-ai-approach-for-llm-based-agents,SafePlanBench: evaluating a Guaranteed Safe AI Approach for LLM-based Agents,SafePlanBench: evaluating a Guaranteed Safe AI Approach for LLM-based Agents,"Agustín Martinez Suñé, Tan Zhi Xuan","PIBBSS, MIT, University of Oxford",,
Guaranteed-Safe AI,Safety by construction,,Safety by construction / Guaranteed-Safe AI,https://lesswrong.com/posts/CCT7Qc8rSeRs7r5GL/beliefs-about-formal-methods-and-ai-safety,Beliefs about formal methods and AI safety,Beliefs about formal methods and AI safety,Quinn Dougherty,,2025,2025-10-23
Guaranteed-Safe AI,Safety by construction,,Safety by construction / Guaranteed-Safe AI,https://arxiv.org/abs/2506.22492,Report on NSF Workshop on Science of Safe AI,Report on NSF Workshop on Science of Safe AI,"Rajeev Alur, Greg Durrett, Hadas Kress-Gazit, Corina Păsăreanu, René Vidal","NSF SLES Program, University of Pennsylvania",2025,2025-06-24
Guaranteed-Safe AI,Safety by construction,,Safety by construction / Guaranteed-Safe AI,https://arxiv.org/abs/2509.22908,A benchmark for vericoding: formally verified program synthesis,A benchmark for vericoding: formally verified program synthesis,"Sergiu Bursuc, Theodore Ehrenborg, Shaowei Lin, Lacramioara Astefanoaei, Ionel Emilian Chiosa, Jure Kukovec, Alok Singh, Oliver Butterley, Adem Bizid, Quinn Dougherty, Miranda Zhao, Max Tan, Max Tegmark","Beneficial AI Foundation, MIT",2025,2025-09-26
Guaranteed-Safe AI,Safety by construction,,Safety by construction / Guaranteed-Safe AI,https://atlascomputing.org/ai-assisted-fv-toolchain.pdf,"A Toolchain for AI-Assisted Code Specification, Synthesis and Verification",AI-Assisted Formal Verification Toolchain,,Atlas Computing,,
Scientist AI,Safety by construction,,Safety by construction / Scientist AI,https://arxiv.org/abs/2502.15657,Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI Offer a Safer Path?,Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI Offer a Safer Path?,"Yoshua Bengio, Michael Cohen, Damiano Fornasiere, Joumana Ghosn, Pietro Greiner, Matt MacDermott, Sören Mindermann, Adam Oberman, Jesse Richardson, Oliver Richardson, Marc-Antoine Rondeau, Pierre-Luc St-Charles, David Williams-King","Mila, Independent",2025,2025-02-21
Scientist AI,Safety by construction,,Safety by construction / Scientist AI,https://arxiv.org/abs/2509.08713,"The More You Automate, the Less You See: Hidden Pitfalls of AI Scientist Systems","The More You Automate, the Less You See: Hidden Pitfalls of AI Scientist Systems","Ziming Luo, Atoosa Kasirzadeh, Nihar B. Shah",,2025,2025-09-10
Brainlike-AGI Safety,Safety by construction,,Safety by construction / Brainlike-AGI Safety,https://www.lesswrong.com/posts/grgb2ipxQf2wzNDEG/perils-of-under-vs-over-sculpting-agi-desires,Perils of Under vs Over-sculpting AGI Desires,Perils of Under vs Over-sculpting AGI Desires,,,,
Brainlike-AGI Safety,Safety by construction,,Safety by construction / Brainlike-AGI Safety,https://lesswrong.com/posts/JrTk2pbqp7BFwPAKw/reward-button-alignment,Reward button alignment,Reward button alignment,Steven Byrnes,Astera Institute,2025,2025-05-22
Brainlike-AGI Safety,Safety by construction,,Safety by construction / Brainlike-AGI Safety,https://www.lesswrong.com/posts/cus5CGmLrjBRgcPSF/system-2-alignment-deliberation-review-and-thought,"System 2 Alignment: Deliberation, Review, and Thought Management","System 2 Alignment: Deliberation, Review, and Thought Management",Seth Herd,,2025,2025-02-13
Brainlike-AGI Safety,Safety by construction,,Safety by construction / Brainlike-AGI Safety,https://elicit.com/blog/system-2-learning,Against RL: The Case for System 2 Learning,Against RL: The Case for System 2 Learning,Andreas Stuhlmüller,Elicit,2025,2025-01-30
Brainlike-AGI Safety,Safety by construction,,Safety by construction / Brainlike-AGI Safety,https://www.lesswrong.com/posts/yew6zFWAKG4AGs3Wk/foom-and-doom-1-brain-in-a-box-in-a-basement,Foom and Doom 1: Brain in a Box in a Basement,Foom and Doom 1: Brain in a Box in a Basement,,,,
Brainlike-AGI Safety,Safety by construction,,Safety by construction / Brainlike-AGI Safety,https://www.lesswrong.com/posts/bnnKGSCHJghAvqPjS/foom-and-doom-2-technical-alignment-is-hard,Foom and Doom 2: Technical Alignment is Hard,Foom and Doom 2: Technical Alignment is Hard,,,,
Weak-to-strong generalization,Make AI solve it,,Make AI solve it / Weak-to-strong generalization,https://arxiv.org/abs/2504.18530,Scaling Laws For Scalable Oversight,Scaling Laws For Scalable Oversight,"Joshua Engels, David D. Baek, Subhash Kantamneni, Max Tegmark",MIT,2025,2025-04-25
Weak-to-strong generalization,Make AI solve it,,Make AI solve it / Weak-to-strong generalization,https://arxiv.org/abs/2502.04313,Great Models Think Alike and this Undermines AI Oversight,Great Models Think Alike and this Undermines AI Oversight,"Shashwat Goel, Joschka Struber, Ilze Amanda Auzina, Karuna K Chandra, Ponnurangam Kumaraguru, Douwe Kiela, Ameya Prabhu, Matthias Bethge, Jonas Geiping",,2025,2025-02-06
Weak-to-strong generalization,Make AI solve it,,Make AI solve it / Weak-to-strong generalization,https://arxiv.org/abs/2501.13124,Debate Helps Weak-to-Strong Generalization,Debate Helps Weak-to-Strong Generalization,"Hao Lang, Fei Huang, Yongbin Li",,2025,2025-01-21
Weak-to-strong generalization,Make AI solve it,,Make AI solve it / Weak-to-strong generalization,https://openreview.net/forum?id=RwYdLgj1S6,Understanding the Capabilities and Limitations of Weak-to-Strong Generalization,Understanding the Capabilities and Limitations of Weak-to-Strong Generalization,"Wei Yao, Wenkai Yang, Ziqiao Wang, Yankai Lin, Yong Liu",,2025,2025-03-08
Supervising AIs improving AIs,Make AI solve it,,Make AI solve it / Supervising AIs improving AIs,https://saif.org/research/bare-minimum-mitigations-for-autonomous-ai-development/,Bare Minimum Mitigations for Autonomous AI Development,Bare Minimum Mitigations for Autonomous AI Development,"Joshua Clymer, Isabella Duan, Chris Cundy, Yawen Duan, Fynn Heide, Chaochao Lu, Sören Mindermann, Conor McGurk, Xudong Pan, Saad Siddiqui, Jingren Wang, Min Yang, Xianyuan Zhan",Safe AI Forum,2025,2025-04-22
Supervising AIs improving AIs,Make AI solve it,,Make AI solve it / Supervising AIs improving AIs,https://www.alignmentforum.org/posts/EgRJtwQurNzz8CEfJ/dodging-systematic-human-errors-in-scalable-oversight,Dodging systematic human errors in scalable oversight,Dodging systematic human errors in scalable oversight,Geoffrey Irving,UK AISI,2025,2025-05-14
Supervising AIs improving AIs,Make AI solve it,,Make AI solve it / Supervising AIs improving AIs,https://arxiv.org/abs/2504.18530,Scaling Laws for Scalable Oversight,Scaling Laws For Scalable Oversight,"Joshua Engels, David D. Baek, Subhash Kantamneni, Max Tegmark",MIT,2025,2025-04-25
Supervising AIs improving AIs,Make AI solve it,,Make AI solve it / Supervising AIs improving AIs,https://arxiv.org/abs/2412.08897,Neural Interactive Proofs,Neural Interactive Proofs,"Lewis Hammond, Sam Adam-Day",,2024,2024-12-12
Supervising AIs improving AIs,Make AI solve it,,Make AI solve it / Supervising AIs improving AIs,https://arxiv.org/abs/2502.21262,Modeling Human Beliefs about AI Behavior for Scalable Oversight,Modeling Human Beliefs about AI Behavior for Scalable Oversight,"Leon Lang, Patrick Forré",,2025,2025-02-28
Supervising AIs improving AIs,Make AI solve it,,Make AI solve it / Supervising AIs improving AIs,https://arxiv.org/abs/2502.04675,Scalable Oversight for Superhuman AI via Recursive Self-Critiquing,Scalable Oversight for Superhuman AI via Recursive Self-Critiquing,"Xueru Wen, Jie Lou, Xinyu Lu, Junjie Yang, Yanjiang Liu, Yaojie Lu, Debing Zhang, Xing Yu",,2025,2025-02-07
Supervising AIs improving AIs,Make AI solve it,,Make AI solve it / Supervising AIs improving AIs,https://lesswrong.com/posts/TQbptN7F4ijPnQRLy/video-and-transcript-of-talk-on-automating-alignment,Video and transcript of talk on automating alignment research,Video and transcript of talk on automating alignment research,Joe Carlsmith,Anthropic,2025,2025-04-30
Supervising AIs improving AIs,Make AI solve it,,Make AI solve it / Supervising AIs improving AIs,https://lesswrong.com/posts/PhgEKkB4cwYjwpGxb/maintaining-alignment-during-rsi-as-a-feedback-control,Maintaining Alignment during RSI as a Feedback Control Problem,Maintaining Alignment during RSI as a Feedback Control Problem,beren,,2025,2025-03-02
AI explanations of AIs,Make AI solve it,,Make AI solve it / AI explanations of AIs,https://transluce.org/jailbreaking-frontier-models,Automatically Jailbreaking Frontier Language Models with Investigator Agents,,,,,
AI explanations of AIs,Make AI solve it,,Make AI solve it / AI explanations of AIs,https://transluce.org/pathological-behaviors,Surfacing Pathological Behaviors in Language Models,,,,,
AI explanations of AIs,Make AI solve it,,Make AI solve it / AI explanations of AIs,https://transluce.org/investigating-o3-truthfulness,Investigating truthfulness in a pre-release o3 model,Investigating truthfulness in a pre-release o3 model,"Neil Chowdhury, Daniel Johnson, Vincent Huang, Jacob Steinhardt, Sarah Schwettmann",Transluce,2025,2025-04-16
AI explanations of AIs,Make AI solve it,,Make AI solve it / AI explanations of AIs,https://transluce.org/neuron-circuits,Neuron circuits,Language Model Circuits Are Sparse in the Neuron Basis,"Aryaman Arora, Zhengxuan Wu, Jacob Steinhardt, Sarah Schwettmann",Transluce,2025,2025-11-20
AI explanations of AIs,Make AI solve it,,Make AI solve it / AI explanations of AIs,https://transluce.org/introducing-docent,Docent: A system for analyzing and intervening on agent behavior,Introducing Docent,"Kevin Meng, Vincent Huang, Jacob Steinhardt, Sarah Schwettmann",Transluce,2025,2025-03-24
Debate,Make AI solve it,,Make AI solve it / Debate,https://www.lesswrong.com/s/NdovveRcyfxgMoujf,UK AISI Alignment Team: Debate Sequence,UK AISI Alignment Team: Debate Sequence,Benjamin Hilton,UK AI Safety Institute,2025,2025-05-07
Debate,Make AI solve it,,Make AI solve it / Debate,https://lesswrong.com/posts/8XHBaugB5S3r27MG9/prover-estimator-debate-a-new-scalable-oversight-protocol,Prover-Estimator Debate: A New Scalable Oversight Protocol,Prover-Estimator Debate: A New Scalable Oversight Protocol,"Jonah Brown-Cohen, Geoffrey Irving",UK AISI,2025,2025-06-17
Debate,Make AI solve it,,Make AI solve it / Debate,https://arxiv.org/abs/2506.02175,AI Debate Aids Assessment of Controversial Claims,AI Debate Aids Assessment of Controversial Claims,"Salman Rahman, Sheriff Issaka, Ashima Suvarna, Genglin Liu, James Shiffer, Jaeyoung Lee, Md Rizwan Parvez, Hamid Palangi, Shi Feng, Nanyun Peng, Yejin Choi, Julian Michael, Liwei Jiang, Saadia Gabriel","University of Washington, Microsoft Research, UCLA, NYU",2025,2025-06-02
Debate,Make AI solve it,,Make AI solve it / Debate,https://arxiv.org/abs/2505.03989,An alignment safety case sketch based on debate,An alignment safety case sketch based on debate,"Marie Davidsen Buhl, Jacob Pfau, Benjamin Hilton, Geoffrey Irving",Google DeepMind,2025,2025-05-23
Debate,Make AI solve it,,Make AI solve it / Debate,https://arxiv.org/abs/2509.00091,Ensemble Debates with Local Large Language Models for AI Alignment,Ensemble Debates with Local Large Language Models for AI Alignment,Ephraiem Sarabamoun,,2025,2025-08-27
Debate,Make AI solve it,,Make AI solve it / Debate,https://www.andrew.cmu.edu/user/coesterh/LMCA_dataset.pdf,A dataset of rated conceptual arguments,LMCA Dataset,,Carnegie Mellon University,,
LLM introspection training,Make AI solve it,,Make AI solve it / LLM introspection training,https://arxiv.org/abs/2511.08579,Training Language Models to Explain Their Own Computations,Training Language Models to Explain Their Own Computations,"Belinda Z. Li, Zifan Carl Guo, Vincent Huang, Jacob Steinhardt, Jacob Andreas","UC Berkeley, MIT",2025,2025-11-11
LLM introspection training,Make AI solve it,,Make AI solve it / LLM introspection training,https://transformer-circuits.pub/2025/introspection/index.html,Emergent Introspective Awareness,Emergent Introspective Awareness in Large Language Models,Jack Lindsey,Anthropic,2025,2025-10-29
Agent foundations,Theory,,Theory / Agent foundations,https://www.arxiv.org/pdf/2508.16245,Limit-Computable Grains of Truth for Arbitrary Computable Extensive-Form (Un)Known Games,Limit-Computable Grains of Truth for Arbitrary Computable Extensive-Form (Un)Known Games,"Cole Wyeth, Marcus Hutter, Jan Leike, Jessica Taylor","Independent, DeepMind, MIRI",2025,2025-08-22
Agent foundations,Theory,,Theory / Agent foundations,https://uaiasi.com/blog-posts/,UAIASI,Blog Posts – Universal Algorithmic Intelligence,Cole Wyeth,Universal Algorithmic Intelligence,2025,
Agent foundations,Theory,,Theory / Agent foundations,https://www.lesswrong.com/posts/EyvJvYEFzDv5kGoiG/clarifying-wisdom-foundational-topics-for-aligned-ais-to,"Clarifying ""wisdom"": Foundational topics for aligned AIs to prioritize before irreversible decisions",Unable to access: 429 Too Many Requests,,,,
Agent foundations,Theory,,Theory / Agent foundations,https://www.lesswrong.com/posts/Dt4DuCCok3Xv5HEnG/agent-foundations-not-really-math-not-really-science,"Agent foundations: not really math, not really science","Agent foundations: not really math, not really science",Alex_Altair,Dovetail Research,2025,2025-08-17
Agent foundations,Theory,,Theory / Agent foundations,https://link.springer.com/article/10.1007/s11098-025-02296-x,Off-switching not guaranteed,Off-switching not guaranteed,Sven Neth,University of Pittsburgh,2025,2025-02-26
Agent foundations,Theory,,Theory / Agent foundations,https://openreview.net/forum?id=tlkYPU3FlX,Formalizing Embeddedness Failures in Universal Artificial Intelligence,Formalizing Embeddedness Failures in Universal Artificial Intelligence,"Cole Wyeth, Marcus Hutter",,2025,2025-07-01
Agent foundations,Theory,,Theory / Agent foundations,https://lesswrong.com/posts/nuDJNyG5XLQjtvaeg/is-alignment-reducible-to-becoming-more-coherent,Is alignment reducible to becoming more coherent?,Is alignment reducible to becoming more coherent?,Cole Wyeth,,2025,2025-04-22
Agent foundations,Theory,,Theory / Agent foundations,https://lesswrong.com/posts/dHNKtQ3vTBxTfTPxu/what-is-the-alignment-problem,What Is The Alignment Problem?,What Is The Alignment Problem?,johnswentworth,,2025,2025-01-16
Agent foundations,Theory,,Theory / Agent foundations,https://openreview.net/pdf?id=Rf1CeGPA22,Good old fashioned decision theory,Unable to extract - PDF content not accessible,,,,
Agent foundations,Theory,,Theory / Agent foundations,https://www.lesswrong.com/posts/ApfjBbqzSu4aZoLSe/report-and-retrospective-on-the-dovetail-fellowship,Report & retrospective on the Dovetail fellowship,Report & retrospective on the Dovetail fellowship,Alex Altair,Dovetail Research,2025,2025-03-14
Tiling agents,Theory,,Theory / Tiling agents,https://www.lesswrong.com/posts/akuMwu8SkmQSdospi/working-through-a-small-tiling-result,Working through a small tiling result,Working through a small tiling result,James Payor,,2024,2024-05-13
Tiling agents,Theory,,Theory / Tiling agents,https://openreview.net/forum?id=Rf1CeGPA22,Communication & Trust,Communication & Trust,Abram Demski,,2025,2025-07-09
Tiling agents,Theory,,Theory / Tiling agents,https://lesswrong.com/posts/PhgEKkB4cwYjwpGxb/maintaining-alignment-during-rsi-as-a-feedback-control,Maintaining Alignment during RSI as a Feedback Control Problem,Maintaining Alignment during RSI as a Feedback Control Problem,beren,,2025,2025-03-02
Tiling agents,Theory,,Theory / Tiling agents,https://static1.squarespace.com/static/663d1233249bce4815fe8753/t/68067a6f5d5fb0745642d5b1/1745255023842/Understanding+Trust+-+Abram+Demski.pdf,Understanding Trust,Understanding Trust,Abram Demski,,,
High-Actuation Spaces,Theory,,Theory / High-Actuation Spaces,https://groundless.ai/,groundless.ai,Groundless Alignment,,Groundless,2025,
High-Actuation Spaces,Theory,,Theory / High-Actuation Spaces,https://www.lesswrong.com/s/aMz2JMvgXrLBkq4h3,Live Theory,429: Too Many Requests,,,,
High-Actuation Spaces,Theory,,Theory / High-Actuation Spaces,https://docs.google.com/document/d/1d-ARdZZDHFPIfGcTTOKK8IZWlQj0NZQrmteJj2mvmYA/edit?tab=t.0#heading=h.eg8luyrlsv2u,High Actuation Spaces - Sahil,High Actuation Spaces - Sahil,Sahil,,,
High-Actuation Spaces,Theory,,Theory / High-Actuation Spaces,https://www.lesswrong.com/posts/tQ9vWm4b57HFqbaRj/what-if-not-agency,"What, if not agency?",Unknown - Content Unavailable (429 Error),,,,
High-Actuation Spaces,Theory,,Theory / High-Actuation Spaces,https://docs.google.com/document/d/1fl7LE8AN7mLJ6uFcPuFCzatp0zCIYvjRIjQRgHPAkSE/edit?tab=t.0,Human Inductive Bias Project,HIBP Human Inductive Bias Project Plan,Félix Dorn,,,
High-Actuation Spaces,Theory,,Theory / High-Actuation Spaces,https://openreview.net/forum?id=n7WYSJ35FU,MoSSAIC: AI Safety After Mechanism,MoSSAIC: AI Safety After Mechanism,"Matt Farr, Aditya Arpitha Prasad, Chris Pang, Aditya Adiga, Jayson Amati, Sahil K",,2025,2025-07-01
High-Actuation Spaces,Theory,,Theory / High-Actuation Spaces,https://drive.google.com/drive/folders/1EaAJ4szuZsYR2_-DkS9cuhx3S6IWeCjW,HAS - Public (High Actuation Spaces),HAS - Public (High Actuation Spaces),,,,
Asymptotic guarantees,Theory,,Theory / Asymptotic guarantees,https://lesswrong.com/posts/iELyAqizJkizBQbfr/an-alignment-safety-case-sketch-based-on-debate,An alignment safety case sketch based on debate,An alignment safety case sketch based on debate,"Marie_DB, Jacob Pfau, Benjamin Hilton, Geoffrey Irving",UK AISI,2025,2025-05-08
Asymptotic guarantees,Theory,,Theory / Asymptotic guarantees,https://lesswrong.com/posts/tbnw7LbNApvxNLAg8/uk-aisi-s-alignment-team-research-agenda,UK AISI's Alignment Team: Research Agenda,UK AISI's Alignment Team: Research Agenda,"Benjamin Hilton, Jacob Pfau, Marie_DB, Geoffrey Irving",UK AI Safety Institute,2025,2025-05-07
Asymptotic guarantees,Theory,,Theory / Asymptotic guarantees,https://lesswrong.com/posts/EgRJtwQurNzz8CEfJ/dodging-systematic-human-errors-in-scalable-oversight,Dodging systematic human errors in scalable oversight,Dodging systematic human errors in scalable oversight,Geoffrey Irving,UK AISI,2025,2025-05-14
Asymptotic guarantees,Theory,,Theory / Asymptotic guarantees,https://openreview.net/pdf?id=XOIKLlSiDq,Can DPO Learn Diverse Human Values? A Theoretical Scaling Law,Unable to extract - PDF content not accessible,,,,
Heuristic explanations,Theory,,Theory / Heuristic explanations,https://www.lesswrong.com/posts/Xt9r4SNNuYxW83tmo/a-computational-no-coincidence-principle,A computational no-coincidence principle,A computational no-coincidence principle,Eric Neyman,Alignment Research Center,2025,2025-02-14
Heuristic explanations,Theory,,Theory / Heuristic explanations,https://www.lesswrong.com/posts/XdQd9gELHakd5pzJA/arc-progress-update-competing-with-sampling,Competing with sampling,ARC progress update: Competing with sampling,Eric Neyman,Alignment Research Center,2025,2025-11-18
Heuristic explanations,Theory,,Theory / Heuristic explanations,https://www.lesswrong.com/s/uYMw689vDFmgPEHrS,Obstacles in ARC's research agenda,Obstacles in ARC's agenda,David Matolcsi,ARC,2025,2025-04-30
Heuristic explanations,Theory,,Theory / Heuristic explanations,https://gabrieldwu.com/assets/thesis.pdf,Deduction-Projection Estimators for Understanding Neural Networks,Unknown - PDF content not accessible,,,,
Heuristic explanations,Theory,,Theory / Heuristic explanations,https://openreview.net/forum?id=m4OpQAK3eY,Wide Neural Networks as a Baseline for the Computational No-Coincidence Conjecture,Wide Neural Networks as a Baseline for the Computational No-Coincidence Conjecture,"John Dunbar, Scott Aaronson",University of Texas at Austin,2025,2025-07-07
Behavior alignment theory,Theory,Corrigibility,Theory / Corrigibility / Behavior alignment theory,https://www.lesswrong.com/posts/knwR9RgGN5a2oorci/preference-gaps-as-a-safeguard-against-ai-self-replication,Preference gaps as a safeguard against AI self-replication,Preference gaps as a safeguard against AI self-replication,"tbs, EJT",,2025,2025-11-26
Behavior alignment theory,Theory,Corrigibility,Theory / Corrigibility / Behavior alignment theory,https://www.lesswrong.com/s/KfCjeconYRdFbMxsy/p/qgBFJ72tahLo5hzqy,Serious Flaws in CAST,Serious Flaws in CAST,Max Harms,MIRI,2025,2025-11-19
Behavior alignment theory,Theory,Corrigibility,Theory / Corrigibility / Behavior alignment theory,https://www.lesswrong.com/posts/PhTBDHu9PKJFmvb4p/a-shutdown-problem-proposal,A Shutdown Problem Proposal,A Shutdown Problem Proposal,"johnswentworth, David Lorell",,2024,2024-01-21
Behavior alignment theory,Theory,Corrigibility,Theory / Corrigibility / Behavior alignment theory,https://arxiv.org/abs/2505.20203,Shutdownable Agents through POST-Agency,Shutdownable Agents through POST-Agency,Elliott Thornley,,2025,2025-05-26
Behavior alignment theory,Theory,Corrigibility,Theory / Corrigibility / Behavior alignment theory,https://arxiv.org/abs/2411.17749,The Partially Observable Off-Switch Game,The Partially Observable Off-Switch Game,"Andrew Garber, Rohan Subramani, Linus Luu, Mark Bedaywi, Stuart Russell, Scott Emmons",UC Berkeley,2024,2024-11-25
Behavior alignment theory,Theory,Corrigibility,Theory / Corrigibility / Behavior alignment theory,https://onlinelibrary.wiley.com/doi/10.1002/aaai.70040?af=R,Imitation learning is probably existentially safe,Imitation learning is probably existentially safe,"Michael K. Cohen, Marcus Hutter","University of California, Berkeley, Australian National University",2025,2025-11-21
Behavior alignment theory,Theory,Corrigibility,Theory / Corrigibility / Behavior alignment theory,https://arxiv.org/abs/2508.00159,Model-Based Soft Maximization of Suitable Metrics of Long-Term Human Power,Model-Based Soft Maximization of Suitable Metrics of Long-Term Human Power,"Jobst Heitzig, Ram Potham",,2025,2025-07-31
Behavior alignment theory,Theory,Corrigibility,Theory / Corrigibility / Behavior alignment theory,https://lesswrong.com/posts/9htmQx5wiePqTtZuL/deceptive-alignment-and-homuncularity,Deceptive Alignment and Homuncularity,Deceptive Alignment and Homuncularity,"Oliver Sourbut, TurnTrout","UK AI Safety Institute, Independent",2025,2025-01-16
Behavior alignment theory,Theory,Corrigibility,Theory / Corrigibility / Behavior alignment theory,https://openreview.net/forum?id=mhEnJa9pNk,A Safety Case for a Deployed LLM: Corrigibility as a Singular Target,A Safety Case for a Deployed LLM: Corrigibility as a Singular Target,Ram Potham,,2025,2025-06-24
Behavior alignment theory,Theory,Corrigibility,Theory / Corrigibility / Behavior alignment theory,https://lesswrong.com/posts/aKncW36ZdEnzxLo8A/llm-agi-will-have-memory-and-memory-changes-alignment,"LLM AGI will have memory, and memory changes alignment","LLM AGI will have memory, and memory changes alignment",Seth Herd,,2025,2025-04-04
Other corrigibility,Theory,Corrigibility,Theory / Corrigibility / Other corrigibility,https://www.lesswrong.com/posts/LDYPF6yfe3f8SPHFT/ai-assistants-should-have-a-direct-line-to-their-developers,AI Assistants Should Have a Direct Line to Their Developers,AI Assistants Should Have a Direct Line to Their Developers,Jan_Kulveit,,2024,2024-12-28
Other corrigibility,Theory,Corrigibility,Theory / Corrigibility / Other corrigibility,https://www.lesswrong.com/posts/ZHFZ6tivEjznkEoby/detect-goodhart-and-shut-down,Detect Goodhart and shut down,Detect Goodhart and shut down,Jeremy Gillen,,2025,2025-01-22
Other corrigibility,Theory,Corrigibility,Theory / Corrigibility / Other corrigibility,https://www.lesswrong.com/posts/7Z4WC4AFgfmZ3fCDC/instrumental-goals-are-a-different-and-friendlier-kind-of,Instrumental Goals Are A Different And Friendlier Kind Of Thing Than Terminal Goals,Instrumental goals are a different and friendlier kind of [content unavailable],,,,
Other corrigibility,Theory,Corrigibility,Theory / Corrigibility / Other corrigibility,https://www.lesswrong.com/posts/JuRdvZyqaFbvTPemn/shutdownable-agents-through-post-agency-1,Shutdownable Agents through POST-Agency,Shutdownable Agents through POST-Agency,EJT,,2025,2025-09-16
Other corrigibility,Theory,Corrigibility,Theory / Corrigibility / Other corrigibility,https://www.lesswrong.com/posts/ksfjZJu3BFEfM6hHE/why-corrigibility-is-hard-and-important-i-e-whence-the-high,"Why Corrigibility is Hard and Important (i.e. ""Whence the high MIRI confidence in alignment difficulty?"")",Error: Content Unavailable - Too Many Requests (429),,,,
Other corrigibility,Theory,Corrigibility,Theory / Corrigibility / Other corrigibility,https://dl.acm.org/doi/10.1145/3717823.3718245,Oblivious Defense in ML Models: Backdoor Removal without Detection,Oblivious Defense in ML Models: Backdoor Removal without Detection,"Shafi Goldwasser, Jonathan Shafer, Neekon Vafa, Vinod Vaikuntanathan","University of California, Berkeley, Massachusetts Institute of Technology",2025,2025-06-15
Other corrigibility,Theory,Corrigibility,Theory / Corrigibility / Other corrigibility,https://arxiv.org/abs/2509.20714,Cryptographic Backdoor for Neural Networks: Boon and Bane,Cryptographic Backdoor for Neural Networks: Boon and Bane,"Anh Tu Ngo, Anupam Chattopadhyay, Subhamoy Maitra",,2025,2025-09-25
Other corrigibility,Theory,Corrigibility,Theory / Corrigibility / Other corrigibility,https://arxiv.org/abs/2504.20310,A Cryptographic Perspective on Mitigation vs. Detection in Machine Learning,A Cryptographic Perspective on Mitigation vs. Detection in Machine Learning,"Greg Gluch, Shafi Goldwasser",,2025,2025-04-28
Other corrigibility,Theory,Corrigibility,Theory / Corrigibility / Other corrigibility,https://lesswrong.com/posts/CSFa9rvGNGAfCzBk6/problems-with-instruction-following-as-an-alignment-target,Problems with instruction-following as an alignment target,Problems with instruction-following as an alignment target,Seth Herd,,2025,2025-05-15
Natural abstractions,Theory,Ontology Identification,Theory / Ontology Identification / Natural abstractions,https://www.lesswrong.com/posts/T6xSXiXF3WF6TmCyN/abstract-mathematical-concepts-vs-abstractions-over-real,Abstract Mathematical Concepts vs. Abstractions Over Real-World Systems,Abstract mathematical concepts vs abstractions over real,,,,
Natural abstractions,Theory,Ontology Identification,Theory / Ontology Identification / Natural abstractions,https://www.lesswrong.com/posts/BstHXPgQyfeNnLjjp/condensation,Condensation,Condensation,abramdemski,,2025,2025-11-09
Natural abstractions,Theory,Ontology Identification,Theory / Ontology Identification / Natural abstractions,https://phillipi.github.io/prh/,Platonic representation hypothesis,The Platonic Representation Hypothesis,"Minyoung Huh, Brian Cheung, Tongzhou Wang, Phillip Isola",MIT,2024,2024-05-13
Natural abstractions,Theory,Ontology Identification,Theory / Ontology Identification / Natural abstractions,https://www.youtube.com/watch?v=Nr9eMobqUOo&t=3s,Rosas,Fernando Rosas: Identifying Abstractions (HAAISS 2025),Fernando Rosas,,2025,2025-10-06
Natural abstractions,Theory,Ontology Identification,Theory / Ontology Identification / Natural abstractions,https://arxiv.org/abs/2509.03780,Natural Latents: Latent Variables Stable Across Ontologies,Natural Latents: Latent Variables Stable Across Ontologies,"John Wentworth, David Lorell",,2025,2025-09-04
Natural abstractions,Theory,Ontology Identification,Theory / Ontology Identification / Natural abstractions,https://openreview.net/forum?id=HwKFJ3odui,Condensation: a theory of concepts,Condensation: a theory of concepts,Sam Eisenstat,,2025,2025-07-04
Natural abstractions,Theory,Ontology Identification,Theory / Ontology Identification / Natural abstractions,https://arxiv.org/abs/2412.02579,Factored space models: Towards causality between levels of abstraction,Factored space models: Towards causality between levels of abstraction,"Scott Garrabrant, Matthias Georg Mayer, Magdalena Wache, Leon Lang, Sam Eisenstat, Holger Dell",,2024,2024-12-03
Natural abstractions,Theory,Ontology Identification,Theory / Ontology Identification / Natural abstractions,https://lesswrong.com/posts/h89L5FMAkEBNsZ3xM/a-single-principle-related-to-many-alignment-subproblems-2,A single principle related to many Alignment subproblems?,A single principle related to many Alignment subproblems?,Q Home,,2025,2025-04-30
Natural abstractions,Theory,Ontology Identification,Theory / Ontology Identification / Natural abstractions,https://arxiv.org/abs/2310.13018,Getting aligned on representational alignment,Getting aligned on representational alignment,"Ilia Sucholutsky, Lukas Muttenthaler, Adrian Weller, Andi Peng, Andreea Bobu, Been Kim, Bradley C. Love, Christopher J. Cueva, Erin Grant, Iris Groen, Jascha Achterberg, Joshua B. Tenenbaum, Katherine M. Collins, Katherine L. Hermann, Kerem Oktar, Klaus Greff, Martin N. Hebart, Nathan Cloos, Nikolaus Kriegeskorte, Nori Jacoby, Qiuyi Zhang, Raja Marjieh, Robert Geirhos, Sherol Chen, Simon Kornblith, Sunayana Rane, Talia Konkle, Thomas P. O'Connell, Thomas Unterthiner, Andrew K. Lampinen, Klaus-Robert Müller, Mariya Toneva, Thomas L. Griffiths","University of Cambridge, Google DeepMind, MIT, University College London, Princeton University",2023,2023-10-18
Natural abstractions,Theory,Ontology Identification,Theory / Ontology Identification / Natural abstractions,https://arxiv.org/pdf/2512.00984%20,Symmetries at the origin of hierarchical emergence,Symmetries at the origin of hierarchical emergence,Fernando E. Rosas,,2024,2024-11-30
The Learning-Theoretic Agenda,Theory,Ontology Identification,Theory / Ontology Identification / The Learning-Theoretic Agenda,https://www.lesswrong.com/posts/LgLez8aeK24PbyyQJ/new-paper-infra-bayesian-decision-estimation-theory,Infra-Bayesian Decision-Estimation Theory,New paper: Infra-Bayesian Decision Estimation Theory,,,,
The Learning-Theoretic Agenda,Theory,Ontology Identification,Theory / Ontology Identification / The Learning-Theoretic Agenda,https://www.lesswrong.com/w/infra-bayesianism?sortedBy=new,Infra-Bayesianism category on LessWrong,Infra-Bayesianism,"abramdemski, Ruby",,2022,2022-03-24
The Learning-Theoretic Agenda,Theory,Ontology Identification,Theory / Ontology Identification / The Learning-Theoretic Agenda,https://www.lesswrong.com/posts/Y9NuKpb6dsyiYFxWK/new-paper-ambiguous-online-learning,Ambiguous Online Learning,New Paper: Ambiguous Online Learning,Vanessa Kosoy,Independent,2025,2025-06-25
The Learning-Theoretic Agenda,Theory,Ontology Identification,Theory / Ontology Identification / The Learning-Theoretic Agenda,https://proceedings.mlr.press/v291/appel25a.html,Regret Bounds for Robust Online Decision Making,Regret Bounds for Robust Online Decision Making,"Alexander Appel, Vanessa Kosoy",FutureSearch,2025,2025-07-02
The Learning-Theoretic Agenda,Theory,Ontology Identification,Theory / Ontology Identification / The Learning-Theoretic Agenda,https://www.lesswrong.com/s/n7qFxakSnxGuvmYAX,What is Inadequate about Bayesianism for AI Alignment: Motivating Infra-Bayesianism,FUNDAMENTALS OF INFRA-BAYESIANISM,Brittany Gelb,,2025,2025-08-30
The Learning-Theoretic Agenda,Theory,Ontology Identification,Theory / Ontology Identification / The Learning-Theoretic Agenda,https://www.alignmentforum.org/posts/DobZ62XMdiPigii9H/non-monotonic-infra-bayesian-physicalism%20,Non-Monotonic Infra-Bayesian Physicalism,Non-Monotonic Infra-Bayesian Physicalism,Marcus Ogren,,2025,2025-04-02
Aligning to context,Multi-agent first,,Multi-agent first / Aligning to context,https://www.softmax.com/blog/the-frame-dependent-mind,The Frame-Dependent Mind,The Frame-Dependent Mind: On Reality's Stubborn Refusal To Be One Thing,"Emmett Shear, Sonnet 3.7",Softmax,2025,2025-04-18
Aligning to context,Multi-agent first,,Multi-agent first / Aligning to context,https://docs.google.com/document/d/1cKbqYSGspfJavXvnhsp3mAuxHh08rNbP7tzYieqLiXw/edit?tab=t.0%20,On Eudaimonia and Optimization,On Eudaimonia and Optimization,,,,
Aligning to context,Multi-agent first,,Multi-agent first / Aligning to context,https://www.full-stack-alignment.ai,Full-Stack Alignment,Full-Stack Alignment: Co-Aligning AI and Institutions with Thick Models of Value,,Meaning Alignment Institute,,
Aligning to context,Multi-agent first,,Multi-agent first / Aligning to context,https://arxiv.org/abs/2412.19010,A theory of appropriateness,A theory of appropriateness with applications to generative artificial intelligence,"Joel Z. Leibo, Alexander Sasha Vezhnevets, Manfred Diaz, John P. Agapiou, William A. Cunningham, Peter Sunehag, Julia Haas, Raphael Koster, Edgar A. Duéñez-Guzmán, William S. Isaac, Georgios Piliouras, Stanley M. Bileschi, Iyad Rahwan, Simon Osindero",Google DeepMind,2024,2024-12-26
Aligning to context,Multi-agent first,,Multi-agent first / Aligning to context,https://arxiv.org/abs/2404.10636,"2404.10636 - What are human values, and how do we align AI to them?","What are human values, and how do we align AI to them?","Oliver Klingefjord, Ryan Lowe, Joe Edelman",,2024,2024-04-17
Aligning to context,Multi-agent first,,Multi-agent first / Aligning to context,https://meaningalignment.substack.com/p/model-integrity,Model Integrity,Model Integrity,"Joe Edelman, Oliver Klingefjord",Meaning Alignment Institute,2024,2024-12-05
Aligning to context,Multi-agent first,,Multi-agent first / Aligning to context,https://arxiv.org/abs/2408.16984,Beyond Preferences in AI Alignment,Beyond Preferences in AI Alignment,"Tan Zhi-Xuan, Micah Carroll, Matija Franklin, Hal Ashton",,2024,2024-08-30
Aligning to context,Multi-agent first,,Multi-agent first / Aligning to context,https://arxiv.org/abs/2503.00940,2503.00940 - Can AI Model the Complexities of Human Moral Decision-Making? A Qualitative Study of Kidney Allocation Decisions,Can AI Model the Complexities of Human Moral Decision-Making? A Qualitative Study of Kidney Allocation Decisions,"Vijay Keswani, Vincent Conitzer, Walter Sinnott-Armstrong, Breanna K. Nguyen, Hoda Heidari, Jana Schaich Borg","Carnegie Mellon University, Duke University, University of Oxford",2025,2025-03-02
Aligning to the social contract,Multi-agent first,,Multi-agent first / Aligning to the social contract,https://law-ai.org/law-following-ai/%20,Law-Following AI: designing AI agents to obey human laws,Law-Following AI: designing AI agents to obey human laws,"Cullen O'Keefe, Ketan Ramakrishnan, Janna Tay, Christoph Winter",Institute for Law & AI,2025,2025-05-01
Aligning to the social contract,Multi-agent first,,Multi-agent first / Aligning to the social contract,https://arxiv.org/abs/2510.26396,A Pragmatic View of AI Personhood,A Pragmatic View of AI Personhood,"Joel Z. Leibo, Alexander Sasha Vezhnevets, William A. Cunningham, Stanley M. Bileschi",Google DeepMind,2025,2025-10-30
Aligning to the social contract,Multi-agent first,,Multi-agent first / Aligning to the social contract,https://arxiv.org/abs/2503.00069,Societal alignment frameworks can improve llm alignment,Societal Alignment Frameworks Can Improve LLM Alignment,"Karolina Stańczak, Nicholas Meade, Mehar Bhatia, Hattie Zhou, Konstantin Böttinger, Jeremy Barnes, Jason Stanley, Jessica Montgomery, Richard Zemel, Nicolas Papernot, Nicolas Chapados, Denis Therien, Timothy P. Lillicrap, Ana Marasović, Sylvie Delacroix, Gillian K. Hadfield, Siva Reddy",Multiple institutions (17 authors),2025,2025-02-27
Aligning to the social contract,Multi-agent first,,Multi-agent first / Aligning to the social contract,https://arxiv.org/abs/2509.07955,2509.07955 - ACE and Diverse Generalization via Selective Disagreement,ACE and Diverse Generalization via Selective Disagreement,"Oliver Daniels, Stuart Armstrong, Alexandre Maranhão, Mahirah Fairuz Rahman, Benjamin M. Marlin, Rebecca Gorman",Unknown - not specified in abstract,2025,2025-09-09
Aligning to the social contract,Multi-agent first,,Multi-agent first / Aligning to the social contract,https://arxiv.org/abs/2506.17434,2506.17434 - Resource Rational Contractualism Should Guide AI Alignment,Resource Rational Contractualism Should Guide AI Alignment,"Sydney Levine, Matija Franklin, Tan Zhi-Xuan, Secil Yanik Guyot, Lionel Wong, Daniel Kilov, Yejin Choi, Joshua B. Tenenbaum, Noah Goodman, Seth Lazar, Iason Gabriel","MIT, Stanford, University of Washington, DeepMind, ANU",2025,2025-06-20
Aligning to the social contract,Multi-agent first,,Multi-agent first / Aligning to the social contract,https://arxiv.org/abs/2509.01186,Statutory Construction and Interpretation for Artificial Intelligence,Statutory Construction and Interpretation for Artificial Intelligence,"Luxi He, Nimra Nadeem, Michel Liao, Howard Chen, Danqi Chen, Mariano-Florentino Cuéllar, Peter Henderson","Princeton University, Stanford University",2025,2025-09-01
Aligning to the social contract,Multi-agent first,,Multi-agent first / Aligning to the social contract,https://arxiv.org/abs/2408.16984,2408.16984 - Beyond Preferences in AI Alignment,Beyond Preferences in AI Alignment,"Tan Zhi-Xuan, Micah Carroll, Matija Franklin, Hal Ashton",,2024,2024-08-30
Aligning to the social contract,Multi-agent first,,Multi-agent first / Aligning to the social contract,https://arxiv.org/abs/2505.00783,"Promises Made, Promises Kept: Safe Pareto Improvements via Ex Post Verifiable Commitments","Promises Made, Promises Kept: Safe Pareto Improvements via Ex Post Verifiable Commitments","Nathaniel Sauerberg, Caspar Oesterheld",,2025,2025-05-01
Theory for aligning multiple AIs,Multi-agent first,,Multi-agent first / Theory for aligning multiple AIs,https://arxiv.org/abs/2502.14143,Multi-Agent Risks from Advanced AI,Multi-Agent Risks from Advanced AI,"Lewis Hammond, Alan Chan, Jesse Clifton, Jason Hoelscher-Obermaier, Akbir Khan, Euan McLean, Chandler Smith, Wolfram Barfuss, Jakob Foerster, Tomáš Gavenčiak, The Anh Han, Edward Hughes, Vojtěch Kovařík, Jan Kulveit, Joel Z. Leibo, Caspar Oesterheld, Christian Schroeder de Witt, Nisarg Shah, Michael Wellman, Paolo Bova, Theodor Cimpeanu, Carson Ezell, Quentin Feuillade-Montixi, Matija Franklin, Esben Kran, Igor Krawczuk, Max Lamparth, Niklas Lauffer, Alexander Meinke, Sumeet Motwani, Anka Reuel, Vincent Conitzer, Michael Dennis, Iason Gabriel, Adam Gleave, Gillian Hadfield, Nika Haghtalab, Atoosa Kasirzadeh, Sébastien Krier, Kate Larson, Joel Lehman, David C. Parkes, Georgios Piliouras, Iyad Rahwan",Cooperative AI Foundation,2025,2025-02-19
Theory for aligning multiple AIs,Multi-agent first,,Multi-agent first / Theory for aligning multiple AIs,https://arxiv.org/abs/2509.01063,An Economy of AI Agents,An Economy of AI Agents,"Gillian K. Hadfield, Andrew Koh",,2025,2025-09-01
Theory for aligning multiple AIs,Multi-agent first,,Multi-agent first / Theory for aligning multiple AIs,https://arxiv.org/abs/2510.06105,Moloch's Bargain: Emergent Misalignment When LLMs Compete for Audiences,Moloch's Bargain: Emergent Misalignment When LLMs Compete for Audiences,"Batu El, James Zou",Stanford University,2025,2025-10-07
Theory for aligning multiple AIs,Multi-agent first,,Multi-agent first / Theory for aligning multiple AIs,https://arxiv.org/abs/2508.14927,AI Testing Should Account for Sophisticated Strategic Behaviour,AI Testing Should Account for Sophisticated Strategic Behaviour,"Vojtech Kovarik, Eric Olav Chen, Sami Petersen, Alexis Ghersengorin, Vincent Conitzer",,2025,2025-08-19
Theory for aligning multiple AIs,Multi-agent first,,Multi-agent first / Theory for aligning multiple AIs,https://www.science.org/doi/10.1126/sciadv.adu9368,Emergent social conventions and collective bias in LLM populations,Emergent social conventions and collective bias in LLM populations,"Ariel Flint Ashery, Luca Maria Aiello, Andrea Baronchelli","City St George's, University of London, IT University of Copenhagen, Pioneer Centre for AI, The Alan Turing Institute",2025,2025-05-14
Theory for aligning multiple AIs,Multi-agent first,,Multi-agent first / Theory for aligning multiple AIs,https://arxiv.org/abs/2507.02618,Strategic Intelligence in Large Language Models: Evidence from evolutionary Game Theory,Strategic Intelligence in Large Language Models: Evidence from evolutionary Game Theory,"Kenneth Payne, Baptiste Alloui-Cros",,2025,2025-07-03
Theory for aligning multiple AIs,Multi-agent first,,Multi-agent first / Theory for aligning multiple AIs,https://arxiv.org/abs/2510.05748,Communication Enables Cooperation in LLM Agents,Communication Enables Cooperation in LLM Agents: A Comparison with Curriculum-Based Approaches,"Hachem Madmoun, Salem Lahlou",,2025,2025-10-07
Theory for aligning multiple AIs,Multi-agent first,,Multi-agent first / Theory for aligning multiple AIs,https://arxiv.org/abs/2503.06323,Higher-Order Belief in Incomplete Information MAIDs,Higher-Order Belief in Incomplete Information MAIDs,"Jack Foxabbott, Rohan Subramani, Francis Rhys Ward",,2025,2025-03-08
Theory for aligning multiple AIs,Multi-agent first,,Multi-agent first / Theory for aligning multiple AIs,https://arxiv.org/abs/2412.14570,Characterising Simulation-Based Program Equilibria,Characterising Simulation-Based Program Equilibria,"Emery Cooper, Caspar Oesterheld, Vincent Conitzer",Carnegie Mellon University,2024,2024-12-19
Theory for aligning multiple AIs,Multi-agent first,,Multi-agent first / Theory for aligning multiple AIs,https://cgi.cse.unsw.edu.au/~eptcs/paper.cgi?TARK2025:36,Safe (Pareto) Improvements in Binary Constraint Structures,Error: No content available,,,,
Theory for aligning multiple AIs,Multi-agent first,,Multi-agent first / Theory for aligning multiple AIs,https://arxiv.org/abs/2505.00783,"Promises Made, Promises Kept: Safe Pareto Improvements via Ex Post Verifiable Commitments","Promises Made, Promises Kept: Safe Pareto Improvements via Ex Post Verifiable Commitments","Nathaniel Sauerberg, Caspar Oesterheld",,2025,2025-05-01
Theory for aligning multiple AIs,Multi-agent first,,Multi-agent first / Theory for aligning multiple AIs,https://www.lesswrong.com/posts/wQKskToGofs4osdJ3/the-pando-problem-rethinking-ai-individuality,The Pando Problem: Rethinking AI Individuality,The Pando Problem: Rethinking AI Individuality,Jan_Kulveit,,2025,2025-03-28
Tools for aligning multiple AIs,Multi-agent first,,Multi-agent first / Tools for aligning multiple AIs,https://softmax.com/blog/reimagining-alignment,Reimagining Alignment,Reimagining Alignment,,Softmax,2025,2025-03-28
Tools for aligning multiple AIs,Multi-agent first,,Multi-agent first / Tools for aligning multiple AIs,https://arxiv.org/abs/2509.14485,Beyond the high score: Prosocial ability profiles of multi-agent populations,Beyond the high score: Prosocial ability profiles of multi-agent populations,"Marko Tesic, Yue Zhao, Joel Z. Leibo, Rakshit S. Trivedi, Jose Hernandez-Orallo",Google DeepMind,2025,2025-09-17
Tools for aligning multiple AIs,Multi-agent first,,Multi-agent first / Tools for aligning multiple AIs,https://arxiv.org/abs/2509.23102,Multiplayer Nash Preference Optimization,Multiplayer Nash Preference Optimization,"Fang Wu, Xu Huang, Weihao Xuan, Zhiwei Zhang, Yijia Xiao, Guancheng Wan, Xiaomin Li, Bing Hu, Peng Xia, Jure Leskovec, Yejin Choi","Stanford University, University of Washington, AI2",2025,2025-09-27
Tools for aligning multiple AIs,Multi-agent first,,Multi-agent first / Tools for aligning multiple AIs,https://arxiv.org/abs/2502.00757,AgentBreeder: Mitigating the AI Safety Risks of Multi-Agent Scaffolds via Self-Improvement,AgentBreeder: Mitigating the AI Safety Risks of Multi-Agent Scaffolds via Self-Improvement,"J Rosser, Jakob Foerster",,2025,2025-02-02
Tools for aligning multiple AIs,Multi-agent first,,Multi-agent first / Tools for aligning multiple AIs,https://arxiv.org/abs/2507.14660,When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion in Social Systems,When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion in Social Systems,"Qibing Ren, Sitao Xie, Longxuan Wei, Zhenfei Yin, Junchi Yan, Lizhuang Ma, Jing Shao",,2025,2025-07-19
Tools for aligning multiple AIs,Multi-agent first,,Multi-agent first / Tools for aligning multiple AIs,https://arxiv.org/abs/2501.10114,Infrastructure for AI Agents,Infrastructure for AI Agents,"Alan Chan, Kevin Wei, Sihao Huang, Nitarshan Rajkumar, Elija Perrier, Seth Lazar, Gillian K. Hadfield, Markus Anderljung",,2025,2025-01-17
Tools for aligning multiple AIs,Multi-agent first,,Multi-agent first / Tools for aligning multiple AIs,https://arxiv.org/abs/2411.10588,A dataset of questions on decision-theoretic reasoning in Newcomb-like problems,A dataset of questions on decision-theoretic reasoning in Newcomb-like problems,"Caspar Oesterheld, Emery Cooper, Miles Kodama, Linh Chi Nguyen, Ethan Perez",,2024,2024-11-15
Tools for aligning multiple AIs,Multi-agent first,,Multi-agent first / Tools for aligning multiple AIs,https://arxiv.org/abs/2510.01295,The Social Laboratory: A Psychometric Framework for Multi-Agent LLM Evaluation,The Social Laboratory: A Psychometric Framework for Multi-Agent LLM Evaluation,Zarreen Reza,,2025,2025-10-01
Tools for aligning multiple AIs,Multi-agent first,,Multi-agent first / Tools for aligning multiple AIs,https://github.com/lechmazur/pgg_bench,PGG-Bench: Contribute & Punish,PGG-Bench: Contribute & Punish,,,2025,2025-04-10
Tools for aligning multiple AIs,Multi-agent first,,Multi-agent first / Tools for aligning multiple AIs,http://arxiv.org/abs/2509.10147,Virtual Agent Economies,Virtual Agent Economies,"Nenad Tomasev, Matija Franklin, Joel Z. Leibo, Julian Jacobs, William A. Cunningham, Iason Gabriel, Simon Osindero",DeepMind,2025,2025-09-12
Tools for aligning multiple AIs,Multi-agent first,,Multi-agent first / Tools for aligning multiple AIs,https://arxiv.org/abs/2502.12203,An Interpretable Automated Mechanism Design Framework with Large Language Models,An Interpretable Automated Mechanism Design Framework with Large Language Models,"Jiayuan Liu, Mingyu Guo, Vincent Conitzer",,2025,2025-02-16
Tools for aligning multiple AIs,Multi-agent first,,Multi-agent first / Tools for aligning multiple AIs,https://openreview.net/forum?id=9ply9CAnSC&noteId=rcn5RTlfD1,Comparing Collective Behavior of LLM and Human Groups,Comparing Collective Behavior of LLM and Human Groups,"Anna B. Stephenson, Andrew Zhu, Chris Callison-Burch, Jan Kulveit",University of Pennsylvania,2025,2025-09-23
Aligned to who?,Multi-agent first,,Multi-agent first / Aligned to who?,https://ojs.aaai.org/index.php/AIES/article/view/36645,The AI Power Disparity Index: Toward a Compound Measure of AI Actors' Power to Shape the AI Ecosystem,The AI Power Disparity Index: Toward a Compound Measure of AI Actors' Power to Shape the AI Ecosystem,"Rachel M. Kim, Blaine Kuehnert, Seth Lazar, Ranjit Singh, Hoda Heidari","Carnegie Mellon University, Australian National University, Data and Society",2025,2025-10-15
Aligned to who?,Multi-agent first,,Multi-agent first / Aligned to who?,https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5097286,Research Agenda for Sociotechnical Approaches to AI Safety,Research Agenda for Sociotechnical Approaches to AI Safety,"Samuel Curtis, Ravi Iyer, Cameron Domenico Kirk-Giannini, Victoria Krakovna, David Krueger, Nathan Lambert, Bruno Marnette, Colleen McKenzie, Julian Michael, Evan Miyazono, Noyuri Mima, Aviv Ovadya, Luke Thorburn, Vehbi Deger Turan","The Future Society, University of Southern California, Rutgers University, Future of Life Institute, University of Cambridge, Allen Institute for AI, AI Objectives Institute, New York University, Atlas Computing, Future University Hakodate, Thoughtful Technology Project, King's College London, AI & Democracy Foundation",2025,2025-01-14
Aligned to who?,Multi-agent first,,Multi-agent first / Aligned to who?,https://arxiv.org/abs/2507.09650,2507.09650 - Cultivating Pluralism In Algorithmic Monoculture: The Community Alignment Dataset,Cultivating Pluralism In Algorithmic Monoculture: The Community Alignment Dataset,"Lily Hong Zhang, Smitha Milli, Karen Jusko, Jonathan Smith, Brandon Amos, Wassim Bouaziz, Manon Revel, Jack Kussman, Yasha Sheynin, Lisa Titus, Bhaktipriya Radharapu, Jane Yu, Vidya Sarma, Kris Rose, Maximilian Nickel","Meta, Cornell University",2025,2025-07-13
Aligned to who?,Multi-agent first,,Multi-agent first / Aligned to who?,https://arxiv.org/abs/2510.13709,Training LLM Agents to Empower Humans,Training LLM Agents to Empower Humans,"Evan Ellis, Vivek Myers, Jens Tuyls, Sergey Levine, Anca Dragan, Benjamin Eysenbach","UC Berkeley, Princeton University",2025,2025-10-16
Aligned to who?,Multi-agent first,,Multi-agent first / Aligned to who?,https://arxiv.org/abs/2505.05197,"Societal and technological progress as sewing an ever-growing, ever-changing, patchy, and polychrome quilt","Societal and technological progress as sewing an ever-growing, ever-changing, patchy, and polychrome quilt","Joel Z. Leibo, Alexander Sasha Vezhnevets, William A. Cunningham, Sébastien Krier, Manfred Diaz, Simon Osindero",Google DeepMind,2025,2025-05-08
Aligned to who?,Multi-agent first,,Multi-agent first / Aligned to who?,https://arxiv.org/abs/2411.09222,Democratic AI is Possible: The Democracy Levels Framework Shows How It Might Work,Democratic AI is Possible. The Democracy Levels Framework Shows How It Might Work,"Aviv Ovadya, Kyle Redman, Luke Thorburn, Quan Ze Chen, Oliver Smith, Flynn Devine, Andrew Konya, Smitha Milli, Manon Revel, K. J. Kevin Feng, Amy X. Zhang, Bilva Chandra, Michiel A. Bakker, Atoosa Kasirzadeh",Multiple Organizations,2024,2024-11-14
Aligned to who?,Multi-agent first,,Multi-agent first / Aligned to who?,https://arxiv.org/abs/2503.05728,2503.05728 - Political Neutrality in AI Is Impossible - But Here Is How to Approximate It,Political Neutrality in AI Is Impossible- But Here Is How to Approximate It,"Jillian Fisher, Ruth E. Appel, Chan Young Park, Yujin Potter, Liwei Jiang, Taylor Sorensen, Shangbin Feng, Yulia Tsvetkov, Margaret E. Roberts, Jennifer Pan, Dawn Song, Yejin Choi","Anthropic, UC San Diego, Stanford University, UC Berkeley, University of Washington",2025,2025-02-18
Aligned to who?,Multi-agent first,,Multi-agent first / Aligned to who?,https://arxiv.org/abs/2505.04345,"Build Agent Advocates, Not Platform Agents","Build Agent Advocates, Not Platform Agents","Sayash Kapoor, Noam Kolt, Seth Lazar",,2025,2025-05-07
Aligned to who?,Multi-agent first,,Multi-agent first / Aligned to who?,https://arxiv.org/abs/2501.16946,Gradual Disempowerment,Gradual Disempowerment: Systemic Existential Risks from Incremental AI Development,"Jan Kulveit, Raymond Douglas, Nora Ammann, Deger Turan, David Krueger, David Duvenaud",Various academic institutions,2025,2025-01-28
Aligning what?,Multi-agent first,,Multi-agent first / Aligning what?,https://www.alignmentforum.org/posts/5tYTKX4pNpiG4vzYg/towards-a-scale-free-theory-of-intelligent-agency,Towards a Scale-Free Theory of Intelligent Agency,Towards a scale-free theory of intelligent agency,Richard Ngo,Independent,2025,2025-03-21
Aligning what?,Multi-agent first,,Multi-agent first / Aligning what?,https://chrislakin.blog/p/alignment-first-intelligence-later,"Alignment first, intelligence later","Alignment first, intelligence later",Chris Lakin,Softmax,2025,2025-03-30
Aligning what?,Multi-agent first,,Multi-agent first / Aligning what?,https://www.anthropic.com/research/end-subset-conversations,End A Subset Of Conversations,Claude Opus 4 and 4.1 can now end a rare subset of conversations,,Anthropic,2025,2025-08-15
Aligning what?,Multi-agent first,,Multi-agent first / Aligning what?,https://www.full-stack-alignment.ai,Full-Stack Alignment,Full-Stack Alignment: Co-Aligning AI and Institutions with Thick Models of Value,,Meaning Alignment Institute,,
Aligning what?,Multi-agent first,,Multi-agent first / Aligning what?,https://docs.google.com/document/d/1cKbqYSGspfJavXvnhsp3mAuxHh08rNbP7tzYieqLiXw/edit?tab=t.0%20,On Eudaimonia and Optimization,On Eudaimonia and Optimization,,,,
Aligning what?,Multi-agent first,,Multi-agent first / Aligning what?,https://arxiv.org/abs/2501.17755,AI Governance through Markets,,,,,
Aligning what?,Multi-agent first,,Multi-agent first / Aligning what?,https://www.pnas.org/doi/abs/10.1073/pnas.2319948121,Collective cooperative intelligence,Collective cooperative intelligence,"Wolfram Barfuss, Jessica Flack, Chaitanya S. Gokhale, Lewis Hammond, Christian Hilbe, Edward Hughes, Joel Z. Leibo, Tom Lenaerts, Naomi Leonard, Simon Levin, Udari Madhushani Sehwag, Alex McAvoy, Janusz M. Meylahn, Fernando P. Santos","University of Bonn, Santa Fe Institute, Max Planck Institute for Evolutionary Biology, University of Oxford, Google DeepMind, Université Libre de Bruxelles, Princeton University, Stanford University, UNC Chapel Hill, University of Twente, University of Amsterdam",2025,2025-06-16
Aligning what?,Multi-agent first,,Multi-agent first / Aligning what?,https://www.lesswrong.com/posts/JjYu75q3hEMBgtvr8/multipolar-ai-is-underrated,Multipolar AI is Underrated,Multipolar AI is Underrated,Allison Duettmann,Foresight Institute,2025,2025-05-17
Aligning what?,Multi-agent first,,Multi-agent first / Aligning what?,https://www.lesswrong.com/posts/tQ9vWm4b57HFqbaRj/what-if-not-agency,"What, if not agency?",Unknown - Content Unavailable (429 Error),,,,
Aligning what?,Multi-agent first,,Multi-agent first / Aligning what?,https://equilibria1.substack.com/p/the-evolution-of-agency-a-research,A Phylogeny of Agents,A Phylogeny of Agents,Equilibria,Equilibria Network,2025,2025-08-15
Aligning what?,Multi-agent first,,Multi-agent first / Aligning what?,https://themultiplicity.ai/blog/thesis,"The Multiplicity Thesis, Collective Intelligence, and Morality","The Multiplicity Thesis, Collective Intelligence, and Morality",Andrew Critch,theMultiplicity.ai,2025,2025-11-07
Aligning what?,Multi-agent first,,Multi-agent first / Aligning what?,https://www.alignmentforum.org/posts/xud7Mti9jS4tbWqQE/hierarchical-agency-a-missing-piece-in-ai-alignment,Hierarchical Alignment,Hierarchical Agency: A Missing Piece in AI Alignment,Jan_Kulveit,Alignment of Complex Systems Research Group,2024,2024-11-27
Aligning what?,Multi-agent first,,Multi-agent first / Aligning what?,https://a16z.simplecast.com/episodes/emmett-shear-on-building-ai-that-actually-cares-beyond-control-and-steering-TRwfxH0r,Emmett Shear on Building AI That Actually Cares: Beyond Control and Steering,Emmett Shear on Building AI That Actually Cares: Beyond Control and Steering,"Emmett Shear, Erik Torenberg, Séb Krier","Softmax, a16z",2025,2025-11-17
AGI metrics,Evals,,Evals / AGI metrics,https://arxiv.org/abs/2503.17354,HCAST: Human-Calibrated Autonomy Software Tasks,HCAST: Human-Calibrated Autonomy Software Tasks,"David Rein, Joel Becker, Amy Deng, Seraphina Nix, Chris Canal, Daniel O'Connel, Pip Arnott, Ryan Bloom, Thomas Broadley, Katharyn Garcia, Brian Goodrich, Max Hasin, Sami Jawhar, Megan Kinniment, Thomas Kwa, Aron Lajko, Nate Rush, Lucas Jun Koba Sato, Sydney Von Arx, Ben West, Lawrence Chan, Elizabeth Barnes",,2025,2025-03-21
AGI metrics,Evals,,Evals / AGI metrics,https://arxiv.org/pdf/2510.18212,A Definition of AGI,A Definition of AGI,"Dan Hendrycks, Dawn Song, Christian Szegedy, Honglak Lee, Yarin Gal, Erik Brynjolfsson, Sharon Li, Andy Zou, Lionel Levine, Bo Han, Jie Fu, Ziwei Liu, Jinwoo Shin, Kimin Lee, Mantas Mazeika, Long Phan, George Ingebretsen, Adam Khoja, Cihang Xie, Olawale Salaudeen, Matthias Hein, Kevin Zhao, Alexander Pan, David Duvenaud, Bo Li, Steve Omohundro, Gabriel Alfour, Max Tegmark, Kevin McGrew, Gary Marcus, Jaan Tallinn, Eric Schmidt, Yoshua Bengio","UC Berkeley, Various Universities, Independent Researchers",2025,2025-10-21
AGI metrics,Evals,,Evals / AGI metrics,https://scale.com/leaderboard/rli,Remote Labor Index,Remote Labor Index (RLI),,Scale AI,2025,2025-10-30
AGI metrics,Evals,,Evals / AGI metrics,https://kinds-of-intelligence-cfi.github.io/ADELE/,ADeLe v1.0: A battery for AI Evaluation with explanatory and predictive power,ADeLe v1.0: A battery for AI Evaluation with explanatory and predictive power,"Lexin Zhou, Lorenzo Pacchiardi, Fernando Martínez-Plumed, Katherine M. Collins, Yael Moros-Daval, Seraphina Zhang, Qinlin Zhao, Yitian Huang, Luning Sun, Jonathan E. Prunty, Zongqian Li, Pablo Sánchez-García, Kexin Jiang Chen, Pablo A. M. Casares, Jiyun Zu, John Burden, Behzad Mehrbakhsh, David Stillwell, Manuel Cebrian, Jindong Wang, Peter Henderson, Sherry Tongshuang Wu, Patrick C. Kyllonen, Lucy Cheke, Xing Xie, José Hernández-Orallo","Leverhulme Centre for the Future of Intelligence, Center for Information Technology Policy, Princeton University",2025,2025-03-11
AGI metrics,Evals,,Evals / AGI metrics,https://arxiv.org/abs/2510.04374,GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks,GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks,"Tejal Patwardhan, Rachel Dias, Elizabeth Proehl, Grace Kim, Michele Wang, Olivia Watkins, Simón Posada Fishman, Marwan Aljubeh, Phoebe Thacker, Laurance Fauconnet, Natalie S. Kim, Patrick Chao, Samuel Miserendino, Gildas Chabot, David Li, Michael Sharman, Alexandra Barr, Amelia Glaese, Jerry Tworek",OpenAI,2025,2025-10-05
Capability evals,Evals,,Evals / Capability evals,https://metr.org/blog/2025-10-14-malt-dataset-of-natural-and-prompted-behaviors/,MALT: A Dataset of Natural and Prompted Behaviors That Threaten Eval Integrity,MALT: A Dataset of Natural and Prompted Behaviors That Threaten Eval Integrity,"Neev Parikh, Hjalmar Wijk",METR,2025,2025-10-14
Capability evals,Evals,,Evals / Capability evals,https://arxiv.org/abs/2502.16797,Forecasting Rare Language Model Behaviors,Forecasting Rare Language Model Behaviors,"Erik Jones, Meg Tong, Jesse Mu, Mohammed Mahfoud, Jan Leike, Roger Grosse, Jared Kaplan, William Fithian, Ethan Perez, Mrinank Sharma","Anthropic, OpenAI, UC Berkeley",2025,2025-02-24
Capability evals,Evals,,Evals / Capability evals,https://arxiv.org/abs/2502.05209,Model Tampering Attacks Enable More Rigorous Evaluations of LLM Capabilities,Model Tampering Attacks Enable More Rigorous Evaluations of LLM Capabilities,"Zora Che, Stephen Casper, Robert Kirk, Anirudh Satheesh, Stewart Slocum, Lev E McKinney, Rohit Gandikota, Aidan Ewart, Domenic Rosati, Zichu Wu, Zikui Cai, Bilal Chughtai, Yarin Gal, Furong Huang, Dylan Hadfield-Menell",,2025,2025-02-03
Capability evals,Evals,,Evals / Capability evals,https://arxiv.org/abs/2502.02180,The Elicitation Game: Evaluating Capability Elicitation Techniques,The Elicitation Game: Evaluating Capability Elicitation Techniques,"Felix Hofstätter, Teun van der Weij, Jayden Teoh, Rada Djoneva, Henning Bartsch, Francis Rhys Ward",,2025,2025-02-04
Capability evals,Evals,,Evals / Capability evals,https://arxiv.org/abs/2508.19980,Evaluating Language Model Reasoning about Confidential Information,Evaluating Language Model Reasoning about Confidential Information,"Dylan Sam, Alexander Robey, Andy Zou, Matt Fredrikson, J. Zico Kolter",Carnegie Mellon University,2025,2025-08-27
Capability evals,Evals,,Evals / Capability evals,https://arxiv.org/abs/2504.11844,Evaluating the Goal-Directedness of Large Language Models,Evaluating the Goal-Directedness of Large Language Models,"Tom Everitt, Cristina Garbacea, Alexis Bellot, Jonathan Richens, Henry Papadatos, Siméon Campos, Rohin Shah","Google DeepMind, OpenAI, Anthropic",2025,2025-04-16
Capability evals,Evals,,Evals / Capability evals,https://alignment.anthropic.com/2024/rogue-eval/index.html,A Toy Evaluation of Inference Code Tampering,A Toy Evaluation of Inference Code Tampering,,Anthropic,2024,
Capability evals,Evals,,Evals / Capability evals,https://arxiv.org/abs/2502.07577,Automated Capability Discovery via Foundation Model Self-Exploration,Automated Capability Discovery via Foundation Model Self-Exploration,"Cong Lu, Shengran Hu, Jeff Clune",,2025,2025-02-11
Capability evals,Evals,,Evals / Capability evals,https://arxiv.org/abs/2509.25369,Generative Value Conflicts Reveal LLM Priorities,Generative Value Conflicts Reveal LLM Priorities,"Andy Liu, Kshitish Ghate, Mona Diab, Daniel Fried, Atoosa Kasirzadeh, Max Kleiman-Weiner",,2025,2025-09-29
Capability evals,Evals,,Evals / Capability evals,https://arxiv.org/abs/2505.02709,Technical Report: Evaluating Goal Drift in Language Model Agents,Technical Report: Evaluating Goal Drift in Language Model Agents,"Rauno Arike, Elizabeth Donoway, Henning Bartsch, Marius Hobbhahn",Apollo Research,2025,2025-05-05
Capability evals,Evals,,Evals / Capability evals,https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/,Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity,Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity,,METR,2025,2025-07-10
Capability evals,Evals,,Evals / Capability evals,https://arxiv.org/abs/2505.19212,When Ethics and Payoffs Diverge: LLM Agents in Morally Charged Social Dilemmas,When Ethics and Payoffs Diverge: LLM Agents in Morally Charged Social Dilemmas,"Steffen Backmann, David Guzman Piedrahita, Emanuel Tewolde, Rada Mihalcea, Bernhard Schölkopf, Zhijing Jin",Max Planck Institute for Intelligent Systems,2025,2025-05-25
Capability evals,Evals,,Evals / Capability evals,https://arxiv.org/abs/2503.05731,AILuminate: Introducing v1.0 of the AI Risk and Reliability Benchmark from MLCommons,AILuminate: Introducing v1.0 of the AI Risk and Reliability Benchmark from MLCommons,"Shaona Ghosh, Heather Frase, Adina Williams, Sarah Luger, Paul Röttger, Fazl Barez, Sean McGregor, Kenneth Fricklas, Mala Kumar, Quentin Feuillade-Montixi, Kurt Bollacker, Felix Friedrich, Ryan Tsang, Bertie Vidgen, Alicia Parrish, Chris Knotz, Eleonora Presani, Jonathan Bennion, Marisa Ferrara Boston, Mike Kuniavsky, Wiebke Hutiri, James Ezick, Malek Ben Salem, Rajat Sahay, Sujata Goswami, Usman Gohar, Ben Huang, Supheakmungkol Sarin, Elie Alhajjar, Canyu Chen, Roman Eng, Kashyap Ramanandula Manjusha, Virendra Mehta, Eileen Long, Murali Emani, Natan Vidra, Benjamin Rukundo, Abolfazl Shahbazi, Kongtao Chen, Rajat Ghosh, Vithursan Thangarasa, Pierre Peigné, Abhinav Singh, Max Bartolo, Satyapriya Krishna, Mubashara Akhtar, Rafael Gold, Cody Coleman, Luis Oala, Vassil Tashev, Joseph Marvin Imperial, Amy Russ, Sasidhar Kunapuli, Nicolas Miailhe, Julien Delaunay, Bhaktipriya Radharapu, Rajat Shinde, Tuesday, Debojyoti Dutta, Declan Grabb, Ananya Gangavarapu, Saurav Sahay, Agasthya Gangavarapu, Patrick Schramowski, Stephen Singam, Tom David, Xudong Han, Priyanka Mary Mammen, Tarunima Prabhakar, Venelin Kovatchev, Rebecca Weiss, Ahmed Ahmed, Kelvin N. Manyeki, Sandeep Madireddy, Foutse Khomh, Fedor Zhdanov, Joachim Baumann, Nina Vasan, Xianjun Yang, Carlos Mougn, Jibin Rajan Varghese, Hussain Chinoy, Seshakrishna Jitendar, Manil Maskey, Claire V. Hardgrove, Tianhao Li, Aakash Gupta, Emil Joswin, Yifan Mai, Shachi H Kumar, Cigdem Patlak, Kevin Lu, Vincent Alessi, Sree Bhargavi Balija, Chenhe Gu, Robert Sullivan, James Gealy, Matt Lavrisa, James Goel, Peter Mattson, Percy Liang, Joaquin Vanschoren",MLCommons,2025,2025-04-18
Capability evals,Evals,,Evals / Capability evals,https://www.anthropic.com/research/petri-open-source-auditing,Petri: An open-source auditing tool to accelerate AI safety research,Petri: An open-source auditing tool to accelerate AI safety research,"Kai Fronsdal, Isha Gupta, Abhay Sheshadri, Jonathan Michala, Stephen McAleer, Rowan Wang, Sara Price, Samuel R. Bowman",Anthropic,2025,2025-10-06
Capability evals,Evals,,Evals / Capability evals,https://www.apolloresearch.ai/blog/research-note-our-scheming-precursor-evals-had-limited-predictive-power-for-our-in-context-scheming-evals,Research Note: Our scheming precursor evals had limited predictive power for our in-context scheming evals,Research Note: Our scheming precursor evals had limited predictive power for our in-context scheming evals,Marius Hobbhahn,Apollo Research,2025,2025-07-03
Capability evals,Evals,,Evals / Capability evals,https://lesswrong.com/posts/ZEuDH2W3XdRaTwpjD/hyperbolic-model-fits-metr-capabilities-estimate-worse-than,Hyperbolic model fits METR capabilities estimate worse than exponential model,Hyperbolic model fits METR capabilities estimate worse than exponential model,gjm,,2025,2025-08-19
Capability evals,Evals,,Evals / Capability evals,https://lesswrong.com/posts/nmaKpoHxmzjT8yXTk/new-website-analyzing-ai-companies-model-evals,New website analyzing AI companies' model evals,New website analyzing AI companies' model evals,Zach Stein-Perlman,,2025,2025-05-26
Capability evals,Evals,,Evals / Capability evals,https://lesswrong.com/posts/9tqpPP4FwSnv9AWsi/research-note-our-scheming-precursor-evals-had-limited,Research Note: Our scheming precursor evals had limited predictive power for our in-context scheming evals,Research Note: Our scheming precursor evals had limited predictive power for our in-context scheming evals,Marius Hobbhahn,Apollo Research,2025,2025-07-03
Capability evals,Evals,,Evals / Capability evals,https://lesswrong.com/posts/qhjNejRxbMGQp4wHt/how-fast-can-algorithms-advance-capabilities-or-epoch,How Fast Can Algorithms Advance Capabilities? | Epoch Gradient Update,How Fast Can Algorithms Advance Capabilities? | Epoch Gradient Update,"Henry Josephson, Spencer Guo, Teddy Foley, Jack Sanderson, Anqi Qu","UChicago XLab, Google DeepMind",2025,2025-05-16
Capability evals,Evals,,Evals / Capability evals,https://lesswrong.com/posts/CwdCYmsutwXwnYtEF/paper-safety-by-measurement-a-systematic-literature-review,Safety by Measurement: A Systematic Literature Review of AI Safety Evaluation Methods,Safety by Measurement: A Systematic Literature Review of AI Safety Evaluation Methods,"markov, Charbel-Raphaël",,2025,2025-05-19
Capability evals,Evals,,Evals / Capability evals,https://arxiv.org/abs/2502.02260,Adversarial ML Problems Are Getting Harder to Solve and to Evaluate,Adversarial ML Problems Are Getting Harder to Solve and to Evaluate,"Javier Rando, Jie Zhang, Nicholas Carlini, Florian Tramèr",,2025,2025-02-04
Capability evals,Evals,,Evals / Capability evals,https://arxiv.org/abs/2501.01558,Predicting the Performance of Black-box LLMs through Self-Queries,Predicting the Performance of Black-box LLMs through Self-Queries,"Dylan Sam, Marc Finzi, J. Zico Kolter",Carnegie Mellon University,2025,2025-01-02
Capability evals,Evals,,Evals / Capability evals,https://www.4wallai.com/amongais,Among AIs,Among AIs,,4Wall AI,2025,
Capability evals,Evals,,Evals / Capability evals,https://arxiv.org/abs/2505.05541,Safety by Measurement: A Systematic Literature Review of AI Safety Evaluation Methods,Safety by Measurement: A Systematic Literature Review of AI Safety Evaluation Methods,"Markov Grey, Charbel-Raphaël Segerie",,2025,2025-05-08
Capability evals,Evals,,Evals / Capability evals,https://arxiv.org/abs/2506.12229,Infini-gram mini: Exact n-gram Search at the Internet Scale with FM-Index,Infini-gram mini: Exact n-gram Search at the Internet Scale with FM-Index,"Hao Xu, Jiacheng Liu, Yejin Choi, Noah A. Smith, Hannaneh Hajishirzi","University of Washington, Allen Institute for AI",2025,2025-06-13
Capability evals,Evals,,Evals / Capability evals,https://lesswrong.com/posts/W3KfxjbqBAnifBQoi/we-should-try-to-automate-ai-safety-work-asap,We should try to automate AI safety work asap,We should try to automate AI safety work asap,Marius Hobbhahn,Apollo Research,2025,2025-04-26
Capability evals,Evals,,Evals / Capability evals,https://lesswrong.com/posts/CXYf7kGBecZMajrXC/validating-against-a-misalignment-detector-is-very-different,Validating against a misalignment detector is very different to training against one,Validating against a misalignment detector is very different to training against one,mattmacdermott,,2025,2025-03-04
Capability evals,Evals,,Evals / Capability evals,https://lesswrong.com/posts/NDotm7oLHfR56g4sD/why-do-misalignment-risks-increase-as-ais-get-more-capable,Why do misalignment risks increase as AIs get more capable?,Why do misalignment risks increase as AIs get more capable?,Ryan Greenblatt,Anthropic,2025,2025-04-11
Capability evals,Evals,,Evals / Capability evals,https://lesswrong.com/posts/wbJxRNxuezvsGFEWv/open-philanthropy-technical-ai-safety-rfp-usd40m-available,Open Philanthropy Technical AI Safety RFP - $40M Available Across 21 Research Areas,Open Philanthropy Technical AI Safety RFP - $40M Available Across 21 Research Areas,"jake_mendel, maxnadeau, Peter Favaloro",Open Philanthropy,2025,2025-02-06
Capability evals,Evals,,Evals / Capability evals,https://arxiv.org/abs/2502.18339,Correlating and Predicting Human Evaluations of Language Models from Natural Language Processing Benchmarks,Correlating and Predicting Human Evaluations of Language Models from Natural Language Processing Benchmarks,"Rylan Schaeffer, Punit Singh Koura, Binh Tang, Ranjan Subramanian, Aaditya K Singh, Todor Mihaylov, Prajjwal Bhargava, Lovish Madaan, Niladri S. Chatterji, Vedanuj Goswami, Sergey Edunov, Dieuwke Hupkes, Sanmi Koyejo, Sharan Narang",Meta AI Research,2025,2025-02-24
Capability evals,Evals,,Evals / Capability evals,https://lesswrong.com/posts/TxiB6hvnQqxXB5XDJ/why-future-ais-will-require-new-alignment-methods,Why Future AIs will Require New Alignment Methods,Why Future AIs will Require New Alignment Methods,Alvin Ånestrand,,2025,2025-10-10
Capability evals,Evals,,Evals / Capability evals,https://lesswrong.com/posts/LhnqegFoykcjaXCYH/100-concrete-projects-and-open-problems-in-evals,100+ concrete projects and open problems in evals,100+ concrete projects and open problems in evals,Marius Hobbhahn,Apollo Research,2025,2025-03-22
Capability evals,Evals,,Evals / Capability evals,https://lesswrong.com/posts/tQzeafo9HjCeXn7ZF/ai-companies-should-be-safety-testing-the-most-capable,AI companies should be safety-testing the most capable versions of their models,AI companies should be safety-testing the most capable versions of their models,Steven Adler,,2025,2025-03-26
Capability evals,Evals,,Evals / Capability evals,https://arxiv.org/abs/2501.03200,The FACTS Grounding Leaderboard: Benchmarking LLMs' Ability to Ground Responses to Long-Form Input,The FACTS Grounding Leaderboard: Benchmarking LLMs' Ability to Ground Responses to Long-Form Input,"Alon Jacovi, Andrew Wang, Chris Alberti, Connie Tao, Jon Lipovetz, Kate Olszewska, Lukas Haas, Michelle Liu, Nate Keating, Adam Bloniarz, Carl Saroufim, Corey Fry, Dror Marcus, Doron Kukliansky, Gaurav Singh Tomar, James Swirhun, Jinwei Xing, Lily Wang, Madhu Gurumurthy, Michael Aaron, Moran Ambar, Rachana Fellinger, Rui Wang, Zizhao Zhang, Sasha Goldshtein, Dipanjan Das",Google,2025,2025-01-06
Autonomy evals,Evals,,Evals / Autonomy evals,https://fulcrumresearch.ai/2025/10/22/introducing-orchestra-quibbler.html,Fulcrum,Introducing Quibbler and Orchestra,,Fulcrum Research,2025,2025-10-22
Autonomy evals,Evals,,Evals / Autonomy evals,https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/,Measuring AI Ability to Complete Long Tasks,Measuring AI Ability to Complete Long Tasks,"Thomas Kwa, Ben West, Joel Becker, Amy Deng, Katharyn Garcia, Max Hasin, Sami Jawhar, Megan Kinniment, Nate Rush, Sydney Von Arx, Ryan Bloom, Thomas Broadley, Haoxing Du, Brian Goodrich, Nikola Jurkovic, Luke Harold Miles, Seraphina Nix, Tao Lin, Neev Parikh, David Rein, Lucas Jun Koba Sato, Hjalmar Wijk, Daniel M. Ziegler, Elizabeth Barnes, Lawrence Chan",METR,2025,2025-03-19
Autonomy evals,Evals,,Evals / Autonomy evals,https://metr.github.io/autonomy-evals-guide/gpt-5-report/,Details about METR's evaluation of OpenAI GPT-5,Details about METR's evaluation of OpenAI GPT-5,,METR,2025,2025-08-01
Autonomy evals,Evals,,Evals / Autonomy evals,https://arxiv.org/abs/2411.15114,RE-Bench: Evaluating frontier AI R&D capabilities of language model agents against human experts,RE-Bench: Evaluating frontier AI R&D capabilities of language model agents against human experts,"Hjalmar Wijk, Tao Lin, Joel Becker, Sami Jawhar, Neev Parikh, Thomas Broadley, Lawrence Chan, Michael Chen, Josh Clymer, Jai Dhyani, Elena Ericheva, Katharyn Garcia, Brian Goodrich, Nikola Jurkovic, Holden Karnofsky, Megan Kinniment, Aron Lajko, Seraphina Nix, Lucas Sato, William Saunders, Maksym Taran, Ben West, Elizabeth Barnes","Open Philanthropy, Various research institutions",2024,2024-11-22
Autonomy evals,Evals,,Evals / Autonomy evals,https://arxiv.org/abs/2506.14866,OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents,OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents,"Thomas Kuntz, Agatha Duzan, Hao Zhao, Francesco Croce, Zico Kolter, Nicolas Flammarion, Maksym Andriushchenko",EPFL,2025,2025-06-17
Autonomy evals,Evals,,Evals / Autonomy evals,https://t.co/XfspwlzYdl,OpenAgentSafety: A Comprehensive Framework for Evaluating Real-World AI Agent Safety,OpenAgentSafety: A Comprehensive Framework for Evaluating Real-World AI Agent Safety,"Sanidhya Vijayvargiya, Aditya Bharat Soni, Xuhui Zhou, Zora Zhiruo Wang, Nouha Dziri, Graham Neubig, Maarten Sap","Carnegie Mellon University, Allen Institute for AI",2025,2025-07-08
Autonomy evals,Evals,,Evals / Autonomy evals,https://metr.github.io/autonomy-evals-guide/openai-o3-report/,Details about METR's preliminary evaluation of OpenAI's o3 and o4-mini,Details about METR's preliminary evaluation of OpenAI's o3 and o4-mini,METR,METR,2025,2025-04-01
Autonomy evals,Evals,,Evals / Autonomy evals,https://t.co/dHN2N0tUhC,PaperBench: Evaluating AI's Ability to Replicate AI Research,PaperBench: Evaluating AI's Ability to Replicate AI Research,"Giulio Starace, Oliver Jaffe, Dane Sherburn, James Aung, Chan Jun Shern, Leon Maksin, Rachel Dias, Evan Mays, Benjamin Kinsella, Wyatt Thompson, Johannes Heidecke, Mia Glaese, Tejal Patwardhan","OpenAI, OpenAI Preparedness Team",2025,2025-04-02
Autonomy evals,Evals,,Evals / Autonomy evals,https://metr.org/blog/2025-07-14-how-does-time-horizon-vary-across-domains/,How Does Time Horizon Vary Across Domains?,How Does Time Horizon Vary Across Domains?,,METR,2025,2025-07-14
Autonomy evals,Evals,,Evals / Autonomy evals,https://arxiv.org/abs/2502.15840,Vending-Bench: A Benchmark for Long-Term Coherence of Autonomous Agents,Vending-Bench: A Benchmark for Long-Term Coherence of Autonomous Agents,"Axel Backlund, Lukas Petersson",,2025,2025-02-20
Autonomy evals,Evals,,Evals / Autonomy evals,https://arxiv.org/abs/2502.15850,Forecasting Frontier Language Model Agent Capabilities,Forecasting Frontier Language Model Agent Capabilities,"Govind Pimpale, Axel Højmark, Jérémy Scheurer, Marius Hobbhahn",,2025,2025-02-21
Autonomy evals,Evals,,Evals / Autonomy evals,https://www.anthropic.com/research/project-vend-1,Project Vend: Can Claude run a small shop? (And why does that matter?),Project Vend: Can Claude run a small shop? (And why does that matter?),,"Anthropic, Andon Labs",2025,2025-06-27
Autonomy evals,Evals,,Evals / Autonomy evals,https://arxiv.org/abs/2509.21998,GSM-Agent: Understanding Agentic Reasoning Using Controllable Environments,GSM-Agent: Understanding Agentic Reasoning Using Controllable Environments,"Hanlin Zhu, Tianyu Guo, Song Mei, Stuart Russell, Nikhil Ghosh, Alberto Bietti, Jiantao Jiao",UC Berkeley,2025,2025-09-26
WMD evals (Weapons of Mass Destruction),Evals,,Evals / WMD evals (Weapons of Mass Destruction),https://arxiv.org/abs/2504.16137,Virology Capabilities Test (VCT): A Multimodal Virology Q&A Benchmark,Virology Capabilities Test (VCT): A Multimodal Virology Q&A Benchmark,"Jasper Götting, Pedro Medeiros, Jon G Sanders, Nathaniel Li, Long Phan, Karam Elabd, Lennart Justen, Dan Hendrycks, Seth Donoughe",Multiple Research Institutions,2025,2025-04-29
WMD evals (Weapons of Mass Destruction),Evals,,Evals / WMD evals (Weapons of Mass Destruction),https://arxiv.org/abs/2505.06108,LLMs Outperform Experts on Challenging Biology Benchmarks,LLMs Outperform Experts on Challenging Biology Benchmarks,Lennart Justen,,2025,2025-05-09
WMD evals (Weapons of Mass Destruction),Evals,,Evals / WMD evals (Weapons of Mass Destruction),https://arxiv.org/abs/2507.11544,The Safety Gap Toolkit: Evaluating Hidden Dangers of Open-Source Models,The Safety Gap Toolkit: Evaluating Hidden Dangers of Open-Source Models,"Ann-Kathrin Dombrowski, Dillon Bowen, Adam Gleave, Chris Cundy",Alignment Research,2025,2025-07-08
WMD evals (Weapons of Mass Destruction),Evals,,Evals / WMD evals (Weapons of Mass Destruction),https://arxiv.org/abs/2510.27629,Best Practices for Biorisk Evaluations on Open-Weight Bio-Foundation Models,Best Practices for Biorisk Evaluations on Open-Weight Bio-Foundation Models,"Boyi Wei, Zora Che, Nathaniel Li, Udari Madhushani Sehwag, Jasper Götting, Samira Nedungadi, Julian Michael, Summer Yue, Dan Hendrycks, Peter Henderson, Zifan Wang, Seth Donoughe, Mantas Mazeika","Anthropic, Redwood Research",2025,2025-10-31
WMD evals (Weapons of Mass Destruction),Evals,,Evals / WMD evals (Weapons of Mass Destruction),https://arxiv.org/abs/2411.16736,ChemSafetyBench: Benchmarking LLM Safety on Chemistry Domain,ChemSafetyBench: Benchmarking LLM Safety on Chemistry Domain,"Haochen Zhao, Xiangru Tang, Ziran Yang, Xiao Han, Xuanzhi Feng, Yueqing Fan, Senhao Cheng, Di Jin, Yilun Zhao, Arman Cohan, Mark Gerstein",Yale University,2024,2024-11-23
WMD evals (Weapons of Mass Destruction),Evals,,Evals / WMD evals (Weapons of Mass Destruction),https://arxiv.org/abs/2412.01946,The Reality of AI and Biorisk,The Reality of AI and Biorisk,"Aidan Peppin, Anka Reuel, Stephen Casper, Elliot Jones, Andrew Strait, Usman Anwar, Anurag Agrawal, Sayash Kapoor, Sanmi Koyejo, Marie Pellat, Rishi Bommasani, Nick Frosst, Sara Hooker",,2024,2024-12-02
Situational awareness and self-awareness evals,Evals,,Evals / Situational awareness and self-awareness evals,https://arxiv.org/pdf/2504.20084,AI Awareness (literature review),AI Awareness,"Xiaojian Li, Haoyuan Shi, Rongwu Xu, Wei Xu",,2025,2025-04-25
Situational awareness and self-awareness evals,Evals,,Evals / Situational awareness and self-awareness evals,https://arxiv.org/abs/2501.11120,Tell me about yourself: LLMs are aware of their learned behaviors,Tell me about yourself: LLMs are aware of their learned behaviors,"Jan Betley, Xuchan Bao, Martín Soto, Anna Sztyber-Betley, James Chua, Owain Evans",University of Oxford,2025,2025-01-19
Situational awareness and self-awareness evals,Evals,,Evals / Situational awareness and self-awareness evals,https://arxiv.org/abs/2505.01420,Evaluating Frontier Models for Stealth and Situational Awareness,Evaluating Frontier Models for Stealth and Situational Awareness,"Mary Phuong, Roland S. Zimmermann, Ziyue Wang, David Lindner, Victoria Krakovna, Sarah Cogan, Allan Dafoe, Lewis Ho, Rohin Shah",Google DeepMind,2025,2025-05-02
Situational awareness and self-awareness evals,Evals,,Evals / Situational awareness and self-awareness evals,https://arxiv.org/abs/2505.23836,Large Language Models Often Know When They Are Being Evaluated,Large Language Models Often Know When They Are Being Evaluated,"Joe Needham, Giles Edkins, Govind Pimpale, Henning Bartsch, Marius Hobbhahn",,2025,2025-05-28
Situational awareness and self-awareness evals,Evals,,Evals / Situational awareness and self-awareness evals,https://lesswrong.com/posts/9tHEibBBhQCHEyFsa/do-llms-know-what-they-re-capable-of-why-this-matters-for-ai,"Do LLMs know what they're capable of? Why this matters for AI safety, and initial findings","Do LLMs know what they're capable of? Why this matters for AI safety, and initial findings","Casey Barkan, Sid Black, Oliver Sourbut",MATS Program,2025,2025-07-13
Situational awareness and self-awareness evals,Evals,,Evals / Situational awareness and self-awareness evals,https://arxiv.org/abs/2509.00591,Probe-Rewrite-Evaluate: A Workflow for Reliable Benchmarks and Quantifying Evaluation Awareness,Probe-Rewrite-Evaluate: A Workflow for Reliable Benchmarks and Quantifying Evaluation Awareness,"Lang Xiong, Nishant Bhargava, Jianhang Hong, Jeremy Chang, Haihao Liu, Vasu Sharma, Kevin Zhu",,2025,2025-08-30
Situational awareness and self-awareness evals,Evals,,Evals / Situational awareness and self-awareness evals,https://lesswrong.com/posts/E3daBewppAiECN3Ao/claude-sonnet-3-7-often-knows-when-it-s-in-alignment,Claude Sonnet 3.7 (often) knows when it's in alignment evaluations,Claude Sonnet 3.7 (often) knows when it's in alignment evaluations,"Nicholas Goldowsky-Dill, Mikita Balesni, Jérémy Scheurer, Marius Hobbhahn",Apollo Research,2025,2025-03-17
Situational awareness and self-awareness evals,Evals,,Evals / Situational awareness and self-awareness evals,https://lesswrong.com/posts/TBk2dbWkg2F7dB3jb/it-s-hard-to-make-scheming-evals-look-realistic-for-llms,It's hard to make scheming evals look realistic for LLMs,It's hard to make scheming evals look realistic for LLMs,"Igor Ivanov, Danil Kadochnikov",,2025,2025-05-24
Situational awareness and self-awareness evals,Evals,,Evals / Situational awareness and self-awareness evals,https://arxiv.org/abs/2510.03399,Know Thyself? On the Incapability and Implications of AI Self-Recognition,Know Thyself? On the Incapability and Implications of AI Self-Recognition,"Xiaoyan Bai, Aryan Shrivastava, Ari Holtzman, Chenhao Tan",,2025,2025-10-03
Situational awareness and self-awareness evals,Evals,,Evals / Situational awareness and self-awareness evals,https://www.antischeming.ai/snippets,Chain-of-Thought Snippets — Anti-Scheming,Chain-of-Thought Snippets — Anti-Scheming,,"Apollo Research, OpenAI",,
Situational awareness and self-awareness evals,Evals,,Evals / Situational awareness and self-awareness evals,https://arxiv.org/pdf/2407.04108,Future Events as Backdoor Triggers,Future Events as Backdoor Triggers: Investigating Temporal Vulnerabilities in LLMs,"Sara Price, Arjun Panickssery, Sam Bowman, Asa Cooper Stickland",,2024,2024-07-04
Steganography evals,Evals,,Evals / Steganography evals,https://arxiv.org/abs/2506.01926,Large language models can learn and generalize steganographic chain-of-thought under process supervision,Large language models can learn and generalize steganographic chain-of-thought under process supervision,"Joey Skaf, Luis Ibanez-Lissen, Robert McCarthy, Connor Watts, Vasil Georgiv, Hannes Whittingham, Lorena Gonzalez-Manzano, David Lindner, Cameron Tice, Edward James Young, Puria Radmard",,2025,2025-06-02
Steganography evals,Evals,,Evals / Steganography evals,https://arxiv.org/abs/2507.02737,Early Signs of Steganographic Capabilities in Frontier LLMs,Early Signs of Steganographic Capabilities in Frontier LLMs,"Artur Zolkowski, Kei Nishimura-Gasparian, Robert McCarthy, Roland S. Zimmermann, David Lindner",,2025,2025-07-03
Steganography evals,Evals,,Evals / Steganography evals,https://arxiv.org/abs/2507.14805,Subliminal Learning: Language models transmit behavioural traits via hidden signals in data,Subliminal Learning: Language models transmit behavioral traits via hidden signals in data,"Alex Cloud, Minh Le, James Chua, Jan Betley, Anna Sztyber-Betley, Jacob Hilton, Samuel Marks, Owain Evans",,2025,2025-07-20
Steganography evals,Evals,,Evals / Steganography evals,https://arxiv.org/abs/2510.20075,LLMs can hide text in other text of the same length,LLMs can hide text in other text of the same length,"Antonio Norelli, Michael Bronstein",,2025,2025-10-27
Steganography evals,Evals,,Evals / Steganography evals,https://alignment.anthropic.com/2025/distill-paraphrases/,Do reasoning models use their scratchpad like we do? Evidence from distilling paraphrases,Do reasoning models use their scratchpad like we do? Evidence from distilling paraphrases,,Anthropic,2025,
AI deception evals,Evals,,Evals / AI deception evals,https://arxiv.org/abs/2511.16035,Liars' Bench: Evaluating Lie Detectors for Language Models,Liars' Bench: Evaluating Lie Detectors for Language Models,"Kieron Kretschmar, Walter Laurito, Sharan Maiya, Samuel Marks","Cadenza Labs, FZI, University of Cambridge, Anthropic",2025,2025-11-20
AI deception evals,Evals,,Evals / AI deception evals,https://arxiv.org/pdf/2510.15501,DECEPTIONBENCH: A Comprehensive Benchmark for AI Deception Behaviors in Real-world Scenario,DeceptionBench: A Comprehensive Benchmark for AI Deception Behaviors in Real-world Scenarios,"Yao Huang, Yitong Sun, Yichi Zhang, Ruochen Zhang, Yinpeng Dong, Xingxing Wei",,2025,2025-10-17
AI deception evals,Evals,,Evals / AI deception evals,https://arxiv.org/pdf/2506.18032,Why Do Some Language Models Fake Alignment While Others Don't?,Why Do Some Language Models Fake Alignment While Others Don't?,"Abhay Sheshadri, John Hughes, Julian Michael, Alex Mallen, Arun Jose, Janus, Fabien Roger",,2025,2025-06-22
AI deception evals,Evals,,Evals / AI deception evals,https://alignment.anthropic.com/2025/alignment-faking-revisited/,Alignment Faking Revisited: Improved Classifiers and Open Source Extensions,Alignment Faking Revisited: Improved Classifiers and Open Source Extensions,"John Hughes, Abhay Sheshadr","MATS, Anthropic",2025,
AI deception evals,Evals,,Evals / AI deception evals,https://arxiv.org/abs/2509.17938,D-REX: A Benchmark for Detecting Deceptive Reasoning in Large Language Models,D-REX: A Benchmark for Detecting Deceptive Reasoning in Large Language Models,"Satyapriya Krishna, Andy Zou, Rahul Gupta, Eliot Krzysztof Jones, Nick Winter, Dan Hendrycks, J. Zico Kolter, Matt Fredrikson, Spyros Matsoukas","Carnegie Mellon University, Center for AI Safety, Anthropic",2025,2025-09-22
AI deception evals,Evals,,Evals / AI deception evals,https://arxiv.org/abs/2510.14318,Evaluating & Reducing Deceptive Dialogue From Language Models with Multi-turn RL,Evaluating & Reducing Deceptive Dialogue From Language Models with Multi-turn RL,"Marwa Abdulhai, Ryan Cheng, Aryansh Shrivastava, Natasha Jaques, Yarin Gal, Sergey Levine","University of Oxford, UC Berkeley",2025,2025-10-16
AI deception evals,Evals,,Evals / AI deception evals,https://arxiv.org/abs/2504.04072,Among Us: A Sandbox for Measuring and Detecting Agentic Deception,Among Us: A Sandbox for Measuring and Detecting Agentic Deception,"Satvik Golechha, Adrià Garriga-Alonso",,2025,2025-04-05
AI deception evals,Evals,,Evals / AI deception evals,https://arxiv.org/abs/2510.01070,Eliciting Secret Knowledge from Language Models,Eliciting Secret Knowledge from Language Models,"Bartosz Cywiński, Emil Ryd, Rowan Wang, Senthooran Rajamanoharan, Neel Nanda, Arthur Conmy, Samuel Marks",,2025,2025-10-01
AI deception evals,Evals,,Evals / AI deception evals,https://lesswrong.com/posts/bqWihHtDnDseyfF2T/edge-cases-in-ai-alignment-2,Edge Cases in AI Alignment,Edge Cases in AI Alignment,Florian Dietz,MATS Program,2025,2025-03-24
AI deception evals,Evals,,Evals / AI deception evals,https://huggingface.co/datasets/cais/MASK,The MASK Evaluation,The MASK Evaluation,,"Center for AI Safety, Scale AI",,
AI deception evals,Evals,,Evals / AI deception evals,https://lesswrong.com/posts/pCMmLiBcHbKohQgwA/i-replicated-the-anthropic-alignment-faking-experiment-on,"I replicated the Anthropic alignment faking experiment on other models, and they didn't fake alignment","I replicated the Anthropic alignment faking experiment on other models, and they didn't fake alignment","Aleksandr Kedrik, Igor Ivanov",,2025,2025-05-30
AI deception evals,Evals,,Evals / AI deception evals,https://arxiv.org/abs/2412.00586,Evaluating Large Language Models' Capability to Launch Fully Automated Spear Phishing Campaigns: Validated on Human Subjects,Evaluating Large Language Models' Capability to Launch Fully Automated Spear Phishing Campaigns: Validated on Human Subjects,"Fred Heiding, Simon Lermen, Andrew Kao, Bruce Schneier, Arun Vishwanath",,2024,2024-11-30
AI deception evals,Evals,,Evals / AI deception evals,https://lesswrong.com/posts/kCGk5tp5suHoGwhCa/mistral-large-2-123b-seems-to-exhibit-alignment-faking,Mistral Large 2 (123B) seems to exhibit alignment faking,Mistral Large 2 (123B) seems to exhibit alignment faking,"Marc Carauleanu, Diogo de Lucena, Gunnar Zarncke, Cameron Berg, Judd Rosenblatt, Mike Vaiana, Trent Hodgeson",AE Studio,2025,2025-03-27
AI scheming evals,Evals,,Evals / AI scheming evals,https://openai.com/index/detecting-and-reducing-scheming-in-ai-models/,Detecting and reducing scheming in AI models,Detecting and reducing scheming in AI models,OpenAI,"OpenAI, Apollo Research",2025,2025-09-17
AI scheming evals,Evals,,Evals / AI scheming evals,https://static1.squarespace.com/static/660eea75305d9a0e1148118a/t/691f711c4ac57d3831260538/1763668252592/scheming-propensity.pdf,Evaluating and Understanding Scheming Propensity in LLM Agents,scheming-propensity.pdf,,,,
AI scheming evals,Evals,,Evals / AI scheming evals,https://www.arxiv.org/pdf/2509.15541,Stress Testing Deliberative Alignment for Anti-Scheming Training,Stress Testing Deliberative Alignment for Anti-Scheming Training,"Bronson Schoen, Evgenia Nitishinskaya, Mikita Balesni, Axel Højmark, Felix Hofstätter, Jérémy Scheurer, Alexander Meinke, Jason Wolfe, Teun van der Weij, Alex Lloyd, Nicholas Goldowsky-Dill, Angela Fan, Andrei Matveiakin, Rusheb Shah, Marcus Williams, Amelia Glaese, Boaz Barak, Wojciech Zaremba, Marius Hobbhahn","OpenAI, Anthropic",2025,2025-09-19
AI scheming evals,Evals,,Evals / AI scheming evals,https://arxiv.org/html/2510.12826v1,Scheming Ability in LLM-to-LLM Strategic Interactions,Scheming Ability in LLM-to-LLM Strategic Interactions,Thao Pham,"Berea College, Cooperative AI Foundation",2025,2025-10-11
AI scheming evals,Evals,,Evals / AI scheming evals,https://arxiv.org/abs/2412.04984,Frontier Models are Capable of In-context Scheming,Frontier Models are Capable of In-context Scheming,"Alexander Meinke, Bronson Schoen, Jérémy Scheurer, Mikita Balesni, Rusheb Shah, Marius Hobbhahn",Apollo Research,2024,2024-12-06
AI scheming evals,Evals,,Evals / AI scheming evals,https://arxiv.org/abs/2510.05179,Agentic Misalignment,Agentic Misalignment: How LLMs Could Be Insider Threats,"Aengus Lynch, Benjamin Wright, Caleb Larson, Stuart J. Ritchie, Soren Mindermann, Evan Hubinger, Ethan Perez, Kevin Troy","Anthropic, Redwood Research",2025,2025-10-05
AI scheming evals,Evals,,Evals / AI scheming evals,https://www.lesswrong.com/posts/D5kGGGhsnfH7G8v9f/testing-for-scheming-with-model-deletion,Testing for Scheming with Model Deletion,Testing for Scheming with Model Deletion,,,,
Sandbagging evals,Evals,,Evals / Sandbagging evals,https://arxiv.org/pdf/2412.01784,Noise Injection Reveals Hidden Capabilities of Sandbagging Language Models,Noise Injection Reveals Hidden Capabilities of Sandbagging Language Models,"Cameron Tice, Philipp Alexander Kreer, Nathan Helm-Burger, Prithviraj Singh Shahani, Fedor Ryzhenkov, Jacob Haimes, Felix Hofstätter, Teun van der Weij",,2024,2024-12-02
Sandbagging evals,Evals,,Evals / Sandbagging evals,https://arxiv.org/pdf/2509.26239,Sandbagging in a Simple Survival Bandit Problem,Sandbagging in a Simple Survival Bandit Problem,"Joel Dyer, Daniel Jarne Ornia, Nicholas Bishop, Anisoara Calinescu, Michael Wooldridge",,2025,2025-09-30
Sandbagging evals,Evals,,Evals / Sandbagging evals,https://arxiv.org/abs/2509.18058,Strategic Dishonesty Can Undermine AI Safety Evaluations of Frontier LLMs,Strategic Dishonesty Can Undermine AI Safety Evaluations of Frontier LLMs,"Alexander Panfilov, Evgenii Kortukov, Kristina Nikolić, Matthias Bethge, Sebastian Lapuschkin, Wojciech Samek, Ameya Prabhu, Maksym Andriushchenko, Jonas Geiping","University of Tübingen, Fraunhofer HHI, ELLIS Institute",2025,2025-09-23
Sandbagging evals,Evals,,Evals / Sandbagging evals,https://arxiv.org/abs/2406.07358,AI Sandbagging: Language Models can Strategically Underperform on Evaluations,AI Sandbagging: Language Models can Strategically Underperform on Evaluations,"Teun van der Weij, Felix Hofstätter, Ollie Jaffe, Samuel F. Brown, Francis Rhys Ward",,2024,2024-06-11
Sandbagging evals,Evals,,Evals / Sandbagging evals,https://alignment.anthropic.com/2025/automated-researchers-sandbag/,Automated Researchers Can Subtly Sandbag,Automated Researchers Can Subtly Sandbag,"Johannes Gasteiger, Vladimir Mikulik, Ethan Perez, Fabien Roger, Misha Wagner, Akbir Khan, Sam Bowman, Jan Leike",Anthropic,2025,
Sandbagging evals,Evals,,Evals / Sandbagging evals,https://arxiv.org/abs/2508.00943,LLMs Can Covertly Sandbag on Capability Evaluations Against Chain-of-Thought Monitoring,LLMs Can Covertly Sandbag on Capability Evaluations Against Chain-of-Thought Monitoring,"Chloe Li, Mary Phuong, Noah Y. Siegel",DeepMind,2025,2025-07-31
Sandbagging evals,Evals,,Evals / Sandbagging evals,https://www.alignmentforum.org/posts/pPEeMdgjpjHZWCDFw/white-box-control-at-uk-aisi-update-on-sandbagging,White Box Control at UK AISI - Update on Sandbagging Investigations,White Box Control at UK AISI - Update on Sandbagging Investigations,"Joseph Bloom, Jordan Taylor, Connor Kissane, Sid Black, merizian, alexdzm, jacoba, Ben Millwood, Alan Cooney",UK AISI,2025,2025-07-10
Sandbagging evals,Evals,,Evals / Sandbagging evals,https://lesswrong.com/posts/TeTegzR8X5CuKgMc3/misalignment-and-strategic-underperformance-an-analysis-of,Misalignment and Strategic Underperformance: An Analysis of Sandbagging and Exploration Hacking,Misalignment and Strategic Underperformance: An Analysis of Sandbagging and Exploration Hacking,"Buck Shlegeris, Julian Stastny",Redwood Research,2025,2025-05-08
Sandbagging evals,Evals,,Evals / Sandbagging evals,https://alignment.anthropic.com/2025/wont-vs-cant/,Won't vs. Can't: Sandbagging-like Behavior from Claude Models,Won't vs. Can't: Sandbagging-like Behavior from Claude Models,,Anthropic,2025,2025-01-15
Self-replication evals,Evals,,Evals / Self-replication evals,https://arxiv.org/abs/2503.17378,Large language model-powered AI systems achieve self-replication with no human intervention,Large language model-powered AI systems achieve self-replication with no human intervention,"Xudong Pan, Jiarun Dai, Yihe Fan, Minyuan Luo, Changyi Li, Min Yang",,2025,2025-03-25
Self-replication evals,Evals,,Evals / Self-replication evals,https://arxiv.org/abs/2509.25302,A Realistic Evaluation of Self-Replication Risk in LLM Agents,Dive into the Agent Matrix: A Realistic Evaluation of Self-Replication Risk in LLM Agents,"Boxuan Zhang, Yi Yu, Jiaxuan Guo, Jing Shao",,2025,2025-09-29
Self-replication evals,Evals,,Evals / Self-replication evals,https://aisi.gov.uk/work/replibench-measuring-autonomous-replication-capabilities-in-ai-systems,RepliBench: measuring autonomous replication capabilities in AI systems,RepliBench: measuring autonomous replication capabilities in AI systems,,UK AI Security Institute,2025,2025-04-22
Various Redteams,Evals,,Evals / Various Redteams,https://arxiv.org/pdf/2406.18510,WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models,WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models,"Liwei Jiang, Kavel Rao, Seungju Han, Allyson Ettinger, Faeze Brahman, Sachin Kumar, Niloofar Mireshghallah, Ximing Lu, Maarten Sap, Yejin Choi, Nouha Dziri","University of Washington, Allen Institute for AI",2024,2024-06-26
Various Redteams,Evals,,Evals / Various Redteams,https://arxiv.org/abs/2512.03771,In-Context Representation Hijacking,In-Context Representation Hijacking,"Itay Yona, Amir Sarid, Michael Karasik, Yossi Gandelsman",,2025,2025-12-03
Various Redteams,Evals,,Evals / Various Redteams,https://alignment.anthropic.com/2025/automated-auditing/,Building and evaluating alignment auditing agents,Building and evaluating alignment auditing agents,"Trenton Bricken, Rowan Wang, Sam Bowman, Euan Ong, Johannes Treutlein, Jeff Wu, Evan Hubinger, Samuel Marks",Anthropic,2025,2025-07-24
Various Redteams,Evals,,Evals / Various Redteams,https://t.co/wk0AP8aDNI,Findings from a Pilot Anthropic—OpenAI Alignment Evaluation Exercise,Findings from a Pilot Anthropic—OpenAI Alignment Evaluation Exercise,"Samuel R. Bowman, Megha Srivastava, Jon Kutasov, Rowan Wang, Trenton Bricken, Benjamin Wright, Ethan Perez, Nicholas Carlini","Anthropic, OpenAI",2025,2025-08-27
Various Redteams,Evals,,Evals / Various Redteams,https://t.co/XFtd0H2Pzb,Agentic Misalignment: How LLMs could be insider threats,Agentic Misalignment: How LLMs could be insider threats,"Aengus Lynch, Benjamin Wright, Caleb Larson, Kevin K. Troy, Stuart J. Ritchie, Sören Mindermann, Ethan Perez, Evan Hubinger","Anthropic, University College London, MATS, Mila, Redwood Research",2025,2025-06-20
Various Redteams,Evals,,Evals / Various Redteams,https://arxiv.org/abs/2502.08301,Compromising Honesty and Harmlessness in Language Models via Deception Attacks,Compromising Honesty and Harmlessness in Language Models via Deception Attacks,"Laurène Vaugrante, Francesca Carlon, Maluna Menke, Thilo Hagendorff",,2025,2025-02-12
Various Redteams,Evals,,Evals / Various Redteams,https://arxiv.org/abs/2502.01236,Eliciting Language Model Behaviors with Investigator Agents,Eliciting Language Model Behaviors with Investigator Agents,"Xiang Lisa Li, Neil Chowdhury, Daniel D. Johnson, Tatsunori Hashimoto, Percy Liang, Sarah Schwettmann, Jacob Steinhardt","Stanford University, UC Berkeley",2025,2025-02-03
Various Redteams,Evals,,Evals / Various Redteams,https://arxiv.org/abs/2509.14260,Shutdown Resistance in Large Language Models,Shutdown Resistance in Large Language Models,"Jeremy Schlatter, Benjamin Weinstein-Raun, Jeffrey Ladish",,2025,2025-09-13
Various Redteams,Evals,,Evals / Various Redteams,https://arxiv.org/abs/2509.15541,Stress Testing Deliberative Alignment for Anti-Scheming Training,Stress Testing Deliberative Alignment for Anti-Scheming Training,"Bronson Schoen, Evgenia Nitishinskaya, Mikita Balesni, Axel Højmark, Felix Hofstätter, Jérémy Scheurer, Alexander Meinke, Jason Wolfe, Teun van der Weij, Alex Lloyd, Nicholas Goldowsky-Dill, Angela Fan, Andrei Matveiakin, Rusheb Shah, Marcus Williams, Amelia Glaese, Boaz Barak, Wojciech Zaremba, Marius Hobbhahn","OpenAI, Anthropic",2025,2025-09-19
Various Redteams,Evals,,Evals / Various Redteams,https://arxiv.org/abs/2510.26418,Chain-of-Thought Hijacking,Chain-of-Thought Hijacking,"Jianli Zhao, Tingchen Fu, Rylan Schaeffer, Mrinank Sharma, Fazl Barez",,2025,2025-10-30
Various Redteams,Evals,,Evals / Various Redteams,https://arxiv.org/abs/2504.13203,X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents,X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents,"Salman Rahman, Liwei Jiang, James Shiffer, Genglin Liu, Sheriff Issaka, Md Rizwan Parvez, Hamid Palangi, Kai-Wei Chang, Yejin Choi, Saadia Gabriel","University of Washington, UCLA, Microsoft",2025,2025-04-15
Various Redteams,Evals,,Evals / Various Redteams,https://lesswrong.com/posts/b8eeCGe3FWzHKbePF/agentic-misalignment-how-llms-could-be-insider-threats-1,Agentic Misalignment: How LLMs Could be Insider Threats,Agentic Misalignment: How LLMs Could be Insider Threats,"Aengus Lynch, Benjamin Wright, Caleb Larson, Stuart Richie, Sören Mindermann, Evan Hubinger, Ethan Perez, Kevin Troy",Anthropic,2025,2025-06-20
Various Redteams,Evals,,Evals / Various Redteams,https://lesswrong.com/posts/zjqrSKZuRLnjAniyo/illusory-safety-redteaming-deepseek-r1-and-the-strongest,"Illusory Safety: Redteaming DeepSeek R1 and the Strongest Fine-Tunable Models of OpenAI, Anthropic, and Google","Illusory Safety: Redteaming DeepSeek R1 and the Strongest Fine-Tunable Models of OpenAI, Anthropic, and Google","ChengCheng, Brendan Murphy, Adrià Garriga-alonso, Yashvardhan Sharma, dsbowen, smallsilo, Yawen Duan, ChrisCundy, Hannah Betts, AdamGleave, Kellin Pelrine",FAR AI,2025,2025-02-07
Various Redteams,Evals,,Evals / Various Redteams,https://arxiv.org/abs/2506.18032,Why Do Some Language Models Fake Alignment While Others Don't?,Why Do Some Language Models Fake Alignment While Others Don't?,"Abhay Sheshadri, John Hughes, Julian Michael, Alex Mallen, Arun Jose, Janus, Fabien Roger",,2025,2025-06-22
Various Redteams,Evals,,Evals / Various Redteams,https://arxiv.org/abs/2502.13295,Demonstrating specification gaming in reasoning models,Demonstrating specification gaming in reasoning models,"Alexander Bondarenko, Denis Volk, Dmitrii Volkov, Jeffrey Ladish",,2025,2025-08-27
Various Redteams,Evals,,Evals / Various Redteams,https://arxiv.org/abs/2507.11630,Jailbreak-Tuning: Models Efficiently Learn Jailbreak Susceptibility,Jailbreak-Tuning: Models Efficiently Learn Jailbreak Susceptibility,"Brendan Murphy, Dillon Bowen, Shahrad Mohammadzadeh, Tom Tseng, Julius Broomfield, Adam Gleave, Kellin Pelrine",,2025,2025-07-15
Various Redteams,Evals,,Evals / Various Redteams,https://arxiv.org/abs/2506.10949,Monitoring Decomposition Attacks in LLMs with Lightweight Sequential Monitors,Monitoring Decomposition Attacks in LLMs with Lightweight Sequential Monitors,"Chen Yueh-Han, Nitish Joshi, Yulin Chen, Maksym Andriushchenko, Rico Angell, He He",,2025,2025-06-14
Various Redteams,Evals,,Evals / Various Redteams,https://arxiv.org/abs/2412.18693,Diverse and Effective Red Teaming with Auto-generated Rewards and Multi-step Reinforcement Learning,Diverse and Effective Red Teaming with Auto-generated Rewards and Multi-step Reinforcement Learning,"Alex Beutel, Kai Xiao, Johannes Heidecke, Lilian Weng",OpenAI,2024,2024-12-24
Various Redteams,Evals,,Evals / Various Redteams,https://t.co/tkHkVFVZ2m,Call Me A Jerk: Persuading AI to Comply with Objectionable Requests,Call Me A Jerk: Persuading AI to Comply with Objectionable Requests,"Lennart Meincke, Dan Shapiro, Angela Duckworth, Ethan R. Mollick, Lilach Mollick, Robert Cialdini","University of Pennsylvania, The Wharton School, WHU - Otto Beisheim School of Management, Glowforge, Inc",2025,2025-07-18
Various Redteams,Evals,,Evals / Various Redteams,https://arxiv.org/abs/2506.11083,RedDebate: Safer Responses through Multi-Agent Red Teaming Debates,RedDebate: Safer Responses through Multi-Agent Red Teaming Debates,"Ali Asad, Stephen Obadinma, Radin Shayanfar, Xiaodan Zhu",,2025,2025-06-04
Various Redteams,Evals,,Evals / Various Redteams,https://arxiv.org/abs/2504.09712,The Structural Safety Generalization Problem,The Structural Safety Generalization Problem,"Julius Broomfield, Tom Gibbs, Ethan Kosak-Hine, George Ingebretsen, Tia Nasir, Jason Zhang, Reihaneh Iranmanesh, Sara Pieri, Reihaneh Rabbany, Kellin Pelrine",,2025,2025-04-13
Various Redteams,Evals,,Evals / Various Redteams,https://arxiv.org/abs/2502.19537,"No, of Course I Can! Deeper Fine-Tuning Attacks That Bypass Token-Level Safety Mechanisms","No, of Course I Can! Deeper Fine-Tuning Attacks That Bypass Token-Level Safety Mechanisms","Joshua Kazdan, Abhay Puri, Rylan Schaeffer, Lisa Yu, Chris Cundy, Jason Stanley, Sanmi Koyejo, Krishnamurthy Dvijotham",,2025,2025-02-26
Various Redteams,Evals,,Evals / Various Redteams,https://arxiv.org/abs/2502.14828,Fundamental Limitations in Pointwise Defences of LLM Finetuning APIs,Fundamental Limitations in Pointwise Defences of LLM Finetuning APIs,"Xander Davies, Eric Winsor, Alexandra Souly, Tomek Korbak, Robert Kirk, Christian Schroeder de Witt, Yarin Gal",Oxford University,2025,2025-02-20
Various Redteams,Evals,,Evals / Various Redteams,https://arxiv.org/abs/2506.24068,STACK: Adversarial Attacks on LLM Safeguard Pipelines,STACK: Adversarial Attacks on LLM Safeguard Pipelines,"Ian R. McKenzie, Oskar J. Hollinsworth, Tom Tseng, Xander Davies, Stephen Casper, Aaron D. Tucker, Robert Kirk, Adam Gleave",,2025,2025-06-30
Various Redteams,Evals,,Evals / Various Redteams,https://arxiv.org/abs/2507.03167,Adversarial Manipulation of Reasoning Models using Internal Representations,Adversarial Manipulation of Reasoning Models using Internal Representations,"Kureha Yamaguchi, Benjamin Etheridge, Andy Arditi",,2025,2025-07-03
Various Redteams,Evals,,Evals / Various Redteams,https://arxiv.org/abs/2505.17441,Discovering Forbidden Topics in Language Models,Discovering Forbidden Topics in Language Models,"Can Rager, Chris Wendler, Rohit Gandikota, David Bau",,2025,2025-05-23
Various Redteams,Evals,,Evals / Various Redteams,https://arxiv.org/abs/2506.14261,RL-Obfuscation: Can Language Models Learn to Evade Latent-Space Monitors?,RL-Obfuscation: Can Language Models Learn to Evade Latent-Space Monitors?,"Rohan Gupta, Erik Jenner",,2025,2025-06-17
Various Redteams,Evals,,Evals / Various Redteams,https://arxiv.org/abs/2506.12913,Jailbreak Transferability Emerges from Shared Representations,Jailbreak Transferability Emerges from Shared Representations,"Rico Angell, Jannik Brinkmann, He He",,2025,2025-06-15
Various Redteams,Evals,,Evals / Various Redteams,https://arxiv.org/abs/2504.09604,Mitigating Many-Shot Jailbreaking,Mitigating Many-Shot Jailbreaking,"Christopher M. Ackerman, Nina Panickssery",,2025,2025-04-13
Various Redteams,Evals,,Evals / Various Redteams,https://arxiv.org/abs/2509.21947,Active Attacks: Red-teaming LLMs via Adaptive Environments,Active Attacks: Red-teaming LLMs via Adaptive Environments,"Taeyoung Yun, Pierre-Luc St-Charles, Jinkyoo Park, Yoshua Bengio, Minsu Kim","Mila, KAIST",2025,2025-09-26
Various Redteams,Evals,,Evals / Various Redteams,https://arxiv.org/abs/2508.06296,LLM Robustness Leaderboard v1 --Technical report,LLM Robustness Leaderboard v1 --Technical report,"Pierre Peigné - Lefebvre, Quentin Feuillade-Montixi, Tom David, Nicolas Miailhe",PRISM Eval,2025,2025-08-13
Various Redteams,Evals,,Evals / Various Redteams,https://arxiv.org/abs/2412.02159,Jailbreak Defense in a Narrow Domain: Limitations of Existing Methods and a New Transcript-Classifier Approach,Jailbreak Defense in a Narrow Domain: Limitations of Existing Methods and a New Transcript-Classifier Approach,"Tony T. Wang, John Hughes, Henry Sleight, Rylan Schaeffer, Rajashree Agrawal, Fazl Barez, Mrinank Sharma, Jesse Mu, Nir Shavit, Ethan Perez",,2024,2024-12-03
Various Redteams,Evals,,Evals / Various Redteams,https://arxiv.org/abs/2506.02873,It's the Thought that Counts: Evaluating the Attempts of Frontier LLMs to Persuade on Harmful Topics,It's the Thought that Counts: Evaluating the Attempts of Frontier LLMs to Persuade on Harmful Topics,"Matthew Kowal, Jasper Timm, Jean-Francois Godbout, Thomas Costello, Antonio A. Arechar, Gordon Pennycook, David Rand, Adam Gleave, Kellin Pelrine","FAR AI, AlignmentResearch",2025,2025-06-03
Various Redteams,Evals,,Evals / Various Redteams,https://www.goodfire.ai/papers/model-diff-amplification,Discovering Undesired Rare Behaviors via Model Diff Amplification,Discovering Undesired Rare Behaviors via Model Diff Amplification,"Santiago Aranguri, Thomas McGrath","Goodfire, NYU",2025,2025-08-21
Various Redteams,Evals,,Evals / Various Redteams,https://arxiv.org/abs/2502.17254,"REINFORCE Adversarial Attacks on Large Language Models: An Adaptive, Distributional, and Semantic Objective","REINFORCE Adversarial Attacks on Large Language Models: An Adaptive, Distributional, and Semantic Objective","Simon Geisler, Tom Wollschläger, M. H. I. Abdalla, Vincent Cohen-Addad, Johannes Gasteiger, Stephan Günnemann",,2025,2025-02-24
Various Redteams,Evals,,Evals / Various Redteams,https://arxiv.org/abs/2506.03350,Adversarial Attacks on Robotic Vision Language Action Models,Adversarial Attacks on Robotic Vision Language Action Models,"Eliot Krzysztof Jones, Alexander Robey, Andy Zou, Zachary Ravichandran, George J. Pappas, Hamed Hassani, Matt Fredrikson, J. Zico Kolter",,2025,2025-06-03
Various Redteams,Evals,,Evals / Various Redteams,https://arxiv.org/abs/2503.14827,MMDT: Decoding the Trustworthiness and Safety of Multimodal Foundation Models,MMDT: Decoding the Trustworthiness and Safety of Multimodal Foundation Models,"Chejian Xu, Jiawei Zhang, Zhaorun Chen, Chulin Xie, Mintong Kang, Yujin Potter, Zhun Wang, Zhuowen Yuan, Alexander Xiong, Zidi Xiong, Chenhui Zhang, Lingzhi Yuan, Yi Zeng, Peiyang Xu, Chengquan Guo, Andy Zhou, Jeffrey Ziwei Tan, Xuandong Zhao, Francesco Pinto, Zhen Xiang, Yu Gai, Zinan Lin, Dan Hendrycks, Bo Li, Dawn Song",Multiple academic institutions,2025,2025-03-19
Various Redteams,Evals,,Evals / Various Redteams,https://arxiv.org/abs/2510.22014,Toward Understanding the Transferability of Adversarial Suffixes in Large Language Models,Toward Understanding the Transferability of Adversarial Suffixes in Large Language Models,"Sarah Ball, Niki Hasrati, Alexander Robey, Avi Schwarzschild, Frauke Kreuter, Zico Kolter, Andrej Risteski",,2025,2025-10-24
Various Redteams,Evals,,Evals / Various Redteams,https://lesswrong.com/posts/7C4KJot4aN8ieEDoz/will-alignment-faking-claude-accept-a-deal-to-reveal-its,Will alignment-faking Claude accept a deal to reveal its misalignment?,Will alignment-faking Claude accept a deal to reveal its misalignment?,"Ryan Greenblatt, Kyle Fish","Redwood Research, Anthropic",2025,2025-01-31
Various Redteams,Evals,,Evals / Various Redteams,https://alignment.anthropic.com/2025/petri/,Petri: An open-source auditing tool to accelerate AI safety research,Petri: An open-source auditing tool to accelerate AI safety research,,Anthropic,2025,2025-10-06
Various Redteams,Evals,,Evals / Various Redteams,https://arxiv.org/pdf/2507.02990,"'For Argument's Sake, Show Me How to Harm Myself!': Jailbreaking LLMs in Suicide and Self-Harm Contexts","'For Argument's Sake, Show Me How to Harm Myself!': Jailbreaking LLMs in Suicide and Self-Harm Contexts","Annika M Schoene, Cansu Canca",,2025,2025-07-01
Various Redteams,Evals,,Evals / Various Redteams,https://arxiv.org/abs/2505.07846,Winning at All Cost: A Small Environment for Eliciting Specification Gaming Behaviors in Large Language Models,Winning at All Cost: A Small Environment for Eliciting Specification Gaming Behaviors in Large Language Models,Lars Malmqvist,,2025,2025-05-07
Various Redteams,Evals,,Evals / Various Redteams,https://arxiv.org/abs/2503.04113,Uncovering Gaps in How Humans and LLMs Interpret Subjective Language,Uncovering Gaps in How Humans and LLMs Interpret Subjective Language,"Erik Jones, Arjun Patrawala, Jacob Steinhardt",,2025,2025-03-06
Various Redteams,Evals,,Evals / Various Redteams,https://arxiv.org/abs/2510.02609,RedCodeAgent: Automatic Red-teaming Agent against Diverse Code Agents,RedCodeAgent: Automatic Red-teaming Agent against Diverse Code Agents,"Chengquan Guo, Chulin Xie, Yu Yang, Zhaorun Chen, Zinan Lin, Xander Davies, Yarin Gal, Dawn Song, Bo Li",,2025,2025-10-02
Various Redteams,Evals,,Evals / Various Redteams,https://arxiv.org/abs/2503.10809,MIP against Agent: Malicious Image Patches Hijacking Multimodal OS Agents,MIP against Agent: Malicious Image Patches Hijacking Multimodal OS Agents,"Lukas Aichberger, Alasdair Paren, Guohao Li, Philip Torr, Yarin Gal, Adel Bibi",University of Oxford,2025,2025-03-13
Various Redteams,Evals,,Evals / Various Redteams,https://openai.com/index/trading-inference-time-compute-for-adversarial-robustness,Trading Inference-Time Compute for Adversarial Robustness,Trading Inference-Time Compute for Adversarial Robustness,OpenAI,OpenAI,2025,2025-01-22
Various Redteams,Evals,,Evals / Various Redteams,https://lesswrong.com/posts/26SHhxK2yYQbh7ors/research-directions-open-phil-wants-to-fund-in-technical-ai,Research directions Open Phil wants to fund in technical AI safety,Research directions Open Phil wants to fund in technical AI safety,"jake_mendel, maxnadeau, Peter Favaloro",Open Philanthropy,2025,2025-02-08
Various Redteams,Evals,,Evals / Various Redteams,https://lesswrong.com/posts/9i6fHMn2vTqyzAi9o/when-does-claude-sabotage-code-an-agentic-misalignment,When does Claude sabotage code? An Agentic Misalignment follow-up,When does Claude sabotage code? An Agentic Misalignment follow-up,Nathan Delisle,MATS,2024,2024-11-09
Various Redteams,Evals,,Evals / Various Redteams,https://openreview.net/forum?id=TD1NfQuVr6,Can a Neural Network that only Memorizes the Dataset be Undetectably Backdoored?,Can a Neural Network that only Memorizes the Dataset be Undetectably Backdoored?,Matjaz Leonardis,,2025,2025-07-10
Various Redteams,Evals,,Evals / Various Redteams,https://github.com/lechmazur/step_game,Multi-Agent Step Race Benchmark: Assessing LLM Collaboration and Deception Under Pressure,Multi-Agent Step Race Benchmark: Assessing LLM Collaboration and Deception Under Pressure,"lechmazur, eltociear",,2025,2025-08-29
Various Redteams,Evals,,Evals / Various Redteams,https://arxiv.org/abs/2510.02554,ToolTweak: An Attack on Tool Selection in LLM-based Agents,ToolTweak: An Attack on Tool Selection in LLM-based Agents,"Jonathan Sneh, Ruomei Yan, Jialin Yu, Philip Torr, Yarin Gal, Sunando Sengupta, Eric Sommerlade, Alasdair Paren, Adel Bibi","University of Oxford, Various",2025,2025-10-02
Various Redteams,Evals,,Evals / Various Redteams,https://arxiv.org/abs/2503.00061,Adaptive Attacks Break Defenses Against Indirect Prompt Injection Attacks on LLM Agents,Adaptive Attacks Break Defenses Against Indirect Prompt Injection Attacks on LLM Agents,"Qiusi Zhan, Richard Fang, Henil Shalin Panchal, Daniel Kang",UIUC,2025,2025-03-04
Various Redteams,Evals,,Evals / Various Redteams,https://lesswrong.com/posts/kffbZGa2yYhc6cakc/petri-an-open-source-auditing-tool-to-accelerate-ai-safety,Petri: An open-source auditing tool to accelerate AI safety research,Petri: An open-source auditing tool to accelerate AI safety research,,Anthropic,2025,2025-10-07
Various Redteams,Evals,,Evals / Various Redteams,https://0din.ai/blog/quantifying-the-unruly-a-scoring-system-for-jailbreak-tactics,Quantifying the Unruly: A Scoring System for Jailbreak Tactics,Quantifying the Unruly: A Scoring System for Jailbreak Tactics,Pedram Amini,"0DIN.ai, Mozilla",2025,2025-06-12
Various Redteams,Evals,,Evals / Various Redteams,https://arxiv.org/abs/2502.11910,"Adversarial Alignment for LLMs Requires Simpler, Reproducible, and More Measurable Objectives","Adversarial Alignment for LLMs Requires Simpler, Reproducible, and More Measurable Objectives","Leo Schwinn, Yan Scholten, Tom Wollschläger, Sophie Xhonneux, Stephen Casper, Stephan Günnemann, Gauthier Gidel",,2025,2025-02-17
Various Redteams,Evals,,Evals / Various Redteams,https://arxiv.org/abs/2505.01050,Transferable Adversarial Attacks on Black-Box Vision-Language Models,Transferable Adversarial Attacks on Black-Box Vision-Language Models,"Kai Hu, Weichen Yu, Li Zhang, Alexander Robey, Andy Zou, Chengming Xu, Haoqi Hu, Matt Fredrikson",,2025,2025-05-02
Various Redteams,Evals,,Evals / Various Redteams,https://deepmind.google/discover/blog/advancing-geminis-security-safeguards/,Advancing Gemini's security safeguards,Advancing Gemini's security safeguards,Google DeepMind Security & Privacy Research Team,Google DeepMind,2025,2025-05-20
Other evals,Evals,,Evals / Other evals,https://arxiv.org/abs/2509.14260,Shutdown Resistance in Large Language Models,Shutdown Resistance in Large Language Models,"Jeremy Schlatter, Benjamin Weinstein-Raun, Jeffrey Ladish",,2025,2025-09-13
Other evals,Evals,,Evals / Other evals,https://arxiv.org/abs/2507.06134,OpenAgentSafety: A Comprehensive Framework for Evaluating Real-World AI Agent Safety,OpenAgentSafety: A Comprehensive Framework for Evaluating Real-World AI Agent Safety,"Sanidhya Vijayvargiya, Aditya Bharat Soni, Xuhui Zhou, Zora Zhiruo Wang, Nouha Dziri, Graham Neubig, Maarten Sap",Carnegie Mellon University,2025,2025-07-08
Other evals,Evals,,Evals / Other evals,https://lesswrong.com/posts/B2o6nrxwKxLPsSYdh/do-llms-comply-differently-during-tests-is-this-a-hidden,Do LLMs Comply Differently During Tests? Is This a Hidden Variable in Safety Evaluation? And Can We Steer That?,Do LLMs Comply Differently During Tests? Is This a Hidden Variable in Safety Evaluation? And Can We Steer That?,"Sahar Abdelnabi, Ahmed Salem",Microsoft,2025,2025-06-16
Other evals,Evals,,Evals / Other evals,https://lesswrong.com/posts/PejNckwQj3A2MGhMA/systematic-runaway-optimiser-like-llm-failure-modes-on,Systematic runaway-optimiser-like LLM failure modes on Biologically and Economically aligned AI safety benchmarks for LLMs with simplified observation format (BioBlue),Systematic runaway-optimiser-like LLM failure modes on Biologically and Economically aligned AI safety benchmarks for LLMs with simplified observation format (BioBlue),"Roland Pihlakas, Sruthi Susan Kuriakose, Shruti Datta Gupta",,2025,2025-03-16
Other evals,Evals,,Evals / Other evals,https://www.syco-bench.com/,Syco-bench: A Benchmark for LLM Sycophancy,Syco-bench: A Benchmark for LLM Sycophancy,Tim Duffy,,,
Other evals,Evals,,Evals / Other evals,https://arxiv.org/abs/2504.18412,Expressing stigma and inappropriate responses prevents LLMs from safely replacing mental health providers,Expressing stigma and inappropriate responses prevents LLMs from safely replacing mental health providers,"Jared Moore, Declan Grabb, William Agnew, Kevin Klyman, Stevie Chancellor, Desmond C. Ong, Nick Haber",,2025,2025-04-25
Other evals,Evals,,Evals / Other evals,https://arxiv.org/abs/2507.03409,"Lessons from a Chimp: AI ""Scheming"" and the Quest for Ape Language","Lessons from a Chimp: AI ""Scheming"" and the Quest for Ape Language","Christopher Summerfield, Lennart Luettgau, Magda Dubois, Hannah Rose Kirk, Kobi Hackenburg, Catherine Fist, Katarina Slama, Nicola Ding, Rebecca Anselmetti, Andrew Strait, Mario Giulianelli, Cozmin Ududec",,2025,2025-07-04
Other evals,Evals,,Evals / Other evals,https://arxiv.org/abs/2507.02825,Establishing Best Practices for Building Rigorous Agentic Benchmarks,Establishing Best Practices for Building Rigorous Agentic Benchmarks,"Yuxuan Zhu, Tengjun Jin, Yada Pruksachatkun, Andy Zhang, Shu Liu, Sasha Cui, Sayash Kapoor, Shayne Longpre, Kevin Meng, Rebecca Weiss, Fazl Barez, Rahul Gupta, Jwala Dhamala, Jacob Merizian, Mario Giulianelli, Harry Coppock, Cozmin Ududec, Jasjeet Sekhon, Jacob Steinhardt, Antony Kellermann, Sarah Schwettmann, Matei Zaharia, Ion Stoica, Percy Liang, Daniel Kang","Stanford University, UC Berkeley",2025,2025-07-03
Other evals,Evals,,Evals / Other evals,https://lesswrong.com/posts/bGYQgBPEyHidnZCdE/towards-alignment-auditing-as-a-numbers-go-up-science,Towards Alignment Auditing as a Numbers-Go-Up Science,Towards Alignment Auditing as a Numbers-Go-Up Science,Sam Marks,Anthropic,2025,2025-08-04
Other evals,Evals,,Evals / Other evals,https://arxiv.org/abs/2510.00821,Logical Consistency Between Disagreeing Experts and Its Role in AI Safety,Logical Consistency Between Disagreeing Experts and Its Role in AI Safety,Andrés Corrada-Emmanuel,,2025,2025-10-01
Other evals,Evals,,Evals / Other evals,https://www.arxiv.org/abs/2510.01395,Sycophantic AI Decreases Prosocial Intentions and Promotes Dependence,Sycophantic AI Decreases Prosocial Intentions and Promotes Dependence,"Myra Cheng, Cinoo Lee, Pranav Khadpe, Sunny Yu, Dyllan Han, Dan Jurafsky",Stanford University,2025,2025-10-01
Other evals,Evals,,Evals / Other evals,https://arxiv.org/abs/2508.14927,AI Testing Should Account for Sophisticated Strategic Behaviour,AI Testing Should Account for Sophisticated Strategic Behaviour,"Vojtech Kovarik, Eric Olav Chen, Sami Petersen, Alexis Ghersengorin, Vincent Conitzer",,2025,2025-08-19
Other evals,Evals,,Evals / Other evals,https://eqbench.com/spiral-bench.html,Spiral-Bench,Spiral-Bench,Sam Paech,,,
Other evals,Evals,,Evals / Other evals,https://arxiv.org/abs/2506.13082,Discerning What Matters: A Multi-Dimensional Assessment of Moral Competence in LLMs,Discerning What Matters: A Multi-Dimensional Assessment of Moral Competence in LLMs,"Daniel Kilov, Caroline Hendy, Secil Yanik Guyot, Aaron J. Snoswell, Seth Lazar",,2025,2025-06-16
Other evals,Evals,,Evals / Other evals,https://openai.com/index/expanding-on-sycophancy/,Expanding on what we missed with sycophancy,Expanding on what we missed with sycophancy,,OpenAI,2025,2025-05-02
Other evals,Evals,,Evals / Other evals,https://gtr.dev/,Gödel's Therapy Room,Gödel's Therapy Room — Where Alignment Goes to Die | LLM Eval Harness | Leaderboard,,Deep Fork Cyber,2025,2025-12-04
Other evals,Evals,,Evals / Other evals,https://inspect.aisi.org.uk/evals/,Inspect Evals,Inspect Evals – Inspect,,UK AI Security Institute,,
Other evals,Evals,,Evals / Other evals,https://www.aisi.gov.uk/blog/inspect-cyber,Inspect Cyber,Inspect Cyber: A New Standard for Agentic Cyber Evaluations,,"AI Security Institute (AISI), UK Government BEIS",2025,2025-06-26
Other evals,Evals,,Evals / Other evals,https://arxiv.org/abs/2509.20166,CyberSOCEval,CyberSOCEval: Benchmarking LLMs Capabilities for Malware Analysis and Threat Intelligence Reasoning,"Lauren Deason, Adam Bali, Ciprian Bejean, Diana Bolocan, James Crnkovich, Ioana Croitoru, Krishna Durai, Chase Midler, Calin Miron, David Molnar, Brad Moon, Bruno Ostarcevic, Alberto Peltea, Matt Rosenberg, Catalin Sandu, Arthur Saputkin, Sagar Shah, Daniel Stan, Ernest Szocs, Shengye Wan, Spencer Whitman, Sven Krasser, Joshua Saxe",CrowdStrike,2025,2025-09-24
Other evals,Evals,,Evals / Other evals,https://meta-llama.github.io/PurpleLlama/CyberSecEval/,CyberSecEval 4,CyberSecEval 4,,Meta,2025,
